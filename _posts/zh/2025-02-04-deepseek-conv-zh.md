---
audio: false
generated: false
lang: zh
layout: post
title: Deepseek - 对话
translated: true
type: note
---

A：我最近在研读DeepSeek-V3技术报告，对这个模型的规模印象深刻。6710亿参数，但每个token只激活370亿？这真是庞大的混合专家架构。它究竟是如何运作的？

B：确实是个壮举！DeepSeek-V3基于混合专家框架构建，每个token仅激活参数子集。具体来说，它使用256个路由专家，但每个token只激活8个。与稠密模型相比，这种设计极具效率优势——后者需要为每个token激活全部参数。

A：有道理。但它如何决定激活哪些专家？是随机选择还是存在某种路由机制？

B：好问题！路由基于token与专家的亲和度评分。每个token会获得针对每个专家的评分，得分最高的前K个专家被激活。DeepSeek-V3使用sigmoid函数计算这些评分，有助于平衡专家间的工作负载。

A：所以这不是随机的，而是在训练过程中学习得到的。但这不会导致专家使用不均衡吗？我听说这是MoE模型的常见问题。

B：没错！专家负载不均衡确实是个问题，但DeepSeek-V3引入了无辅助损失策略来解决。它没有添加额外的损失项来促进负载均衡，而是动态调整每个专家的偏置项——过载专家降低偏置，欠载专家增加偏置。这样既保持负载均衡，又不影响模型性能。

A：很巧妙的设计。没有辅助损失意味着对主要训练目标的干扰更小。那与传统使用辅助损失的MoE模型相比如何？

B：传统MoE模型常使用辅助损失促进负载均衡，但这些损失有时会损害性能。DeepSeek-V3的无辅助损失方法避免了这种权衡。消融实验表明，它在代码和数学等任务上持续优于依赖辅助损失的模型。

A：有意思。说到代码和数学，我注意到DeepSeek-V3在HumanEval和MATH等基准测试中表现异常出色。这背后的秘诀是什么？

B：关键因素之一是多token预测目标。DeepSeek-V3不仅预测下一个token，还在每个位置预测多个未来token。这使训练信号更密集，帮助模型进行前瞻规划，对需要序列推理的任务（如编程和数学）特别有用。

A：等等，所以它能同时预测多个token？推理时如何运作？仍然使用MTP还是仅用于训练？

B：推理时MTP模块可被丢弃，模型表现如标准自回归模型。但巧妙之处在于：MTP模块可重用于推测解码，通过并行预测多个token再进行验证，从而加速生成过程。

A：这个技巧很精妙。相当于训练时获得MTP益处，推理时又用它加速。那注意力机制呢？我注意到有多头潜在注意力，这又是如何运作的？

B：MLA是另一项关键创新。它通过压缩键值缓存来减少内存占用，使用低秩联合压缩表示注意力键值，而非存储完整键值。这在保持性能与标准多头注意力相当的同时，显著减小了推理时的KV缓存大小。

A：这对效率提升意义重大。但压缩不会导致信息损失吗？如何保持性能？

B：问得好。压缩设计通过聚焦捕捉键值本质特征的潜在向量来保留最关键信息。模型还使用旋转位置编码保持位置信息，有助于缓解压缩带来的损失。

A：明白了。所以MLA实现了效率与性能的兼得。那训练方面呢？训练如此规模的模型成本极高，DeepSeek-V3如何控制成本？

B：训练效率是重点优化方向。DeepSeek-V3使用FP8混合精度框架降低内存使用并加速计算，同时采用双管道算法进行流水线并行，最小化流水线气泡，实现计算与通信重叠。这些优化使模型仅用278.8万H800 GPU小时就完成了14.8万亿token的训练。

A：令人惊叹。但FP8训练可能存在问题——如何处理精度问题？我听说低精度训练可能导致不稳定。

B：确实如此。FP8训练因动态范围有限而充满挑战。DeepSeek-V3通过细粒度量化解决：将激活值和权重分组为更小的区块独立缩放，减少异常值影响保持训练稳定。关键操作还使用高精度累加确保准确性。

A：有道理。这是在效率与精度间取得的平衡。那数据方面呢？14.8万亿token的数据集规模巨大，包含哪些类型的数据？

B：数据集兼具多样性与高质量，侧重中英文文本，同时包含大量数学和编程数据，这解释了模型在这些领域的卓越表现。数据管道经过优化在保持多样性的同时减少冗余，并使用文档打包等技术确保数据完整性。

A：这解释了在编程和数学任务上的强劲表现。那多语言性能呢？对其他语言处理效果如何？

B：DeepSeek-V3基于多语言语料训练，在包含非英语任务的MMMLU等基准上表现良好。中文能力尤其突出，在C-Eval和CMMLU等中文基准上超越Qwen2.5等模型。

A：令人印象深刻。长上下文任务处理呢？我看到它支持128K token，如何应对如此长的输入？

B：DeepSeek-V3通过YaRN技术分两阶段扩展上下文长度：先扩展到32K，再扩展到128K。这使其能有效处理文档摘要和检索等长上下文任务，在评估长上下文理解的“大海捞针”测试中表现优异。

A：相对前代模型是巨大改进。那部署方面呢？如何处理如此大模型的推理？

B：推理在H800集群上进行，通过NVLink和InfiniBand互联GPU。部署策略将填充阶段与解码阶段分离，确保高吞吐与低延迟。推理时还使用冗余专家平衡负载维持效率。

A：优化措施真多。但有哪些局限性呢？如此规模的模型必然存在权衡。

B：一个限制是部署单元规模。DeepSeek-V3需要较大集群实现高效推理，对小团队可能构成挑战。生成速度也有提升空间，不过MTP推测解码有所助益。

A：可以理解。但总体而言仍是巨大进步。DeepSeek-V3的未来规划是什么？有哪些探索方向？

B：他们正在多个领域探索：改进架构支持无限上下文长度，开发更多训练信号源，增强模型推理能力。同时致力于建立更全面的评估方法以更好衡量模型性能。

A：看来他们不会放缓脚步。感谢你详细讲解——DeepSeek-V3无疑是开源LLM领域的颠覆者。

B：确实！开源模型的进展令人振奋。DeepSeek-V3正在突破边界，我迫不及待想看到他们的下一步成果。

A：你提到DeepSeek-V3使用FP8混合精度训练。我很好奇——与BF16或FP16相比如何？FP8真能稳定训练如此大模型吗？

B：好问题。FP8因动态范围有限确实更具挑战性，但DeepSeek-V3通过细粒度量化策略缓解。例如激活值分组为1x128区块，权重分组为128x128模块，每组独立缩放，有助于处理异常值保持训练稳定。

A：有意思。所以不是简单的全局FP8量化，而是更精细的策略。但管理这些组别和缩放因子不会引入额外开销吗？

B：确实有开销，但相比收益微不足道。关键在于FP8降低内存使用加速计算，这对训练大模型至关重要。关键操作（如矩阵乘法）仍使用高精度累加确保数值稳定性。

A：明白了。这是在精度与效率间的权衡，但他们找到了良好平衡点。双管道算法呢？如何运作？

B：双管道旨在最小化流水线并行的气泡效应。它将工作块划分为四个组件：注意力、全分发、MLP和全组合，通过重叠计算与通信实现优化。反向传播时进一步将计算拆分为“输入反向”和“权重反向”，实现更高效的重叠。

A：听起来复杂但合理。本质上是通过重叠计算隐藏通信开销。与1F1B或零气泡等其他流水线并行方法相比如何？

B：双管道相比1F1B和零气泡具有更少流水线气泡。还支持双向调度，从流水线两端同时输入微批次，进一步减少空闲时间提升整体效率。实际上双管道实现了近乎零的全通信开销，这对扩展MoE模型至关重要。

A：令人惊叹。那内存使用呢？双管道是否需要更多内存？

B：因保留两份模型参数副本，内存占用稍多，但增加幅度可控。通过重计算RMSNorm和MLA上投影等技术优化内存占用，无需存储中间激活值。

A：相当于用少量内存换取更佳效率，这是合理的权衡。说到内存，如何处理128K token长上下文的KV缓存？必定需要巨大缓存空间。

B：这正是MLA的优势所在。通过压缩KV缓存显著减小其体积，存储压缩后的潜在向量而非完整注意力键值，使DeepSeek-V3能处理长上下文而不受内存限制。

A：聪明的解决方案。但注意力质量如何？压缩是否影响模型关注正确token的能力？

B：压缩设计保留了最关键信息，对注意力质量影响甚微。同时使用RoPE保持位置信息，帮助模型理解token相对位置——即使键值经过压缩。

A：有道理。所以MLA是双赢方案——减少内存占用且几乎不牺牲性能。那训练数据呢？你提到的14.8万亿token数据集，如何保证如此庞大数据集的质量与多样性？

B：数据集经过精心策划，包含高质量多样化token。数据管道经过优化在保持多样性的同时减少冗余，使用文档打包等技术确保数据完整性。语料库混合中英文文本，侧重数学和编程样本。

A：这解释了在编程和数学任务上的强劲表现。多语言任务呢？对其他语言处理效果如何？

B：DeepSeek-V3基于多语言语料训练，在包含非英语任务的MMMLU等基准上表现良好。中文能力尤其突出，在C-Eval和CMMLU等中文基准上超越Qwen2.5等模型。

A：令人印象深刻。长上下文任务处理呢？我看到它支持128K token，如何应对如此长的输入？

B：DeepSeek-V3通过YaRN技术分两阶段扩展上下文长度：先扩展到32K，再扩展到128K。这使其能有效处理文档摘要和检索等长上下文任务，在评估长上下文理解的“大海捞针”测试中表现优异。

A：相对前代模型是巨大改进。那部署方面呢？如何处理如此大模型的推理？

B：推理在H800集群上进行，通过NVLink和InfiniBand互联GPU。部署策略将填充阶段与解码阶段分离，确保高吞吐与低延迟。推理时还使用冗余专家平衡负载维持效率。

A：优化措施真多。但有哪些局限性呢？如此规模的模型必然存在权衡。

B：一个限制是部署单元规模。DeepSeek-V3需要较大集群实现高效推理，对小团队可能构成挑战。生成速度也有提升空间，不过MTP推测解码有所助益。

A：可以理解。但总体而言仍是巨大进步。DeepSeek-V3的未来规划是什么？有哪些探索方向？

B：他们正在多个领域探索：改进架构支持无限上下文长度，开发更多训练信号源，增强模型推理能力。同时致力于建立更全面的评估方法以更好衡量模型性能。

A：看来他们不会放缓脚步。感谢你详细讲解——DeepSeek-V3无疑是开源LLM领域的颠覆者。

B：确实！开源模型的进展令人振奋。DeepSeek-V3正在突破边界，我迫不及待想看到他们的下一步成果。