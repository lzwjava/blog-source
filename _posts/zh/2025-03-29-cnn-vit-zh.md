---
audio: false
generated: true
lang: zh
layout: post
title: åŸºäºCNNä¸è§†è§‰Transformer(ViT)çš„å›¾åƒå¤„ç†
translated: true
type: note
---

ä»¥ä¸‹æ˜¯ä½¿ç”¨ **å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNsï¼‰** å’Œ **è§†è§‰å˜æ¢å™¨ï¼ˆViTï¼‰** åœ¨ Python ä¸­é€šè¿‡ **PyTorch** å¤„ç†å›¾åƒçš„ä»£ç ç¤ºä¾‹ã€‚

---

## **1. ä½¿ç”¨ CNNï¼ˆå·ç§¯ç¥ç»ç½‘ç»œï¼‰è¿›è¡Œå›¾åƒå¤„ç†**
CNN å¹¿æ³›ç”¨äºå›¾åƒåˆ†ç±»ã€ç›®æ ‡æ£€æµ‹å’Œç‰¹å¾æå–ã€‚

### **ç¤ºä¾‹ï¼šä½¿ç”¨é¢„è®­ç»ƒçš„ CNNï¼ˆResNetï¼‰**
```python
import torch
import torchvision.models as models
import torchvision.transforms as transforms
from PIL import Image

# åŠ è½½é¢„è®­ç»ƒçš„ ResNet æ¨¡å‹
model = models.resnet18(pretrained=True)
model.eval()  # è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼

# å®šä¹‰å›¾åƒé¢„å¤„ç†æµç¨‹
preprocess = transforms.Compose([
    transforms.Resize(256),
    transforms.CenterCrop(224),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
])

# åŠ è½½å¹¶é¢„å¤„ç†å›¾åƒ
image = Image.open("example.jpg")  # æ›¿æ¢ä¸ºä½ çš„å›¾åƒè·¯å¾„
input_tensor = preprocess(image)
input_batch = input_tensor.unsqueeze(0)  # æ·»åŠ æ‰¹æ¬¡ç»´åº¦

# å¦‚æœå¯ç”¨åˆ™ç§»è‡³ GPU
if torch.cuda.is_available():
    input_batch = input_batch.to('cuda')
    model.to('cuda')

# æå–ç‰¹å¾ï¼ˆåœ¨æœ€ç»ˆåˆ†ç±»å±‚ä¹‹å‰ï¼‰
with torch.no_grad():
    features = model(input_batch)

print("ç‰¹å¾å‘é‡å½¢çŠ¶:", features.shape)  # ä¾‹å¦‚ï¼štorch.Size([1, 1000])
```
**è¯´æ˜**ï¼š
1. **ResNet18** æ˜¯åœ¨ ImageNet ä¸Šé¢„è®­ç»ƒçš„ CNN æ¶æ„ã€‚
2. å›¾åƒç»è¿‡é¢„å¤„ç†ï¼ˆè°ƒæ•´å¤§å°ã€æ ‡å‡†åŒ–ï¼‰ã€‚
3. æ¨¡å‹å°†å›¾åƒè½¬æ¢ä¸º**ç‰¹å¾å‘é‡**ï¼ˆä¾‹å¦‚ï¼ŒResNet18 ä¸º 1000 ç»´ï¼‰ã€‚

---

## **2. ä½¿ç”¨è§†è§‰å˜æ¢å™¨ï¼ˆViTï¼‰è¿›è¡Œå›¾åƒå¤„ç†**
ViT å°†å›¾åƒè§†ä¸ºè¡¥ä¸åºåˆ—ï¼Œå¹¶ä½¿ç”¨è‡ªæ³¨æ„åŠ›æœºåˆ¶ï¼ˆç±»ä¼¼äº NLPï¼‰ã€‚

### **ç¤ºä¾‹ï¼šä½¿ç”¨é¢„è®­ç»ƒçš„ ViTï¼ˆHugging Faceï¼‰**
```python
from transformers import ViTFeatureExtractor, ViTModel
from PIL import Image
import torch

# åŠ è½½é¢„è®­ç»ƒçš„è§†è§‰å˜æ¢å™¨ï¼ˆViTï¼‰
model_name = "google/vit-base-patch16-224-in21k"
feature_extractor = ViTFeatureExtractor.from_pretrained(model_name)
model = ViTModel.from_pretrained(model_name)

# åŠ è½½å›¾åƒ
image = Image.open("example.jpg")  # æ›¿æ¢ä¸ºä½ çš„å›¾åƒè·¯å¾„

# é¢„å¤„ç†å›¾åƒï¼ˆè½¬æ¢ä¸ºè¡¥ä¸ï¼‰
inputs = feature_extractor(images=image, return_tensors="pt")

# æå–ç‰¹å¾ï¼ˆCLS æ ‡è®°æˆ–è¡¥ä¸åµŒå…¥ï¼‰
with torch.no_grad():
    outputs = model(**inputs)

# è·å–ç‰¹å¾å‘é‡ï¼ˆCLS æ ‡è®°ï¼‰
features = outputs.last_hidden_state[:, 0, :]  # å½¢çŠ¶ï¼š[1, 768]

print("ç‰¹å¾å‘é‡å½¢çŠ¶:", features.shape)  # ä¾‹å¦‚ï¼štorch.Size([1, 768])
```
**è¯´æ˜**ï¼š
1. **ViT** å°†å›¾åƒåˆ†å‰²ä¸º **16x16 è¡¥ä¸**ï¼Œå¹¶åƒ NLP ä¸­çš„æ ‡è®°ä¸€æ ·å¤„ç†å®ƒä»¬ã€‚
2. `CLS æ ‡è®°`ï¼ˆç¬¬ä¸€ä¸ªæ ‡è®°ï¼‰ä»£è¡¨æ•´ä¸ªå›¾åƒçš„ç‰¹å¾å‘é‡ã€‚
3. è¾“å‡ºæ˜¯ä¸€ä¸ª **768 ç»´å‘é‡**ï¼ˆå¯¹äº `vit-base`ï¼‰ã€‚

---

## **3. CNN ä¸ ViT ç‰¹å¾æå–å¯¹æ¯”**

| æ¨¡å‹ | æ–¹æ³• | ç‰¹å¾å‘é‡å¤§å° | åº“ |
|-------|----------|---------------------|-----------|
| **CNN (ResNet18)** | å·ç§¯å±‚ + æ± åŒ– | 1000ï¼ˆImageNet ç±»åˆ«æ•°ï¼‰ | `torchvision` |
| **ViT (Google ViT-Base)** | è¡¥ä¸åµŒå…¥ + å˜æ¢å™¨ | 768ï¼ˆéšè—ç»´åº¦ï¼‰ | `transformers` |

---

## **4. å›¾åƒç‰¹å¾å‘é‡çš„åº”ç”¨**
- **å›¾åƒæœç´¢**ï¼šæ¯”è¾ƒç‰¹å¾å‘é‡ï¼ˆä¾‹å¦‚ï¼Œä½™å¼¦ç›¸ä¼¼åº¦ï¼‰ã€‚
- **è¿ç§»å­¦ä¹ **ï¼šä½¿ç”¨é¢„è®­ç»ƒç‰¹å¾è¿›è¡Œè‡ªå®šä¹‰ä»»åŠ¡ã€‚
- **ç›®æ ‡æ£€æµ‹**ï¼šä»ç‰¹å¾ä¸­æå–æ„Ÿå…´è¶£åŒºåŸŸï¼ˆROIï¼‰ã€‚

```python
# ç¤ºä¾‹ï¼šè®¡ç®—ä¸¤å¼ å›¾åƒçš„ç›¸ä¼¼åº¦
from sklearn.metrics.pairwise import cosine_similarity

# å‡è®¾ features1 å’Œ features2 æ˜¯æå–çš„å‘é‡
similarity = cosine_similarity(features1, features2)
print("ç›¸ä¼¼åº¦å¾—åˆ†:", similarity[0][0])
```

---

## **å…³é”®è¦ç‚¹**
1. **CNN**ï¼ˆä¾‹å¦‚ ResNetï¼‰ä½¿ç”¨**å·ç§¯å±‚**æå–å±‚æ¬¡åŒ–ç‰¹å¾ã€‚
2. **ViT**ï¼ˆä¾‹å¦‚ Google çš„ ViTï¼‰å°†å›¾åƒè§†ä¸º**è¡¥ä¸åºåˆ—**å¹¶ä½¿ç”¨**è‡ªæ³¨æ„åŠ›**ã€‚
3. ä¸¤è€…éƒ½å°†å›¾åƒè½¬æ¢ä¸º**ç‰¹å¾å‘é‡**ä»¥ç”¨äºä¸‹æ¸¸ä»»åŠ¡ã€‚

éœ€è¦å…³äº**ä»å¤´è®­ç»ƒè‡ªå®šä¹‰ CNN/ViT** çš„ç¤ºä¾‹å—ï¼Ÿå‘Šè¯‰æˆ‘å§ï¼ğŸš€