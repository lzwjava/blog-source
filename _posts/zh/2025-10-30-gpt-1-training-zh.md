---
audio: false
generated: true
image: false
lang: zh
layout: post
model: x-ai/grok-4-fast
title: GPT-1架构与训练概述
translated: true
type: note
---

### GPT-1 架构概述

GPT-1（生成式预训练 Transformer 1）是 OpenAI 于 2018 年推出的奠基性模型。它开创了通过语言建模目标在无标注文本上预训练大型语言模型，再针对下游任务进行微调的新范式。该架构采用**仅含解码器的 Transformer**，源自原版 Transformer 论文（Vaswani 等，2017），但为自回归生成任务移除了编码器堆栈。这种设计使模型能够预测序列中的下一个标记，适用于连续性文本生成任务。

与 BERT 等双向模型不同，GPT-1 使用**掩码自注意力机制**确保因果性——每个位置仅能关注之前的位置，防止未来标记的信息泄露。

### 核心组件与超参数

- **模型类型**：具有掩码多头自注意力和逐位置前馈网络的多层 Transformer 解码器
- **层数**：12 个 Transformer 块（层）
- **注意力机制**：每层 12 个注意力头，每个头处理 64 维状态（模型总维度：768）
- **嵌入维度**：
  - 隐藏层大小（d_model）：768
  - 前馈网络内部维度（d_ff）：3072（隐藏层大小的 4 倍，Transformer 标准配置）
- **位置编码**：可学习的位置嵌入与标记嵌入相加（未使用正弦编码）
- **激活函数**：前馈网络中使用高斯误差线性单元（GELU）
- **词汇与标记化**：字节对编码（BPE）包含 40,000 个合并规则，基于训练语料库训练
- **参数量**：约 1.17 亿
- **序列长度**：训练时使用 512 个标记的序列
- **正则化**：
  - 残差连接、嵌入层和注意力层使用 0.1 的 dropout
  - 非偏置/非层归一化权重使用 0.01 的修正 L2 正则化
- **初始化**：权重从正态分布 N(0, 0.02) 初始化

### 训练细节

- **预训练阶段**：
  - **数据集**：BooksCorpus 包含约 7,000 本未出版书籍（总计约 8 亿词），涵盖奇幻、言情、冒险等体裁。文本经过清洗（如使用 ftfy 库）并通过 spaCy 进行标记化
  - **目标**：无监督语言建模（下一标记预测）
  - **优化器**：Adam（β1=0.9, β2=0.999, ε=1e-8）
  - **学习率调度**：经过 2,000 次更新的线性预热至峰值 2.5×10⁻⁴，随后采用余弦退火降至 0
  - **批大小**：每小批 64 个序列
  - **训练轮数**：100
  - **性能指标**：在训练语料上达到 18.4 的困惑度
- **微调阶段**：
  - 最小化改动：添加线性输出层进行任务特定预测，使用分隔符标记（如 <s>, <e>, $）将输入构建为单一序列
  - 微调时辅助语言建模损失的权重为 0.5
  - 超参数：较低学习率（6.25×10⁻⁵），批大小 32，3 个训练周期，包含 0.2% 预热和线性衰减
  - 这种迁移学习方法在文本蕴含、问答等任务上取得了强劲表现，且无需任务特定架构

### 重要创新与局限

GPT-1 证明了单一预训练模型通过简单输入格式化即可有效迁移至多种 NLP 任务，当时超越了部分经过任务调优的模型。但与现代大语言模型（如 GPT-3 参数量达 1750 亿）相比规模较小，且上下文长度限制为 512 个标记。该架构为整个 GPT 系列奠定了基础，强调了预训练的可扩展性。

[通过生成式预训练提升语言理解能力](https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf)  
[GPT-1 - 维基百科](https://en.wikipedia.org/wiki/GPT-1)