---
audio: false
generated: true
lang: zh
layout: post
title: 余弦相似度
translated: true
type: note
---

余弦相似度是机器学习中广泛使用的度量方法，用于衡量高维空间中两个向量的相似程度。它特别适用于自然语言处理（NLP）、信息检索和推荐系统等领域，因为它能够捕捉向量之间的方向（或角度）而非幅度。这一特性使其在比较文本文档、用户偏好或嵌入向量等对象时具有鲁棒性——当向量的方向比其长度更重要时尤为有效。

### 什么是余弦相似度？

余弦相似度通过计算两个向量间夹角的余弦值来量化它们的相似程度。数学上定义为：

\\[
\text{余弦相似度} = \cos(\theta) = \frac{A \cdot B}{\|A\| \|B\|}
\\]

其中：
- \\( A \\) 和 \\( B \\) 是两个向量（例如代表文档、嵌入或特征集）
- \\( A \cdot B \\) 是向量的点积，计算公式为 \\( \sum_{i=1}^n A_i B_i \\)
- \\( \|A\| \\) 和 \\( \|B\| \\) 分别是向量 \\( A \\) 和 \\( B \\) 的欧几里得范数（模长），计算方式为 \\( \sqrt{\sum_{i=1}^n A_i^2} \\) 和 \\( \sqrt{\sum_{i=1}^n B_i^2} \\)
- \\( \theta \\) 是向量间的夹角

结果范围如下：
- **1**：向量方向完全相同（夹角 0°）
- **0**：向量相互正交（夹角 90°），表示无相似性
- **-1**：向量方向完全相反（夹角 180°），表示最大相异性

### 关键特性

1. **值域范围**：余弦相似度取值在 -1 到 1 之间，易于解释
2. **幅度无关性**：由于向量经过模长归一化，余弦相似度关注方向而非长度，适用于比较不同长度的文档或不同尺度的嵌入向量
3. **非负特征**：在许多应用场景中（如使用词频的文本数据），向量分量均为非负值，此时相似度通常落在 0 到 1 之间
4. **计算高效性**：点积和模长计算简单直接，使余弦相似度在处理高维数据时具有计算效率优势

### 在机器学习中的应用

余弦相似度凭借其通用性广泛应用于多种机器学习任务：

1. **文本分析与自然语言处理**：
   - **文档相似度**：在聚类或搜索引擎任务中，文档被表示为向量（如 TF-IDF 或 Word2Vec、GloVe、BERT 等词嵌入），余弦相似度可基于内容衡量文档间相似程度
   - **情感分析**：比较文本片段的情感向量
   - **抄袭检测**：通过对比文本的向量表征识别相似性

2. **推荐系统**：
   - 用于比较用户或物品画像（如协同过滤中），例如根据用户评分或行为衡量偏好相似度
   - 在基于内容的过滤中表现优异，其中物品（如电影、商品）被表示为特征向量

3. **图像与音频处理**：
   - 计算机视觉中，通过比较图像提取的特征向量（如来自 CNN 的特征）来衡量视觉相似度
   - 音频处理中，用于比较声谱图或音频片段的嵌入向量

4. **聚类与分类**：
   - 在聚类算法中（如处理文本数据的 K-means），余弦相似度作为距离度量对相似项进行分组
   - 分类任务中，用于比较输入向量与类别原型

5. **异常检测**：
   - 通过将数据点与质心或预期模式进行比较，低相似度可能指示潜在异常值

### 示例：文本分析中的余弦相似度

假设有两个文档的 TF-IDF 向量表示：
- 文档 1：\\( A = [2, 1, 0, 3] \\)（四个词的词频）
- 文档 2：\\( B = [1, 1, 1, 0] \\)

**步骤 1：计算点积**：
\\[
A \cdot B = (2 \cdot 1) + (1 \cdot 1) + (0 \cdot 1) + (3 \cdot 0) = 2 + 1 + 0 + 0 = 3
\\]

**步骤 2：计算模长**：
\\[
\|A\| = \sqrt{2^2 + 1^2 + 0^2 + 3^2} = \sqrt{4 + 1 + 0 + 9} = \sqrt{14} \approx 3.742
\\]
\\[
\|B\| = \sqrt{1^2 + 1^2 + 1^2 + 0^2} = \sqrt{1 + 1 + 1 + 0} = \sqrt{3} \approx 1.732
\\]

**步骤 3：计算余弦相似度**：
\\[
\cos(\theta) = \frac{A \cdot B}{\|A\| \|B\|} = \frac{3}{3.742 \cdot 1.732} \approx \frac{3}{6.483} \approx 0.462
\\]

余弦相似度约为 0.462，表明文档间存在中等程度的相似性。

### 余弦相似度的优势

- **尺度不变性**：不受向量幅度影响，特别适合文档长度不一的文本数据
- **高维数据处理能力**：在稀疏的高维空间（如具有数千特征的文本数据）中表现优异
- **直观可解释性**：余弦值与角度直接关联，提供清晰的相似度度量

### 局限性

- **忽略幅度信息**：当绝对数量差异很重要时可能不适用
- **假设线性关系**：默认角度距离最能表征相似性，这一假设并非始终成立
- **稀疏数据敏感性**：在极度稀疏的向量中，由于许多维度对点积贡献微小，区分度可能降低

### 与其他度量方法的对比

- **欧氏距离**：衡量直线距离，对幅度敏感，当方向比绝对差异更重要时优选余弦相似度
- **杰卡德相似度**：适用于集合数据（如二元数据），关注共享元素而非向量方向
- **皮尔逊相关系数**：衡量线性相关性，处理经过均值中心化的数据，而余弦相似度直接处理原始向量

### 实践实现

余弦相似度在多种机器学习库中均有实现：
- **Python**：`scikit-learn` 在 `sklearn.metrics.pairwise` 中提供 `cosine_similarity`
  ```python
  from sklearn.metrics.pairwise import cosine_similarity
  import numpy as np

  A = np.array([[2, 1, 0, 3]])
  B = np.array([[1, 1, 1, 0]])
  similarity = cosine_similarity(A, B)
  print(similarity)  # 输出：[[0.46225063]]
  ```
- **TensorFlow/PyTorch**：用于深度学习模型中的嵌入向量比较
- **Spark**：支持大规模分布式相似度计算

### 进阶应用

1. **词嵌入**：余弦相似度是比较词或句嵌入向量（如 BERT、Sentence-BERT）语义相似度的标准方法
2. **图模型**：在图神经网络中用于节点相似度计算
3. **个性化搜索**：Elasticsearch 等搜索引擎使用余弦相似度按查询相关度对文档排序

### 结语

余弦相似度是一种强大、直观且计算高效的机器学习相似度度量方法。其对向量方向的聚焦特性，使其特别适用于文本、嵌入向量和推荐系统等场景，但在幅度信息重要的任务中表现相对受限。通过理解其优势与局限，从业者可在从自然语言处理到计算机视觉的多样化应用中有效运用该技术。如果您希望深入了解特定用例或实现细节，欢迎随时交流！