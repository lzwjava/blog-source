---
audio: false
generated: true
image: false
lang: zh
layout: post
model: x-ai/grok-4.1-fast
title: 具身智能概览
translated: true
type: note
---

### 具身智能：概览

具身智能（Embodied AI）是指通过物理载体（如机器人）置身于现实世界，并通过与环境直接交互进行学习、推理和行动的人工智能系统。与传统AI处理孤立抽象数据（如GPT等语言模型）不同，具身AI强调真正的智能源于感知-行动闭环：感知世界、采取行动、根据反馈进行适应。这一范式借鉴了认知科学的观点，即认知植根于物理具身而非纯粹计算。

核心原则包括：
- **多模态感知**：融合视觉、触觉、本体感觉，有时还包括语言或声音
- **交互驱动学习**：智能体通过现实世界或高保真仿真中的试错进行改进（仿真到现实迁移）
- **泛化与适应**：处理非结构化动态环境中的长周期任务、多模态融合（如视觉与语言结合）及扰动鲁棒性

截至2025年，得益于基础模型（大规模预训练视觉语言模型）、扩散技术和Open X-Embodiment等海量数据集，具身智能实现爆发式发展，推动人形机器人、操作控制、自主导航和人机交互等领域进步。实时性能、安全性、仿真到现实的差距以及开放世界任务扩展仍是当前挑战。谷歌RT系列、OpenVLA和基于扩散的策略等领先研究正致力于打造通用机器人。

### 关键技术：Diffusion Policy、RT-2与ACT

这三项技术代表通过模仿学习（基于人类或专家示范数据而非显式奖励）来训练机器人策略（从观察到动作的映射）的最前沿方法。

#### ACT（基于Transformer的动作分块）
- **起源**：2023年由Tony Zhao等人（Covariant.ai，原UC伯克利团队）在低成本双手操作系统ALOHA中提出
- **核心思想**：基于Transformer的策略直接预测未来动作块（如一次性输出100步动作），而非逐时间步预测。这减少了时序误差（长周期任务中的错误累积），支持平滑的高频控制（如50Hz）
- **架构**：采用变分自编码器或Transformer主干网络。输入：多视角RGB图像+本体感觉（关节状态）。输出：分块化的关节位置/速度
- **优势**：
  - 极高的样本效率（仅需约50次演示即可学会复杂任务）
  - 可在消费级硬件上实时运行
  - 在精细灵巧任务（穿针引线、折叠衣物）中表现卓越
- **局限**：主要基于模仿学习，缺乏对语言指令或网络规模泛化的原生支持
- **现实影响**：已应用于ALOHA移动操作机器人系统，被广泛用于双手协同任务

#### Diffusion Policy
- **起源**：2023年由Cheng Chi等人（哥伦比亚大学、丰田研究院、MIT）提出，后续发展出3D Diffusion Policy和ScaleDP（2025年参数规模达10亿）
- **核心思想**：将机器人动作视为扩散模型的生成样本（受Stable Diffusion等图像生成器启发）。从噪声动作开始，基于观测条件迭代去噪，生成高质量多模态动作序列
- **架构**：条件去噪扩散模型（常采用Transformer），学习动作分布的梯度函数。推理采用滑动窗口控制：规划序列→执行首动作→重新规划
- **优势**：
  - 天然处理多模态行为（如多种抓取方式——扩散模型能连贯采样而非取平均）
  - 对高维动作和噪声演示数据具有鲁棒性
  - 在基准测试中实现46%+性能提升（2025年仍保持竞争力）
  - 3D Diffusion Policy等变体使用点云提升三维理解能力
- **局限**：推理速度较慢（需10-100步去噪），通过优化（减少步数、蒸馏等）可实现实时运行
- **现实影响**：广泛用于视觉运动操作任务，已集成至PoCo（策略组合）等系统

#### RT-2（机器人Transformer 2）
- **起源**：2023年由谷歌DeepMind在RT-1基础上开发，属于视觉-语言-动作模型家族
- **核心思想**：对预训练视觉语言模型（如550亿参数的PaLM-E或PaLI-X）进行机器人轨迹联合微调。动作被 token化为文本字符串，使模型能直接输出动作并利用网络规模知识（图像+文本）
- **架构**：接收图像+语言指令→token化动作的Transformer。涌现出来自网络预训练的推理能力（符号推理、思维链等）
- **优势**：
  - 语义泛化能力：理解新颖指令（如“捡起灭绝动物”→抓起恐龙玩具）无需机器人专项训练
  - 将网络知识迁移至机器人任务（如通过网络图像识别垃圾）
  - 在涌现技能上较前代模型提升最高达3倍
- **局限**：大模型导致算力需求高；在低层级灵巧控制上精度低于ACT/扩散方法（更擅长高层推理）
- **现实影响**：支撑谷歌机器人舰队数据收集系统AutoRT，已演进为RT-X并集成至后续系统

### 对比表格

| 维度                | ACT                                      | Diffusion Policy                          | RT-2                                      |
|---------------------|------------------------------------------|-------------------------------------------|-------------------------------------------|
| **核心方法**        | Transformer+动作分块（确定性/回归式）    | 去噪扩散（生成式）                        | 视觉语言动作模型（LLM/VLM中的token化动作）|
| **输入**            | 多视角图像+本体感觉                      | 图像/点云+本体感觉                        | 图像+语言指令                             |
| **输出**            | 分块关节动作                             | 去噪后动作序列                            | Token化动作字符串                         |
| **核心优势**        | 样本效率、精度、实时性                   | 多模态性、鲁棒性、强表现力                | 语义推理、网络数据泛化                    |
| **推理速度**        | 快速（单次前向）                         | 较慢（迭代去噪）                          | 中等（Transformer自回归）                 |
| **数据效率**        | 极高（约50次演示/任务）                  | 高                                        | 中等（受益于网络预训练）                  |
| **最佳适用场景**    | 精密灵巧操作                             | 复杂多模态任务                            | 语言引导的新颖/涌现任务                   |
| **典型成功率提升**  | 在真实机器人基准测试表现强劲             | 较基线提升46%（2023年基准）              | 涌现技能提升2-3倍                        |
| **2025年现状**      | 低成本硬件基准方案                       | 参数规模扩展至数十亿                      | OpenVLA/RT-X后继模型的基础                |

### 在具身智能中的定位

- **ACT与Diffusion Policy**：纯视觉运动策略——擅长低层级感知运动技能，构成具身AI的“身体”，使机器人能根据原始像素在物理世界中流畅行动
- **RT-2（及VLA模型）**：通往高层认知的桥梁——通过语言实现类人交互与推理，推动通用具身智能体发展
- **混合架构是未来趋势**：2025年趋势显示三者正在融合（如OpenVLA=RT-2架构+扩散头；OCTO=高效小模型+扩散元素）。最终目标是通过扩散/分块实现多模态感知、LLM/世界模型推理和精准行动的机器人

这些技术推动了灵巧机器人技术的普及：过去需要多年工程开发的任务，现在仅需数十次人类演示即可完成。随着Open X-Embodiment等数据集扩张和Figure 01、特斯拉Optimus等人形机器人硬件进步，具身智能正在快速迈向能够胜任现实世界任务的通用机器人。

[Diffusion Policy项目页](https://diffusion-policy.cs.columbia.edu/)  
[RT-2技术公告](https://deepmind.google/discover/blog/rt-2-new-model-translates-vision-and-language-into-action/)  
[ALOHA/ACT论文](https://tonyzhaozh.github.io/aloha/)  
[Open X-Embodiment数据集](https://robotics-transformer-x.github.io/)