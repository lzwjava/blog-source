---
audio: false
generated: true
lang: zh
layout: post
title: 长短期记忆网络
translated: true
type: note
---

长短期记忆网络（LSTM）是一种特殊的循环神经网络架构，专门用于解决梯度消失问题，并能够学习序列数据中的长期依赖关系。这是通过一种更复杂的内部结构——**LSTM单元**——来实现的。

以下是LSTM单元的工作原理详解：

**核心思想：细胞状态**

LSTM的核心概念是**细胞状态**（通常记为 'C<sub>t</sub>'）。可以将细胞状态想象成一条贯穿整个序列的传送带。它承载着与序列长期历史相关的信息。信息可以通过称为**门**的结构，在网络中流动时被添加到细胞状态或从细胞状态中移除。

**门结构**

LSTM单元有三个主要门来控制信息流：

1.  **遗忘门：** 该门决定应从先前的细胞状态中丢弃哪些信息。
    *   它接收先前的隐藏状态 (h<sub>t-1</sub>) 和当前输入 (x<sub>t</sub>)。
    *   这些信息通过一个神经网络层和一个 **sigmoid激活函数**。
    *   sigmoid函数输出介于0和1之间的值。接近0的值表示"完全忘记此信息"，而接近1的值表示"完全保留此信息"。
    *   数学上，遗忘门的输出 (f<sub>t</sub>) 计算如下：
        ```
        f_t = σ(W_f * [h_{t-1}, x_t] + b_f)
        ```
        其中：
        *   σ 是sigmoid函数。
        *   W<sub>f</sub> 是遗忘门的权重矩阵。
        *   [h<sub>t-1</sub>, x_t] 是先前的隐藏状态和当前输入的拼接向量。
        *   b<sub>f</sub> 是遗忘门的偏置向量。

2.  **输入门：** 该门决定应将当前输入中的哪些新信息添加到细胞状态中。此过程涉及两个步骤：
    *   **输入门层：** 一个sigmoid层决定我们将更新哪些值。
        ```
        i_t = σ(W_i * [h_{t-1}, x_t] + b_i)
        ```
        其中：
        *   σ 是sigmoid函数。
        *   W<sub>i</sub> 是输入门的权重矩阵。
        *   [h<sub>t-1</sub>, x_t] 是先前的隐藏状态和当前输入的拼接向量。
        *   b<sub>i</sub> 是输入门的偏置向量。
    *   **候选值层：** 一个tanh层创建一个新的候选值向量（候选细胞状态，记为 'C̃<sub>t</sub>'），这些值可能会被添加到细胞状态中。tanh函数输出介于-1和1之间的值，这有助于调节网络。
        ```
        C̃_t = tanh(W_C * [h_{t-1}, x_t] + b_C)
        ```
        其中：
        *   tanh 是双曲正切函数。
        *   W<sub>C</sub> 是候选细胞状态的权重矩阵。
        *   [h<sub>t-1</sub>, x_t] 是先前的隐藏状态和当前输入的拼接向量。
        *   b<sub>C</sub> 是候选细胞状态的偏置向量。

3.  **输出门：** 该门决定应将当前细胞状态中的哪些信息输出，作为当前时间步的隐藏状态。
    *   它接收先前的隐藏状态 (h<sub>t-1</sub>) 和当前输入 (x<sub>t</sub>)。
    *   这些信息通过一个神经网络层和一个 **sigmoid激活函数**，以决定要输出细胞状态的哪些部分。
        ```
        o_t = σ(W_o * [h_{t-1}, x_t] + b_o)
        ```
        其中：
        *   σ 是sigmoid函数。
        *   W<sub>o</sub> 是输出门的权重矩阵。
        *   [h<sub>t-1</sub>, x_t] 是先前的隐藏状态和当前输入的拼接向量。
        *   b<sub>o</sub> 是输出门的偏置向量。
    *   然后，细胞状态通过一个 **tanh函数**，将其值压缩到-1和1之间。
    *   最后，sigmoid门的输出与应用于细胞状态的tanh函数的输出进行逐元素相乘。这就成为了新的隐藏状态 (h<sub>t</sub>)，它被传递到下一个时间步，也可用于进行预测。
        ```
        h_t = o_t * tanh(C_t)
        ```

**更新细胞状态**

细胞状态根据遗忘门和输入门的决策进行更新：

```
C_t = f_t * C_{t-1} + i_t * C̃_t
```

*   遗忘门 (f<sub>t</sub>) 决定保留多少先前的细胞状态 (C<sub>t-1</sub>)。如果 f<sub>t</sub> 接近0，则信息大部分被遗忘。如果接近1，则信息大部分被保留。
*   输入门 (i<sub>t</sub>) 决定应将多少新的候选值 (C̃<sub>t</sub>) 添加到细胞状态中。如果 i<sub>t</sub> 接近0，则新信息大部分被忽略。如果接近1，则新信息大部分被添加。

**LSTM如何解决梯度消失问题：**

LSTM处理长期依赖关系的关键在于细胞状态和门控机制。细胞状态充当了信息跨越多个时间步的直接通路，而无需反复与权重相乘——这正是传统RNN中梯度消失的主要原因。

这些门使得LSTM能够学习何时存储、访问和忘记细胞状态中的信息。这种选择性记忆机制使网络能够在长序列中保留相关信息并在需要时使用它，从而有效缓解梯度消失问题，并允许学习长期依赖关系。

本质上，LSTM为网络提供了一种在较长时间内维持"记忆"的方式，这使得它们在处理具有长期依赖关系的序列数据任务时非常有效。