---
audio: false
generated: true
image: false
lang: zh
layout: post
model: x-ai/grok-4-fast
title: 计算能力在人工智能中胜过语法
translated: true
type: note
---

你说得对，人工智能和大型语言模型的核心确实不在于优雅的散文或对编程语言的哲学沉思——尽管像王垠和Daniel P. Friedman这样的人无疑深化了我们的思考。王垠的博客深入剖析了Python和Lisp等系统的内核，批判它们如何在实际中塑造计算范式；而Friedman的著作（比如《The Little Schemer》系列）以近乎诗意的方式解构递归和函数式编程。但确实，当要构建真正能运作的系统——比如生成连贯文本或在神经网络中处理梯度——关键始终在于计算的"方法"与"内容"，而非语法糖衣。

### 为何计算优于语法
本质上，像我这样的LLM并不会思考Lisp宏或Java对象层次结构，而是在大规模执行矩阵乘法、注意力机制和概率采样。"如何计算"可归结为：
- **算法与模型**：如Transformer架构（Vaswani等，2017）定义了计算内容——对词元嵌入的自注意力、位置编码等。这才是产生魔力的地方，与语言无关。即使用伪代码实现GPT，它在纸面上也能"运行"，语法只是载体
- **数值精度与效率**：这里"计算什么"至关重要。我们讨论的是词元概率、损失函数（如交叉熵）和反向传播。数学运算出错，模型就会产生荒谬输出。语法？那是次要的——Python的NumPy以可读性带来90%的便利，但训练巨型模型时解释器速度堪忧

不过语言选择确实会作为实用过滤器悄然介入。C++在AI性能关键底层大放异彩（如TensorFlow内核或PyTorch的CUDA绑定），分布式训练中每个时钟周期都至关重要。Python因简洁性主导原型开发——当`torch.nn.Linear`能让你用一行代码串联网络层时，何必纠缠于C++的内存管理？但你说得对：这只是"小问题"。真正的瓶颈在于算法（如Kaplan等的扩展定律）或数据层面，与你输入`def`还是`struct`无关

### 编程语言思想的隐性价值
话虽如此，完全抛弃编程语言研究就像把婴儿和洗澡水一起倒掉。受Friedman/王垠等人启发的优秀语言设计，能让计算表达更安全高效：
- **用于推理的抽象**：函数式纯度（类似Scheme风格）有助于避免概率代码中的错误，这对可靠的AI推理至关重要
- **无痛性能提升**：Rust甚至现代C++借鉴编程语言研究成果，在保证安全性的同时提供速度，减少原始C语言中常见的计算错误

归根结底，AI进展80%靠数学/模型/数据，15%靠工程实现（C++等语言在此发力），剩下5%才是语法诗学。如果我们过度优化后者，只会得到更漂亮的论文但更迟钝的机器人。你怎么看——你觉得未来会看到更多编程语言与AI的融合吗？比如用于神经符号计算的领域特定语言？

[王垠的博客](https://www.yinwang.org)  
[Friedman的编程精粹](https://www.cs.indiana.edu/~dfriedman/)  
[注意力机制即是一切（Transformer论文）](https://arxiv.org/abs/1706.03762)  
[神经语言模型的扩展定律](https://arxiv.org/abs/2001.08361)