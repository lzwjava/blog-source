---
audio: false
generated: true
image: false
lang: zh
layout: post
model: x-ai/grok-4-fast
title: 梯度提升机简介
translated: true
type: note
---

### 概述
Jerome H. Friedman 于 2001 年发表的论文《贪婪函数逼近：梯度提升机》提出了梯度提升机（GBM）——一种强大的集成学习方法，适用于回归和分类等监督学习任务。该框架将提升方法视为函数空间中的梯度下降过程，通过顺序添加简单的“弱”学习者（通常是决策树）来构建加性模型，以最小化指定的损失函数。这种方法推广了早期的提升算法（如 AdaBoost），并强调函数空间中的贪婪优化，从而产生高精度、鲁棒性强且可解释的模型。

### 摘要（意译）
GBM 通过以顺序加性的方式组合弱学习器，逼近可微损失函数的最小值，从而构建灵活的预测模型。使用回归树作为基学习器，可为回归和分类任务提供具有竞争力且鲁棒的方法。实证测试表明，该方法在多个数据集上的错误率较低，性能优于多元自适应回归样条（MARS）等替代方法。

### 核心方法
核心思想是迭代地将新学习器拟合到损失函数相对于当前模型预测的负梯度（伪残差），模拟函数空间中的梯度下降。

- **模型结构**：最终模型为 \\( F_M(x) = \sum_{m=1}^M \beta_m h_m(x) \\)，其中每个 \\( h_m(x) \\) 是一个弱学习器（例如小型回归树）。
- **更新规则**：在第 \\( m \\) 次迭代中，计算残差 \\( r_{im} = -\left[ \frac{\partial L(y_i, F(x_i))}{\partial F(x_i)} \right]_{F=F_{m-1}} \\)，然后通过最小二乘法将 \\( h_m \\) 拟合到这些残差。步长 \\( \gamma_m \\) 通过线性搜索优化：\\( \gamma_m = \arg\min_\gamma \sum_i L(y_i, F_{m-1}(x_i) + \gamma h_m(x_i)) \\)。
- **收缩**：通过缩放因子 \\( \nu \in (0,1] \\)（例如 \\( \nu = 0.1 \\)）缩放每次添加的模型，以减少过拟合并允许更多次迭代。
- **随机变体**：在每一步对数据进行子采样（例如 50%），以加速训练并提高泛化能力。
- **TreeBoost 算法**（伪代码概述）：
  1. 初始化 \\( F_0(x) \\) 为最小化损失的常数。
  2. 对于 \\( m = 1 \\) 到 \\( M \\)：
     - 计算伪残差 \\( r_{im} \\)。
     - 将树 \\( h_m \\) 拟合到 \\( \{ (x_i, r_{im}) \} \\)。
     - 通过线性搜索找到最优 \\( \gamma_m \\)。
     - 更新 \\( F_m(x) = F_{m-1}(x) + \nu \gamma_m h_m(x) \\)。
  3. 根据迭代次数或损失改进情况停止。

支持的损失函数包括：
- 最小二乘（回归）：\\( L(y, F) = \frac{1}{2}(y - F)^2 \\)，残差 = \\( y - F \\)。
- 最小绝对偏差（鲁棒回归）：\\( L(y, F) = |y - F| \\)。
- 对数似然（二分类）：\\( L = -\sum [y \log p + (1-y) \log(1-p)] \\)，其中 \\( p = \frac{1}{1 + e^{-F}} \\)；残差 = \\( y - p \\)。
- Huber 损失（抗异常值）。

诸如 LogitBoost 的变体针对特定损失函数（例如二项偏差）进行了适配。

### 贡献
- **统一框架**：通过梯度将提升方法扩展到任何可微损失函数，统一了 AdaBoost（指数损失）和 LogitBoost。
- **实用增强**：引入收缩和随机子采样，以更好地控制过拟合和计算量。
- **灵活性**：适用于各种基学习器和任务（回归、分类、排序）；树结构能高效处理高维数据。
- **可解释性工具**：讨论了变量重要性（通过总不纯度减少量）和部分依赖图。

### 主要发现
- **性能**：GBM 在实证中表现卓越，在真实数据集（如金融、物理模拟）上，相较于单棵树、装袋法、神经网络和 MARS 等基线方法，错误率降低了 10-50%。
- **鲁棒性**：随机 GBM 收敛更快；更深的树（6-8 层）可提高准确性，但需要正则化。LAD/Huber 损失比最小二乘更能缓解异常值的影响。
- **效率**：对于大规模数据训练速度快；最优配置为 \\( \nu = 0.1 \\)、1000+ 次迭代和子采样。
- 图表显示，与替代方法相比，GBM 能快速收敛损失，且方差/偏差较低。

这篇论文为现代实现（如 XGBoost 和 LightGBM）奠定了基础。

### 参考文献
- [Greedy Function Approximation: A Gradient Boosting Machine (PDF)](https://jerryfriedman.su.domains/ftp/trebst.pdf)