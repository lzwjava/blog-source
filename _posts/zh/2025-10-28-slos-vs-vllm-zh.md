---
audio: false
generated: true
image: false
lang: zh
layout: post
model: x-ai/grok-4-fast
title: SLOs-Serve与vLLM功能特性对比
translated: true
type: note
---

### 对比分析：SLOs-Serve 与 vLLM

SLOs-Serve 通过将 vLLM 集成为核心批处理引擎并进行多 SLO（服务水平目标）保障层级扩展，使其更适用于具有多样化、延迟敏感工作负载的生产环境。vLLM 主要专注于单 SLO 或吞吐量最大化场景的高吞吐量推理，采用 PagedAttention 等技术提升内存效率。以下基于 SLOs-Serve 论文与 vLLM 设计的关键维度进行结构化对比。

| 维度                 | SLOs-Serve                                                                 | vLLM                                                                 |
|----------------------|----------------------------------------------------------------------------|----------------------------------------------------------------------|
| **核心定位**         | 面向多阶段LLM应用的多SLO服务（如推理任务中预填充阶段的严格TTFT、编程任务中解码阶段的严格TPOT）。通过阶段级保障应对突发混合工作负载。 | 面向连续解码的高吞吐量批处理，通过PagedAttention优化内存受限场景。假定统一SLO或优先考虑聚合吞吐量。 |
| **SLO处理能力**      | 显式多SLO支持：支持阶段级（预填充/解码）和应用级SLO（如编程任务50ms TPOT vs 聊天任务100ms TPOT）。采用柔性准入控制拒绝/延迟违规请求。 | 无原生多SLO支持，依赖静态配置（如最大批处理规模）。资源争用时易出现SLO违规（如突发流量下延迟峰值>2倍）。 |
| **调度器**           | 基于动态规划（DP）：按SLO层级优化预填充预算、批处理规模和推测长度。通过Roofline模型预测执行时间（R² > 0.8准确率）。 | 连续批处理调度器：动态贪婪打包请求，专注解码密集型场景。无SLO感知规划能力。 |
| **预填充优化**       | 分块预填充与自适应推测（每SLO 1-10个令牌）。通过"预填充预算"分配平衡解码阶段。 | 单请求单次预填充；支持分块但缺乏SLO适配。混合负载易出现队头阻塞。 |
| **解码优化**         | SLO自适应批处理（支持512+令牌）与面向TPOT目标的推测解码。 | 具备前瞻批处理的高效连续解码；吞吐量显著提升（如较Hugging Face快10-20倍）但忽略单请求截止时间。 |
| **资源管理**         | 基于Ray的多副本路由；通过尽力而为队列与抢占机制应对突发。支持分离式架构。 | 单节点或基础分布式架构（通过Ray集成）；无主动路由或SLO优先分配机制。 |
| **吞吐量与容量**     | 较vLLM平均容量提升2.2倍（跨6类场景几何平均：聊天机器人、编程等）。推理突发场景达2.4倍。副本数量实现超线性扩展。 | 吞吐量基线：解码密集型场景较替代方案快24倍，但在SLO约束下性能下降（混合负载容量损失50%）。 |
| **系统开销**         | 单次调度<10ms；因DP高效性（O(n)状态）保持最低开销。 | 批处理开销低（<1ms）；因无SLO逻辑故无附加开销。 |
| **适用场景**         | 严格SLA的生产应用：工具调用智能体、推理链、突发流量（如Azure轨迹数据）。 | 研发场景吞吐量需求：简单服务、长文本生成、均匀负载。 |
| **局限性**           | 需预设SLO/截止时间；未来需扩展未知长度处理。基于vLLM构建，继承其解码优势。 | 多阶段/可变SLO处理能力不足；共享GPU环境尾延迟较高（评估中p99>目标值2倍）。 |
| **实现方案**         | 基于vLLM+Ray的开源原型；支持A100/H100显卡的OPT/ToolLlama模型。 | 成熟开源方案（GitHub）；支持100+模型，部署便捷，生态广泛。 |

综上所述，vLLM在面向吞吐量的原始效率方面表现卓越，但在复杂现实场景的SLO合规性上存在不足——这也正是SLOs-Serve通过针对性优化实现2倍以上性能增益的关键所在。若您的工作负载纯属吞吐量导向且无需延迟保障，vLLM仍是更简单快捷的部署选择。

[SLOs-Serve：多SLO大语言模型优化服务框架](https://arxiv.org/abs/2504.08784)  
[vLLM：基于PagedAttention的轻量快速LLM服务方案](https://arxiv.org/abs/2309.06180)