---
audio: false
generated: true
lang: zh
layout: post
title: 变压器
translated: true
type: note
---

**Transformer** 是一种革命性的神经网络架构，已成为大多数现代大语言模型（LLMs）的基础。它由 Vaswani 等人在 2017 年的开创性论文《Attention Is All You Need》中提出，并从此从根本上改变了自然语言处理（NLP）领域。

与之前占主导地位的架构（如循环神经网络 RNNs）逐步处理序列数据不同，Transformer **并行地**处理整个输入序列。这一关键差异使得训练速度显著提高，并且能够捕捉文本中的长距离依赖关系。

以下是在 LLMs 背景下，对 Transformer 架构关键组件和概念的深入探讨：

**1. 核心理念：注意力机制**

Transformer 的核心创新是**注意力机制**，特别是**自注意力**。该机制允许模型在处理特定词语（或标记）时，权衡输入序列中不同词语的重要性。自注意力使模型能够考虑整个上下文来理解词语的含义和关系，而不是仅仅依赖于紧邻的前面的词语（像 RNNs 那样）。

可以这样理解：当你阅读一个句子时，你并不是孤立地处理每个单词。你的大脑会同时考虑所有单词，以理解整体含义以及每个单词对其的贡献。自注意力机制模仿了这种行为。

**自注意力工作原理（简化版）：**

对于输入序列中的每个单词，Transformer 会计算三个向量：

*   **查询向量：** 代表当前单词在其他单词中"寻找"什么。
*   **键向量：** 代表其他每个单词"包含"什么信息。
*   **值向量：** 代表其他每个单词所持有的、可能相关的实际信息。

然后，自注意力机制执行以下步骤：

1.  **计算注意力分数：** 计算一个单词的 Query 向量与序列中其他所有单词的 Key 向量的点积。这些分数表明其他每个单词的信息与当前单词的相关程度。
2.  **缩放分数：** 将分数除以 Key 向量维度的平方根。这种缩放有助于在训练过程中稳定梯度。
3.  **应用 Softmax：** 将缩放后的分数通过 softmax 函数，将其归一化为 0 到 1 之间的概率。这些概率代表了**注意力权重**——当前单词应对其他每个单词给予多少"注意力"。
4.  **计算加权值：** 将每个单词的 Value 向量乘以其对应的注意力权重。
5.  **求和加权值：** 将加权的 Value 向量相加以生成当前单词的**输出向量**。这个输出向量现在包含了输入序列中所有其他相关单词的信息，并根据它们的重要性进行了加权。

**2. 多头注意力**

为了进一步增强模型捕捉不同类型关系的能力，Transformer 采用了**多头注意力**。它不是只执行一次自注意力机制，而是使用不同的 Query、Key 和 Value 权重矩阵集并行执行多次。每个"头"学习关注词语之间关系的不同方面（例如，语法依赖关系、语义连接）。所有注意力头的输出随后被连接起来，并进行线性变换，以产生多头注意力层的最终输出。

**3. 位置编码**

由于 Transformer 并行处理所有单词，它丢失了序列中单词**顺序**的信息。为了解决这个问题，**位置编码**被添加到输入嵌入中。这些编码是表示每个单词在序列中位置的向量。它们通常是固定的模式（例如，正弦函数）或学习得到的嵌入。通过添加位置编码，Transformer 可以理解语言的顺序特性。

**4. 编码器和解码器堆栈**

Transformer 架构通常由两个主要部分组成：一个**编码器**和一个**解码器**，两者都由多个相同的层堆叠而成。

*   **编码器：** 编码器的角色是处理输入序列并创建其丰富的表示。每个编码器层通常包含：
    *   一个**多头自注意力**子层。
    *   一个**前馈神经网络**子层。
    *   围绕每个子层的**残差连接**，后接**层归一化**。残差连接有助于训练期间的梯度流动，层归一化则稳定了激活值。

*   **解码器：** 解码器的角色是生成输出序列（例如，在机器翻译或文本生成中）。每个解码器层通常包含：
    *   一个**掩码多头自注意力**子层。"掩码"阻止解码器在训练期间查看目标序列中未来的标记，确保它只使用先前生成的标记来预测下一个标记。
    *   一个**多头注意力**子层，该子层关注编码器的输出。这使得解码器在生成输出时能够聚焦于输入序列的相关部分。
    *   一个**前馈神经网络**子层。
    *   与编码器类似的**残差连接**和**层归一化**。

**5. 前馈网络**

每个编码器和解码器层都包含一个前馈神经网络（FFN）。该网络独立地应用于每个标记，有助于进一步处理由注意力机制学习到的表示。它通常由两个线性变换组成，中间有一个非线性激活函数（例如 ReLU）。

**Transformer 如何在 LLMs 中使用：**

LLMs 主要基于**仅解码器**的 Transformer 架构（如 GPT 模型）或**编码器-解码器**架构（如 T5）。

*   **仅解码器模型：** 这些模型被训练为在给定前面标记的情况下预测序列中的下一个标记。它们堆叠了多个解码器层。输入的提示词通过这些层传递，最后一层预测词汇表中下一个标记的概率分布。通过自回归地从该分布中采样，模型可以生成连贯且与上下文相关的文本。

*   **编码器-解码器模型：** 这些模型接收一个输入序列并生成一个输出序列。它们使用完整的编码器-解码器架构。编码器处理输入，解码器利用编码器的输出来生成目标序列。这些模型非常适合翻译、摘要和问答等任务。

**深入理解其意义：**

Transformer 架构对 LLMs 的影响是深远的：

*   **处理长距离依赖：** 自注意力机制允许模型直接连接序列中相距较远的单词，克服了 RNNs 在捕捉长距离上下文方面的局限性。
*   **并行处理：** 并行处理整个序列显著减少了训练时间，并允许使用更大的数据集和模型。
*   **可扩展性：** Transformer 架构能够很好地适应数据和模型规模的增加，从而催生了具有数十亿甚至数万亿参数的极其强大的 LLMs。
*   **上下文理解：** 注意力机制使模型能够以更细致的方式理解单词的上下文，从而实现更好的语言理解和生成。
*   **创新的基础：** Transformer 架构已成为众多进步和变体的基础，造就了现代 LLMs 多样化的格局。

总之，Transformer 架构以其核心的自注意力机制、并行处理能力以及编码器-解码器结构（或其仅解码器的变体），彻底改变了 NLP 领域，并且是现代大语言模型卓越能力背后的驱动力。