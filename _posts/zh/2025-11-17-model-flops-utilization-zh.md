---
audio: false
generated: true
image: false
lang: zh
layout: post
model: openai/gpt-5
title: 理解模型浮点运算利用率
translated: true
type: note
---

在NanoGPT中，**MFU**代表**模型浮点运算利用率**。

它表示**模型在训练过程中实际使用的GPU理论计算能力（浮点运算次数）占比**。
可以将其粗略理解为“GPU效率指标”。

实际含义如下：

**• 0–20% MFU：**
常见于小模型、小批量训练或GPU未充分利用的情况

**• 30–50% MFU：**
在消费级GPU上运行中等规模模型时的理想表现

**• 50–70% MFU：**
非常高的利用率——通常出现在大批量训练、内核优化良好的场景

**• 80%+ MFU：**
接近极限值，仅出现在采用优化内核（FlashAttention、融合操作、bf16等）的大型模型中

您的数据：

```
mfu 10.69%
```

对于在RTX 4070上运行的1.25亿参数模型，这个数值是正常的。小模型无法完全利用GPU的原因包括：

* 矩阵乘法运算量过小，无法饱和流式多处理器
* 批量训练尺寸较小
* 数据加载器和Python等开销占主导
* GPU专为大型矩阵运算设计，而非微型计算

若想提升MFU，可尝试：

* 增加**批量大小**（每批最大令牌数）
* 扩大**模型规模**
* 采用**FlashAttention**
* 使用**CUDA图捕获**（新版配置已默认启用）

但对于小模型而言，10–20%的MFU属于正常范围且完全可接受。

如果需要，我可以协助分析您的配置文件，具体说明MFU与批量大小和参数数量之间的关联。