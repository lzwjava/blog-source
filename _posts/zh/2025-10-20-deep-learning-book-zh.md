---
audio: false
generated: true
image: false
lang: zh
layout: post
model: x-ai/grok-4-fast
title: Goodfellow、Bengio与Courville的深度学习
translated: true
type: note
---

**Ian Goodfellow**  
**Yoshua Bengio**  
**Aaron Courville**

## 目录

- 网站 viii  
- 致谢 ix  
- 符号说明 xiii  

### 1 引言 1

- 1.1 本书面向的读者 8  
- 1.2 深度学习的历史趋势 12  

## I 应用数学与机器学习基础 27

### 2 线性代数 29

- 2.1 标量、向量、矩阵和张量 29  
- 2.2 矩阵与向量相乘 32  
- 2.3 单位矩阵和逆矩阵 34  
- 2.4 线性相关和生成子空间 35  
- 2.5 范数 37  
- 2.6 特殊类型的矩阵和向量 38  
- 2.7 特征分解 40  
- 2.8 奇异值分解 42  
- 2.9 Moore-Penrose 伪逆 43  
- 2.10 迹运算 44  
- 2.11 行列式 45  
- 2.12 实例：主成分分析 45  

### 3 概率与信息论 51

- 3.1 为什么要使用概率？ 52  
- 3.2 随机变量 54  
- 3.3 概率分布 54  
- 3.4 边缘概率 56  
- 3.5 条件概率 57  
- 3.6 条件概率的链式法则 57  
- 3.7 独立性和条件独立性 58  
- 3.8 期望、方差和协方差 58  
- 3.9 常用概率分布 60  
- 3.10 常用函数的有用性质 65  
- 3.11 贝叶斯规则 68  
- 3.12 连续型变量的技术细节 69  
- 3.13 信息论 71  
- 3.14 结构化概率模型 73  

### 4 数值计算 78

- 4.1 上溢和下溢 78  
- 4.2 病态条件 80  
- 4.3 基于梯度的优化方法 80  
- 4.4 约束优化 91  
- 4.5 实例：线性最小二乘 94  

### 5 机器学习基础 96

- 5.1 学习算法 97  
- 5.2 容量、过拟合和欠拟合 108  
- 5.3 超参数和验证集 118  
- 5.4 估计、偏差和方差 120  
- 5.5 最大似然估计 129  
- 5.6 贝叶斯统计 133  
- 5.7 监督学习算法 137  
- 5.8 无监督学习算法 142  
- 5.9 随机梯度下降 149  
- 5.10 构建机器学习算法 151  
- 5.11 促使深度学习发展的挑战 152  

## II 深度网络：现代实践 162

### 6 深度前馈网络 164

- 6.1 实例：学习 XOR 167  
- 6.2 基于梯度的学习 172  
- 6.3 隐藏单元 187  
- 6.4 架构设计 193  
- 6.5 反向传播和其他微分算法 200  
- 6.6 历史小记 220  

### 7 深度学习中的正则化 224

- 7.1 参数范数惩罚 226  
- 7.2 作为约束优化的范数惩罚 233  
- 7.3 正则化和欠约束问题 235  
- 7.4 数据集增强 236  
- 7.5 噪声鲁棒性 238  
- 7.6 半监督学习 240  
- 7.7 多任务学习 241  
- 7.8 提前终止 241  
- 7.9 参数绑定和参数共享 249  
- 7.10 稀疏表示 251  
- 7.11 Bagging 和其他集成方法 253  
- 7.12 Dropout 255  
- 7.13 对抗训练 265  
- 7.14 切面距离、正切传播和流形正切分类器 267  

### 8 深度模型中的优化 271

- 8.1 学习和纯优化的区别 272  
- 8.2 神经网络优化中的挑战 279  
- 8.3 基本算法 290  
- 8.4 参数初始化策略 296  
- 8.5 自适应学习率算法 302  
- 8.6 近似二阶方法 307  
- 8.7 优化策略和元算法 313  

### 9 卷积网络 326

- 9.1 卷积运算 327  
- 9.2 动机 329  
- 9.3 池化 335  
- 9.4 卷积与池化作为一种无限强的先验 339  
- 9.5 基本卷积函数的变体 342  
- 9.6 结构化输出 352  
- 9.7 数据类型 354  
- 9.8 高效的卷积算法 356  
- 9.9 随机或无监督的特征 356  
- 9.10 卷积网络的神经科学基础 358  
- 9.11 卷积网络与深度学习的历史 365  

### 10 序列建模：循环和递归网络 367

- 10.1 展开计算图 369  
- 10.2 循环神经网络 372  
- 10.3 双向 RNN 388  
- 10.4 基于编码器-解码器的序列到序列架构 390  
- 10.5 深度循环网络 392  
- 10.6 递归神经网络 394  
- 10.7 长期依赖的挑战 396  
- 10.8 回声状态网络 399  
- 10.9 渗漏单元和其他多时间尺度的策略 402  
- 10.10 长短期记忆和其他门控 RNN 404  
- 10.11 优化长期依赖 408  
- 10.12 显式记忆 412  

### 11 实践方法论 416

- 11.1 性能度量  
- 11.2 默认的基准模型  
- 11.3 决定是否收集更多数据  
- 11.4 选择超参数  
- 11.5 调试策略  
- 11.6 实例：多位数字识别  

## III 深度学习研究 482

### 12 线性因子模型 485

- 12.1 概率 PCA 和因子分析  
- 12.2 独立成分分析 (ICA)  
- 12.3 慢特征分析  
- 12.4 稀疏编码  
- 12.5 PCA 的流形解释  

### 13 自编码器 500

- 13.1 欠完备自编码器  
- 13.2 正则化自编码器  
- 13.3 表示能力、层的大小和深度  
- 13.4 随机编码器和解码器  
- 13.5 去噪自编码器  
- 13.6 使用自编码器学习流形  
- 13.7 收缩自编码器  
- 13.8 预测稀疏分解  
- 13.9 自编码器的应用  

### 14 表示学习 525

- 14.1 贪心逐层无监督预训练  
- 14.2 迁移学习和领域自适应  
- 14.3 半监督解耦因果因素  
- 14.4 分布式表示  
- 14.5 得益于深度的指数增益  
- 14.6 提供发现潜在原因的线索  

### 15 深度学习的结构化概率模型 540

- 15.1 非结构化建模的挑战  
- 15.2 使用图描述模型结构  
- 15.3 从图模型中采样  
- 15.4 结构化建模的优势  
- 15.5 学习依赖关系  
- 15.6 推断和近似推断  
- 15.7 深度学习在结构化概率模型中的应用方法  

### 16 蒙特卡罗方法 557

- 16.1 采样和蒙特卡罗方法  
- 16.2 重要采样  
- 16.3 马尔可夫链蒙特卡罗方法  
- 16.4 Gibbs 采样  
- 16.5 分离模态之间的混合挑战  

### 17 直面配分函数 567

- 17.1 对数似然梯度  
- 17.2 随机最大似然和对比散度  
- 17.3 伪似然  
- 17.4 得分匹配和比率匹配  
- 17.5 去噪得分匹配  
- 17.6 噪声对比估计  
- 17.7 估计配分函数  

### 18 近似推断 579

- 18.1 把推断视作优化  
- 18.2 期望最大化  
- 18.3 MAP 推断和稀疏编码  
- 18.4 变分推断和学习  
- 18.5 学成的近似推断  

### 19 深度生成模型 594

- 19.1 玻尔兹曼机  
- 19.2 受限玻尔兹曼机  
- 19.3 深度信念网络  
- 19.4 深度玻尔兹曼机  
- 19.5 实值数据上的玻尔兹曼机  
- 19.6 卷积玻尔兹曼机  
- 19.7 用于结构化或序列输出的玻尔兹曼机  
- 19.8 其他玻尔兹曼机  
- 19.9 通过随机操作的反向传播  
- 19.10 有向生成网络  
- 19.11 从自编码器采样  
- 19.12 生成随机网络  
- 19.13 其他生成方案  
- 19.14 生成模型的评估  
- 19.15 结论  

[《深度学习》目录](https://www.deeplearningbook.org/contents/toc.html)