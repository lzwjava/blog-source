---
audio: false
generated: true
image: false
lang: zh
layout: post
title: vLLM：高性能大语言模型服务引擎
translated: true
type: note
---

vLLM 是一个专为大语言模型（LLM）推理与服务设计的高性能库。以下是对其关键特性的详细解析：

### 1. **顶尖服务吞吐量**
   - **含义**：vLLM 旨在最大化每秒处理的请求或令牌数量，为 LLM 推理提供高吞吐能力。
   - **实现方式**：通过优化从请求处理到模型执行的完整推理流水线，降低系统开销并高效利用硬件加速器（如 GPU），确保即使在重负载下也能实现快速响应。

### 2. **通过 PagedAttention 实现注意力键值内存的高效管理**
   - **含义**：PagedAttention 是一种面向基于 Transformer 的 LLM 注意力机制的内存管理技术。
   - **原理**：在 Transformer 中，注意力机制需要存储每个令牌的键值（KV）张量，这会消耗大量 GPU 内存。PagedAttention 将 KV 缓存拆分为更小的可管理“页”，类似于操作系统中的虚拟内存机制。这种设计减少了内存碎片，支持内存高效复用，并能在不耗尽 GPU 内存的前提下处理更大模型或更长序列。

### 3. **持续批处理传入请求**
   - **含义**：持续批处理技术动态地将传入请求分组进行联合处理，以提升效率。
   - **原理**：vLLM 实时将多个到达的请求动态组合成批次进行处理，通过动态调整批次大小和构成来最小化空闲时间，最大化 GPU 利用率。这种机制特别适用于现实服务场景中处理动态变化的工作负载。

### 4. **基于 CUDA/HIP 图的快速模型执行**
   - **含义**：通过预定义操作序列的 CUDA/HIP 图来优化 GPU 执行效率。
   - **原理**：传统 GPU 操作涉及多次内核启动，会产生额外开销。CUDA/HIP 图允许 vLLM 将一系列操作（如矩阵乘法、激活函数）捕获为单个可执行图，从而减少启动开销并提升执行速度。这对 LLM 推理中的重复性任务尤为有效。

### 5. **量化支持：GPTQ、AWQ、AutoRound、INT4、INT8 与 FP8**
   - **含义**：量化通过降低模型权重和激活值的精度（如从 32 位浮点转换为低位格式）来节省内存并加速计算。
   - **技术解析**：
     - **GPTQ**：一种训练后量化方法，可将权重压缩至 4 位或更低精度，同时保持高准确度。
     - **AWQ（激活感知权重量化）**：通过考虑激活值分布来优化量化过程，提升特定模型的性能表现。
     - **AutoRound**：通过自动微调舍入决策来最小化精度损失的量化技术。
     - **INT4/INT8**：基于整数的量化方案（4 位或 8 位），可降低内存占用并在兼容硬件上实现更快计算。
     - **FP8**：8 位浮点格式，在现代支持 FP8 的 GPU（如 NVIDIA H100）上实现精度与效率的平衡。
   - **影响**：这些量化方法显著降低内存消耗，使得更大模型可部署于 GPU，并在保证精度的前提下加速推理。

### 6. **优化 CUDA 内核：集成 FlashAttention 与 FlashInfer**
   - **含义**：vLLM 使用为 LLM 定制的深度优化 CUDA 内核，包含 FlashAttention 与 FlashInfer 等先进注意力机制。
   - **原理**：
     - **CUDA 内核**：针对 LLM 特定操作（如矩阵乘法、注意力计算）优化的定制 GPU 程序，可减少执行时间。
     - **FlashAttention**：通过重构注意力机制减少内存访问与计算量的高效算法，对长序列处理尤为迅捷。
     - **FlashInfer**：作为 FlashAttention 的扩展或替代方案，针对特定使用场景或硬件进一步优化注意力计算。
   - **影响**：这些优化使注意力计算更快、更节省内存，对基于 Transformer 的 LLM 至关重要。

### 7. **推测解码**
   - **含义**：通过并行预测多个令牌并后续验证的方式加速文本生成。
   - **原理**：vLLM 使用更轻量的快速模型（或启发式方法）并行预测若干令牌，再由主模型单次验证预测结果。若预测正确则减少模型评估次数，若错误则回退至标准解码流程以确保准确性。
   - **影响**：该技术可显著提升对话、故事生成等文本生成任务的吞吐量。

### 8. **分块预填充**
   - **含义**：将输入提示词的初始处理（预填充）拆分为小块以优化内存和计算资源。
   - **原理**：在 LLM 中，预填充阶段需要在生成令牌前为输入提示计算注意力 KV 缓存。面对长提示时，这一过程可能消耗大量内存。分块预填充将提示词分割为小段处理，降低峰值内存使用，实现对长序列的高效处理。
   - **影响**：使 vLLM 能够在不占用过量 GPU 内存的前提下处理更长输入，提升系统扩展性。

### 总结
vLLM 的高性能源于多重技术组合：优化的内存管理（PagedAttention、分块预填充）、高效的请求处理（持续批处理）、硬件加速（CUDA/HIP 图、优化内核、FlashAttention/FlashInfer）以及模型压缩（量化）。推测解码等技术进一步增强了生成速度。这些特性共同使 vLLM 成为支撑高吞吐、低延迟大规模 LLM 部署的强大工具。

**参考文献**：  
[vLLM 文档](https://vllm.ai/)  
[FlashAttention 论文](https://arxiv.org/abs/2205.14135)