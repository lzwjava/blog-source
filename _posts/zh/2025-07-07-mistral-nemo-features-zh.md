---
audio: false
generated: true
lang: zh
layout: post
title: Mistral NeMo：强大的120亿参数大语言模型
translated: true
type: note
---

**Mistral NeMo** 是由 **Mistral AI 与 NVIDIA 联合开发**的强大 120 亿参数大语言模型（LLM）。该模型旨在提供最先进的自然语言处理能力，在其规模级别中特别擅长推理、世界知识和代码生成准确性。

以下是 Mistral NeMo 的主要特性与亮点：

* **120 亿参数**：使其成为相对紧凑但仍具备强大能力的模型，在性能与效率间取得平衡
* **大上下文窗口**：支持高达 128K token 的上下文长度，能更有效地处理和理解长文本、复杂文档及多轮对话
* **顶尖性能表现**：为同规模模型树立新标杆，在推理、通用知识掌握和代码生成任务中展现卓越能力
* **多语言支持**：专为全球应用设计，精通英语、法语、德语、西班牙语、意大利语、葡萄牙语、中文、日语、韩语、阿拉伯语和印地语等十余种语言
* **Tekken 分词器**：采用基于 Tiktoken 的新型 Tekken 分词器，对超百种语言的自然语言文本和源代码压缩效率显著提升
* **函数调用能力**：经过函数调用专项训练，显著增强其根据自然语言指令交互并执行特定程序功能的能力
* **量化感知训练**：支持 FP8 推理且不损失性能，为高效部署提供关键技术支持
* **开源许可**：基于 Apache 2.0 许可证发布，推动研究机构与企业实现广泛采用、定制化与集成
* **便捷集成**：标准架构设计可轻松替代现有使用 Mistral 7B 的系统
* **与 NVIDIA 深度合作**：基于 NVIDIA DGX Cloud AI 平台训练，深度融合 TensorRT-LLM 加速推理技术与 NeMo 开发平台，同时作为 NVIDIA NIM 推理微服务提供

本质上，Mistral NeMo 致力于打造能单 GPU 运行的通用型高性能 LLM，为聊天机器人、文本摘要、语言翻译及代码生成等企业级应用场景提供更易获取的先进 AI 能力。