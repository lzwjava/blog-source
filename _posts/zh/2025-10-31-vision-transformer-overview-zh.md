---
audio: false
generated: true
image: false
lang: zh
layout: post
model: x-ai/grok-4-fast
title: 视觉变换器概览
translated: true
type: note
---

### Vision Transformer (ViT) 简介

Vision Transformer (ViT) 是一种将原本为自然语言处理设计的Transformer模型适配到计算机视觉任务（特别是图像分类）的神经网络架构。该架构由Google的Alexey Dosovitskiy及其团队在2020年的论文《An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale》中提出。ViT将图像视为图块序列进行处理，而非使用传统卷积神经网络。这使得它能够利用Transformer在处理长距离依赖和并行计算方面的优势。

在ImageNet等大规模数据集上，ViT已展现出与CNN相当或更优的性能，尤其是在海量数据（如JFT-300M）上进行预训练时。DeiT（数据高效图像Transformer）等变体使其在较小数据集上更具效率。如今，受ViT启发的模型为DALL-E、Stable Diffusion等现代分类器中的众多视觉任务提供核心能力。

### ViT工作原理：整体架构与流程

ViT的核心思想是将图像"分词化"为固定尺寸的图块序列，类似于将文本拆分为单词或词元。该序列随后通过标准Transformer编码器进行处理（与生成式文本模型不同，不包含解码器）。以下是其工作原理的逐步解析：

1. **图像预处理与图块提取**：
   - 输入尺寸为 \\(H \times W \times C\\) 的图像（例如RGB图像为224×224×3）
   - 将图像划分为固定尺寸 \\(P \times P\\) 的非重叠图块（如16×16像素），得到 \\(N = \frac{HW}{P^2}\\) 个图块（224×224图像按16×16划分可得196个图块）
   - 每个图块被展平为长度为 \\(P^2 \cdot C\\) 的一维向量（如16×16×3对应768维）
   - 为何使用图块？原始像素会产生过长的序列（高清图像可达数百万），图块作为"视觉单词"可有效降低维度

2. **图块嵌入**：
   - 对每个展平后的图块向量应用可学习的线性投影（全连接层），映射到固定嵌入维度 \\(D\\)（如768维，与BERT类Transformer保持一致）
   - 生成 \\(N\\) 个尺寸为 \\(D\\) 的嵌入向量
   - 可选地在序列前端添加特殊[CLS]标记嵌入（可学习的 \\(D\\) 维向量），类似于BERT的分类任务处理方式

3. **位置嵌入**：
   - 向图块嵌入添加可学习的一维位置嵌入，以编码空间信息（若无此步骤，Transformer具有排列不变性）
   - 完整输入序列为：\\([ \text{[CLS]}, \text{patch}_1, \text{patch}_2, \dots, \text{patch}_N ] + \text{positions}\\)，构成尺寸为 \\((N+1) \times D\\) 的矩阵

4. **Transformer编码器块**：
   - 将序列输入 \\(L\\) 个堆叠的Transformer编码层（如12层）
   - 每层包含：
     - **多头自注意力机制**：计算所有图块对（含[CLS]）间的注意力分数，使模型能捕捉全局关系（如"猫耳与100个图块外的胡须关联"），突破CNN的局部感受野限制
       - 计算公式：Attention(Q, K, V) = \\(\text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right) V\\)，其中Q、K、V为输入投影
     - **多层感知机**：前馈网络（两个线性层+GELU激活函数），逐位置应用
     - 层归一化与残差连接：输入 + MSA → 归一化 → MLP → 归一化 + 输入
   - 输出：精炼后的嵌入序列，保持 \\((N+1) \times D\\) 尺寸

5. **分类头**：
   - 对于图像分类，提取[CLS]标记的输出（或取所有图块嵌入的均值）
   - 通过简单MLP头（如1-2个线性层）输出类别logits
   - 训练时使用带标签数据的交叉熵损失，预训练常采用掩码图块预测等自监督任务

**关键超参数**（原始ViT-Base模型）：
- 图块尺寸 \\(P\\)：16
- 嵌入维度 \\(D\\)：768
- 层数 \\(L\\)：12
- 注意力头数：12
- 参数量：约8600万

ViT具备良好扩展性：更大模型（如ViT-Large，\\(D=1024\\), \\(L=24\\)）性能更优，但需要更多数据/算力支持

**训练与推理**：
- **训练**：基于标注数据端到端训练；海量图像预训练能显著提升效果
- **推理**：编码器前向传播（注意力机制时间复杂度约O(N²)，但可通过FlashAttention等优化技术加速）
- 与CNN不同，ViT不具备平移不变性等归纳偏置——所有特性均需学习获得

### 与文本Transformer的对比：相似与相异

ViT本质上与文本Transformer（如BERT）的编码器架构相同，但适配于二维视觉数据。以下是并置对比：

| 对比维度            | 文本Transformer（如BERT）                     | 视觉Transformer（ViT）                        |
|---------------------|-----------------------------------------------|-----------------------------------------------|
| **输入表示**        | 嵌入向量的词元序列（单词/子词）                | 嵌入向量的图像图块序列，图块类似"视觉词元"     |
| **序列长度**        | 可变（如句子512个词元）                       | 根据图像尺寸/图块尺寸固定（如含[CLS]共197个） |
| **位置编码**        | 一维（绝对/相对）编码词序                     | 一维（可学习）编码图块顺序（如行优先展开），无内置二维结构 |
| **核心机制**        | 词元间自注意力建模依赖关系                    | 图块间自注意力——数学原理相同，但关注空间"关系"而非句法关系 |
| **输出/任务**       | 编码器用于分类/掩码语言模型；解码器用于生成    | 仅编码器用于分类；可扩展至检测/分割任务        |
| **优势**            | 处理长距离文本依赖                            | 图像全局上下文理解（如整体场景感知）           |
| **劣势**            | 需要大规模文本语料                            | 数据饥渴型；无CNN预训练时在小数据集表现不佳    |
| **预测模式**        | 解码器中的下一词元预测（自回归）              | 无固有"下一项"预测机制——整体分类图像           |

本质上，ViT是"即插即用"的替换方案：将词嵌入替换为图块嵌入，即可获得视觉模型。两者都依赖注意力机制权衡序列中的关系，但文本本质是序列/线性的，而图像是空间性的（ViT通过注意力学习这种特性）。

### 辨析ViT中的"下一词元"与"下一像素"

ViT并不像文本Transformer预测"下一词元"那样预测"下一像素"。原因如下：

- **文本Transformer（自回归）**：如GPT等模型，解码器按序列生成——每次生成一个词元，以所有前序词元为条件。对于图像而言，部分生成模型（如PixelRNN）会逐像素生成，但效率较低

- **ViT的处理方式**：ViT是非自回归且整体处理的。它通过编码器并行处理完整图像（所有图块）。推理过程中不存在序列化的"下一项"预测——注意力机制会同步计算所有图块间的关系。对于生成任务（如图像修复），ViT变体可能使用解码器以自回归方式预测掩码图块，但基础ViT仅用于分类而非生成

- **为何不处理像素？** 将单个像素作为词元会使序列长度爆炸（如224×224×3=约15万个词元），导致二次方注意力计算不可行（O(N²)=数万亿次运算）。图块将约256个像素（16×16）分组为一个"词元"，使N保持可控（约200个）

若需生成式视觉能力（如下一像素/图块预测），可参考VQ-VAE + Transformer或Parti等基于ViT思想的模型

更多细节请参阅原始论文：[An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale](https://arxiv.org/abs/2010.11929)。如需代码示例（如PyTorch实现）或特定变体对比，欢迎告知！