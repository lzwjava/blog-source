---
audio: false
generated: true
lang: zh
layout: post
title: 神经网络中的激活函数
translated: true
type: note
---

**Sigmoid**、**ReLU** 和 **Leaky ReLU** 都是神经网络中使用的激活函数。它们各自具有独特特性，根据待解决问题的类型适用于不同任务。以下是它们的区别分析：

### 1. **Sigmoid**：
   - **公式**：  
     \\[
     \sigma(x) = \frac{1}{1 + e^{-x}}
     \\]
   - **值域**：(0, 1)
   - **优点**：
     - 平滑梯度，有助于优化
     - 输出限定在0到1之间，适用于概率或二分类场景
   - **缺点**：
     - **梯度消失问题**：当输入值极大或极小时，梯度会变得极小（接近零），这可能减缓训练速度，在深度网络中尤为明显
     - 输出非零中心化，可能导致梯度更新被单一方向主导
   - **适用场景**：常用于二分类任务的输出层（如逻辑回归）

### 2. **ReLU（线性整流单元）**：
   - **公式**：  
     \\[
     f(x) = \max(0, x)
     \\]
   - **值域**：[0, ∞)
   - **优点**：
     - **计算高效**：计算简单且实践效率高
     - 通过良好梯度传播缓解梯度消失问题
     - 促进稀疏性（可使大量神经元处于非活跃状态）
   - **缺点**：
     - **神经元死亡问题**：当输出恒为零时（即输入为负值），神经元可能在训练过程中"死亡"，导致部分神经元永久失活
   - **适用场景**：深度网络的隐藏层默认选择，尤其适用于卷积神经网络和深度神经网络

### 3. **Leaky ReLU**：
   - **公式**：  
     \\[
     f(x) = \max(\alpha x, x)
     \\]
     其中 \\( \alpha \\) 为极小常数（通常取0.01）
   - **值域**：(-∞, ∞)
   - **优点**：
     - 通过对负值输入引入微小负斜率，避免神经元死亡问题
     - 与ReLU类似，能缓解梯度消失问题并促进稀疏性
   - **缺点**：
     - 仍存在负值输出可能，但相比标准ReLU影响更小
     - \\( \alpha \\) 取值需要调优，选择存在难度
   - **适用场景**：用于规避ReLU缺陷的深度学习模型，特别在可能出现神经元死亡问题的网络中

---

### **对比总结**：

- **Sigmoid**：  
  最适用于二分类任务的输出层，但存在梯度消失问题
  
- **ReLU**：  
  隐藏层的通用激活函数，能加速训练并适用于大型网络，但可能存在神经元死亡问题

- **Leaky ReLU**：  
  通过为负输入保留微小梯度来解决ReLU缺陷的改进方案，广泛应用于深度模型

### **实践建议**：
- 当输出需限定在(0,1)范围时（如二分类最终层）常使用 **Sigmoid**
- 由于简单高效且能有效处理梯度消失，**ReLU** 通常作为隐藏层的默认选择
- 当担心训练中出现神经元死亡问题时，可用 **Leaky ReLU** 替代 ReLU

根据所构建神经网络的结构和目标，这些激活函数各有其适用场景！