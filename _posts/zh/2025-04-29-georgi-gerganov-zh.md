---
audio: false
generated: true
lang: zh
layout: post
title: 乔治·格加诺夫
translated: true
type: note
---

来自保加利亚、拥有医学物理背景的开发者Georgi Gerganov，通过其开发的**llama.cpp**——一个用于运行Meta AI的LLaMA等大型语言模型的高效C/C++库，成为推动AI民主化的关键人物。他最初对神经网络持怀疑态度，但凭借技术好奇心和优化天赋，最终在设备端AI推理领域做出了突破性贡献。

### 背景与早期工作
- **教育与成就**：Gerganov就读于索非亚大学物理学院，专攻医学物理。他早年便展露才华，曾于2006年获得国际物理奥林匹克竞赛银牌，并在2008年由保加利亚软件公司协会组织的编程比赛中获奖。
- **最初的AI怀疑论**：2022年之前，Gerganov自称是“非AI信仰者”，对神经网络的潜力持怀疑态度，更倾向于保守的技术观。
- **Whisper.cpp**：他的首个重要AI项目是**whisper.cpp**（2022年），这是OpenAI语音转文本模型Whisper的C/C++移植版。这个项目得益于时机与运气，通过对Whisper进行优化使其能在CPU上运行，让没有GPU的设备（如笔记本电脑甚至智能手机）也能使用。该项目因实现高效的音频转录和翻译而广受欢迎。

### llama.cpp的诞生
- **背景**：2023年2月，Meta AI发布了LLaMA系列高效大型语言模型（参数量70亿至650亿），但运行这些模型需要大量计算资源，通常依赖GPU。
- **挑战**：受whisper.cpp成功的启发，Gerganov开始尝试在消费级硬件（特别是MacBook）上运行LLaMA，“只是为了好玩”。2023年3月，他开发了**llama.cpp**，这是一个极简的C/C++实现的LLaMA推理代码，无外部依赖。
- **关键创新**：Gerganov利用了他的**GGML**（Georgi Gerganov模型语言）库，这是一个基于C的张量代数框架，始于2022年9月，灵感来自Fabrice Bellard的LibNC。GGML强调严格的内存管理和多线程支持，实现了高效的基于CPU的推理。
- **量化突破**：llama.cpp的核心特性是4位量化，通过压缩模型权重来减少内存使用并加速推理，同时精度损失极小（例如4位量化下困惑度仅增加4%）。这使得70亿参数的LLaMA模型能够在仅4GB内存的设备上运行，包括安卓手机和树莓派。

### 影响与发展
- **可访问性**：llama.cpp让没有专业硬件的爱好者和开发者也能使用大型语言模型。它可以在MacBook、Pixel手机甚至树莓派4上运行（尽管速度较慢，约1个词元/秒）。这引发了一波实验热潮，黑客和研究人员在各种平台上运行LLaMA。
- **社区与规模**：该项目迅速走红，在GitHub上获得了超过69,000颗星，发布了2,600多个版本，拥有900多名贡献者。其开源特性和简洁性（例如在单个C++文件中实现CUDA后端）促进了协作，包括支持AMD设备的ROCm后端和通过MPI实现的分布式推理。
- **GGUF格式**：2023年8月，Gerganov推出了**GGUF**（GGML通用文件）格式，取代了GGML。GGUF将模型权重、元数据和词元整合到单个二进制文件中，支持2位到8位量化，并确保向后兼容，进一步优化了模型存储和加载。
- **多模态支持**：到2023年10月，llama.cpp添加了对LLaVA等多模态模型的支持，将其应用范围从文本扩展到了视觉任务。

### 技术贡献
- **优化技术**：Gerganov使用SIMD向量指令（如AVX2/AVX-512）将CPU变成了矩阵运算的“迷你GPU”，提升了性能。他在Apple Silicon上的基准测试凸显了其在LLM推理方面的内存带宽优势。
- **理念转变**：Llama.cpp将AI竞争从原始模型性能转向了优化和可访问性，实现了本地推理，减少了对基于云的GPU的依赖。
- **边缘AI**：该项目符合设备端AI的愿景，例如在六个树莓派上分布式运行650亿参数LLaMA的实验，展示了低成本、去中心化AI的潜力。

### 更广泛的影响
- **ggml.ai**：Gerganov创立了**ggml.ai**，并得到了Nat Friedman和Daniel Gross的支持，以推动GGML和llama.cpp的发展。该公司聘请贡献者来推进设备端推理技术。
- **文化影响**：在X平台上被称为“欧洲阿尔法程序员”的Gerganov，以其快速的开发速度（例如一晚内完成llama.cpp的核心开发）和开源精神，使他成为AI社区的民间英雄。他的工作被比作Stable Diffusion的开放发布，标志着大型语言模型的“Stable Diffusion时刻”。
- **民主化**：通过让大型语言模型在消费设备上运行，llama.cpp降低了AI实验的门槛，促进了边缘AI的创新，并挑战了以GPU为中心的框架的主导地位。

### 核心故事
Georgi Gerganov与llama.cpp的故事展现了技术智慧和时机把握的力量。从最初的AI怀疑论者，到在LLaMA发布时抓住机遇，运用其在C/C++和张量优化方面的专业知识，让尖端的大型语言模型能够在日常硬件上运行。通过llama.cpp，他不仅解决了技术挑战，还催生了一场向可访问、本地化AI迈进的运动，证明了一位来自保加利亚的独立开发者能够重塑AI格局。他的工作仍在不断发展，llama.cpp作为开源创新力量的见证屹立不倒。