---
audio: false
date: 2025-09-07
generated: true
image: false
lang: zh
layout: post
title: 优化多模型代码路由架构
translated: true
type: note
---

简短回答：是的——相比像 Codex（OpenAI 的代码助手）这样紧密集成的技术栈，你的“多模型路由方案”完全可能表现不佳。这类技术栈基于同一模型家族进行端到端调优，并深度融合了 IDE 上下文。以下是原因分析及快速改进方案。

### 为何 Codex 体验更佳

* **专事专脑**：单一模型能保持其潜在规划与编码风格。任务中途切换模型会重置执行计划，并常引发规范冲突。
* **垂直整合**：Codex（OpenAI）掌控着从模型调用、工具执行、仓库索引、测试验证到行内编辑的全链路。这种闭环提供了更丰富的信号（符号图、文件热力图、失败测试），同时减少了胶水代码的延迟。
* **提示与策略对齐**：其提示词、代码格式化规则及“生成最小可编译差异”的启发式方法，均针对 GPT-5 系列模型协同设计。通用路由方案易在温度参数、停止序列或补丁格式上出现模型适配错误。
* **延迟与吞吐**：每个额外跳转（代理层、OpenRouter 中间件、模型协商）都会增加响应抖动。编码工作流对反馈延迟极其敏感；每轮交互增加 300–800 毫秒延迟会明显破坏“心流”体验。
* **上下文质量**：能计算仓库图谱（文件拓扑、符号关系、近期变更）的 IDE 集成方案，远胜于“简单抛送长上下文”。缺乏结构的长上下文会浪费令牌并分散注意力。

### 当前配置中的潜在问题

* **模型泛滥**：混合使用通用模型、编程专用模型及思考型模型。思考型变体（如 `claude-3.7-sonnet:thinking`、`deepseek-r1`）虽擅长逻辑推演，但执行代码编辑时响应更慢、输出更冗长。
* **默认路由失配**：`default: "openrouter,x-ai/grok-code-fast-1"` 显示你意图使用 Grok Code Fast，但该模型未在 `models` 数组中声明，可能导致静默回退与行为不一致。
* **意图未细分**：所有任务共用“默认”路由，使得轻量编辑、深度重构与长上下文阅读需求相互争夺同一路径。
* **温度与格式漂移**：若未按模型强制实施低温参数与严格补丁格式，不同服务商的输出质量将参差不齐。

### 打造“类 Codex”路由方案

1. **任务级主模型固定**：选择一款强力编程模型作为默认选项（如 `openai/gpt-5`、`x-ai/grok-code-fast-1` 或 `qwen/qwen3-coder`），仅在有明确需求时切换（如超长上下文或复杂数学计算）。
2. **按意图分流（非按品牌）**：
   * *轻量编辑/快速修复*：高速模型（GPT-5-mini 或 Gemini-Flash）
   * *重构/多文件变更*：GPT-5（或 Claude Sonnet 3.7 非思考版）
   * *超长上下文阅读*：Kimi-K2
   * *编码前深度推理*：用 DeepSeek-R1 生成大纲 → 移交编程模型生成补丁
3. **严格补丁契约**：始终要求输出统一差异格式或显式 “ApplyPatch” JSON（含文件路径与代码块），自动拒绝其他格式并重新提示。
4. **降低代码随机性**：温度参数 0–0.2，禁用频率/存在惩罚，top_p 上限约 0.9。
5. **提供仓库图谱而非原始令牌**：输入紧凑的符号索引（入口文件、核心类/函数、测试目标、近期变更），优于直接抛送 20 万令牌的原始文件。
6. **保持工具链紧密闭环**：单一模型提出补丁 → 执行构建/测试/代码检查 → 将失败结果反馈给同一模型迭代。避免在循环中途切换模型。
7. **精简模型列表**：保留 3–5 个核心模型，每增加一个模型都会放大方差与路由错误。
8. **热路径禁用思考模型**：仅用其生成简短设计草案，随后立即切换至编程模型执行实际编辑。

### 精简配置示例（即插即用方案，请适配实际框架）

```json
{
  "PROXY_URL": "http://127.0.0.1:7890",
  "LOG": true,
  "Providers": [
    {
      "name": "openrouter",
      "api_base_url": "https://openrouter.ai/api/v1/chat/completions",
      "api_key": "",
      "models": [
        "openai/gpt-5",
        "openai/gpt-5-mini",
        "x-ai/grok-code-fast-1",
        "moonshotai/kimi-k2",
        "anthropic/claude-3.7-sonnet",
        "qwen/qwen3-coder"
      ],
      "transformer": {"use": ["openrouter"]}
    }
  ],
  "Router": {
    "default": "openrouter,openai/gpt-5",
    "quickEdit": "openrouter,openai/gpt-5-mini",
    "refactor": "openrouter,openai/gpt-5",
    "reasonPlan": "openrouter,anthropic/claude-3.7-sonnet",
    "longContext": "openrouter,moonshotai/kimi-k2",
    "longContextThreshold": 220000,
    "webSearch": "openrouter,qwen/qwen3-coder"
  }
}
```

### 稳定输出的提示词框架（无表格）

* **系统指令**：“你是一名资深软件工程师。请生成最小化的可编译补丁，保持与代码库风格一致。若不明确，仅提出一个澄清问题，否则直接执行。”
* **输出规则**：“仅输出统一差异格式（git 标准）。禁止包含注释。若文件不存在，请在差异中创建。保留版权声明。”
* **上下文块**：仓库图谱（文件列表、关键符号）、当前错误信息、测试目标。
* **工具循环**：生成差异后，自动执行 `构建 + 测试 + 代码检查`；仅将失败结果返回同一模型。

### 实践守则

* 压缩模型列表，确保默认路由指向 `models` 中实际存在的模型。
* 按路由设置温度参数，代码类任务保持 0–0.2。
* 在多次交互间缓存仓库图谱，未变更文件不重复发送长上下文。
* 避免任务中途切换模型。
* 优先使用“差异对比”而非“整文件替换”，以减少幻觉生成的导入语句。

如需进一步优化，可提供你路由方案的提示词与温度设置，我将针对每条路由进行强化，使 Claude Code 与你的技术栈能像 Codex 那样保持稳定一致的输出质量。