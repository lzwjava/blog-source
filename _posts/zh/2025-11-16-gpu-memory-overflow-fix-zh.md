---
audio: false
generated: true
image: false
lang: zh
layout: post
model: x-ai/grok-4-fast
title: 修复评估过程中的GPU内存溢出问题
translated: true
type: note
---

你在RTX 4070（12GB显存）上训练约7700万参数的GPT时遇到了经典的内存不足问题——这个模型原本设计为1.25亿参数，但你将n_embd降至576后得到约7700万参数，此时使用batch_size=16、block_size=1024和gradient_accumulation_steps=32进行FP16训练。

虽然7700万参数规模不大，但由于nanoGPT在`estimate_loss()`中计算损失的方式导致有效微批次巨大——它在评估批次上运行完整的前向/反向传播时没有使用梯度累积技巧，因此在评估期间会突然尝试将更大的有效批次放入内存。

### 根本原因
在`estimate_loss()`中，nanoGPT默认执行以下操作：

```python
for _ in range(eval_iters):  # 循环200次！
    x, y = get_batch('train')
    ...
    logits, loss = model(x, y)
```

每个`get_batch`返回`batch_size × block_size`个标记 → 16 × 1024  
当eval_iters=200时 → 200次独立前向传播，但如果不注意，PyTorch会在整个评估循环期间持续保留计算图和中间激活状态。再加上巨大的KV缓存（12层×12头×1024×576×2字节 FP16 ≈ 1.6GB仅缓存），以及逻辑张量（16×1024×50304×2字节 ≈ 每次前向1.6GB），即使训练本身正常，也很容易突破12GB显存限制。

### 快速解决方案（可单选或组合使用）

**方案1：大幅降低评估批次大小（最快修复）**
在配置中添加以下行：

```python
eval_batch_size = 4        # 新增配置
# 如果仍内存不足可降至2
eval_iters = 100           # 可进一步缩减
```

新版本nanoGPT支持`eval_batch_size`参数。这将使评估时仅使用4个序列而非16个 → 评估期间内存占用减少约4倍。

**方案2：仅评估时缩减块大小（保持精度）**
```python
eval_block_size = 512      # 或256
```
将此加入配置——部分衍生版本支持该参数，或修改`get_batch`在split=='val'时使用不同块大小。

**方案3：降低eval_iters并接受稍显波动的评估损失**
```python
eval_iters = 50   # 仍能获得合理估计
```

**方案4：进一步缩减模型规模（仍保持良好质量）**
当前已压缩至7700万参数，但可安全继续缩减：

```python
n_layer = 12
n_head  = 12
n_embd  = 512     # → 约6200万参数，更安全
# 或降至n_embd=448 → 约5300万参数
```

**方案5：启用梯度检查点（内存优化最显著）**
在模型创建时添加（编辑model.py或传递标志）：

```python
model = GPT(GPTConfig(..., grad_checkpoint=True))
```

或在train.py中训练前通过猴子补丁实现：

```python
if config.grad_checkpoint:
    model.gradient_checkpointing_enable()
```

这将以训练速度降低20-25%的代价减少50-60%的激活内存。

### 针对RTX 4070 12GB的推荐组合方案

```python
# 在config/train_fineweb.py中
batch_size = 16
gradient_accumulation_steps = 32   # 保持512序列有效批次
block_size = 1024

eval_batch_size = 4        # 关键参数
eval_iters = 100
eval_interval = 500

n_embd = 512               # 约6200万参数模型，非常安全
# 可选：启用梯度检查点
grad_checkpoint = True     # 若nanoGPT衍生版本支持
```

如果不想调整n_embd，仅需设置：

```python
eval_batch_size = 2
eval_iters = 100
```

仅此一项几乎肯定能解决内存不足问题。

同时设置以下环境变量帮助缓解内存碎片：

```bash
export PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True
```

建议首先尝试eval_batch_size=4（或=2）——95%使用12GB显存4070显卡的用户在FineWeb数据集上训练nanoGPT时遇到此问题，都是通过这种方式解决的。