---
audio: false
generated: true
image: false
lang: zh
layout: post
model: x-ai/grok-4-fast
title: FineWeb NanoGPT GPUå†…å­˜æŒ‡å—
translated: true
type: note
---

ä½ åœ¨ä½¿ç”¨**æœ‰é™GPUå†…å­˜**ï¼ˆå¯èƒ½æ˜¯16-24 GBï¼Œå¦‚RTX 3090/4090æˆ–A6000ï¼‰çš„æƒ…å†µä¸‹ï¼ŒåŸºäº**FineWebæ•°æ®é›†**ï¼ˆç”¨äºè®­ç»ƒLLaMAã€Mistralç­‰çš„Common Crawlæ¸…æ´—ç‰ˆ100B tokenç‰ˆæœ¬ï¼‰è®­ç»ƒä¸€ä¸ªå°å‹GPTï¼ˆnanoGPTé£æ ¼ï¼‰ã€‚

ä½ å½“å‰çš„é…ç½®å·²ç»ç›¸å½“ä¿å®ˆä¸”é€‚åˆå•å¼ æ¶ˆè´¹çº§GPUï¼Œä½†ä»¥ä¸‹æ˜¯å…³äº**å¦‚ä½•ä¸ºFineWebä¸Šçš„nanoGPTé€‰æ‹©æœ€ä½³å‚æ•°**çš„è¯¦ç»†æŒ‡å—ï¼ŒæŒ‰é‡è¦æ€§å’Œå†…å­˜å½±å“æ’åºã€‚

### æŒ‰GPUå†…å­˜æ¨èçš„é…ç½®

| GPUæ˜¾å­˜       | æ¨¡å‹å¤§å°ï¼ˆå‚æ•°ï¼‰ | n_layer | n_head | n_embd | batch_size | grad_acc | æœ‰æ•ˆæ‰¹æ¬¡å¤§å° | block_size | å¤‡æ³¨ |
|----------------|---------------------|---------|--------|--------|------------|----------|--------------|------------|-------|
| 80 GB (A100/H100) | ~350M              | 12      | 12     | 768    | 64         | 8        | 512          | 1024       | æ¥è¿‘åŸå§‹GPT-2 mediumï¼Œè®­ç»ƒæ•ˆæœå¥½ |
| 48 GB (A6000/3090) | ~250â€“300M         | 12      | 12     | 672    | 32         | 16       | 512          | 1024       | éå¸¸å¥½çš„æŠ˜è¡·æ–¹æ¡ˆ |
| 24 GB (4090/3090) | ~160â€“200M          | 10      | 10     | 640    | 32         | 12â€“16    | 384â€“512      | 1024       | ä½ çš„æœ€ä½³é€‰æ‹© |
| 16â€“20 GB (4080, 3090 Ti) | ~125M        | 8       | 8      | 512    | 32         | 12       | 384          | 1024       | ç¨³å®š |
| <16 GB (4070 Ti, ç­‰) | ~84M           | 6       | 6      | 384    | 16â€“32      | 16â€“24    | 256â€“512      | 512â€“1024   | ä½ å½“å‰çš„é…ç½® |

ä½ å½“å‰çš„é…ç½®ï¼ˆ`6L 6H 384embd`ï¼‰æ˜¯å®‰å…¨çš„ï¼Œä½†æœ‰ç‚¹å°ã€‚ä½ å¯ä»¥ä½¿ç”¨æ›´å¤§çš„é…ç½®ã€‚

### 24 GB GPUï¼ˆRTX 4090 / 3090ï¼‰åœ¨FineWebä¸Šçš„æœ€ä½³é…ç½®
è¿™æ˜¯ç›®å‰nanoGPTç¤¾åŒºä¸­æœ€æµè¡Œçš„è®¾ç½®ï¼š

```python
out_dir = 'out-fineweb-160M'
eval_interval = 1000
eval_iters = 200
log_interval = 100
always_save_checkpoint = True

wandb_log = True
wandb_project = 'fineweb'
wandb_run_name = '160M-fineweb'

dataset = 'fineweb'
gradient_accumulation_steps = 16   # 32 * 16 = 512 æœ‰æ•ˆæ‰¹æ¬¡å¤§å°
batch_size = 32
block_size = 1024                  # é‡è¦ï¼šFineWebä½¿ç”¨1024+è¿›è¡Œè®­ç»ƒ

n_layer = 10
n_head = 10
n_embd = 640
dropout = 0.0                      # åç»­å¯ä»¥å°è¯•0.1
learning_rate = 6e-4               # å¯¹äºè¾ƒå°æ¨¡å‹ï¼Œå­¦ä¹ ç‡ç¨é«˜
max_iters = 50000                  # æ€»è®¡çº¦50â€“100B tokenæ˜¯ç†æƒ³çš„
warmup_iters = 2000
lr_decay_iters = 50000
min_lr = 6e-5
beta2 = 0.99
```

â†’ è¿™å¤§çº¦æ˜¯1.6äº¿å‚æ•°ï¼Œåœ¨4090ä¸Šè¿è¡Œèˆ’é€‚ï¼Œæ˜¾å­˜ä½¿ç”¨é‡çº¦ä¸º20â€“22 GBã€‚

### æ›´å¥½ï¼š2äº¿+æ¨¡å‹ï¼ˆå¦‚æœä½ æœ‰24 GB+æ˜¾å­˜ï¼‰
```python
n_layer = 12
n_head = 12
n_embd = 768    # â†’ ~3.5äº¿å‚æ•°ï¼ˆåŸå§‹GPT-2 mediumå¤§å°ï¼‰
batch_size = 32
gradient_accumulation_steps = 16   # æœ‰æ•ˆæ‰¹æ¬¡å¤§å°512
block_size = 1024
learning_rate = 5e-4
max_iters = 60000
```
è®¸å¤šäººåœ¨å•å¼ 4090ä¸ŠæˆåŠŸè®­ç»ƒäº†è¿™ä¸ªé…ç½®ã€‚

### FineWeb + nanoGPTçš„å…³é”®ç»éªŒæ³•åˆ™

1. **å¼ºçƒˆæ¨è block_size = 1024**  
   FineWebæ˜¯ä½¿ç”¨1024ä¸Šä¸‹æ–‡è¿›è¡Œè¿‡æ»¤å’Œè®­ç»ƒçš„ã€‚ä½¿ç”¨512å¯¹å›°æƒ‘åº¦çš„æŸå®³æ¯”ä½ æƒ³è±¡çš„è¦å¤§ã€‚

2. **æœ‰æ•ˆæ‰¹æ¬¡å¤§å° â‰ˆ 512 æ˜¯æœ€ä½³ç‚¹**  
   åŸå§‹LLaMAä½¿ç”¨çº¦4M tokenæ¯æ‰¹æ¬¡ã€‚å¯¹äºnanoGPTï¼Œ512ä¸ªåºåˆ— Ã— 1024ä¸ªtoken = çº¦0.5M token/æ‰¹æ¬¡ â†’ ç¼©æ”¾å®šå¾‹è¶³å¤Ÿæ¥è¿‘ã€‚

3. **å­¦ä¹ ç‡**  
   - ~1äº¿å‚æ•°ï¼š6eâ€“8e-4  
   - ~3.5äº¿å‚æ•°ï¼š5eâ€“6e-4  
   - ~7.7äº¿å‚æ•°ï¼š3e-4  

4. **è‡³å°‘è®­ç»ƒ500äº¿tokenï¼ˆæœ€å¥½1000äº¿+ï¼‰**  
   å¯¹äº1.6äº¿æ¨¡å‹ï¼š  
   - 50k è¿­ä»£æ¬¡æ•° Ã— 32 Ã— 16 Ã— 1024 token = çº¦830äº¿token  
   æ”¶æ•›è‰¯å¥½ã€‚

5. **ä½¿ç”¨å®˜æ–¹FineWebæ•°æ®é›†ï¼ˆè€Œéopenwebtextï¼‰**  
   ä½ çš„è„šæœ¬å·²ç»æ˜¯æ­£ç¡®çš„ï¼š  
   FineWeb >> OpenWebText åœ¨è´¨é‡ä¸Šã€‚  
   ä½¿ç”¨100Bå­é›†ï¼š`fineweb100B` æˆ–å®Œæ•´ `fineweb`ï¼ˆæ ¹æ®ä½ çš„ç£ç›˜ç©ºé—´æˆªæ–­ï¼‰ã€‚

6. **å¯é€‰ä½†æœ‰å¸®åŠ©**
```python
compile = True          # PyTorch 2.0ç¼–è¯‘ï¼Œé€Ÿåº¦æå‡çº¦20â€“30%
bias = False            # Linearå±‚æ— åç½®ï¼ˆç±»ä¼¼LLaMAï¼‰
weight_decay = 0.1
```

### ä¸‹è½½FineWebï¼ˆ100Bç‰ˆæœ¬ï¼‰çš„å¿«é€Ÿå‘½ä»¤
```bash
pip install datasets
python -c "
from datasets import load_dataset
ds = load_dataset('HuggingFaceFW/fineweb', name='sample-100BT', split='train')
ds.save_to_disk('data/fineweb100B')
"
```
ç„¶åå°†nanoGPTæŒ‡å‘è¯¥æ–‡ä»¶å¤¹ã€‚

### æ€»ç»“ï¼šå‡çº§ä½ å½“å‰çš„é…ç½®
å°†ä½ å½“å‰çš„æ¨¡å‹éƒ¨åˆ†æ›¿æ¢ä¸ºä»¥ä¸‹å†…å®¹ï¼ˆé’ˆå¯¹24 GB GPUï¼‰ï¼š

```python
n_layer = 10
n_head = 10
n_embd = 640
batch_size = 32
gradient_accumulation_steps = 16
block_size = 1024
learning_rate = 6e-4
max_iters = 50000
```

ä½ å°†ä»¥å‡ ä¹é›¶é¢å¤–æˆæœ¬è·å¾—ä¸€ä¸ªæ›´å¼ºå¤§çš„æ¨¡å‹ã€‚

ä½¿ç”¨è¿™äº›è®¾ç½®ï¼Œäººä»¬æ­£åœ¨FineWebä¸Šè·å¾—**éå¸¸ä¼˜ç§€çš„1.6äº¿â€“3.5äº¿æ¨¡å‹**â€”â€”é€šå¸¸è¶…è¿‡åŸå§‹GPT-2çš„æ€§èƒ½ã€‚

å‘Šè¯‰æˆ‘ä½ çš„ç¡®åˆ‡GPUå‹å·ï¼Œæˆ‘å¯ä»¥ä¸ºä½ æä¾›æœ€ä¼˜é…ç½®ï¼ğŸš€