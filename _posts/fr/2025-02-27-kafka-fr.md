---
audio: false
generated: true
lang: fr
layout: post
title: Utilisation d'Apache Kafka en Java
translated: true
type: note
---

L'utilisation d'Apache Kafka en Java implique la configuration d'un client Kafka dans votre application Java pour produire et consommer des messages. Kafka est une plateforme de streaming distribuée, et ses bibliothèques client Java facilitent l'intégration. Ci-dessous, je vais vous guider à travers les étapes de base pour commencer.

Tout d'abord, vous devez configurer votre environnement. Assurez-vous d'avoir Kafka installé et en cours d'exécution sur votre système ou sur un serveur. Vous pouvez le télécharger depuis le site officiel d'Apache Kafka et démarrer le serveur ZooKeeper et Kafka en utilisant les scripts fournis. Par souci de simplicité, je suppose que vous exécutez Kafka localement avec les paramètres par défaut (par exemple, `localhost:9092` comme serveur bootstrap).

Ensuite, ajoutez la dépendance client Kafka à votre projet Java. Si vous utilisez Maven, incluez ceci dans votre `pom.xml` :

```xml
<dependency>
    <groupId>org.apache.kafka</groupId>
    <artifactId>kafka-clients</artifactId>
    <version>3.6.0</version> <!-- Utilisez la dernière version -->
</dependency>
```

Maintenant, écrivons un peu de code. Je vais vous montrer comment créer un producteur et un consommateur simples.

### Exemple de Producteur Kafka
Le producteur envoie des messages vers un topic Kafka. Voici un exemple basique :

```java
import org.apache.kafka.clients.producer.*;
import java.util.Properties;

public class SimpleProducer {
    public static void main(String[] args) {
        // Configurer les propriétés du producteur
        Properties props = new Properties();
        props.put("bootstrap.servers", "localhost:9092"); // Adresse du serveur Kafka
        props.put("key.serializer", "org.apache.kafka.common.serialization.StringSerializer");
        props.put("value.serializer", "org.apache.kafka.common.serialization.StringSerializer");

        // Créer une instance du producteur
        try (Producer<String, String> producer = new KafkaProducer<>(props)) {
            // Envoyer un message vers un topic appelé "test-topic"
            String topic = "test-topic";
            for (int i = 0; i < 10; i++) {
                String key = "key" + i;
                String value = "Hello, Kafka " + i;
                ProducerRecord<String, String> record = new ProducerRecord<>(topic, key, value);

                producer.send(record, (metadata, exception) -> {
                    if (exception == null) {
                        System.out.println("Sent message: " + value + " to partition " + metadata.partition());
                    } else {
                        exception.printStackTrace();
                    }
                });
            }
        }
    }
}
```

Dans ce code :
- `bootstrap.servers` spécifie l'emplacement d'exécution de Kafka.
- Les sérialiseurs définissent comment les clés et les valeurs (toutes deux des chaînes de caractères ici) sont converties en octets.
- `ProducerRecord` représente le message, et `send()` l'envoie de manière asynchrone avec un callback pour gérer le succès ou l'échec.

### Exemple de Consommateur Kafka
Le consommateur s'abonne à un topic et lit les messages. Voici un exemple :

```java
import org.apache.kafka.clients.consumer.*;
import java.util.Collections;
import java.util.Properties;

public class SimpleConsumer {
    public static void main(String[] args) {
        // Configurer les propriétés du consommateur
        Properties props = new Properties();
        props.put("bootstrap.servers", "localhost:9092");
        props.put("group.id", "test-group"); // ID du groupe de consommateurs
        props.put("key.deserializer", "org.apache.kafka.common.serialization.StringDeserializer");
        props.put("value.deserializer", "org.apache.kafka.common.serialization.StringDeserializer");
        props.put("auto.offset.reset", "earliest"); // Commencer depuis le début du topic

        // Créer une instance du consommateur
        try (KafkaConsumer<String, String> consumer = new KafkaConsumer<>(props)) {
            // S'abonner au topic
            consumer.subscribe(Collections.singletonList("test-topic"));

            // Poll pour récupérer les messages
            while (true) {
                ConsumerRecords<String, String> records = consumer.poll(1000); // Timeout en ms
                for (ConsumerRecord<String, String> record : records) {
                    System.out.printf("Received: key=%s, value=%s, partition=%d, offset=%d%n",
                            record.key(), record.value(), record.partition(), record.offset());
                }
            }
        }
    }
}
```

Ici :
- `group.id` identifie le groupe de consommateurs pour la coordination.
- Les désérialiseurs convertissent les données octets en chaînes de caractères.
- `poll()` récupère les messages dans une boucle, et `auto.offset.reset` garantit que vous commencez depuis le message le plus ancien si aucun offset n'est validé.

### Exécution du Code
1. **Démarrez Kafka** : Assurez-vous que ZooKeeper et Kafka sont en cours d'exécution (par exemple, `bin/zookeeper-server-start.sh config/zookeeper.properties` et `bin/kafka-server-start.sh config/server.properties` sur les systèmes de type Unix).
2. **Créez un Topic** : Utilisez l'outil en ligne de commande Kafka pour créer `test-topic` s'il n'existe pas : `bin/kafka-topics.sh --create --topic test-topic --bootstrap-server localhost:9092 --partitions 1 --replication-factor 1`.
3. **Exécutez le Producteur** : Exécutez la classe `SimpleProducer` pour envoyer des messages.
4. **Exécutez le Consommateur** : Exécutez la classe `SimpleConsumer` pour voir les messages consommés.

### Conseils
- **Gestion des Erreurs** : Ajoutez des blocs try-catch ou de la journalisation pour le code de production.
- **Configuration** : Ajustez les propriétés comme `acks` (pour la fiabilité du producteur) ou `max.poll.records` (pour la taille des lots du consommateur) en fonction de vos besoins.
- **Bibliothèques** : Pour des applications plus complexes, envisagez d'utiliser Spring Kafka, qui simplifie la configuration et l'utilisation.

Il s'agit d'une configuration de base pour vous lancer. Faites-moi savoir si vous souhaitez approfondir des fonctionnalités spécifiques comme le partitionnement, la gestion des erreurs ou l'intégration avec une application plus large !