---
audio: false
generated: true
image: false
lang: fr
layout: post
model: x-ai/grok-4-fast
title: Liste de lectures sur l'IA d'Ilya Sutskever
translated: true
type: note
---

Ilya Sutskever, co-fondateur d'OpenAI, a partagé une liste de lecture recommandée d'environ 30 articles clés, billets de blog, cours et ressources sur l'apprentissage profond et l'IA avec John Carmack vers 2020. Il aurait déclaré que les maîtriser couvrirait "90% de ce qui est important aujourd'hui" dans le domaine. Bien que la liste originale complète de 30 éléments ne soit pas intégralement publique (certains éléments ont peut-être été perdus en raison de suppressions d'e-mails), une version largement diffusée comprend 27 éléments fondamentaux couvrant les réseaux neuronaux convolutifs, les réseaux neuronaux récurrents, les transformers, la théorie de l'information, et plus encore. Voici cette liste organisée, regroupée par catégorie pour plus de clarté. Chaque entrée comprend le titre, les auteurs, l'année et le type.

### Réseaux Neuronaux Convolutifs
1. **CS231n: Convolutional Neural Networks for Visual Recognition** - Fei-Fei Li, Andrej Karpathy, Justin Johnson - 2017 - Cours Stanford
2. **ImageNet Classification with Deep Convolutional Neural Networks (AlexNet)** - Alex Krizhevsky, Ilya Sutskever, Geoffrey E. Hinton - 2012 - Article
3. **Deep Residual Learning for Image Recognition (ResNet)** - Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun - 2015 - Article
4. **Identity Mappings in Deep Residual Networks** - Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun - 2016 - Article
5. **Multi-Scale Context Aggregation by Dilated Convolutions** - Fisher Yu, Vladlen Koltun - 2015 - Article

### Réseaux Neuronaux Récurrents
6. **Understanding LSTM Networks** - Christopher Olah - 2015 - Billet de blog
7. **The Unreasonable Effectiveness of Recurrent Neural Networks** - Andrej Karpathy - 2015 - Billet de blog
8. **Recurrent Neural Network Regularization** - Wojciech Zaremba, Ilya Sutskever, Oriol Vinyals - 2014 - Article
9. **Neural Turing Machines** - Alex Graves, Greg Wayne, Ivo Danihelka - 2014 - Article
10. **Deep Speech 2: End-to-End Speech Recognition in English and Mandarin** - Dario Amodei et al. - 2016 - Article
11. **Neural Machine Translation by Jointly Learning to Align and Translate (RNNsearch)** - Dzmitry Bahdanau, Kyunghyun Cho, Yoshua Bengio - 2015 - Article
12. **Pointer Networks** - Oriol Vinyals, Meire Fortunato, Navdeep Jaitly - 2015 - Article
13. **Order Matters: Sequence to Sequence for Sets (Set2Set)** - Oriol Vinyals, Samy Bengio, Manjunath Kudlur - 2016 - Article
14. **A Simple Neural Network Module for Relational Reasoning (Relation Networks)** - Adam Santoro, David Raposo, David G. Barrett, Mateusz Malinowski, Razvan Pascanu, Peter Battaglia, Timothy Lillicrap - 2017 - Article
15. **Relational Recurrent Neural Networks** - Adam Santoro, Ryan Faulkner, David Raposo, Jack Rae, Mike Chrzanowski, Theophane Weber, Daan Wierstra, Oriol Vinyals, Razvan Pascanu, Timothy Lillicrap - 2018 - Article

### Transformers
16. **Attention Is All You Need** - Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin - 2017 - Article
17. **The Annotated Transformer** - Sasha Rush et al. - 2017 (annoté 2020) - Billet de blog
18. **Scaling Laws for Neural Language Models** - Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B. Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, Dario Amodei - 2020 - Article

### Théorie de l'Information
19. **A Tutorial Introduction to the Minimum Description Length Principle** - Peter Grünwald - 2004 - Chapitre de livre
20. **Kolmogorov Complexity and Algorithmic Randomness (Chapter 14)** - Alexander Shen, Vladimir A. Uspensky, Nikolay Vereshchagin - 2017 - Chapitre de livre
21. **The First Law of Complexodynamics** - Scott Aaronson - 2011 - Billet de blog
22. **Quantifying the Rise and Fall of Complexity in Closed Systems: The Coffee Automaton** - Scott Aaronson, Sean M. Carroll, Lauren Ouellette - 2014 - Article
23. **Machine Super Intelligence** - Shane Legg - 2008 - Thèse

### Divers
24. **Keeping Neural Networks Simple by Minimizing the Description Length of the Weights** - Geoffrey E. Hinton, Drew van Camp - 1993 - Article
25. **Variational Lossy Autoencoder** - Xi Chen, Diederik P. Kingma, Tim Salimans, Yan Duan, Prafulla Dhariwal, John Schulman, Ilya Sutskever, Pieter Abbeel - 2017 - Article
26. **GPipe: Easy Scaling with Micro-Batch Pipeline Parallelism** - Yanping Huang, Youlong Cheng, Ankur Bapna, Orhan Firat, Mia Xu Chen, Dehao Chen, HyoukJoong Lee, Jiquan Ngiam, Quoc V. Le, Yonghui Wu, Zhifeng Chen - 2018 - Article
27. **Neural Message Passing for Quantum Chemistry** - Justin Gilmer, Samuel S. Schoenholz, Patrick F. Riley, Oriol Vinyals, George E. Dahl - 2017 - Article

Les spéculations sur les 3 éléments "manquants" pointent souvent vers des articles sur le méta-apprentissage (par exemple, Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks par Chelsea Finn et al., 2017) ou des travaux sur l'apprentissage par renforcement par auto-joueur, basées sur les présentations de Sutskever de l'époque, mais cela n'est pas confirmé.

[Liste de lecture sur l'apprentissage profond d'Ilya Sutskever](https://github.com/dzyim/ilya-sutskever-recommended-reading)
[Résumé de la liste de lecture en IA d'Ilya Sutskever](https://tensorlabbet.com/2024/09/24/ai-reading-list/)
[Les éléments de lecture perdus de la liste de lecture en IA d'Ilya Sutskever](https://tensorlabbet.com/2024/11/11/lost-reading-items/)