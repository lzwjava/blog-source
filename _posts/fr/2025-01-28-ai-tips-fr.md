---
audio: false
lang: fr
layout: post
title: AI Tools Tips
translated: true
---

* Copiez l'exemple de code dans Cursor, et utilisez des commentaires pour fournir du contexte.

* Dans la région de Hong Kong, il est acceptable d'utiliser les services DeepSeek ou Mistral, tandis qu'il n'est pas acceptable d'utiliser ChatGPT, Claude ou Gemini.

* La politique API d'une région est approximativement la même que sa politique d'application.

* Utilisez Cursor au lieu de Visual Studio Code.

* Il y a encore des cas où vous devez utiliser Visual Studio Code, par exemple pour les scénarios d'éditeur de fusion git, où j'utilise toujours `git config --global core.editor "code --wait"`.

* À partir du jour de la sortie de Deepseek V3, nous n'avons plus besoin de souscrire à des outils d'IA.

* Utilisez Gemini ou Grok pour générer des images de célébration de festival avec des invites comme "Générer une image joyeuse du Nouvel An du Serpent Lunaire avec les noms de texte inclus".

* Dans certains cas, même en fournissant un texte original aux modèles d'IA pour créer un tableau, quelques endroits dans la sortie peuvent différer de l'entrée. Par exemple, lors de l'utilisation du modèle Deepseek V3 dans Cursor pour générer un tableau de liste pip, il peut inclure des versions comme `1.极狐0`. Ici, `极狐` fait référence à la plateforme GitLab chinoise.

* Lorsque vous utilisez l'API DeepSeek ou Mistral pour traduire des titres avec des invites comme `Vous êtes un traducteur professionnel. Vous traduisez un fichier markdown pour un article de blog Jekyll de l'anglais au chinois. {texte}`, cela peut conduire à des traductions incorrectes. Outre le texte que vous fournissez, la sortie comprend souvent une traduction excessive.

* Bien que parfois les modèles d'IA dans Cursor donnent un texte partiellement correct, nous pouvons les accepter, car nous pouvons ajouter des instructions de suivi qui feront régénérer les parties correctes par les modèles d'IA.

* Évitez de fournir un contexte excessif aux grands modèles linguistiques s'il est peu probable qu'il soit utile. Par exemple, lors de la génération de lignes de dialogue conversationnelles, évitez de fournir 100 points sur un sujet. Les grands modèles linguistiques contiennent déjà d'énormes quantités de données.

* Lorsque vous fournissez un contexte suffisant pour des tâches telles que la traduction ou la génération de paroles de dialogue, évitez d'utiliser les fonctionnalités de chaîne de pensée, car cela peut être lent et conduire à des réponses verbeuses ou inutiles.

* Une façon de tester si un chatbot peut suivre les instructions d'un utilisateur est de lui demander d'expliquer quelque chose en anglais, puis de continuer la saisie en chinois, en observant si le chatbot maintient sa sortie en anglais.

* Une raison pour laquelle nous n'utilisons pas Postman ou Figma à l'ère de l'IA est que leurs fonctionnalités ne peuvent pas être générées par le texte. Ils manquent également d'un raccourci commande + K pour déclencher le remplacement de composants.

* Les interfaces utilisateur deviennent un obstacle à l'ère de l'IA. Pourquoi mettre à niveau Postman pour qu'il soit alimenté par l'IA pour tester les applications alors que nous pouvons directement utiliser la bibliothèque requests de Python ou d'autres langages de programmation pour tester le code, car ce dernier sera alimenté par l'IA ?

* Pourquoi mettre à niveau Figma pour qu'il soit alimenté par l'IA pour la création d'interfaces utilisateur alors que la génération d'interfaces utilisateur basée sur le code, améliorée par l'IA, offre une approche plus directe et potentiellement plus puissante ?

* Les LLM changeront d'abord les applications liées au texte.


* [Messages de validation Git alimentés par l'IA](./gitmessageai-en)

* [Génération audio de conversation](./conversation-style-en)

* [Transcription Google Cloud](./speech-to-text-en)

* [Essayer llama.cpp](./llama-cpp-en)

* [Un cas d'utilisation de la recherche dans ChatGPT](./ai-search-en)

* [Deepseek, Mistral et Anthropic](./dma-en)
