---
audio: false
lang: fr
layout: post
title: PyTorch
translated: true
type: note
---

### Fonctions torch

*   `torch.Tensor` : Un tableau multidimensionnel contenant des éléments d'un seul type de données.
*   `torch.tensor` : Construit un tensor avec des données et des propriétés.
*   `torch.zeros` : Retourne un tensor rempli de zéros.
*   `torch.ones` : Retourne un tensor rempli de uns.
*   `torch.arange` : Retourne un tensor 1-D de valeurs régulièrement espacées.
*   `torch.linspace` : Retourne un tensor 1-D de valeurs régulièrement espacées sur un intervalle spécifié.
*   `torch.rand` : Retourne un tensor rempli de nombres aléatoires d'une distribution uniforme sur l'intervalle [0, 1).
*   `torch.randn` : Retourne un tensor rempli de nombres aléatoires d'une distribution normale de moyenne 0 et de variance 1.
*   `torch.empty` : Retourne un tensor avec des données non initialisées.
*   `torch.full` : Crée un tensor d'une taille spécifiée rempli d'une valeur spécifiée.
*   `torch.eye` : Retourne un tensor 2-D avec des uns sur la diagonale et des zéros ailleurs.

### Opérations sur les tenseurs

*   `torch.add` : Additionne deux tenseurs élément par élément.
*   `torch.sub` : Soustrait deux tenseurs élément par élément.
*   `torch.mul` : Multiplie deux tenseurs élément par élément.
*   `torch.div` : Divise deux tenseurs élément par élément.
*   `torch.matmul` : Effectue une multiplication matricielle.
*   `torch.pow` : Élève chaque élément d'un tensor à une puissance.
*   `torch.exp` : Calcule l'exponentielle de chaque élément d'un tensor.
*   `torch.log` : Calcule le logarithme naturel de chaque élément d'un tensor.
*   `torch.sqrt` : Calcule la racine carrée de chaque élément d'un tensor.
*   `torch.abs` : Calcule la valeur absolue de chaque élément d'un tensor.
*   `torch.neg` : Inverse le signe de chaque élément d'un tensor.
*   `torch.round` : Arrondit chaque élément d'un tensor à l'entier le plus proche.
*   `torch.floor` : Retourne la partie entière par défaut de chaque élément d'un tensor.
*   `torch.ceil` : Retourne la partie entière par excès de chaque élément d'un tensor.
*   `torch.clamp` : Limite tous les éléments de l'entrée à l'intervalle [min, max].
*   `torch.sum` : Retourne la somme de tous les éléments du tensor d'entrée.
*   `torch.mean` : Retourne la moyenne de tous les éléments du tensor d'entrée.
*   `torch.std` : Retourne l'écart type de tous les éléments du tensor d'entrée.
*   `torch.var` : Retourne la variance de tous les éléments du tensor d'entrée.
*   `torch.max` : Retourne la valeur maximale de tous les éléments du tensor d'entrée.
*   `torch.min` : Retourne la valeur minimale de tous les éléments du tensor d'entrée.
*   `torch.argmax` : Retourne l'indice de la valeur maximale de tous les éléments du tensor d'entrée.
*   `torch.argmin` : Retourne l'indice de la valeur minimale de tous les éléments du tensor d'entrée.
*   `torch.sort` : Trie les éléments du tensor d'entrée le long d'une dimension donnée.
*   `torch.topk` : Retourne les k plus grands éléments du tensor d'entrée le long d'une dimension donnée.
*   `torch.reshape` : Retourne un tensor avec les mêmes données et le même nombre d'éléments que l'entrée, mais avec la forme spécifiée.
*   `torch.transpose` : Retourne une vue du tensor d'entrée avec ses dimensions permutées.
*   `torch.squeeze` : Retourne un tensor avec toutes les dimensions de taille 1 supprimées.
*   `torch.unsqueeze` : Retourne un nouveau tensor avec une dimension de taille un insérée à la position spécifiée.
*   `torch.cat` : Concatène les tenseurs donnés dans la dimension donnée.
*   `torch.stack` : Concatène une séquence de tenseurs le long d'une nouvelle dimension.
*   `torch.chunk` : Divise un tensor en un nombre spécifique de morceaux.
*   `torch.split` : Divise un tensor en morceaux d'une taille spécifique.

### Modules de réseaux neuronaux

*   `torch.nn.Module` : Classe de base pour tous les modules de réseaux neuronaux.
*   `torch.nn.Linear` : Applique une transformation linéaire aux données entrantes.
*   `torch.nn.Conv2d` : Applique une convolution 2D sur un signal d'entrée composé de plusieurs plans d'entrée.
*   `torch.nn.MaxPool2d` : Applique un pooling max 2D sur un signal d'entrée.
*   `torch.nn.ReLU` : Applique la fonction ReLU (Unité Linéaire Rectifiée) élément par élément.
*   `torch.nn.Sigmoid` : Applique la fonction sigmoïde élément par élément.
*   `torch.nn.Tanh` : Applique la fonction tangente hyperbolique élément par élément.
*   `torch.nn.BatchNorm2d` : Applique la normalisation par lots sur une entrée 4D.
*   `torch.nn.Dropout` : Pendant l'entraînement, met aléatoirement à zéro certains éléments du tensor d'entrée avec une probabilité p.
*   `torch.nn.Embedding` : Une simple table de consultation qui stocke les embeddings d'un dictionnaire et d'une taille fixes.

### Fonctions de perte

*   `torch.nn.MSELoss` : Crée un critère qui mesure l'erreur quadratique moyenne (norme L2 au carré) entre chaque élément de l'entrée et de la cible.
*   `torch.nn.CrossEntropyLoss` : Ce critère calcule la perte d'entropie croisée entre l'entrée et la cible.
*   `torch.nn.BCELoss` : Crée un critère qui mesure l'entropie croisée binaire entre la cible et la sortie.
*   `torch.nn.L1Loss` : Crée un critère qui mesure l'erreur absolue moyenne (MAE) entre chaque élément de l'entrée et de la cible.

### Optimiseurs

*   `torch.optim.SGD` : Implémente la descente de gradient stochastique (éventuellement avec momentum).
*   `torch.optim.Adam` : Implémente l'algorithme Adam.
*   `torch.optim.RMSprop` : Implémente l'algorithme RMSprop.

### Autograd

*   `torch.autograd.grad` : Calcule et retourne la somme des gradients des sorties par rapport aux entrées.

### Utilitaires

*   `torch.device` : Représente le device sur lequel un torch.Tensor est ou sera alloué.
*   `torch.cuda.is_available` : Retourne True si CUDA est disponible.
*   `torch.save` : Sauvegarde un tensor sur le disque.
*   `torch.load` : Charge un tensor depuis le disque.

### Autres

*   `torch.no_grad` : Gestionnaire de contexte qui désactive le calcul du gradient.
*   `torch.set_grad_enabled` : Active ou désactive le gradient, selon son argument.