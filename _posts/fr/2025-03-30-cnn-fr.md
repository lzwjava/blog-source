---
audio: false
generated: true
lang: fr
layout: post
title: Réseau de Neurones Convolutif
translated: true
type: note
---

### Qu'est-ce qu'un CNN (Réseau de Neurones Convolutif) ?

Un **Réseau de Neurones Convolutif (CNN)** est un type de réseau de neurones profond principalement utilisé pour des tâches liées aux images telles que la classification, la détection d'objets et la segmentation. Les CNN sont conçus pour apprendre automatiquement et de manière adaptative des hiérarchies spatiales de caractéristiques à partir d'images d'entrée. Ils sont très efficaces pour capturer les dépendances locales et réduire le nombre de paramètres par rapport aux couches entièrement connectées traditionnelles dans les réseaux de neurones.

### Composants Clés d'un CNN :
1. **Couche Convolutive** :  
   Cette couche applique des opérations de convolution aux données d'entrée, ce qui aide à extraire des caractéristiques de l'image d'entrée (par exemple, les bords, les textures, les motifs). L'opération de convolution utilise des filtres (également appelés noyaux) qui glissent sur l'image d'entrée.

2. **Couche de Pooling** :  
   Les couches de pooling sont utilisées pour sous-échantillonner les cartes de caractéristiques, réduisant leurs dimensions spatiales et rendant le réseau plus efficace sur le plan computationnel, tout en aidant à l'invariance translationnelle (la capacité à reconnaître des objets même s'ils sont déplacés dans l'image).

3. **Couche Entièrement Connectée** :  
   Après les couches de convolution et de pooling, des couches entièrement connectées sont utilisées pour classer les caractéristiques extraites des couches précédentes. La couche de sortie finale utilise généralement une fonction d'activation softmax ou sigmoïde pour les tâches de classification.

4. **Fonction d'Activation (ReLU)** :  
   Après chaque couche convolutive ou entièrement connectée, une fonction d'activation comme **ReLU** (Unité Linéaire Rectifiée) est souvent utilisée pour introduire de la non-linéarité dans le modèle, lui permettant d'apprendre des motifs plus complexes.

### Exemple d'Architecture d'un CNN :
- **Couche d'Entrée** : Une image ou un lot d'images.
- **Couche Convolutive 1** : Appliquer un ensemble de filtres convolutifs (noyaux).
- **Activation ReLU** : Appliquer ReLU pour introduire de la non-linéarité.
- **Couche de Pooling 1** : Max pooling ou average pooling.
- **Couche Convolutive 2** : Appliquer des convolutions supplémentaires.
- **Couche Entièrement Connectée** : Aplatir la sortie et la transmettre à des couches entièrement connectées pour la classification.
- **Couche de Sortie** : Activation softmax ou sigmoïde pour le résultat de classification final.

---

### Implémentation d'un CNN à partir de zéro (sans utiliser de frameworks comme TensorFlow/PyTorch)

Voici une implémentation simple d'un CNN en utilisant **NumPy**. Cela vous donnera une idée de base du fonctionnement des opérations (convolution, ReLU, pooling, etc.) dans un CNN.

Nous allons implémenter un CNN basique avec :
1. Une couche de convolution
2. Une couche d'activation ReLU
3. Une couche de pooling
4. Une couche entièrement connectée

Nous nous concentrerons sur une version très simplifiée d'un CNN, sans fonctionnalités avancées comme la normalisation par lots, le dropout, etc.

### Étape 1 : Couche de Convolution

Nous allons implémenter l'opération de **convolution**, qui consiste à faire glisser un filtre (noyau) sur l'image d'entrée.

```python
import numpy as np

def convolve2d(input_image, kernel):
    kernel_height, kernel_width = kernel.shape
    image_height, image_width = input_image.shape
    
    # Dimensions de sortie après convolution
    output_height = image_height - kernel_height + 1
    output_width = image_width - kernel_width + 1
    
    output = np.zeros((output_height, output_width))
    
    # Faire glisser le noyau sur l'image d'entrée
    for i in range(output_height):
        for j in range(output_width):
            region = input_image[i:i+kernel_height, j:j+kernel_width]
            output[i, j] = np.sum(region * kernel)  # Multiplication et somme élément par élément
    return output
```

### Étape 2 : Activation ReLU

ReLU est appliqué élément par élément sur la sortie de la convolution.

```python
def relu(input_image):
    return np.maximum(0, input_image)  # Opération ReLU
```

### Étape 3 : Couche de Pooling (Max Pooling)

Nous allons implémenter une simple couche de **max pooling** avec une fenêtre 2x2 et un pas (stride) de 2.

```python
def max_pooling(input_image, pool_size=2, stride=2):
    image_height, image_width = input_image.shape
    output_height = (image_height - pool_size) // stride + 1
    output_width = (image_width - pool_size) // stride + 1
    
    output = np.zeros((output_height, output_width))
    
    # Appliquer le max pooling
    for i in range(0, image_height - pool_size + 1, stride):
        for j in range(0, image_width - pool_size + 1, stride):
            region = input_image[i:i+pool_size, j:j+pool_size]
            output[i // stride, j // stride] = np.max(region)
    
    return output
```

### Étape 4 : Couche Entièrement Connectée

La couche entièrement connectée est simplement une couche dense qui prend la sortie des couches précédentes et calcule une somme pondérée.

```python
def fully_connected(input_image, weights, bias):
    # Aplatir l'image d'entrée (si elle est multidimensionnelle)
    flattened_input = input_image.flatten()
    
    # Calculer la sortie de la couche entièrement connectée
    output = np.dot(flattened_input, weights) + bias
    return output
```

### Étape 5 : Assemblage du Tout

Maintenant, définissons un exemple simple où nous allons créer un CNN qui prend une image, applique une convolution, ReLU, un pooling, puis une couche entièrement connectée pour faire une prédiction.

```python
# Exemple d'image (5x5)
image = np.array([[1, 2, 3, 0, 1],
                  [4, 5, 6, 1, 2],
                  [7, 8, 9, 2, 3],
                  [1, 2, 3, 4, 5],
                  [6, 7, 8, 5, 6]])

# Définir un noyau simple (3x3)
kernel = np.array([[1, 0, -1],
                   [1, 0, -1],
                   [1, 0, -1]])

# Opération de convolution
conv_output = convolve2d(image, kernel)
print("Sortie de la Convolution :")
print(conv_output)

# Appliquer l'activation ReLU
relu_output = relu(conv_output)
print("Sortie de ReLU :")
print(relu_output)

# Appliquer le Max Pooling
pool_output = max_pooling(relu_output)
print("Sortie du Max Pooling :")
print(pool_output)

# Couche entièrement connectée (sortie aplatie, poids 1D et biais)
weights = np.random.randn(pool_output.size)  # Poids aléatoires
bias = np.random.randn()  # Biais aléatoire
fc_output = fully_connected(pool_output, weights, bias)
print("Sortie de la Couche Entièrement Connectée :")
print(fc_output)
```

### Explication du Processus :

1. **Image d'Entrée** : Nous créons une image simple 5x5.
2. **Convolution** : Nous appliquons un filtre 3x3 à l'image, ce qui donne une sortie plus petite.
3. **ReLU** : Nous appliquons la fonction ReLU pour introduire de la non-linéarité.
4. **Pooling** : Nous sous-échantillonnons l'image en utilisant le max pooling avec une fenêtre 2x2 et un pas de 2.
5. **Entièrement Connectée** : Nous aplations la sortie du pooling et appliquons une couche entièrement connectée avec des poids et un biais aléatoires.

Il s'agit d'un CNN très simplifié, mais il devrait vous donner une idée du fonctionnement des couches de convolution, ReLU, pooling et entièrement connectées.

### Conclusion

Cette implémentation fournit un cadre de base pour comprendre le fonctionnement d'un CNN à un niveau bas. En pratique, des frameworks comme **TensorFlow**, **Keras** et **PyTorch** gèrent de nombreuses optimisations, améliorations d'efficacité et fonctionnalités supplémentaires (par exemple, la normalisation par lots, le dropout) qui rendent les CNN plus puissants et plus faciles à entraîner sur de grands ensembles de données.

Faites-moi savoir si vous souhaitez plus de détails sur la façon d'étendre cela ou si vous avez besoin d'aide pour une autre partie des CNN !