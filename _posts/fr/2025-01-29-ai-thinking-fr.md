---
audio: true
lang: fr
layout: post
title: Pensée IA
translated: true
---

- Satya Nadella a mentionné le paradoxe de Jevons. Cela vaut la peine d'apprendre.

- Yin Wang : Il n'y a pas d'«intelligence» dans l'intelligence artificielle, pas de «neuronal» dans réseau neuronal, pas d'«apprentissage» dans apprentissage automatique, et pas de «profondeur» dans apprentissage profond. Ce qui fonctionne vraiment dans ce domaine est appelé «calcul». Je préfère donc appeler ce domaine «calcul différentiable», et le processus de construction de modèles est appelé «programmation différentiable».

- Yin Wang : L'apprentissage automatique est vraiment utile, voire une théorie belle, car c'est simplement du calcul après une refonte! C'est la vieille et grande théorie de Newton, Leibniz, sous une forme plus simple, élégante et puissante. L'apprentissage automatique consiste essentiellement à utiliser le calcul pour dériver et ajuster certaines fonctions, et l'apprentissage profond consiste à ajuster des fonctions plus complexes.

- Actuellement, les grands modèles de langage ne peuvent pas filtrer par langage de fichier comme YAML ou Python. Cependant, une partie significative de l'information dans le monde réel est organisée de cette manière. Cela signifie que nous pourrions entraîner des grands modèles de langage en utilisant des fichiers.

- Pour l'entraînement des grands modèles de langage, nous pourrions développer un système qui trouve des correspondances exactes. Peut-être pourrions-nous combiner l'algorithme de recherche KMP (Knuth-Morris-Pratt) avec l'architecture transformer pour améliorer les capacités de recherche.