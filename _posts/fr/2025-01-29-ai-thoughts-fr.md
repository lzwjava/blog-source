---
audio: false
lang: fr
layout: post
title: Pensées IA
translated: true
---

- Satya Nadella a mentionné le paradoxe de Jevons. Cela vaut la peine d'être appris.

- Yin Wang : Il n'y a pas d'"intelligence" dans l'intelligence artificielle, pas de "neuronal" dans réseau neuronal, pas d'"apprentissage" dans apprentissage automatique, et pas de "profondeur" dans apprentissage profond. Ce qui fonctionne vraiment dans ce domaine s'appelle "calcul". Je préfère donc appeler ce domaine "calcul différentiel", et le processus de construction de modèles "programmation différentielle".

- Yin Wang : L'apprentissage automatique est vraiment utile, on pourrait même dire que c'est une théorie belle, parce que c'est simplement du calcul après un lifting ! C'est la vieille et grande théorie de Newton, Leibniz, sous une forme plus simple, élégante et puissante. L'apprentissage automatique consiste essentiellement à utiliser le calcul pour dériver et ajuster certaines fonctions, et l'apprentissage profond consiste à ajuster des fonctions plus complexes.

- Actuellement, les grands modèles de langage ne peuvent pas filtrer par langage de fichier comme YAML ou Python. Cependant, une part significative de l'information dans le monde réel est organisée de cette manière. Cela signifie que nous pourrions former de grands modèles de langage en utilisant des fichiers.

- Pour former de grands modèles de langage, nous pourrions développer un système qui trouve des correspondances exactes. Peut-être pouvons-nous combiner l'algorithme de recherche KMP (Knuth-Morris-Pratt) avec l'architecture transformateur pour améliorer les capacités de recherche.

- Il n'y a pas de secrets technologiques. Le logiciel open source révélera tous les secrets soigneusement gardés.

- [Nouvelles Plateformes Alimentées par des Flux de Travail IA](./ai-workflow-en)

- [La Prochaine Direction des Éditeurs de Code IA](./ai-code-en)

- [Comment Je Vis Bien à l'Ère de l'IA et de la Blockchain](./ai-blockchain-en)