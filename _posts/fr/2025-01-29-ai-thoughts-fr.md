---
audio: true
lang: fr
layout: post
title: Pensées IA
translated: true
---

- Satya Nadella a mentionné le paradoxe de Jevons. Cela vaut la peine d'apprendre.

- Yin Wang : Il n'y a pas d'"intelligence" dans l'intelligence artificielle, de "neuronal" dans les réseaux neuronaux, de "learn" dans l'apprentissage automatique, et de "profondeur" dans l'apprentissage profond. Ce qui fonctionne vraiment dans ce domaine s'appelle "calcul". Je préfère donc appeler ce domaine "calcul différentiel", et le processus de construction des modèles "programmation différentielle".

- Yin Wang : L'apprentissage automatique est vraiment utile, on pourrait même dire qu'il s'agit d'une théorie belle, parce que c'est simplement du calcul après une refonte ! C'est la vieille et grande théorie de Newton, Leibniz, dans une forme plus simple, élégante et puissante. L'apprentissage automatique consiste essentiellement à utiliser le calcul pour dériver et ajuster certaines fonctions, et l'apprentissage profond consiste à ajuster des fonctions plus complexes.

- Actuellement, les grands modèles de langage ne peuvent pas filtrer par langage de fichier comme YAML ou Python. Cependant, une grande partie de l'information dans le monde réel est organisée de cette manière. Cela signifie que nous pourrions former des grands modèles de langage en utilisant des fichiers.

- Pour former des grands modèles de langage, nous pourrions développer un système qui trouve des correspondances exactes. Peut-être pourrions-nous combiner l'algorithme de recherche KMP (Knuth-Morris-Pratt) avec l'architecture Transformer pour améliorer les capacités de recherche.

- Il n'y a pas de secrets technologiques. Le code source ouvert révélera tous les secrets soigneusement gardés.