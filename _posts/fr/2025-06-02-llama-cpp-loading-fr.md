---
audio: false
generated: true
lang: fr
layout: post
title: Correction de l'Erreur de Chargement de Mod√®le Llama.cpp
translated: true
type: note
---

En raison de l'erreur `gguf_init_from_file_impl: failed to read key-value pairs`, votre fichier de mod√®le est soit **corrompu**, **incomplet**, soit **incompatible** avec votre version de llama.cpp. Voici comment r√©soudre le probl√®me :

### 1. **V√©rifier et ret√©l√©charger le mod√®le**
   ```bash
   # V√©rifier la taille actuelle du fichier (la quantification Q6_K devrait √™tre d'environ 6,1 Go pour un mod√®le 8B) :
   ls -lh /home/lzw/Projects/llama.cpp/models/DeepSeek-R1-0528-Qwen3-8B-UD-Q6_K_XL.gguf

   # R√©sultat attendu :
   # -rw-r--r-- 1 lzw lzw 6.1G Jun  2 10:00 ...Q6_K_XL.gguf
   ```
   - Si la taille < 6 Go, **ret√©l√©chargez le mod√®le** :
     ```bash
     rm /home/lzw/Projects/llama.cpp/models/DeepSeek-R1-0528-Qwen3-8B-UD-Q6_K_XL.gguf
     # Ret√©l√©chargez depuis la source originale (par exemple, HuggingFace)
     ```

### 2. **Mettre √† jour llama.cpp**
Votre version (commit `7675c555`) est peut-√™tre obsol√®te :
```bash
cd ~/Projects/llama.cpp
git pull origin master          # Mettre √† jour le code
rm -rf build && mkdir build     # Nettoyer la compilation
cd build
cmake .. -DLLAMA_CUBLAS=ON      # Activer CUDA
make -j                         # Recompiler
```

### 3. **Valider l'int√©grit√© du mod√®le**
Testez le mod√®le avec une inf√©rence simple :
```bash
~/Projects/llama.cpp/build/bin/main \
  -m /home/lzw/Projects/llama.cpp/models/DeepSeek-R1-0528-Qwen3-8B-UD-Q6_K_XL.gguf \
  -p "Bonjour" -n 10
```
- Si cela √©choue, le fichier de mod√®le est d√©finitivement corrompu.

### 4. **Essayer un mod√®le diff√©rent (Test de bon sens)**
Testez avec un mod√®le fonctionnel connu (par exemple, [TinyLlama](https://huggingface.co/TheBloke/TinyLlama-1.1B-1T-OpenOrca-GGUF)) :
```bash
~/Projects/llama.cpp/build/bin/llama-server \
  -m /chemin/vers/tinyllama.Q8_0.gguf \
  --port 8000
```
- Si cela fonctionne, votre fichier de mod√®le original est le probl√®me.

### 5. **Reconvertir le mod√®le (Option avanc√©e)**
Si le ret√©l√©chargement √©choue :
1. Obtenez le mod√®le Hugging Face original :
   ```bash
   huggingface-cli download deepseek-ai/DeepSeek-R1-0528-Qwen3-8B --local-dir ./deepseek-original
   ```
2. Reconvertissez en GGUF :
   ```bash
   python3 ~/Projects/llama.cpp/convert-hf-to-gguf.py \
     ./deepseek-original \
     --outtype q6_k  # Correspond √† votre quantification
   ```

### R√©sum√© des correctifs courants :

| Probl√®me | Solution |
|-------|----------|
| **T√©l√©chargement corrompu** | Ret√©l√©charger le mod√®le |
| **llama.cpp obsol√®te** | `git pull` + recompiler |
| **GGUF incompatible** | Utiliser la derni√®re version de `convert-hf-to-gguf.py` |
| **Permissions de fichier** | `chmod +r /chemin/vers/modele.gguf` |

> üí° **Note :** Le mod√®le DeepSeek-R1 est r√©cent (publi√© le 28 mai 2024). Assurez-vous que votre llama.cpp a √©t√© **mis √† jour durant la derni√®re semaine** pour garantir la compatibilit√©.

Commencez par ret√©l√©charger le mod√®le et mettez √† jour llama.cpp ‚Äì cela r√©sout 90 % de ces probl√®mes. Faites-moi savoir si vous avez besoin d'aide suppl√©mentaire !