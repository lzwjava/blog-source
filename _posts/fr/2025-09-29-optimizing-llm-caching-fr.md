---
audio: false
generated: true
image: false
lang: fr
layout: post
model: openai/gpt-5
title: Optimisation du cache des LLM par la structure des invites
translated: true
type: note
---

Comme les grands mod√®les de langage consomment de plus en plus de tokens, la mise en cache devient de plus en plus importante pour r√©duire les co√ªts. Manus (une entreprise/outil que vous avez mentionn√©) a soulev√© une optimisation : fixer l'ordre de s√©rialisation JSON afin que les requ√™tes r√©p√©t√©es puissent √™tre mises en cache plus efficacement.

Je viens de penser √† une autre id√©e : lors de la construction des prompts ou du contexte, il est pr√©f√©rable de placer les variables ou le contenu dynamique **√† la fin**. De cette fa√ßon, les premi√®res parties restent stables et peuvent toucher le cache plus r√©guli√®rement.

Bien s√ªr, pour les utilisateurs individuels avec une utilisation relativement faible, cette optimisation n'a pas beaucoup d'importance. Mais pour les entreprises proposant des copilots (o√π les utilisateurs paient un abonnement forfaitaire par mois), le fournisseur backend (comme Manus, Claude, Code ou Cursor) a besoin d'une technologie de cache robuste afin de contr√¥ler les co√ªts √† grande √©chelle.

---

‚úÖ Donc oui, votre raisonnement est correct :
- **Un ordre stable** ‚Üí de meilleurs taux de cache.
- **Le contenu dynamique √† la fin** ‚Üí pr√©serve le pr√©fixe r√©utilisable plus long.
- **Les services √† haut volume (style copilot)** b√©n√©ficient beaucoup plus de cette optimisation que les utilisateurs individuels √† faible utilisation.

---

üëâ Souhaitez-vous que j'explique √©galement quelques **strat√©gies pratiques de mise en cache** que les entreprises utilisent r√©ellement avec les LLM ?