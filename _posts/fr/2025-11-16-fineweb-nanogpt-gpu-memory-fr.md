---
audio: false
generated: true
image: false
lang: fr
layout: post
model: x-ai/grok-4-fast
title: Guide de la M√©moire GPU pour FineWeb NanoGPT
translated: true
type: note
---

Vous entra√Ænez un petit GPT (style nanoGPT) sur le jeu de donn√©es **FineWeb** (la version nettoy√©e √† 100B tokens de Common Crawl utilis√©e pour entra√Æner LLaMA, Mistral, etc.), mais avec une m√©moire GPU limit√©e (probablement 16‚Äì24 GB, comme une RTX 3090/4090 ou A6000).

Votre configuration actuelle est d√©j√† assez conservative et adapt√©e √† une seule carte graphique grand public, mais voici un guide d√©taill√© sur **comment choisir les meilleurs param√®tres** pour nanoGPT sur FineWeb, class√©s par importance et impact m√©moire.

### Configurations Recommand√©es par M√©moire GPU

| M√©moire GPU   | Taille mod√®le (params) | n_layer | n_head | n_embd | batch_size | grad_acc | BS effective | block_size | Notes |
|---------------|------------------------|---------|--------|--------|------------|----------|--------------|------------|-------|
| 80 GB (A100/H100) | ~350M              | 12      | 12     | 768    | 64         | 8        | 512          | 1024       | Proche du GPT-2 medium original, bon entra√Ænement |
| 48 GB (A6000/3090) | ~250‚Äì300M         | 12      | 12     | 672    | 32         | 16       | 512          | 1024       | Tr√®s bon compromis |
| 24 GB (4090/3090) | ~160‚Äì200M          | 10      | 10     | 640    | 32         | 12‚Äì16    | 384‚Äì512      | 1024       | Votre point id√©al |
| 16‚Äì20 GB (4080, 3090 Ti) | ~125M        | 8       | 8      | 512    | 32         | 12       | 384          | 1024       | Stable |
| <16 GB (4070 Ti, etc.) | ~84M           | 6       | 6      | 384    | 16‚Äì32      | 16‚Äì24    | 256‚Äì512      | 512‚Äì1024   | Votre configuration actuelle |

Votre configuration actuelle (`6L 6H 384embd`) est s√ªre mais un peu petite. Vous pouvez viser plus grand.

### Meilleure Config pour une Carte 24 GB (RTX 4090 / 3090) sur FineWeb
C'est le setup le plus populaire dans la communaut√© nanoGPT actuellement :

```python
out_dir = 'out-fineweb-160M'
eval_interval = 1000
eval_iters = 200
log_interval = 100
always_save_checkpoint = True

wandb_log = True
wandb_project = 'fineweb'
wandb_run_name = '160M-fineweb'

dataset = 'fineweb'
gradient_accumulation_steps = 16   # 32 * 16 = 512 batch size effectif
batch_size = 32
block_size = 1024                  # important : FineWeb a √©t√© entra√Æn√© avec 1024+

n_layer = 10
n_head = 10
n_embd = 640
dropout = 0.0                      # peut essayer 0.1 plus tard
learning_rate = 6e-4               # l√©g√®rement plus √©lev√© pour les petits mod√®les
max_iters = 50000                  # ~50‚Äì100B tokens total est id√©al
warmup_iters = 2000
lr_decay_iters = 50000
min_lr = 6e-5
beta2 = 0.99
```

‚Üí Cela repr√©sente ~160M param√®tres, fonctionne confortablement sur une 4090 avec ~20‚Äì22 GB de VRAM utilis√©e.

### Encore Mieux : Mod√®le 200M+ (si vous avez 24 GB+)
```python
n_layer = 12
n_head = 12
n_embd = 768    # ‚Üí ~350M params (taille du GPT-2 medium original)
batch_size = 32
gradient_accumulation_steps = 16   # BS effectif 512
block_size = 1024
learning_rate = 5e-4
max_iters = 60000
```
Beaucoup de personnes entra√Ænent ce mod√®le avec succ√®s sur une seule 4090.

### R√®gles G√©n√©rales Cl√©s pour FineWeb + nanoGPT

1. **block_size = 1024** est fortement recommand√©  
   FineWeb a √©t√© filtr√© et entra√Æn√© avec un contexte de 1024. Utiliser 512 nuit √† la perplexit√© plus que vous ne le pensez.

2. **Batch size effectif ‚âà 512** est le point id√©al  
   LLaMA original utilisait ~4M tokens par batch. Pour nanoGPT, 512 s√©quences √ó 1024 tokens = ~0.5M tokens/batch ‚Üí respecte assez bien les lois d'√©chelle.

3. **Taux d'apprentissage (Learning rate)**  
   - ~100M params : 6e‚Äì8e-4  
   - ~350M params : 5e‚Äì6e-4  
   - ~770M params : 3e-4  

4. **Entra√Ænez pendant au moins 50B tokens** (100B+ c'est mieux)  
   Avec un mod√®le de 160M :  
   - 50k iters √ó 32 √ó 16 √ó 1024 tokens = ~83B tokens  
   Bonne convergence.

5. **Utilisez le jeu de donn√©es FineWeb officiel (pas openwebtext)**  
   Votre script est d√©j√† correct :  
   FineWeb >> OpenWebText en qualit√©.  
   Utilisez le sous-ensemble 100B : `fineweb100B` ou le `fineweb` complet (tronqu√© selon votre espace disque).

6. **Optionnel mais utile**
```python
compile = True          # PyTorch 2.0 compile, ~20‚Äì30% plus rapide
bias = False            # pas de biais dans les couches Linear (comme LLaMA)
weight_decay = 0.1
```

### Commande Rapide pour T√©l√©charger FineWeb (version 100B)
```bash
pip install datasets
python -c "
from datasets import load_dataset
ds = load_dataset('HuggingFaceFW/fineweb', name='sample-100BT', split='train')
ds.save_to_disk('data/fineweb100B')
"
```
Puis indiquez √† nanoGPT le chemin de ce dossier.

### R√©sum√© : Am√©liorez Votre Configuration Actuelle
Remplacez la partie mod√®le de votre configuration actuelle par celle-ci (pour une carte 24 GB) :

```python
n_layer = 10
n_head = 10
n_embd = 640
batch_size = 32
gradient_accumulation_steps = 16
block_size = 1024
learning_rate = 6e-4
max_iters = 50000
```

Vous obtiendrez un mod√®le bien plus performant sans co√ªt suppl√©mentaire significatif.

Les personnes obtiennent des **mod√®les 160M‚Äì350M tr√®s performants** sur FineWeb avec ces param√®tres ‚Äî surpassant souvent les performances du GPT-2 original.

Indiquez-moi votre carte graphique exacte et je pourrai vous donner la configuration optimale ! üöÄ