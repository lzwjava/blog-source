---
audio: true
lang: fr
layout: post
title: Benchmark MMLU
translated: true
---

## Avant-propos

Ce billet évalue un modèle de langage en utilisant le benchmark MMLU (Massive Multitask Language Understanding).

Le benchmark MMLU est un test complet de la capacité d'un modèle à effectuer diverses tâches dans un large éventail de sujets. Il consiste en des questions à choix multiples couvrant des domaines variés tels que les mathématiques, l'histoire, le droit et la médecine.

**Liens des jeux de données :**

*   [Papers with Code](https://paperswithcode.com/dataset/mmlu)
*   [Hugging Face Datasets](https://huggingface.co/datasets/cais/mmlu)

## llama-server

Pour exécuter le llama-server :

```bash
build/bin/llama-server -m models/7B/mistral-7b-instruct-v0.2.Q4_K_M.gguf --port 8080
```

## Benchmark MMLU

Ce script évalue le benchmark MMLU en utilisant trois backends différents : `ollama`, `llama-server`, et `deepseek`.

Pour exécuter le code du benchmark MMLU :

```python
import torch
from datasets import load_dataset
import requests
import json
from tqdm import tqdm
import argparse
import os
from openai import OpenAI
from dotenv import load_dotenv

load_dotenv()

# Configuration de l'analyse des arguments
parser = argparse.ArgumentParser(description="Évaluer le jeu de données MMLU avec différents backends.")
parser.add_argument("--type", type=str, default="ollama", choices=["ollama", "llama", "deepseek"], help="Type de backend : ollama, llama, ou deepseek")
args = parser.parse_args()

# Charger le jeu de données MMLU
subject = "college_computer_science"  # Choisissez votre sujet
dataset = load_dataset("cais/mmlu", subject, split="test", cache_dir="./.cache")

# Formater le prompt sans exemples few-shot
def format_mmlu_prompt(example):
    prompt = "Les questions suivantes sont des questions à choix multiples sur {}".format(subject.replace("_", " "))
    prompt += ". Veuillez répondre avec la lettre du choix correct (A, B, C ou D) uniquement."
    prompt += " Répondez uniquement avec la lettre. Aucune explication n'est nécessaire."
    
    # Ajouter la question actuelle
    prompt += f"Question: {example['question']}\n"
    prompt += "Choices:\nA. {}\nB. {}\nC. {}\nD. {}\n".format(*example['choices'])
    return prompt

# Boucle d'évaluation
correct = 0
total = 0

# Initialiser le client DeepSeek si nécessaire
if args.type == "deepseek":
    api_key = os.environ.get("DEEPSEEK_API_KEY")
    if not api_key:
        print("Erreur : la variable d'environnement DEEPSEEK_API_KEY n'est pas définie.")
        exit()
    client = OpenAI(api_key=api_key, base_url="https://api.deepseek.com")


for i, example in tqdm(enumerate(dataset), total=len(dataset), desc="Évaluation"):
    prompt = format_mmlu_prompt(example)
    
    # Envoyer la requête au backend
    if args.type == "ollama":
        url = "http://localhost:11434/v1/chat/completions"
        data = {
            "messages": [{"role": "user", "content": prompt}],
            "model": "mistral:7b"
        }
        headers = {"Content-Type": "application/json"}
        print(f"Entrée de l'API : {data}")
        response = requests.post(url, headers=headers, data=json.dumps(data))
        if response.status_code == 200:
            output_text = response.json()["choices"][0]["message"]["content"]
            predicted_answer = output_text.strip()[0] if len(output_text.strip()) > 0 else ""
            print(f"Sortie de l'API : {output_text}")
        else:
            predicted_answer = ""
            print(f"Erreur : {response.status_code} - {response.text}")
    elif args.type == "llama":
        url = "http://localhost:8080/v1/chat/completions"
        data = {
            "messages": [{"role": "user", "content": prompt}]
        }
        headers = {"Content-Type": "application/json"}
        print(f"Entrée de l'API : {data}")
        response = requests.post(url, headers=headers, data=json.dumps(data))
        if response.status_code == 200:
            output_text = response.json()["choices"][0]["message"]["content"]
            predicted_answer = output_text.strip()[0] if len(output_text.strip()) > 0 else ""
            print(f"Sortie de l'API : {output_text}")
        else:
            predicted_answer = ""
            print(f"Erreur : {response.status_code} - {response.text}")
    elif args.type == "deepseek":
        try:
            response = client.chat.completions.create(
                model="deepseek-chat",
                messages=[
                    {"role": "user", "content": prompt}
                ],
                max_tokens=100
            )
            if response and response.choices:
                output_text = response.choices[0].message.content.strip()
                predicted_answer = output_text.strip()[0] if len(output_text.strip()) > 0 else ""
                print(f"Sortie de l'API : {output_text}")
            else:
                predicted_answer = ""
                print("Erreur : Pas de réponse de l'API.")
        except Exception as e:
            predicted_answer = ""
            print(f"Erreur lors de l'appel API : {e}")
    else:
        raise ValueError("Type de backend invalide")
    
    # Comparer avec la vérité terrain
    
    answer_map = {0: "A", 1: "B", 2: "C", 3: "D"}
    ground_truth_answer = answer_map.get(example["answer"], "")
    is_correct = predicted_answer.upper() == ground_truth_answer
    if is_correct:
        correct += 1
    total += 1
    
    print(f"Question: {example['question']}")
    print(f"Choices: A. {example['choices'][0]}, B. {example['choices'][1]}, C. {example['choices'][2]}, D. {example['choices'][3]}")
    print(f"Réponse prédite : {predicted_answer}, Vérité terrain : {ground_truth_answer}, Correct : {is_correct}")
    print("-" * 30)

    if (i+1) % 10 == 0:
        accuracy = correct / total
        print(f"Traité {i+1}/{len(dataset)}. Précision actuelle : {accuracy:.2%} ({correct}/{total})")


# Calculer la précision
accuracy = correct / total
print(f"Sujet : {subject}")
print(f"Précision : {accuracy:.2%} ({correct}/{total})")
```

## Résultats

### Évaluation Zero-Shot

| Modèle                     | Méthode                      | Sujet                        | Précision   |
|---------------------------|--------------------------|--------------------------------|------------|
| mistral-7b-instruct-v0.2, Q4_K_M | macOS m2, 16GB, llama-server | MMLU college_computer_science | 40.00% (40/100) |
| Mistral-7B-Instruct-v0.3, Q4_0  | macOS m2, 16GB, ollama      | MMLU college_computer_science | 40.00% (40/100) |
| deepseek v3               | API, 2025.1.25           | MMLU college_computer_science | 78.00% (78/100) |
| gemini-1.5-flash          | API, 2025.1.25           | MMLU college_computer_science | 72.00% (72/100) |
| deepseek r1               | API, 2025.1.26           | MMLU college_computer_science | 87.14% (61/70) |

### Figure

Créons une figure basée sur le tableau ci-dessus.

```python
import matplotlib.pyplot as plt
import os

# Données d'exemple (remplacez par vos données réelles)
models = ['mistral-7b-instruct-v0.2 (llama.cpp)', 'Mistral-7B-Instruct-v0.3 (ollama)', 'deepseek v3 (API)', 'gemini-1.5-flash (API)']
accuracy = [40.00, 40.00, 78.00, 72.00]

# Créer le graphique en barres
plt.figure(figsize=(10, 6))
plt.bar(models, accuracy, color=['skyblue', 'lightcoral', 'lightgreen', 'gold'])
plt.xlabel('Modèle')
plt.ylabel('Précision (%)')
plt.title('Précision du benchmark MMLU')
plt.ylim(0, 100)  # Limite de l'axe y à 0-100 pour le pourcentage
plt.xticks(rotation=45, ha="right")  # Rotation des étiquettes de l'axe x pour une meilleure lisibilité
plt.tight_layout()

# Ajouter les valeurs de précision au-dessus des barres
for i, val in enumerate(accuracy):
    plt.text(i, val + 1, f'{val:.2f}%', ha='center', va='bottom')

# Sauvegarder le graphique en tant que fichier JPG dans le répertoire courant
plt.savefig(os.path.join(os.path.dirname(__file__), 'mmlu_accuracy_chart.jpg'))
plt.show()
```

{: .centered }
![](assets/images/mmlu/mmlu_accuracy_chart.jpg)
*Précision du benchmark MMLU*{: .caption }