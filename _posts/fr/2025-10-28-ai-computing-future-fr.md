---
audio: false
generated: true
image: false
lang: fr
layout: post
model: x-ai/grok-4-fast
title: L'informatique pilote l'avenir de l'IA
translated: true
type: note
---

Lors du cours CS25 de Stanford (un cours sur les architectures Transformer), Hyung Won Chung, chercheur chez OpenAI, a donné une conférence invitée intitulée "Shaping the Future of AI" (dans le cadre de la série V4). Il y discute de l'importance d'identifier la "force motrice dominante" derrière les changements majeurs dans tout domaine pour prédire efficacement sa trajectoire future.

### Points Clés de Sa Discussion
- **L'Analogie du Stylo qui Tombe** : Pour illustrer son propos, Chung utilise un exemple simple de physique : si vous lâchez un stylo, sa trajectoire est prévisible parce que nous comprenons la **force dominante** — la gravité — qui l'emporte sur des facteurs mineurs comme la résistance de l'air. En se concentrant sur cette force dominante, nous simplifions les systèmes complexes et faisons des prédictions fiables. Il soutient que nous devrions appliquer le même prisme à l'IA.

- **La Force Dominante dans l'IA** : Pour la recherche en IA, la force motrice dominante est la **diminution exponentielle du coût du calcul** (c'est-à-dire une puissance de calcul moins chère et plus abondante). Cela a été le principal facteur permettant les progrès rapides, autorisant les modèles à augmenter en taille avec plus de données et de paramètres. Chung souligne que comprendre cette force déplace l'attention vers la construction de méthodes générales et évolutives plutôt que vers des conceptions sur-mesurées et biaisées.

- **Lien avec la "Leçon Amère"** : Il fait référence à l'essai "The Bitter Lesson" de Rich Sutton, qui soutient que les avancées en IA proviennent de l'utilisation de plus de puissance de calcul et de données avec des approches d'apprentissage plus simples (comme les Transformers) plutôt que de biais inductifs conçus par l'homme. Avec le calcul comme force dominante, les architectures devraient prioriser l'évolutivité — par exemple, en favorisant les Transformers décodeurs uniquement par rapport aux configurations plus complexes encodeur-décodeur, car la structure supplémentaire devient inutile avec des ressources abondantes.

- **Implications Plus Larges** : Chung présente des graphiques historiques de la croissance de la puissance de calcul par rapport aux performances de l'IA, notant que les tendances passées (comme la loi de Moore) suggèrent qu'une mise à l'échelle continue entraînera les futures avancées. Son conseil : "Quand nous parlons de quelque chose dans le futur, le meilleur endroit pour obtenir des conseils est de regarder l'histoire."

Ce cadre aide à expliquer pourquoi les modèles d'IA récents (par exemple, ceux d'OpenAI) mettent l'accent sur une mise à l'échelle massive plutôt que sur des ajustements fins.

[Stanford CS25: V4 - Shaping the Future of AI with Hyung Won Chung](https://www.linkedin.com/pulse/stanford-cs25-v4-shaping-future-ai-hyung-won-chung-tarazona-md-lf9pe)  
[Vidéo complète de la conférence sur YouTube](https://www.youtube.com/watch?v=orDKvo8h71o)