---
audio: false
generated: true
image: false
lang: fr
layout: post
model: x-ai/grok-4-fast
title: Extraction de texte √† partir des archives Wikip√©dia
translated: true
type: note
---

Voici les m√©thodes les plus courantes et efficaces pour extraire du texte brut des fichiers dumps `.bz2` de Wikip√©dia comme le v√¥tre (`enwiki-latest-pages-articles1.xml-p1p41242.bz2`).

### Outils & M√©thodes recommand√©s (du plus simple au plus efficace)

#### 1. Utiliser wikiextractor (le meilleur pour la plupart des gens ‚Äì rapide et propre)
C'est l'outil standard utilis√© par presque tout le monde en NLP/ML pour les dumps Wikip√©dia.

```bash
# Installation (une fois)
pip install wikiextractor

# Extraction du texte (supprime les mod√®les, tableaux, etc., donne un texte propre par article)
wikiextractor enwiki-latest-pages-articles1.xml-p1p41242.bz2 --output wiki_text
```

- Sortie : cr√©e un dossier `wiki_text/` avec des sous-dossiers AA, AB, ‚Ä¶ contenant des fichiers `.bz2` de JSON/texte propre.
- Pour obtenir du texte brut (un article par ligne) :
```bash
wikiextractor enwiki-latest-pages-articles1.xml-p1p41242.bz2 --processes 8 -o wiki_text --json
# Puis convertir le JSON en texte brut si n√©cessaire
find wiki_text -name '*' -exec cat {} \; | grep '"text":' | sed 's/.*"text": "//;s/"$//' > all_text.txt
```

#### 2. Utiliser WikiCorpus de gensim (excellent si vous faites du NLP en Python)

```python
from gensim.corpora import WikiCorpus

# Cela g√®re automatiquement le .bz2 et extrait le texte + tokenise
wiki = WikiCorpus("enwiki-latest-pages-articles1.xml-p1p41242.bz2", dictionary=False)

# It√©rer sur les articles
with open("output.txt", "w", encoding="utf-8") as f:
    for i, text in enumerate(wiki.get_texts()):
        f.write(" ".join(text) + "\n")
        if i % 10000 == 0:
            print(f"Processed {i} articles")
```

#### 3. Simple bzip2 + grep/awk (rapide & sale, pas de nettoyage)

```bash
# Juste d√©compresser et voir le XML brut
bzip2 -dc enwiki-latest-pages-articles1.xml-p1p41242.bz2 | less

# Extraire uniquement le contenu <text> (tr√®s rudimentaire, conserve le balisage wiki)
bzip2 -dc enwiki-latest-pages-articles1.xml-p1p41242.bz2 | \
  sed -n '/<text /,/<\/text>/p' | \
  sed 's/<text[^>]*>//g;s/<\/text>//g' > raw_text_with_markup.txt
```

#### 4. Si vous avez le dump complet et voulez tout rapidement (parall√®le)
Votre fichier n'est que le premier segment (p1p41242), mais si vous avez tous les ~30+ segments plus tard :

```bash
# Tout extraire en parall√®le
ls *.bz2 | xargs -P 8 -I {} wikiextractor {} --output wiki_extracted
```

### R√©sum√© ‚Äì Ce que je vous recommande maintenant :

```bash
pip install wikiextractor
wikiextractor enwiki-latest-pages-articles1.xml-p1p41242.bz2 -o extracted_wiki --bytes 1M --json
```

Cela vous donnera des articles propres et lisibles en quelques minutes (votre fichier ne fait que ~2-3 Go compress√©, donc tr√®s g√©rable).

Dites-moi si vous voulez :
- Uniquement l'espace de noms principal (les vrais articles, pas les pages de discussion)
- Du texte brut au lieu du JSON
- Une sortie tokenis√©e
- Wikip√©dia en chinois √† la place (zhwiki) ‚Äì le processus est identique

Bonnes extractions ! üöÄ