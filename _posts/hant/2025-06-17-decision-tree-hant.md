---
audio: false
generated: true
lang: hant
layout: post
title: 決策樹
translated: true
type: note
---

**決策樹**是一種在機器學習和數據分析中使用的預測建模工具，用於根據輸入數據做出決策。它以樹狀結構表示決策及其可能的後果，包括隨機事件的結果。決策樹廣泛應用於分類（例如預測客戶是否會購買產品）和回歸（例如預測房價）等任務。它們直觀、易於解釋，並且對簡單和複雜的數據集都有效。

本綜合指南將解釋什麼是決策樹、其工作原理、組成部分、構建過程、優點、局限性及實際應用注意事項，並附帶示例。

---

### **什麼是決策樹？**

決策樹是一種流程圖式的表示法，用於展示決策及其可能的結果。它由節點和分支組成：
- **節點**：代表決策、條件或結果。
- **分支**：代表決策或條件的可能結果。
- **葉節點**：代表最終輸出（例如分類中的類別標籤或回歸中的數值）。

決策樹用於監督學習，模型從標記的訓練數據中學習，以預測新的未見數據的結果。它們用途廣泛，可以處理分類和數值數據。

---

### **決策樹的組成部分**

1. **根節點**：
   - 樹的頂層節點。
   - 代表整個數據集和初始決策點。
   - 根據提供最多信息或最大程度減少不確定性的特徵進行分裂。

2. **內部節點**：
   - 根節點和葉節點之間的節點。
   - 代表基於特定特徵和條件的中間決策點（例如「年齡 > 30？」）。

3. **分支**：
   - 節點之間的連接。
   - 代表決策或條件的結果（例如二元分裂的「是」或「否」）。

4. **葉節點**：
   - 終端節點，代表最終輸出。
   - 在分類中，葉節點代表類別標籤（例如「購買」或「不購買」）。
   - 在回歸中，葉節點代表數值（例如預測價格）。

---

### **決策樹如何工作？**

決策樹通過根據特徵值遞歸地將輸入數據分割成區域，然後根據該區域中的多數類別或平均值做出決策。以下是其運作步驟的逐步解釋：

1. **輸入數據**：
   - 數據集包含特徵（自變量）和目標變量（因變量）。
   - 例如，在預測客戶是否會購買產品的數據集中，特徵可能包括年齡、收入和瀏覽時間，目標為「購買」或「不購買」。

2. **分割數據**：
   - 算法選擇一個特徵和閾值（例如「年齡 > 30」）將數據分割成子集。
   - 目標是創建能夠最大化類別分離（用於分類）或最小化方差（用於回歸）的分割。
   - 分割標準包括 **基尼不純度**、**信息增益** 或 **方差減少** 等指標（下文解釋）。

3. **遞歸分割**：
   - 算法對每個子集重複分割過程，創建新的節點和分支。
   - 此過程持續直到滿足停止條件（例如達到最大深度、節點最小樣本數或無進一步改進）。

4. **分配輸出**：
   - 分割停止後，每個葉節點被分配一個最終輸出。
   - 對於分類，葉節點代表該區域中的多數類別。
   - 對於回歸，葉節點代表該區域中目標值的平均值（或中位數）。

5. **預測**：
   - 要預測新數據點的結果，樹從根節點遍歷到葉節點，根據數據點的特徵值遵循決策規則。
   - 葉節點提供最終預測。

---

### **分割標準**

分割的質量決定了樹分離數據的效果。常見標準包括：

1. **基尼不純度（分類）**：
   - 衡量節點的不純度（類別的混合程度）。
   - 公式：\( \text{Gini} = 1 - \sum_{i=1}^n (p_i)^2 \)，其中 \( p_i \) 是節點中類別 \( i \) 的比例。
   - 較低的基尼不純度表示更好的分割（節點更同質）。

2. **信息增益（分類）**：
   - 基於 **熵**，衡量節點的隨機性或不確定性。
   - 熵：\( \text{Entropy} = - \sum_{i=1}^n p_i \log_2(p_i) \)。
   - 信息增益 = 分割前的熵 - 分割後的加權平均熵。
   - 較高的信息增益表示更好的分割。

3. **方差減少（回歸）**：
   - 衡量分割後目標變量方差的減少。
   - 方差：\( \text{Variance} = \frac{1}{n} \sum_{i=1}^n (y_i - \bar{y})^2 \)，其中 \( y_i \) 是目標值，\( \bar{y} \) 是平均值。
   - 算法選擇最大化方差減少的分割。

4. **卡方（分類）**：
   - 檢驗分割是否顯著改善類別的分佈。
   - 用於某些算法如 CHAID。

算法評估每個特徵的所有可能分割，並選擇得分最佳的分割（例如最低基尼不純度或最高信息增益）。

---

### **如何構建決策樹？**

構建決策樹涉及以下步驟：

1. **選擇最佳特徵**：
   - 使用選定的標準（例如基尼、信息增益）評估所有特徵和可能的分割點。
   - 選擇最能分離數據的特徵和閾值。

2. **分割數據**：
   - 根據選定的特徵和閾值將數據集分成子集。
   - 為每個子集創建子節點。

3. **遞歸重複**：
   - 對每個子節點應用相同過程，直到滿足停止條件：
     - 達到最大樹深度。
     - 節點中樣本數達到最小值。
     - 分割標準無顯著改善。
     - 節點中所有樣本屬於同一類別（分類）或具有相似值（回歸）。

4. **修剪樹（可選）**：
   - 為防止過擬合，通過移除對預測準確性貢獻小的分支來降低樹的複雜度。
   - 修剪可以是 **預修剪**（在構建過程中提前停止）或 **後修剪**（構建後移除分支）。

---

### **示例：分類決策樹**

**數據集**：根據年齡、收入和瀏覽時間預測客戶是否會購買產品。

| 年齡 | 收入   | 瀏覽時間 | 購買？ |
|------|--------|----------|--------|
| 25   | 低     | 短       | 否     |
| 35   | 高     | 長       | 是     |
| 45   | 中     | 中       | 是     |
| 20   | 低     | 短       | 否     |
| 50   | 高     | 長       | 是     |

**步驟 1：根節點**：
- 評估所有特徵（年齡、收入、瀏覽時間）以找到最佳分割。
- 假設「收入 = 高」提供最高的信息增益。
- 分割數據：
  - 收入 = 高：全部為「是」（純節點，停止）。
  - 收入 = 低 或 中：混合（繼續分割）。

**步驟 2：子節點**：
- 對於「低或中收入」子集，評估剩餘特徵。
- 假設「年齡 > 30」提供最佳分割：
  - 年齡 > 30：主要為「是」。
  - 年齡 ≤ 30：全部為「否」。

**步驟 3：停止**：
- 所有節點為純節點（僅包含一個類別）或滿足停止條件。
- 樹結構如下：
  - 根節點：「收入是否為高？」
    - 是 → 葉節點：「購買」
    - 否 → 「年齡 > 30？」
      - 是 → 葉節點：「購買」
      - 否 → 葉節點：「不購買」

**預測**：
- 新客戶：年齡 = 40，收入 = 中，瀏覽時間 = 短。
- 路徑：收入 ≠ 高 → 年齡 = 40 > 30 → 預測「購買」。

---

### **示例：回歸決策樹**

**數據集**：根據面積和位置預測房價。

| 面積（平方英尺） | 位置   | 價格（千美元） |
|------------------|--------|----------------|
| 1000             | 市區   | 300            |
| 1500             | 郊區   | 400            |
| 2000             | 市區   | 600            |
| 800              | 鄉村   | 200            |

**步驟 1：根節點**：
- 評估分割（例如面積 > 1200，位置 = 市區）。
- 假設「面積 > 1200」最小化方差。
- 分割：
  - 面積 > 1200：價格 = {400, 600}（平均值 = 500）。
  - 面積 ≤ 1200：價格 = {200, 300}（平均值 = 250）。

**步驟 2：停止**：
- 節點足夠小或方差減少最小。
- 樹結構：
  - 根節點：「面積 > 1200？」
    - 是 → 葉節點：預測 500 千美元。
    - 否 → 葉節點：預測 250 千美元。

**預測**：
- 新房：面積 = 1800，位置 = 市區 → 面積 > 1200 → 預測 500 千美元。

---

### **決策樹的優點**

1. **可解釋性**：
   - 易於理解和可視化，非常適合向非技術利益相關者解釋決策。
2. **處理混合數據**：
   - 無需大量預處理即可處理分類和數值特徵。
3. **非參數**：
   - 不對底層數據分佈做假設。
4. **特徵重要性**：
   - 識別哪些特徵對預測貢獻最大。
5. **快速預測**：
   - 訓練後，預測速度快，僅涉及簡單比較。

---

### **決策樹的局限性**

1. **過擬合**：
   - 深樹可能記憶訓練數據，導致泛化能力差。
   - 解決方案：使用修剪、限制最大深度或設置節點最小樣本數。
2. **不穩定性**：
   - 數據的微小變化可能導致完全不同的樹。
   - 解決方案：使用集成方法如隨機森林或梯度提升。
3. **偏向主導類別**：
   - 在類別不平衡的數據集中表現不佳，其中一個類別佔主導。
   - 解決方案：使用類別加權或過採樣等技術。
4. **貪婪方法**：
   - 分割基於局部優化選擇，可能無法達到全局最優樹。
5. **線性關係處理能力差**：
   - 對於特徵與目標之間關係為線性或複雜的數據集效果較差。

---

### **實際應用注意事項**

1. **超參數**：
   - **最大深度**：限制樹的深度以防止過擬合。
   - **最小分割樣本數**：分割節點所需的最小樣本數。
   - **最小葉節點樣本數**：葉節點中的最小樣本數。
   - **最大特徵數**：每次分割考慮的特徵數量。

2. **修剪**：
   - 預修剪：在樹構建期間設置約束。
   - 後修剪：根據驗證性能在構建樹後移除分支。

3. **處理缺失值**：
   - 某些算法（例如 CART）將缺失值分配到最小化誤差的分支。
   - 或者，在訓練前填補缺失值。

4. **可擴展性**：
   - 決策樹對於中小型數據集計算效率高，但對於特徵眾多的超大數據集可能較慢。

5. **集成方法**：
   - 為克服局限性，決策樹常被用於集成中：
     - **隨機森林**：結合多個在數據和特徵的隨機子集上訓練的樹。
     - **梯度提升**：順序構建樹，每棵樹修正前一棵的錯誤。

---

### **決策樹的應用**

1. **商業**：
   - 客戶流失預測、信用評分、市場細分。
2. **醫療保健**：
   - 疾病診斷、風險預測（例如心臟病）。
3. **金融**：
   - 欺詐檢測、貸款違約預測。
4. **自然語言處理**：
   - 文本分類（例如情感分析）。
5. **回歸任務**：
   - 預測連續結果，如房價或銷售預測。

---

### **可視化示例**

為說明決策樹如何分割數據，考慮一個簡單的分類數據集，具有兩個特徵（例如年齡和收入）和兩個類別（購買、不購買）。以下是展示決策樹如何劃分特徵空間的概念圖。

```
chartjs
{
  "type": "scatter",
  "data": {
    "datasets": [
      {
        "label": "購買",
        "data": [
          {"x": 35, "y": 50000},
          {"x": 45, "y": 60000},
          {"x": 50, "y": 80000}
        ],
        "backgroundColor": "#4CAF50",
        "pointRadius": 6
      },
      {
        "label": "不購買",
        "data": [
          {"x": 20, "y": 20000},
          {"x": 25, "y": 30000}
        ],
        "backgroundColor": "#F44336",
        "pointRadius": 6
      }
    ]
  },
  "options": {
    "scales": {
      "x": {
        "title": { "display": true, "text": "年齡" },
        "min": 15,
        "max": 60
      },
      "y": {
        "title": { "display": true, "text": "收入（美元）" },
        "min": 10000,
        "max": 100000
      }
    },
    "plugins": {
      "title": { "display": true, "text": "決策樹特徵空間" },
      "legend": { "display": true }
    }
  }
}
```

此圖顯示了二維特徵空間中的數據點。決策樹可能會分割此空間（例如在年齡 = 30 或收入 = 40000 處）以分離「購買」和「不購買」。

---

### **實際實現**

決策樹可以使用以下庫實現：
- **Python**：Scikit-learn（`DecisionTreeClassifier`、`DecisionTreeRegressor`）、XGBoost、LightGBM。
- **R**：`rpart`、`party`。
- **其他工具**：Weka、MATLAB 或 Spark MLlib。

Python 示例（Scikit-learn）：
```python
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score

# 示例數據
X = [[25, 20000], [35, 50000], [45, 60000], [20, 30000], [50, 80000]]  # 特徵：年齡、收入
y = [0, 1, 1, 0, 1]  # 標籤：0 = 不購買，1 = 購買

# 分割數據
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# 訓練決策樹
clf = DecisionTreeClassifier(max_depth=3, random_state=42)
clf.fit(X_train, y_train)

# 預測
y_pred = clf.predict(X_test)

# 評估
print("準確率:", accuracy_score(y_test, y_pred))
```

---

### **結論**

決策樹是強大且可解釋的分類和回歸任務模型。它們通過基於特徵條件遞歸分割數據來工作，使用基尼不純度或信息增益等標準。雖然它們易於使用和可視化，但可能過擬合且不穩定，通常通過修剪或使用集成方法（如隨機森林）來緩解。

如需進一步探索，請使用真實數據集實驗決策樹，調整超參數，並將它們與集成方法結合以提高性能。如果您有特定的數據集或使用案例，我可以提供量身定制的示例或可視化！