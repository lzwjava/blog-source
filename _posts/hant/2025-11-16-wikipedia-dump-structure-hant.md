---
audio: false
generated: true
image: false
lang: hant
layout: post
model: x-ai/grok-4-fast
title: 維基百科數據轉儲檔案結構差異
translated: true
type: note
---

主要區別在於維基百科全文如何分割儲存於多個檔案中——具體來說，就是一個 `.xml.bz2` 檔案是僅包含單一流式的修訂版本（非多重流），還是將多個壓縮流捆綁在一起（多重流）。

### 1. 非多重流備份檔（舊式風格，或預設的「pages-articles」檔案）
範例檔案名稱：  
`enwiki-20251101-pages-articles1.xml-p1p41242.bz2`

- 每個 `.xml.bz2` 檔案包含一部分頁面（例如第 1–41242 頁），按頁面 ID 排序。
- 檔案內部僅有一段連續的 `<page> … <page> …` 序列。
- 單一頁面的所有修訂版本都集中儲存在該頁面標籤內（最新修訂版本 + 若你要求「當前版本 + 歷史記錄」則包含舊版本）。
- 要提取單篇文章，只需使用任何 XML/BZ2 工具正常解壓縮並解析大型 XML 檔案。
- `wget -c` 能完美運作：因為檔案是單一的 bzip2 流，所以能從中斷處準確恢復下載。

### 2. 多重流備份檔（檔案名稱中包含「multistream」）
範例檔案名稱（你連結的檔案）：  
`enwiki-20251101-pages-articles-multistream1.xml-p1p41242.bz2`

- 涵蓋相同的頁面範圍 (p1p41242)，但所有修訂版本的全文不再儲存在主 XML 檔案中。
- 取而代之的是：
  - 主 XML 檔案僅包含元數據（標題、ID、限制、最新修訂時間戳等）以及指向實際修訂文本儲存位置的指標（位元組偏移量 + 長度）。
  - 實際的修訂文本則分開儲存在同一個 `.bz2` 檔案內的許多小型壓縮流中（因此稱為「多重流」）。
- 通常會有一個配套的 `...-multistream-index1.txt.bz2` 檔案，其中包含每個頁面/修訂版本的確切位元組偏移量，以便工具能直接跳轉到正確的壓縮流，僅提取該文本而無需解壓縮整個 10–30 GB 的檔案。

### 這對 `wget -c` 有何影響？

實際上，以下兩個指令：

```bash
wget -c https://.../enwiki-20251101-pages-articles1.xml-p1p41242.bz2
wget -c https://.../enwiki-20251101-pages-articles-multistream1.xml-p1p41242.bz2
```

從 wget 的角度來看行為完全相同：兩者都是正常的單一 bzip2 檔案，因此使用 `-c` 恢復下載在兩種情況下都能完美運作。

差異僅在於你開始處理下載的檔案時才會顯現。

### 摘要對照表

| 特性                               | 非多重流（常規）                      | 多重流                                      |
|------------------------------------|---------------------------------------|---------------------------------------------|
| 檔案名稱包含                       | `-pages-articlesN.xml-...bz2`         | `-pages-articles-multistreamN.xml-...bz2`   |
| 文章文本位置                       | 位於 XML `<page><revision><text>...</text></revision></page>` 內 | 在 XML 外部，位於獨立的壓縮流中            |
| 檔案大小（相同頁面範圍）           | 大得多（通常是 2–3 倍）               | 小得多                                      |
| 隨機存取 / 單頁提取                | 非常慢（必須解壓縮整個檔案）          | 非常快（配合索引檔案使用）                  |
| 能直接理解的工具                   | 任何 XML 解析器（SAX、xmltodict 等）  | wikimedia 的 dump-generator 工具、pywiki、mwxml 等 |
| 需要配套索引檔案？                 | 否                                    | 是（`-multistream-index....txt.bz2`）       |
| `wget -c` 可恢復性                 | 完美                                  | 完美（仍為單一 bzip2 檔案）                 |

### 那麼你應該下載哪一種？

- 如果你只想進行一次完整的順序處理（例如訓練語言模型），非多重流檔案更為簡單。
- 如果你希望快速查找單獨頁面，或者在意磁碟空間和提取速度，則應優先選擇多重流 + 索引檔案組合。現今幾乎所有嚴謹的 Wikimedia 處理工具（WikiExtractor、gensim 的 WikiCorpus、jawiki 腳本等）都預期或強烈推薦使用多重流版本。

這兩種檔案你都可以使用 `wget -c` 無障礙恢復下載；「多重流」部分僅影響檔案內部結構，而不影響 HTTP/wget 對其的處理方式。