---
audio: false
generated: true
image: false
lang: hant
layout: post
model: x-ai/grok-4-fast
title: 卡帕西的納米GPT與納米聊天
translated: true
type: note
---

### 概述
nanoGPT 與 nanochat 皆是由 Andrej Karpathy 建立的開源項目，旨在讓大型語言模型（LLM）訓練變得易於接觸且具教育意義。nanoGPT（2023 年發布）專注於從頭訓練 GPT 風格模型的基礎知識，而 nanochat（2025 年 10 月發布）則在其基礎上發展為更全面的「全端」流程，用於創建類似 ChatGPT 的聊天機器人。兩者的主要區別在於範疇、訓練階段、程式碼庫複雜度以及端到端可用性——nanochat 實質上是將 nanoGPT 演進為一個完整的、接近生產級別的對話式 AI 系統。

### 訓練程式碼的主要差異
nanochat 中的訓練程式碼是對 nanoGPT 方法的延伸與改進，但它針對聊天應用整合了額外的階段、優化與整合。以下是詳細分析：

| 面向                     | nanoGPT                                                                 | nanochat                                                                 |
|-------------------------|-------------------------------------------------------------------------|--------------------------------------------------------------------------|
| **主要焦點**      | 在原始文字資料（例如 OpenWebText 或莎士比亞作品）上預訓練基於 Transformer 的 GPT 模型。教授核心概念，如分詞、模型架構和基礎訓練迴圈。 | 完整流程：預訓練 + 中期訓練（對話/多選題） + 監督式微調（SFT） + 可選的強化學習（透過 GRPO 的 RLHF） + 評估 + 推理。建構可部署的聊天機器人。 |
| **訓練階段**    | - 單階段預訓練。<br>- 基礎評估（例如困惑度）。 | - **預訓練**：與 nanoGPT 類似，但使用 FineWeb 資料集。<br>- **中期訓練**：使用 SmolTalk（使用者-助理對話）、多選題問答以及工具使用資料。<br>- **監督式微調**：針對聊天對齊進行微調，並在 MMLU、ARC-E/C、GSM8K（數學）、HumanEval（程式碼）等基準上進行評估。<br>- **強化學習**：在 GSM8K 上進行可選的 RLHF 以實現偏好對齊。<br>- 自動生成報告卡，包含指標（例如 CORE 分數）。 |
| **程式碼庫規模與結構** | 總計約 600 行（例如 `train.py` 約 300 行，`model.py` 約 300 行）。極簡、可修改的 PyTorch；優先考慮簡潔性而非完整性。已棄用，建議改用 nanochat。 | 約 8,000 行整潔、模組化的 PyTorch 程式碼。包含基於 Rust 的分詞器、高效推理引擎（KV 快取、預填/解碼）、工具整合（例如 Python 沙箱）及網頁 UI。更具凝聚力，但仍可分支修改。 |
| **優化器與超參數** | 標準 AdamW；學習率針對中型模型（例如 GPT-2 1.24 億參數）進行調整。 | Muon + AdamW 混合（靈感來自 modded-nanoGPT）；自適應學習率（例如對較小資料集使用較低學習率以避免過度擬合）。透過 `--depth` 標誌實現模型規模擴展。 |
| **資料處理**      | 原始文字語料庫；基礎 BPE 分詞器訓練。 | 增強功能：訓練自訂分詞器（詞彙量約 65K）；使用 Hugging Face 資料集（預訓練用 FineWeb，對話用 SmolTalk）。支援合成資料以注入個性。 |
| **訓練時間與成本** | 在 8xA100 上約 4 天以訓練 GPT-2 等效模型（約 500 美元以上）。專注於教育性運行。 | 在 8xH100 上約 4 小時訓練基礎 5.6 億參數模型（約 100 美元）；約 12 小時即可超越 GPT-2；可擴展至約 1,000 美元以訓練更強模型（例如 24 小時後 MMLU 達 40%）。 |
| **推理與部署** | 基礎文字生成；無內建聊天功能或 UI。 | 具 KV 快取的優化引擎；CLI 聊天；ChatGPT 風格網頁 UI；工具使用（例如程式碼執行）。模型在小規模下顯得「天真/笨拙」但有趣。 |
| **教育目標**   | 建構語言模型的「核心」（大腦）。非常適合初學者實驗架構。 | 建構完整的 ChatGPT 複製品（從大腦到介面）。是進階課程（如 LLM101n）的頂石項目；具潛力成為研究基準。 |

### 何時使用各自專案
- **選擇 nanoGPT**：如果您是 LLM 新手，並希望輕量入門預訓練（例如調整架構或資料集）。它較簡單，但僅限於非對話式模型。
- **選擇 nanochat**：用於端到端的聊天機器人開發。它在聊天任務上表現優於 nanoGPT（例如某些 RLHF 測試中提升 58%），並包含從資料到可運作 UI 所需的一切，全部集中在一個儲存庫中。

兩者均強調可修改性——例如，nanochat 允許輕鬆擴展（更改模型深度）或自訂（添加合成資料以塑造個性）。nanochat 甚至賦予模型自我意識（例如，它知道自身規模與限制）。

若欲動手探索：
- [nanoGPT GitHub](https://github.com/karpathy/nanoGPT)
- [nanochat GitHub](https://github.com/karpathy/nanochat)

如果您要運行這些專案，建議從 nanochat 的 `speedrun.sh` 腳本開始，並在像 Lambda Labs 這樣的雲端 GPU 上進行，以獲得完整體驗。