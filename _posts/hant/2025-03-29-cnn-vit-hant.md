---
audio: false
generated: true
lang: hant
layout: post
title: åŸºæ–¼å·ç©ç¥ç¶“ç¶²çµ¡èˆ‡è¦–è¦ºè®Šæ›å™¨çš„åœ–åƒè™•ç†
translated: true
type: note
---

ä»¥ä¸‹æ˜¯ä½¿ç”¨ **å·ç©ç¥ç¶“ç¶²çµ¡ (CNNs)** å’Œ **Vision Transformers (ViT)** åœ¨ Python ä¸­é€é **PyTorch** è™•ç†å½±åƒçš„ç¨‹å¼ç¢¼ç¯„ä¾‹ã€‚

---

## **1. ä½¿ç”¨ CNNï¼ˆå·ç©ç¥ç¶“ç¶²çµ¡ï¼‰é€²è¡Œå½±åƒè™•ç†**
CNN å»£æ³›æ‡‰ç”¨æ–¼å½±åƒåˆ†é¡ã€ç‰©ä»¶åµæ¸¬å’Œç‰¹å¾µæå–ã€‚

### **ç¯„ä¾‹ï¼šä½¿ç”¨é è¨“ç·´çš„ CNNï¼ˆResNetï¼‰**
```python
import torch
import torchvision.models as models
import torchvision.transforms as transforms
from PIL import Image

# è¼‰å…¥é è¨“ç·´çš„ ResNet æ¨¡å‹
model = models.resnet18(pretrained=True)
model.eval()  # è¨­å®šç‚ºè©•ä¼°æ¨¡å¼

# å®šç¾©å½±åƒé è™•ç†æµç¨‹
preprocess = transforms.Compose([
    transforms.Resize(256),
    transforms.CenterCrop(224),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
])

# è¼‰å…¥ä¸¦é è™•ç†å½±åƒ
image = Image.open("example.jpg")  # è«‹æ›¿æ›ç‚ºä½ çš„å½±åƒè·¯å¾‘
input_tensor = preprocess(image)
input_batch = input_tensor.unsqueeze(0)  # æ·»åŠ æ‰¹æ¬¡ç¶­åº¦

# å¦‚æœå¯ç”¨å‰‡ç§»è‡³ GPU
if torch.cuda.is_available():
    input_batch = input_batch.to('cuda')
    model.to('cuda')

# æå–ç‰¹å¾µï¼ˆåœ¨æœ€çµ‚åˆ†é¡å±¤ä¹‹å‰ï¼‰
with torch.no_grad():
    features = model(input_batch)

print("ç‰¹å¾µå‘é‡å½¢ç‹€:", features.shape)  # ä¾‹å¦‚ï¼štorch.Size([1, 1000])
```
**èªªæ˜**ï¼š
1. **ResNet18** æ˜¯åœ¨ ImageNet ä¸Šé è¨“ç·´çš„ CNN æ¶æ§‹ã€‚
2. å½±åƒç¶“éé è™•ç†ï¼ˆèª¿æ•´å¤§å°ã€æ¨™æº–åŒ–ï¼‰ã€‚
3. æ¨¡å‹å°‡å½±åƒè½‰æ›ç‚º**ç‰¹å¾µå‘é‡**ï¼ˆä¾‹å¦‚ ResNet18 çš„ 1000 ç¶­å‘é‡ï¼‰ã€‚

---

## **2. ä½¿ç”¨ Vision Transformer (ViT) é€²è¡Œå½±åƒè™•ç†**
ViT å°‡å½±åƒè¦–ç‚ºåœ–å¡Šåºåˆ—ï¼Œä¸¦ä½¿ç”¨è‡ªæ³¨æ„åŠ›æ©Ÿåˆ¶ï¼ˆé¡ä¼¼æ–¼ NLP ä¸­çš„æ–¹æ³•ï¼‰ã€‚

### **ç¯„ä¾‹ï¼šä½¿ç”¨é è¨“ç·´çš„ ViTï¼ˆHugging Faceï¼‰**
```python
from transformers import ViTFeatureExtractor, ViTModel
from PIL import Image
import torch

# è¼‰å…¥é è¨“ç·´çš„ Vision Transformer (ViT)
model_name = "google/vit-base-patch16-224-in21k"
feature_extractor = ViTFeatureExtractor.from_pretrained(model_name)
model = ViTModel.from_pretrained(model_name)

# è¼‰å…¥å½±åƒ
image = Image.open("example.jpg")  # è«‹æ›¿æ›ç‚ºä½ çš„å½±åƒè·¯å¾‘

# é è™•ç†å½±åƒï¼ˆè½‰æ›ç‚ºåœ–å¡Šï¼‰
inputs = feature_extractor(images=image, return_tensors="pt")

# æå–ç‰¹å¾µï¼ˆCLS ç¬¦è¨˜æˆ–åœ–å¡ŠåµŒå…¥ï¼‰
with torch.no_grad():
    outputs = model(**inputs)

# å–å¾—ç‰¹å¾µå‘é‡ï¼ˆCLS ç¬¦è¨˜ï¼‰
features = outputs.last_hidden_state[:, 0, :]  # å½¢ç‹€ï¼š[1, 768]

print("ç‰¹å¾µå‘é‡å½¢ç‹€:", features.shape)  # ä¾‹å¦‚ï¼štorch.Size([1, 768])
```
**èªªæ˜**ï¼š
1. **ViT** å°‡å½±åƒåˆ†å‰²ç‚º **16x16 åœ–å¡Š**ï¼Œä¸¦åƒè™•ç† NLP ä¸­çš„ç¬¦è¨˜ä¸€æ¨£è™•ç†å®ƒå€‘ã€‚
2. `CLS ç¬¦è¨˜`ï¼ˆç¬¬ä¸€å€‹ç¬¦è¨˜ï¼‰ä»£è¡¨æ•´å€‹å½±åƒçš„ç‰¹å¾µå‘é‡ã€‚
3. è¼¸å‡ºæ˜¯ä¸€å€‹ **768 ç¶­å‘é‡**ï¼ˆé‡å° `vit-base`ï¼‰ã€‚

---

## **3. CNN èˆ‡ ViT ç‰¹å¾µæå–æ¯”è¼ƒ**

| æ¨¡å‹ | æ–¹æ³• | ç‰¹å¾µå‘é‡å¤§å° | å‡½å¼åº« |
|-------|----------|---------------------|-----------|
| **CNN (ResNet18)** | å·ç©å±¤ + æ± åŒ– | 1000 (ImageNet é¡åˆ¥) | `torchvision` |
| **ViT (Google ViT-Base)** | åœ–å¡ŠåµŒå…¥ + Transformer | 768 (éš±è—ç¶­åº¦) | `transformers` |

---

## **4. å½±åƒç‰¹å¾µå‘é‡çš„æ‡‰ç”¨**
- **å½±åƒæœå°‹**ï¼šæ¯”è¼ƒç‰¹å¾µå‘é‡ï¼ˆä¾‹å¦‚é¤˜å¼¦ç›¸ä¼¼åº¦ï¼‰ã€‚
- **é·ç§»å­¸ç¿’**ï¼šä½¿ç”¨é è¨“ç·´ç‰¹å¾µé€²è¡Œè‡ªè¨‚ä»»å‹™ã€‚
- **ç‰©ä»¶åµæ¸¬**ï¼šå¾ç‰¹å¾µä¸­æå–æ„Ÿèˆˆè¶£å€åŸŸ (ROI)ã€‚

```python
# ç¯„ä¾‹ï¼šè¨ˆç®—å…©å¼µå½±åƒä¹‹é–“çš„ç›¸ä¼¼åº¦
from sklearn.metrics.pairwise import cosine_similarity

# å‡è¨­ features1 å’Œ features2 æ˜¯æå–çš„å‘é‡
similarity = cosine_similarity(features1, features2)
print("ç›¸ä¼¼åº¦åˆ†æ•¸:", similarity[0][0])
```

---

## **é—œéµè¦é»**
1. **CNN**ï¼ˆä¾‹å¦‚ ResNetï¼‰ä½¿ç”¨**å·ç©å±¤**ä¾†æå–å±¤æ¬¡åŒ–ç‰¹å¾µã€‚
2. **ViT**ï¼ˆä¾‹å¦‚ Google çš„ ViTï¼‰å°‡å½±åƒè¦–ç‚º**åœ–å¡Šåºåˆ—**ä¸¦ä½¿ç”¨**è‡ªæ³¨æ„åŠ›æ©Ÿåˆ¶**ã€‚
3. å…©è€…éƒ½å°‡å½±åƒè½‰æ›ç‚º**ç‰¹å¾µå‘é‡**ä»¥ä¾›å¾ŒçºŒä»»å‹™ä½¿ç”¨ã€‚

éœ€è¦é—œæ–¼**å¾é ­è¨“ç·´è‡ªè¨‚ CNN/ViT** çš„ç¯„ä¾‹å—ï¼Ÿå‘Šè¨´æˆ‘å§ï¼ğŸš€