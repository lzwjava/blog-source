---
audio: true
lang: hant
layout: post
title: MMLU 評估
translated: true
---

## 序言

這篇文章評估了一個語言模型，使用MMLU（Massive Multitask Language Understanding）基準來評估其性能。

MMLU基準是一個全面的測試，旨在評估模型在廣泛範圍的科目中執行各種任務的能力。它包括多選問題，涵蓋了數學、歷史、法律和醫學等多個領域。

**數據集鏈接：**

*   [Papers with Code](https://paperswithcode.com/dataset/mmlu)
*   [Hugging Face Datasets](https://huggingface.co/datasets/cais/mmlu)

## llama-server

要運行llama-server：

```bash
build/bin/llama-server -m models/7B/mistral-7b-instruct-v0.2.Q4_K_M.gguf --port 8080
```

## MMLU 基準

這個腳本使用三個不同的後端（`ollama`、`llama-server`和`deepseek`）來評估MMLU基準。

要運行MMLU基準的代碼：

```python
import torch
from datasets import load_dataset
import requests
import json
from tqdm import tqdm
import argparse
import os
from openai import OpenAI
from dotenv import load_dotenv
import time

load_dotenv()

# 設置參數解析
parser = argparse.ArgumentParser(description="使用不同後端評估MMLU數據集。")
parser.add_argument("--type", type=str, default="ollama", choices=["ollama", "llama", "deepseek", "gemini", "deepseek-r1"], help="後端類型：ollama, llama, deepseek或gemini")
args = parser.parse_args()

# 加載MMLU數據集
subject = "college_computer_science"  # 選擇你的科目
dataset = load_dataset("cais/mmlu", subject, split="test")

# 格式化提示，不包含少量示例
def format_mmlu_prompt(example):
    prompt = "以下是關於{}的多選問題。請只用正確選項的字母（A, B, C或D）回答。".format(subject.replace("_", " "))
    prompt += " 本問題僅需回答，無需解釋。"

    # 添加當前問題
    prompt += f"問題: {example['question']}\n"
    prompt += "選項:\nA. {}\nB. {}\nC. {}\nD. {}\n".format(*example['choices'])
    return prompt

# 初始化DeepSeek客戶端（如果需要）
def initialize_deepseek_client():
    api_key = os.environ.get("DEEPSEEK_API_KEY")
    if not api_key:
        print("錯誤：未設置DEEPSEEK_API_KEY環境變量。")
        exit()
    return OpenAI(api_key=api_key, base_url="https://api.deepseek.com")

def call_gemini_api(prompt, retries=3, backoff_factor=1):
    gemini_api_key = os.environ.get("GEMINI_API_KEY")
    if not gemini_api_key:
        print("錯誤：未設置GEMINI_API_KEY環境變量。")
        exit()
    url = f"https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash:generateContent"
    params = {"key": gemini_api_key}
    payload = {"contents": [{"parts": [{"text": prompt}]}]}

    for attempt in range(retries):
        response = requests.post(url, json=payload, params=params)
        if response.status_code != 429:
            return response.json()
        time.sleep(backoff_factor * (2 ** attempt))  # 指數退避
    return None

import re

def process_ollama_response(response):
    if response.status_code == 200:
        output_text = response.json()["choices"][0]["message"]["content"]
        match = re.search(r"Answer:\s*([A-D])", output_text, re.IGNORECASE)
        if match:
            predicted_answer = match.group(1).upper()
        else:
            predicted_answer = output_text.strip()[0] if len(output_text.strip()) > 0 else ""
        print(f"API輸出: {output_text}")
        return predicted_answer
    else:
        print(f"錯誤: {response.status_code} - {response.text}")
        return ""

def process_llama_response(response):
    if response.status_code == 200:
        output_text = response.json()["choices"][0]["message"]["content"]
        predicted_answer = output_text.strip()[0] if len(output_text.strip()) > 0 else ""
        print(f"API輸出: {output_text}")
        return predicted_answer
    else:
        print(f"錯誤: {response.status_code} - {response.text}")
        return ""

def process_deepseek_response(client, prompt):
    try:
        response = client.chat.completions.create(
            model="deepseek-chat",
            messages=[
                {"role": "user", "content": prompt}
            ],
            max_tokens=100
        )
        if response and response.choices:
            output_text = response.choices[0].message.content.strip()
            predicted_answer = output_text.strip()[0] if len(output_text.strip()) > 0 else ""
            print(f"API輸出: {output_text}")
            return predicted_answer
        else:
            print("錯誤：API沒有返回任何回應。")
            return ""
    except Exception as e:
        print(f"API調用過程中發生錯誤: {e}")
        return ""

def process_deepseek_r1_response(client, prompt, retries=3, backoff_factor=1):
    for attempt in range(retries):
        try:
            response = client.chat.completions.create(
                model="deepseek-reasoner",
                messages=[
                    {"role": "user", "content": prompt}
                ],
                max_tokens=100
            )
            if response and response.choices:
                output_text = response.choices[0].message.content.strip()
                predicted_answer = output_text.strip()[0] if len(output_text.strip()) > 0 else ""
                print(f"API輸出: {output_text}")
                return predicted_answer
            else:
                print("錯誤：API沒有返回任何回應。")
                return ""
        except Exception as e:
            if "502" in str(e):
                print(f"API調用過程中發生502錯誤，重試中，{backoff_factor * (2 ** attempt)}秒後重試...")
                time.sleep(backoff_factor * (2 ** attempt))
            else:
                print(f"API調用過程中發生錯誤: {e}")
                return ""
    print("重試次數達到上限，返回空回應。")
    return ""

def process_gemini_response(prompt):
    json_response = call_gemini_api(prompt)
    if not json_response:
        print("在重試後，Gemini API沒有回應。")
        return ""
    if 'candidates' not in json_response or not json_response['candidates']:
        print("沒有在回應中找到候選答案，重試...")
        json_response = call_gemini_api(prompt)
        if not json_response or 'candidates' not in json_response or not json_response['candidates']:
            print("在重試後，沒有在回應中找到候選答案。")
            return ""

    first_candidate = json_response['candidates'][0]
    if 'content' in first_candidate and 'parts' in first_candidate['content']:
        first_part = first_candidate['content']['parts'][0]
        if 'text' in first_part:
            output_text = first_part['text']
            predicted_answer = output_text.strip()[0] if len(output_text.strip()) > 0 else ""
            print(f"API輸出: {output_text}")
            return predicted_answer
        else:
            print("沒有在回應中找到文本")
            return ""
    else:
        print("回應格式不符：缺少內容或部分")
        return ""


def evaluate_model(args, dataset):
    correct = 0
    total = 0
    client = None
    if args.type == "deepseek" or args.type == "deepseek-r1":
        client = initialize_deepseek_client()

    for i, example in tqdm(enumerate(dataset), total=len(dataset), desc="評估"):
        prompt = format_mmlu_prompt(example)
        predicted_answer = ""

        if args.type == "ollama":
            url = "http://localhost:11434/v1/chat/completions"
            data = {
                "messages": [{"role": "user", "content": prompt}],
                "model": "mistral:7b"
            }
            headers = {"Content-Type": "application/json"}
            print(f"API輸入: {data}")
            response = requests.post(url, headers=headers, data=json.dumps(data))
            predicted_answer = process_ollama_response(response)

        elif args.type == "llama":
            url = "http://localhost:8080/v1/chat/completions"
            data = {
                "messages": [{"role": "user", "content": prompt}]
            }
            headers = {"Content-Type": "application/json"}
            print(f"API輸入: {data}")
            response = requests.post(url, headers=headers, data=json.dumps(data))
            predicted_answer = process_llama_response(response)

        elif args.type == "deepseek":
            predicted_answer = process_deepseek_response(client, prompt)

        elif args.type == "deepseek-r1":
            predicted_answer = process_deepseek_r1_response(client, prompt)

        elif args.type == "gemini":
            predicted_answer = process_gemini_response(prompt)
        else:
            raise ValueError("無效的後端類型")

        answer_map = {0: "A", 1: "B", 2: "C", 3: "D"}
        ground_truth_answer = answer_map.get(example["answer"], "")
        is_correct = predicted_answer.upper() == ground_truth_answer
        if is_correct:
            correct += 1
        total += 1

        print(f"問題: {example['question']}")
        print(f"選項: A. {example['choices'][0]}, B. {example['choices'][1]}, C. {example['choices'][2]}, D. {example['choices'][3]}")
        print(f"預測答案: {predicted_answer}，真實答案: {ground_truth_answer}，正確: {is_correct}")
        print("-" * 30)

        if (i+1) % 10 == 0:
            accuracy = correct / total
            print(f"處理了{i+1}/{len(dataset)}個數據。當前準確率: {accuracy:.2%} ({correct}/{total})")

    return correct, total

# 評估循環
correct, total = evaluate_model(args, dataset)

# 計算準確率
accuracy = correct / total
print(f"科目: {subject}")
print(f"準確率: {accuracy:.2%} ({correct}/{total})")
```

## 結果

### 零次示例評估

| 模型                         | 方式                          | 科目                          | 準確率       |
|--------------------------------|--------------------------------|--------------------------------|--------------|
| mistral-7b-instruct-v0.2, Q4_K_M | macOS m2, 16GB, llama-server   | MMLU college_computer_science | 40.00% (40/100) |
| Mistral-7B-Instruct-v0.3, Q4_0  | macOS m2, 16GB, ollama         | MMLU college_computer_science | 40.00% (40/100) |
| deepseek v3 (API)               | API, 2025.1.25                 | MMLU college_computer_science | 78.00% (78/100)|
| gemini-1.5-flash (API)          | API, 2025.1.25                 | MMLU college_computer_science | 72.00% (72/100)|
| deepseek r1 (API)               | API, 2025.1.26                 | MMLU college_computer_science | 87.14% (61/70)|
| Mistral Small Latest (API)      | API, 2025.01.31                | MMLU college_computer_science | 65.00% (65/100)|
| Mistral Large Latest (API)      | API, 2025.01.31                | MMLU college_computer_science | 73.00% (73/100)|
| Mistral Small 2501 (API)         | API, 2025.01.31                | MMLU college_computer_science | 66.00% (66/100)|