---
audio: false
generated: true
image: false
lang: hant
layout: post
model: x-ai/grok-4-fast
title: nanoGPT CPU 訓練配置詳解
translated: true
type: note
---

### 指令解析

這個完整指令使用 Python 3.11 執行 nanoGPT 儲存庫中的 `train.py` 腳本，載入莎士比亞字符級數據集的預設配置，並覆寫特定超參數以進行較小規模的 CPU 訓練運行。以下將逐步解釋每個組件和參數。

- **python3.11**：指定用於執行腳本的 Python 直譯器版本（3.11）。nanoGPT 要求 Python 3.8+；這確保了與近期功能的相容性。

- **train.py**：nanoGPT 中的主要訓練腳本。負責處理數據載入、模型初始化、訓練循環（前向/反向傳播、優化）、評估、日誌記錄和檢查點保存。

- **config/train_shakespeare_char.py**：一個配置檔案，設定數據集特定的預設值（例如 `dataset = 'shakespeare_char'`、`vocab_size = 65`、初始學習率等）。它定義了任務：在莎士比亞作品的字符級文本上進行訓練。所有後續的 `--` 標誌都會覆寫此配置中的值。

#### 覆寫參數
這些是通過 argparse 傳遞給 `train.py` 的命令列標誌，無需編輯檔案即可自定義設定。它們控制硬體、訓練行為、模型架構和正則化。

| 參數 | 值 | 解釋 |
|-----------|-------|-------------|
| `--device` | `cpu` | 指定運算裝置：`'cpu'` 表示所有運算在主機 CPU 上執行（速度較慢但無需 GPU）。預設為 `'cuda'`（如果 GPU 可用）。適用於測試或低資源環境。 |
| `--compile` | `False` | 啟用/停用 PyTorch 的 `torch.compile()` 模型優化功能（PyTorch 2.0 引入，通過圖編譯加速執行）。設為 `False` 可避免相容性問題（例如在舊硬體或非 CUDA 裝置上）。預設為 `True`。 |
| `--eval_iters` | `20` | 評估期間用於估計驗證損失的前向傳遞（迭代）次數。較高值可提供更準確的估計但耗時較長。預設為 200；此處降低以加快檢查速度。 |
| `--log_interval` | `1` | 將訓練損失列印到控制台的頻率（以迭代次數計）。設為 1 表示每個步驟都輸出詳細資訊；預設為 10 以減少輸出干擾。 |
| `--block_size` | `64` | 模型單次處理的最大上下文長度（序列長度）。影響記憶體使用量以及模型能「記住」的歷史內容量。配置中預設為 256；設為 64 可在有限硬體上加快訓練速度。 |
| `--batch_size` | `12` | 每個訓練步驟中並行處理的序列數量（批次大小）。較大的批次使用更多記憶體，但可通過更好的 GPU/CPU 利用率加速訓練。預設為 64；設為 12 是為 CPU 環境縮減規模。 |
| `--n_layer` | `4` | Transformer 解碼器層的數量（網絡深度）。更多層數會增加模型容量但可能過度擬合且需要更多運算資源。預設為 6；設為 4 可建立更小的模型。 |
| `--n_head` | `4` | 每層的多頭注意力頭數量。控制注意力計算的並行度；必須能被 `n_embd` 整除。預設為 6；設為 4 可降低複雜度。 |
| `--n_embd` | `128` | 模型嵌入和隱藏狀態的維度（模型寬度）。較大值會增加表達能力但需要更多記憶體/運算資源。預設為 384；設為 128 可建立非常小的模型（約 0.5M 參數）。 |
| `--max_iters` | `2000` | 訓練迭代（步驟）的總次數。每次迭代處理一個批次。達到此次數後訓練停止（除非提前終止）。預設為 5000；設為 2000 可縮短運行時間以進行實驗。 |
| `--lr_decay_iters` | `2000` | 學習率（LR）線性衰減至零的迭代次數（餘弦或線性衰減計劃）。此處與 `max_iters` 匹配，使學習率在訓練結束時完全衰減。預設為 5000；將衰減與總步數綁定。 |
| `--dropout` | `0.0` | 訓練期間隨機將單元歸零的正則化概率（dropout 概率）。設為 0.0 表示完全停用（無正則化），對於小數據集/模型很有用。預設為 0.2。 |

此設定在 CPU 上訓練一個微型 GPT-2 風格模型（4 層，128 維），在莎士比亞文本上運行 2000 步，並具有頻繁的日誌記錄且無 dropout——非常適合快速原型設計或除錯。

[nanoGPT 儲存庫](https://github.com/karpathy/nanoGPT)