---
audio: false
generated: true
image: false
lang: hant
layout: post
model: x-ai/grok-4-fast
title: 高效大型語言模型的4位元量化
translated: true
type: note
---

### 什麼是 4-bit 量化（在 QLoRA 或 GPTQ 情境下）？

4-bit 量化是一種技術，透過僅使用 4 位元來儲存每個權重，而非通常的 16 位元（FP16/BF16）或 32 位元（FP32）精度，從而大幅減少大型語言模型（LLM）的記憶體佔用。

正常（全精度）模型：
- FP32：每個參數 4 位元組 → 70 億參數模型 ≈ 28 GB
- FP16/BF16：每個參數 2 位元組 → 70 億參數模型 ≈ 14 GB

4-bit 量化：
- 每個參數約 0.5 位元組 → 70 億參數模型 ≈ 3.5–4 GB（約為原始大小的 1/4 到 1/8，具體取決於所用方法）

目前常見的兩種主要 4-bit 方法如下：

| 方法    | 全名                       | 主要論文 / 年份     | 典型使用場景                      | 主要特性                                                                               |
|---------|----------------------------|---------------------|-----------------------------------|---------------------------------------------------------------------------------------|
| GPTQ    | GPTQ                       | 2023                | 訓練後量化（僅推論）              | 一次性、非常精確、無需重新訓練。在訓練後將權重四捨五入為 4-bit。                        |
| QLoRA   | Quantized Low-Rank Adaptation | 2023（6月）       | 高效微調 / 指令調校               | 結合 4-bit 儲存 + LoRA 適配器 + 分頁優化器。允許在單個 24–48 GB GPU 上微調 650 億以上參數模型。 |

#### QLoRA 詳解（人們通常所說的「4-bit QLoRA」）
QLoRA 同時實現了四項巧妙的設計：

1. 4-bit NormalFloat（NF4）量化
   - 一種特殊的 4-bit 資料類型，針對常態分佈的權重進行了優化（大多數 LLM 權重在訓練後近似高斯分佈）。
   - 優於普通的 INT4；理論上對常態分佈資料是最佳的。

2. 雙重量化
   - 連量化常數（縮放因子）也從 FP16 量化為 8-bit，節省了更多 MB 級記憶體。

3. 分頁優化器
   - 優化器狀態（AdamW 動量）儲存在 CPU RAM 中，並透過 NVIDIA 統一記憶體分頁傳輸到 GPU。防止訓練期間記憶體不足。

4. LoRA 適配器
   - 僅訓練小型低秩矩陣（r=64 或更小），而基礎 4-bit 模型保持凍結。

結果：您可以在單張 48 GB RTX A6000 上完全微調 650 億參數的 Llama/Mistral 模型，甚至可以在單張 80 GB A100 上微調 700 億參數模型，而正常的全參數微調則需要 8 張或更多 A100。

#### GPTQ（專注於推論的方法）
- 在訓練完成後進行。
- 使用二階（Hessian）資訊來最小化將權重壓縮至 4-bit 時的捨入誤差。
- 極度精確 — 通常與 FP16 相比，困惑度下降 <0.1。
- 在 AutoGPTQ、ExLlama、vLLM 和 llama.cpp（GGUF 也有 GPTQ 風格模式）等工具中廣受歡迎。

### 梯度檢查點（也稱為「啟動檢查點」）

一種完全獨立的記憶體節省技術，用於訓練期間。

運作原理：
- 通常，反向傳播需要儲存前向傳遞中的所有中間啟動值 → 記憶體使用隨模型深度/批次大小/序列長度線性增長。
- 梯度檢查點以計算換取記憶體：
  - 在前向傳遞期間，僅保存少數「檢查點」啟動值。
  - 在反向傳遞期間，它即時重新計算缺失的啟動值。

典型的記憶體節省：節省 50–80% 的啟動記憶體，代價是訓練速度減慢約 20–30%。

它幾乎總是與 QLoRA 或任何大型模型微調一起使用，因為即使使用 4-bit 權重，啟動值仍然會耗盡 GPU 記憶體。

### 總結

「4-bit 量化（QLoRA 風格或 GPTQ）+ 梯度檢查點」是現代實現以下目標的配方：

- 在消費級或單一企業級 GPU 上執行或微調巨型模型（330 億、700 億、4050 億參數）。
- QLoRA 風格的 4-bit → 以極低 GPU 記憶體進行微調。
- GPTQ 風格的 4-bit → 以極低 GPU 記憶體進行快速推論。
- 梯度檢查點 → 使得在長序列上進行實際反向傳遞而不會記憶體不足成為可能。

正是這些技術的結合，使得人們今天能夠在單張 3090/4090 上微調 Llama-3-700 億參數模型。