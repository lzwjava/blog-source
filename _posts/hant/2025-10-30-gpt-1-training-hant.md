---
audio: false
generated: true
image: false
lang: hant
layout: post
model: x-ai/grok-4-fast
title: GPT-1 架構與訓練概述
translated: true
type: note
---

### GPT-1 架構概述

GPT-1（Generative Pre-trained Transformer 1）是 OpenAI 於 2018 年推出的基礎模型。它開創了先透過語言建模目標在未標記文本上預訓練大型語言模型，再針對下游任務進行微調的創新方法。該架構採用**僅解碼器的 Transformer**，改編自原始 Transformer 論文（Vaswani 等人，2017），但精簡為僅保留解碼器堆疊以實現自迴歸生成。這種設計使模型能夠預測序列中的下一個 token，使其適用於涉及連續文本的任務。

與 BERT 等雙向模型不同，GPT-1 使用**遮蔽自注意力機制**來確保因果關係——每個位置僅能關注先前位置，防止未來 token 的資訊洩漏。

### 關鍵組件與超參數

- **模型類型**：多層 Transformer 解碼器，配備遮蔽多頭自注意力機制及位置前饋網絡。
- **層數**：12 個 Transformer 區塊（層）。
- **注意力機制**：每層 12 個注意力頭，每個頭處理 64 維狀態（總模型維度：768）。
- **嵌入維度**：
  - 隱藏層大小（d_model）：768。
  - 前饋網絡內部維度（d_ff）：3072（隱藏層大小的 4 倍，為 Transformer 標準配置）。
- **位置編碼**：學習式位置嵌入，與 token 嵌入相加（未使用正弦編碼）。
- **激活函數**：前饋網絡中使用高斯誤差線性單元（GELU）。
- **詞彙與分詞**：位元組對編碼（BPE），包含 40,000 個合併操作，基於語料庫訓練。
- **總參數量**：約 1.17 億。
- **序列長度**：訓練時使用 512 個 token 的序列。
- **正則化**：
  - 丟棄法：在殘差連接、嵌入層及注意力層應用 0.1 的丟棄率。
  - 權重衰減：對非偏置/非層歸一化權重應用修正的 L2 正則化（0.01）。
- **初始化**：權重從常態分佈 N(0, 0.02) 初始化。

### 訓練細節

- **預訓練階段**：
  - **資料集**：BooksCorpus，包含約 7,000 本未出版書籍（總計約 8 億詞），涵蓋奇幻、浪漫、冒險等多種類型。文本經過清理（例如使用 ftfy 庫）並透過 spaCy 進行分詞。
  - **目標**：無監督語言建模（下一個 token 預測）。
  - **優化器**：Adam，參數設為 β1=0.9、β2=0.999、ε=1e-8。
  - **學習率調度**：線性暖身 2,000 次更新至最高 2.5×10⁻⁴，隨後以餘弦退火降至 0。
  - **批次大小**：每個小批次 64 個序列。
  - **訓練週期**：100 個 epoch。
  - **效能指標**：在訓練語料上達到 18.4 的困惑度。
- **微調階段**：
  - 最小化變更：添加線性輸出層以進行任務特定預測，並使用分隔符 token（如 <s>、<e>、$）將輸入結構化為單一序列。
  - 微調期間加權 0.5 的輔助語言建模損失。
  - 超參數：較低學習率（6.25×10⁻⁵）、批次大小 32、3 個訓練週期，配合 0.2% 暖身及線性衰減。
  - 這種遷移學習方法使模型在文本蘊涵和問答等任務上表現出色，無需任務特定架構。

### 重要創新與限制

GPT-1 證明了單一預訓練模型可透過簡單的輸入格式調整，有效遷移至多種 NLP 任務，表現超越當時部分經過任務調優的模型。然而，與現代大型語言模型（如 GPT-3 的 1,750 億參數）相比，其規模相對較小，且僅支援 512 個 token 的上下文長度。該架構為整個 GPT 系列奠定了基礎，強調了預訓練的可擴展性。

[Improving Language Understanding by Generative Pre-Training](https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf)  
[GPT-1 - Wikipedia](https://en.wikipedia.org/wiki/GPT-1)