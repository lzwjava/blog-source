---
audio: false
generated: true
lang: hant
layout: post
title: 長短期記憶網絡
translated: true
type: note
---

長短期記憶（LSTM）網絡是一種循環神經網絡架構，專門設計用於解決梯度消失問題，並能學習序列數據中的長期依賴關係。其實現方式是透過一種更精密的內部結構，稱為 **LSTM 單元**。

以下是 LSTM 單元運作原理的解析：

**核心概念：細胞狀態**

LSTM 的核心概念是 **細胞狀態**（通常表示為 'C<sub>t</sub>'）。可以將細胞狀態想像成一條貫穿整個序列的傳送帶。它承載著與序列長期歷史相關的資訊。資訊可以透過稱為 **門** 的結構，在流經網絡時被添加到細胞狀態或從中移除。

**門控機制**

LSTM 單元具有三個主要門控，用於調節資訊流動：

1.  **遺忘門：** 此門決定應丟棄先前細胞狀態中的哪些資訊。
    * 它接收先前的隱藏狀態 (h<sub>t-1</sub>) 和當前輸入 (x<sub>t</sub>)。
    * 這些資訊通過一個神經網絡層，接著是 **sigmoid 激活函數**。
    * sigmoid 函數輸出介於 0 到 1 之間的值。接近 0 表示「完全忘記此資訊」，而接近 1 表示「完全保留此資訊」。
    * 數學上，遺忘門的輸出 (f<sub>t</sub>) 計算如下：
        ```
        f_t = σ(W_f * [h_{t-1}, x_t] + b_f)
        ```
        其中：
        * σ 是 sigmoid 函數。
        * W<sub>f</sub> 是遺忘門的權重矩陣。
        * [h<sub>t-1</sub>, x_t] 是先前隱藏狀態與當前輸入的串接。
        * b<sub>f</sub> 是遺忘門的偏置向量。

2.  **輸入門：** 此門決定應將當前輸入中的哪些新資訊添加到細胞狀態。此過程包含兩個步驟：
    * **輸入門層：** 一個 sigmoid 層決定我們將更新哪些值。
        ```
        i_t = σ(W_i * [h_{t-1}, x_t] + b_i)
        ```
        其中：
        * σ 是 sigmoid 函數。
        * W<sub>i</sub> 是輸入門的權重矩陣。
        * [h<sub>t-1</sub>, x_t] 是先前隱藏狀態與當前輸入的串接。
        * b<sub>i</sub> 是輸入門的偏置向量。
    * **候選值層：** 一個 tanh 層創建一個新的候選值向量（候選細胞狀態，表示為 'C̃<sub>t</sub>'），這些值可能會被添加到細胞狀態中。tanh 函數輸出介於 -1 到 1 之間的值，有助於調節網絡。
        ```
        C̃_t = tanh(W_C * [h_{t-1}, x_t] + b_C)
        ```
        其中：
        * tanh 是雙曲正切函數。
        * W<sub>C</sub> 是候選細胞狀態的權重矩陣。
        * [h<sub>t-1</sub>, x_t] 是先前隱藏狀態與當前輸入的串接。
        * b<sub>C</sub> 是候選細胞狀態的偏置向量。

3.  **輸出門：** 此門決定應將當前細胞狀態中的哪些資訊輸出，作為當前時間步的隱藏狀態。
    * 它接收先前的隱藏狀態 (h<sub>t-1</sub>) 和當前輸入 (x<sub>t</sub>)。
    * 這些資訊通過一個神經網絡層，接著是 **sigmoid 激活函數**，以決定要輸出細胞狀態的哪些部分。
        ```
        o_t = σ(W_o * [h_{t-1}, x_t] + b_o)
        ```
        其中：
        * σ 是 sigmoid 函數。
        * W<sub>o</sub> 是輸出門的權重矩陣。
        * [h<sub>t-1</sub>, x_t] 是先前隱藏狀態與當前輸入的串接。
        * b<sub>o</sub> 是輸出門的偏置向量。
    * 接著，細胞狀態通過 **tanh 函數**，將其值壓縮到 -1 到 1 之間。
    * 最後，sigmoid 門的輸出與應用於細胞狀態的 tanh 函數輸出進行逐元素相乘。這成為新的隱藏狀態 (h<sub>t</sub>)，它會被傳遞到下一個時間步，並可用於進行預測。
        ```
        h_t = o_t * tanh(C_t)
        ```

**更新細胞狀態**

細胞狀態是根據遺忘門和輸入門的決策來更新的：

```
C_t = f_t * C_{t-1} + i_t * C̃_t
```

* 遺忘門 (f<sub>t</sub>) 決定要保留多少先前的細胞狀態 (C<sub>t-1</sub>)。如果 f<sub>t</sub> 接近 0，則資訊大部分被遺忘。如果接近 1，則資訊大部分被保留。
* 輸入門 (i<sub>t</sub>) 決定應將多少新的候選值 (C̃<sub>t</sub>) 添加到細胞狀態中。如果 i<sub>t</sub> 接近 0，則新資訊大部分被忽略。如果接近 1，則新資訊大部分被添加。

**LSTM 如何解決梯度消失問題：**

LSTM 能夠處理長期依賴關係的關鍵在於細胞狀態和門控機制。細胞狀態充當資訊跨越多個時間步的直接通路，而無需反覆乘以權重，這正是傳統 RNN 中梯度消失的主要原因。

這些門控讓 LSTM 能夠學習何時儲存、讀取和忘記細胞狀態中的資訊。這種選擇性記憶機制使網絡能夠在長序列中保留相關資訊，並在需要時使用它，從而有效緩解梯度消失問題，並實現對長期依賴關係的學習。

本質上，LSTM 為網絡提供了一種在較長時間內維持「記憶」的方法，這使得它們在處理具有長期依賴關係的序列數據任務時非常有效。