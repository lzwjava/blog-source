---
audio: false
generated: true
image: false
lang: hant
layout: post
model: x-ai/grok-4-fast
title: 梯度提升機器介紹
translated: true
type: note
---

### 概述
Jerome H. Friedman 於 2001 年發表的論文《貪婪函數逼近：梯度提升機器》提出了梯度提升機器（GBMs），這是一種強大的集成學習方法，適用於回歸和分類等監督任務。該方法將提升框架視為函數空間中的梯度下降過程，通過順序添加簡單的「弱」學習器（通常是決策樹）來構建加法模型，以最小化指定的損失函數。這種方法推廣了早期的提升算法（例如 AdaBoost），並強調在函數空間中進行貪婪優化，從而產生高精度、穩健且可解釋的模型。

### 摘要（改寫）
GBMs 通過以順序加法方式組合弱學習器來逼近可微損失函數的最小值，從而構建靈活的預測模型。使用回歸樹作為基礎學習器，可產生具有競爭力且穩健的回歸和分類程序。實證測試表明，該方法在多種數據集上均表現出低錯誤率，勝過多元自適應回歸樣條（MARS）等替代方法。

### 關鍵方法
核心思想是迭代地將新學習器擬合到損失函數相對於當前模型預測的*負梯度*（偽殘差），模擬函數空間中的梯度下降。

- **模型結構**：最終模型為 \\( F_M(x) = \sum_{m=1}^M \beta_m h_m(x) \\)，其中每個 \\( h_m(x) \\) 是一個弱學習器（例如小型回歸樹）。
- **更新規則**：在迭代 \\( m \\) 時，計算殘差 \\( r_{im} = -\left[ \frac{\partial L(y_i, F(x_i))}{\partial F(x_i)} \right]_{F=F_{m-1}} \\)，然後通過最小二乘法將 \\( h_m \\) 擬合到這些殘差。步長 \\( \gamma_m \\) 通過線性搜索優化：\\( \gamma_m = \arg\min_\gamma \sum_i L(y_i, F_{m-1}(x_i) + \gamma h_m(x_i)) \\)。
- **收縮**：通過 \\( \nu \in (0,1] \\)（例如 \\( \nu = 0.1 \\)）縮放加法項，以減少過擬合並允許更多迭代。
- **隨機變體**：在每一步對數據進行子採樣（例如 50%），以加快訓練速度並提高泛化能力。
- **TreeBoost 算法**（偽代碼概述）：
  1. 將 \\( F_0(x) \\) 初始化為最小化損失的常數。
  2. 對於 \\( m = 1 \\) 到 \\( M \\)：
     - 計算偽殘差 \\( r_{im} \\)。
     - 將樹 \\( h_m \\) 擬合到 \\( \{ (x_i, r_{im}) \} \\)。
     - 通過線性搜索找到最優 \\( \gamma_m \\)。
     - 更新 \\( F_m(x) = F_{m-1}(x) + \nu \gamma_m h_m(x) \\)。
  3. 根據迭代次數或損失改進情況停止。

支持的損失函數包括：
- 最小二乘法（回歸）：\\( L(y, F) = \frac{1}{2}(y - F)^2 \\)，殘差 = \\( y - F \\)。
- 最小絕對偏差（穩健回歸）：\\( L(y, F) = |y - F| \\)。
- 對數似然（二元分類）：\\( L = -\sum [y \log p + (1-y) \log(1-p)] \\)，其中 \\( p = \frac{1}{1 + e^{-F}} \\)；殘差 = \\( y - p \\)。
- Huber 損失（抗異常值）。

像 LogitBoost 這樣的變體針對特定損失（例如二項偏差）進行了適應。

### 貢獻
- **統一框架**：通過梯度將提升擴展到任何可微損失，統一了 AdaBoost（指數損失）和 LogitBoost。
- **實用增強**：引入收縮和隨機子採樣，以更好地控制過擬合和計算。
- **靈活性**：適用於各種基礎學習器和任務（回歸、分類、排序）；樹能夠高效處理高維數據。
- **可解釋性工具**：討論了變量重要性（通過總雜質減少）和部分依賴圖。

### 主要發現
- **性能**：GBMs 在實證中表現卓越，在真實世界數據集（例如金融、物理模擬）上，相比基線方法（如單棵樹、裝袋法、神經網絡和 MARS），錯誤率降低了 10-50%。
- **穩健性**：隨機 GBM 收斂更快；更深的樹（6-8 層）可提高準確性，但需要正則化。LAD/Huber 損失比最小二乘法更能緩解異常值影響。
- **效率**：對於大數據處理快速；最優設置使用 \\( \nu = 0.1 \\)、1000+ 次迭代和子採樣。
- 圖表顯示，與替代方法相比，損失收斂迅速且方差/偏差低。

這篇論文為現代實現（如 XGBoost 和 LightGBM）奠定了基礎。

### 參考文獻
- [Greedy Function Approximation: A Gradient Boosting Machine (PDF)](https://jerryfriedman.su.domains/ftp/trebst.pdf)