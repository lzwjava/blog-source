---
audio: false
generated: true
lang: hant
layout: post
title: DeepSeek 2025 年推出的 128K 標記模型
translated: true
type: note
---

截至2025年6月，DeepSeek擁有多款具備驚人上下文長度的模型，通常採用混合專家架構來提升效率。以下是詳細分析：

* **DeepSeek-R1：** 這款於2025年1月發布的模型，以支援**128,000個詞元**的上下文長度著稱（部分基準測試中顯示為131,072詞元）。憑藉其強化學習後訓練技術，該模型在數學、編程與結構化思考方面表現尤為突出。
* **DeepSeek-V3：** 2024年12月發布的DeepSeek-V3及其基礎模型同樣支援**128,000詞元**的上下文長度。這是一款強大的混合專家模型，總參數量達6710億（每次查詢激活370億參數）。
* **DeepSeek-Coder-V2：** 這款專注於編程的混合專家模型於2024年7月發布，具備**128,000詞元**的重大上下文窗口，支援多達338種程式語言。
* **DeepSeek-V2：** 2024年5月推出的DeepSeek-V2擁有**128,000詞元**的上下文長度。此混合專家模型以經濟型訓練與高效推理為設計目標。
* **DeepSeek-V2.5：** 2024年9月發布的此模型融合通用與編程能力，同樣支援**128,000詞元**的上下文窗口。

**重要注意事項：**

* **原生上下文長度與API限制：** 雖然部分DeepSeek模型的原生上下文長度最高可達163,840詞元，但DeepSeek API可能對特定模型設有上下文窗口限制（例如「deepseek-chat」與「deepseek-reasoner」目前透過API僅提供64K上下文限制）。若需使用完整上下文長度，可能需要本地部署模型或選用支援更長上下文的API服務商。
* **效能衰減問題：** 與其他大型語言模型類似，儘管模型標榜極長的上下文窗口，當使用至最大極限長度時，其效能（準確性、推理能力）可能出現衰減。例如預計推出的DeepSeek R2，正是為了解決R1模型在128,000詞元極限上下文窗口中出現15%效能下降的問題。

總括而言，DeepSeek在長上下文LLM領域表現卓越，截至2025年6月已有數款模型提供128K詞元的上下文窗口，使其特別適合處理大量輸入資料與複雜任務。