---
audio: false
generated: true
image: false
lang: hant
layout: post
model: x-ai/grok-4-fast
title: 梯度提升入門
translated: true
type: note
---

### 什麼是梯度提升？

梯度提升是一種強大的集成機器學習技術，用於監督式學習任務，如回歸、分類和排序。它屬於提升演算法的一種，通過以序列方式組合多個弱學習器（通常是簡單的決策樹）來建立強大的預測模型。其核心思想是通過專注於先前模型所犯的錯誤（殘差）來迭代改進模型，從而有效「提升」整體性能。

#### 核心概念
梯度提升從本質上將學習過程視為一個優化問題。它使用**梯度下降**來最小化損失函數（例如，回歸的均方誤差或分類的對數損失）。序列中的每個新模型都被訓練來預測損失函數相對於當前集成預測的**負梯度**。這樣，演算法逐步「修正」先前模型的錯誤。

#### 運作原理：逐步解析
1. **初始化模型**：從一個簡單的基礎模型開始，通常是目標變量的平均值（用於回歸）或對數勝算（用於分類）。
   
2. **計算殘差（偽殘差）**：在每次迭代中，計算殘差——實際值與預測值之間的差異。這些代表了下一模型需要解決的「錯誤」。

3. **擬合弱學習器**：在這些殘差上訓練一個新的弱學習器（例如，淺層決策樹）。目標是預測所需修正的方向和幅度。

4. **更新集成**：將新的學習器添加到集成中，並按一個小的學習率（收縮參數，通常 <1）進行縮放，以防止過擬合。更新後的預測為：
   \\[
   F_m(x) = F_{m-1}(x) + \eta \cdot h_m(x)
   \\]
   其中 \\( F_m(x) \\) 是經過 \\( m \\) 次迭代後的集成，\\( \eta \\) 是學習率，而 \\( h_m(x) \\) 是新的弱學習器。

5. **重複**：迭代固定次數（或直到收斂），每次使用來自完整集成的最新殘差。

這個過程之所以稱為「梯度」，是因為殘差近似於損失函數的梯度，允許演算法在模型空間中執行一種函數梯度下降。

#### 主要優勢
- **高準確性**：通常在不需太多特徵工程的情況下，能夠捕捉複雜模式，從而勝過其他處理表格數據的演算法。
- **靈活性**：支援自定義損失函數和正則化（例如 L1/L2 懲罰）以處理過擬合。
- **高效性**：在如 XGBoost 等實現中可並行化，該實現優化了提升過程以實現速度和可擴展性。

#### 與 XGBoost 的關係
如您的查詢所述，XGBoost（極限梯度提升）是該框架的一個流行且經過優化的實現。它增加了諸如基於直方圖的分割以加速訓練和內建交叉驗證等增強功能，使其成為像 Kaggle 這類競賽的首選。

要深入了解，梯度提升是如 LightGBM 和 CatBoost 等變體的基礎，每個變體都調整了如樹生長或處理類別數據等方面的細節。

#### 參考資料
- [梯度提升機器：教程](https://www.stat.cmu.edu/~cshalizi/402/lectures/18-gbm.pdf)（卡內基梅隆大學講義）
- [XGBoost 文檔：提升樹簡介](https://xgboost.readthedocs.io/en/stable/tutorials/model.html)
- [理解梯度提升機器](https://towardsdatascience.com/understanding-gradient-boosting-machines-9be756fe76ab)（Towards Data Science 文章）