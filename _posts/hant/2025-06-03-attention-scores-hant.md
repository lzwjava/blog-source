---
audio: false
generated: true
lang: hant
layout: post
title: 穩定變壓器中的注意力分數
translated: true
type: note
---

在縮放點積注意力機制中（如公式 \\(\text{縮放分數} = \frac{Q K^T}{\sqrt{d_k}}\\) 所示），特意將查詢與鍵的點積除以鍵維度的平方根 \\(\sqrt{d_k}\\)，是為了穩定梯度並防止數值過大，尤其在 Transformer 架構中更顯重要。以下說明為何選擇使用 \\(\sqrt{d_k}\\) 而非直接使用 \\(d_k\\)：

1. **點積的方差**：
   - 點積 \\( Q K^T \\) 用於計算查詢向量 (\\( Q \\)) 與鍵向量 (\\( K \\)) 之間的相似度，每個向量的維度均為 \\( d_k \\)。若假設 \\( Q \\) 和 \\( K \\) 的元素相互獨立且均值為 0、方差為 1（常見於初始化或歸一化後），則單對查詢與鍵向量的點積 \\( Q_i \cdot K_j \\) 的方差為 \\( d_k \\)。這是因為 \\( d_k \\) 個獨立標準常態變量乘積之和的方差會隨 \\( d_k \\) 線性增長。
   - 若不進行縮放，\\( Q K^T \\) 的數值大小會隨 \\( d_k \\) 增大而增長（在 Transformer 中，\\( d_k \\) 可能為 64、128 或更大）。注意力分數過大會導致傳遞至 softmax 函數時出現問題。

2. **Softmax 穩定性**：
   - 注意力分數 \\( \frac{Q K^T}{\sqrt{d_k}} \\) 會輸入至 softmax 以計算注意力權重。若分數過大（未縮放或縮放不足時），softmax 函數會產生極尖銳的分佈，即某個元素主導（接近 1）而其他元素接近 0。這會導致多數元素的梯度消失，使模型難以有效學習。
   - 除以 \\(\sqrt{d_k}\\) 可確保縮放後分數的方差約為 1，使分數保持在 softmax 函數行為良好的範圍內，從而產生更平衡的注意力權重和穩定的梯度。

3. **為何不直接使用 \\( d_k \\)？**：
   - 若除以 \\( d_k \\) 而非 \\(\sqrt{d_k}\\)，會對點積進行過度縮放，將分數的方差降低至 \\( \frac{1}{d_k} \\)。當 \\( d_k \\) 較大時，這會使分數變得極小，導致 softmax 產生近乎均勻的分佈（因為 softmax 在輸入值很小時，輸出會接近 \\( \frac{1}{n} \\)）。這將削弱注意力機制聚焦相關鍵的能力，因為分數間的差異會被減弱。
   - 使用 \\( d_k \\) 過度縮放還可能在某些情況下導致數值不穩定，因為極小的數值在浮點運算中可能難以精確處理。

4. **為何選擇 \\(\sqrt{d_k}\\)？**：
   - 除以 \\(\sqrt{d_k}\\) 可將點積的方差歸一化至約為 1，因為 \\( \text{Var}\left(\frac{Q K^T}{\sqrt{d_k}}\right) = \frac{\text{Var}(Q K^T)}{d_k} = \frac{d_k}{d_k} = 1 \)。這使分數保持在合理範圍內，確保 softmax 產生有意義的注意力權重，既不過於尖銳也不過於均勻。
   - 平方根自然源自點積方差的數學特性。它在防止數值爆炸（未縮放時會發生）與避免過度抑制分數（使用 \\( d_k \\) 時會發生）之間取得了平衡。

5. **實證有效性**：
   - 使用 \\(\sqrt{d_k}\\) 的選擇最初在原始 Transformer 論文（Vaswani 等人於 2017 年發表的《Attention is All You Need》）中提出，並在多種基於 Transformer 的模型中獲得實證驗證。在實際應用中，該方法對各種 \\( d_k \\) 值均表現良好，確保了訓練的穩定性和模型性能。

總而言之，除以 \\(\sqrt{d_k}\\) 是基於數學原理的選擇，能穩定注意力分數的方差，確保 softmax 在良好狀態下運作，並避免梯度爆炸或消失的問題。若改為除以 \\( d_k \\)，則會過度降低分數，導致注意力權重的區分度下降，可能損害模型性能。