---
audio: false
generated: true
image: false
lang: hant
layout: post
model: x-ai/grok-4-fast
title: FineWeb NanoGPT é¡¯ç¤ºå¡è¨˜æ†¶é«”æŒ‡å—
translated: true
type: note
---

ä½ æ­£åœ¨ä½¿ç”¨æœ‰é™çš„GPUè¨˜æ†¶é«”ï¼ˆå¯èƒ½æ˜¯16-24 GBï¼Œä¾‹å¦‚RTX 3090/4090æˆ–A6000ï¼‰ä¾†è¨“ç·´ä¸€å€‹å°å‹GPTï¼ˆnanoGPTé¢¨æ ¼ï¼‰ï¼Œè³‡æ–™é›†æ˜¯**FineWeb**ï¼ˆç”¨æ–¼è¨“ç·´LLaMAã€Mistralç­‰çš„Common Crawlæ¸…ç†ç‰ˆæœ¬ï¼ŒåŒ…å«100Bå€‹è©å…ƒï¼‰ã€‚

ä½ ç›®å‰çš„é…ç½®å·²ç¶“ç›¸ç•¶ä¿å®ˆï¼Œé©åˆå–®å¼µæ¶ˆè²»ç´šGPUï¼Œä½†ä»¥ä¸‹æ˜¯é—œæ–¼**å¦‚ä½•ç‚ºnanoGPTåœ¨FineWebä¸Šé¸æ“‡æœ€ä½³åƒæ•¸**çš„è©³ç´°æŒ‡å—ï¼ŒæŒ‰é‡è¦æ€§å’Œè¨˜æ†¶é«”å½±éŸ¿æ’åºã€‚

### æŒ‰GPUè¨˜æ†¶é«”æ¨è–¦çš„é…ç½®

| GPU VRAM       | æ¨¡å‹å¤§å° (åƒæ•¸é‡) | n_layer | n_head | n_embd | batch_size | grad_acc | æœ‰æ•ˆæ‰¹æ¬¡å¤§å° | block_size | å‚™è¨» |
|----------------|---------------------|---------|--------|--------|------------|----------|--------------|------------|-------|
| 80 GB (A100/H100) | ~350M              | 12      | 12     | 768    | 64         | 8        | 512          | 1024       | æ¥è¿‘åŸå§‹GPT-2 mediumï¼Œè¨“ç·´æ•ˆæœè‰¯å¥½ |
| 48 GB (A6000/3090) | ~250â€“300M         | 12      | 12     | 672    | 32         | 16       | 512          | 1024       | éå¸¸å¥½çš„æŠ˜è¡·æ–¹æ¡ˆ |
| 24 GB (4090/3090) | ~160â€“200M          | 10      | 10     | 640    | 32         | 12â€“16    | 384â€“512      | 1024       | ä½ çš„æœ€ä½³é¸æ“‡ |
| 16â€“20 GB (4080, 3090 Ti) | ~125M        | 8       | 8      | 512    | 32         | 12       | 384          | 1024       | ç©©å®š |
| <16 GB (4070 Ti, ç­‰) | ~84M           | 6       | 6      | 384    | 16â€“32      | 16â€“24    | 256â€“512      | 512â€“1024   | ä½ ç›®å‰çš„é…ç½® |

ä½ ç›®å‰çš„é…ç½®ï¼ˆ`6L 6H 384embd`ï¼‰å¾ˆå®‰å…¨ï¼Œä½†æœ‰é»å°ã€‚ä½ å¯ä»¥å˜—è©¦æ›´å¤§çš„æ¨¡å‹ã€‚

### 24 GB GPU (RTX 4090 / 3090) åœ¨ FineWeb ä¸Šçš„æœ€ä½³é…ç½®
é€™æ˜¯ç›®å‰ nanoGPT ç¤¾ç¾¤ä¸­æœ€å—æ­¡è¿çš„è¨­å®šï¼š

```python
out_dir = 'out-fineweb-160M'
eval_interval = 1000
eval_iters = 200
log_interval = 100
always_save_checkpoint = True

wandb_log = True
wandb_project = 'fineweb'
wandb_run_name = '160M-fineweb'

dataset = 'fineweb'
gradient_accumulation_steps = 16   # 32 * 16 = 512 æœ‰æ•ˆæ‰¹æ¬¡å¤§å°
batch_size = 32
block_size = 1024                  # é‡è¦ï¼šFineWeb æ˜¯ä½¿ç”¨ 1024+ è¨“ç·´çš„

n_layer = 10
n_head = 10
n_embd = 640
dropout = 0.0                      # ä¹‹å¾Œå¯ä»¥å˜—è©¦ 0.1
learning_rate = 6e-4               # è¼ƒå°çš„æ¨¡å‹å¯ä»¥ä½¿ç”¨ç¨é«˜çš„å­¸ç¿’ç‡
max_iters = 50000                  # ç¸½å…± ~50â€“100B è©å…ƒæ˜¯ç†æƒ³çš„
warmup_iters = 2000
lr_decay_iters = 50000
min_lr = 6e-5
beta2 = 0.99
```

â†’ é€™å¤§ç´„æ˜¯ 160M åƒæ•¸ï¼Œåœ¨ 4090 ä¸Šé‹è¡Œé †æš¢ï¼ŒVRAM ä½¿ç”¨é‡ç´„ç‚º 20â€“22 GBã€‚

### æ›´å¥½ï¼š200M+ æ¨¡å‹ (å¦‚æœä½ æœ‰ 24 GB+ VRAM)
```python
n_layer = 12
n_head = 12
n_embd = 768    # â†’ ~350M åƒæ•¸ (åŸå§‹ GPT-2 medium å¤§å°)
batch_size = 32
gradient_accumulation_steps = 16   # æœ‰æ•ˆæ‰¹æ¬¡å¤§å° 512
block_size = 1024
learning_rate = 5e-4
max_iters = 60000
```
è¨±å¤šäººåœ¨å–®å¼µ 4090 ä¸ŠæˆåŠŸè¨“ç·´é€™å€‹é…ç½®ã€‚

### FineWeb + nanoGPT çš„é—œéµç¶“é©—æ³•å‰‡

1. **å¼·çƒˆæ¨è–¦ block_size = 1024**  
   FineWeb æ˜¯ä½¿ç”¨ 1024 ä¸Šä¸‹æ–‡é•·åº¦é€²è¡Œéæ¿¾å’Œè¨“ç·´çš„ã€‚ä½¿ç”¨ 512 æœƒå°å›°æƒ‘åº¦é€ æˆæ¯”ä½ æƒ³åƒä¸­æ›´å¤§çš„æå®³ã€‚

2. **æœ‰æ•ˆæ‰¹æ¬¡å¤§å° â‰ˆ 512 æ˜¯æœ€ä½³é»**  
   åŸå§‹ LLaMA ä½¿ç”¨ç´„ 4M è©å…ƒæ¯æ‰¹æ¬¡ã€‚å°æ–¼ nanoGPTï¼Œ512 åºåˆ— Ã— 1024 è©å…ƒ = ç´„ 0.5M è©å…ƒ/æ‰¹æ¬¡ â†’ ç¸®æ”¾å®šå¾‹è¶³å¤ æ¥è¿‘ã€‚

3. **å­¸ç¿’ç‡**  
   - ~100M åƒæ•¸ï¼š6eâ€“8e-4  
   - ~350M åƒæ•¸ï¼š5eâ€“6e-4  
   - ~770M åƒæ•¸ï¼š3e-4  

4. **è‡³å°‘è¨“ç·´ 50B è©å…ƒ (æœ€å¥½ 100B+)**  
   ä½¿ç”¨ 160M æ¨¡å‹ï¼š  
   - 50k è¿­ä»£ Ã— 32 Ã— 16 Ã— 1024 è©å…ƒ = ç´„ 83B è©å…ƒ  
   æ”¶æ–‚è‰¯å¥½ã€‚

5. **ä½¿ç”¨å®˜æ–¹çš„ FineWeb è³‡æ–™é›† (è€Œé openwebtext)**  
   ä½ çš„è…³æœ¬å·²ç¶“æ˜¯æ­£ç¢ºçš„ï¼š  
   FineWeb >> OpenWebText çš„å“è³ªã€‚  
   ä½¿ç”¨ 100B å­é›†ï¼š`fineweb100B` æˆ–å®Œæ•´çš„ `fineweb`ï¼ˆæ ¹æ“šä½ çš„ç¡¬ç¢Ÿç©ºé–“æˆªæ–·ï¼‰ã€‚

6. **å¯é¸ä½†æœ‰ç›Šçš„è¨­å®š**
```python
compile = True          # PyTorch 2.0 ç·¨è­¯ï¼Œé€Ÿåº¦æå‡ç´„ 20â€“30%
bias = False            # Linear å±¤ä¸­ç„¡ bias (é¡ä¼¼ LLaMA)
weight_decay = 0.1
```

### å¿«é€Ÿä¸‹è¼‰ FineWeb (100B ç‰ˆæœ¬) çš„å‘½ä»¤
```bash
pip install datasets
python -c "
from datasets import load_dataset
ds = load_dataset('HuggingFaceFW/fineweb', name='sample-100BT', split='train')
ds.save_to_disk('data/fineweb100B')
"
```
ç„¶å¾Œå°‡ nanoGPT æŒ‡å‘è©²è³‡æ–™å¤¾ã€‚

### ç¸½çµï¼šå‡ç´šä½ ç›®å‰çš„é…ç½®
å°‡ä½ ç›®å‰çš„æ¨¡å‹éƒ¨åˆ†æ›¿æ›ç‚ºä»¥ä¸‹å…§å®¹ï¼ˆé‡å° 24 GB GPUï¼‰ï¼š

```python
n_layer = 10
n_head = 10
n_embd = 640
batch_size = 32
gradient_accumulation_steps = 16
block_size = 1024
learning_rate = 6e-4
max_iters = 50000
```

ä½ å°‡ä»¥å¹¾ä¹é›¶é¡å¤–æˆæœ¬ç²å¾—ä¸€å€‹æ›´å¼·å¤§çš„æ¨¡å‹ã€‚

ä½¿ç”¨é€™äº›è¨­å®šï¼Œäººå€‘åœ¨ FineWeb ä¸Šç²å¾—äº†**éå¸¸å„ªç§€çš„ 160Mâ€“350M æ¨¡å‹**â€”â€”é€šå¸¸è¶…è¶ŠåŸå§‹ GPT-2 çš„è¡¨ç¾ã€‚

å‘Šè¨´æˆ‘ä½ ç¢ºåˆ‡çš„ GPU å‹è™Ÿï¼Œæˆ‘å¯ä»¥çµ¦ä½ æœ€å„ªçš„é…ç½®ï¼ğŸš€