---
audio: false
generated: true
image: false
lang: hant
layout: post
model: anthropic/claude-4.5-sonnet
title: DeepSeek稀疏注意力效率突破
translated: true
type: note
---

DeepSeek的Native Sparse Attention（NSA）代表了大語言模型高效長文本建模的重大突破。與具有二次計算複雜度的傳統全注意力機制不同，NSA透過精密的層次化稀疏注意力策略，在維持甚至超越模型效能的同時，智慧地降低計算成本。[1][2]

## 核心架構與設計理念

NSA解決了長文本建模的根本挑戰：標準注意力機制需要O(n²)計算量（n為序列長度），導致處理超過數千個標記的上下文時計算成本過高。**NSA採用動態層次化稀疏策略，結合粗粒度標記壓縮與細粒度標記選擇，同時保留全局上下文感知與局部精確度**[3]

該機制基於兩大關鍵原則運作：

1. **並非所有標記都需要同等關注** — 部分標記可被壓縮或摘要化
2. **硬體優化至關重要** — 若無法實現快速實際執行，演算法效率便毫無意義

## 三分支架構

NSA透過三個並行分支處理注意力計算，共同建立高效稀疏注意力模式：[4]

### 1. **壓縮分支**
此分支透過將連續標記分組為區塊並壓縮成代表性標記，處理粗粒度上下文聚合。壓縮機制透過建立標記群的摘要表徵，減少模型必須關注的標記數量。例如，32,768個標記的序列可能被壓縮至約2,046個壓縮標記。[5]

壓縮過程使用學習到的門控機制，決定如何將多個標記的資訊聚合為單一代表性標記，在保留全局上下文感知的同時避免完整計算負擔。

### 2. **選擇分支**
此分支透過動態識別最重要的待關注標記，實現細粒度標記選擇。模型並非關注所有標記，而是計算重要性分數，僅選擇性關注與當前查詢最相關的標記。這保留了局部精確度，並捕捉可能因單獨壓縮而遺失的關鍵細節。

選擇過程在訓練期間學習，使模型能自適應地決定不同上下文與任務中哪些標記具有最高資訊價值。[6]

### 3. **滑動窗口分支**
此分支透過讓每個標記關注固定窗口內的相鄰標記，維持局部上下文。這確保無論壓縮或選擇決策為何，短距離依賴關係始終能被捕捉。滑動窗口通常涵蓋定義半徑內的近期標記。

## 數學基礎

NSA中的注意力計算可表示為對三個不同鍵值集合的操作：

- 來自壓縮分支的**壓縮KV對**
- 來自選擇分支的**選擇KV對**  
- 來自滑動窗口的**本地KV對**

NSA並非對所有n個標記計算注意力，而是對結合這三個來源的極小有效集合進行計算。**透過整合層次化標記壓縮與區塊化標記選擇**[3]，該機制將二次複雜度降低至近似線性或近線性級別。

## 硬體對齊優化

NSA的關鍵創新在於其硬體感知設計。過往的稀疏注意力方法因未針對現代GPU架構優化，往往無法實現實際加速效果。[1]

NSA透過以下方式實現顯著加速：

### **區塊化記憶體存取模式**
演算法將資料組織成與GPU記憶體層級和Tensor Core操作對齊的區塊。這最大化合併記憶體載入，並實現GPU計算單元的高效運用。[3]

### **算術強度平衡**
演算法設計維持高算術強度——計算與記憶體存取的比例。這確保GPU保持計算限制而非記憶體限制，最大化硬體使用率。

### **融合核心實現**
NSA將多個操作合併為單一融合核心，消除冗餘KV快取傳輸與中間張量實體化。[5] 這大幅降低記憶體頻寬需求。

### **優化循環排程**
精密的核心層級優化消除冗餘記憶體操作，最大化暫存器重用。

## 效能提升

效率提升相當顯著：[7]

- 訓練期間**前向計算比FlashAttention-2快達9.0倍**
- **反向傳播快6.0倍** 
- 64k長度序列**解碼速度提升11.6倍**
- 在各項基準測試中**維持或超越全注意力效能**

對於較長序列，加速效果尤其顯著。對於64k標記序列，NSA實現約11.6倍解碼加速，因為其從記憶體載入的KV快取資料量大幅減少。[3]

## 原生可訓練性——關鍵進展

有別於許多僅加速推論的過往稀疏注意力方法，**NSA實現端到端訓練，在保持模型效能的同时減少預訓練計算量**[1]。稀疏模式在訓練期間學習，而非固定或基於啟發式方法。

這意味著：
- 模型學習哪些標記該壓縮、哪些該選擇
- 梯度透過稀疏注意力決策流動
- 壓縮與選擇策略適應特定任務與資料分佈

此原生可訓練性至關重要，因為它讓模型能發現最佳稀疏模式，而非依賴手動設計規則。

## 相較傳統注意力的優勢

**計算效率**：將二次複雜度降至近線性，實現100k+標記上下文的實際處理能力。

**記憶體效率**：大幅降低訓練與推論期間的KV快取記憶體需求。

**效能保持**：實驗結果顯示，NSA訓練的模型在通用基準測試、長文本任務與基於指令的推理中，均匹配或超越全注意力模型。[3]

**硬體加速**：有別於某些顯示理論增益但實際改善有限的稀疏方法，NSA在實際GPU硬體上實現顯著測量加速。

**自適應稀疏性**：學習到的注意力模式適應任務需求，而非使用固定模式。

## 技術實現細節

實現運用多項精密技術：

- 根據內容自適應壓縮比率的**動態層次化壓縮**
- 用於智能標記合併的**門控聚合機制**
- 使用學習重要性指標的**基於分數的標記選擇**
- 為GPU快取層級優化的**區塊對齊記憶體操作**
- 超越標準實現的**基於Triton的自訂核心**[8]

## 近期發展

DeepSeek近期發佈的DeepSeek-V3.2-Exp，實現了稱為DeepSeek Sparse Attention（DSA）的先進版本。此更新變體以對輸出品質的最小影響實現細粒度稀疏注意力，進一步提升長文本效能同時降低計算成本。[9]

## 結論

NSA透過同步優化演算法效率、硬體使用率與模型可訓練性，代表了注意力機制設計的典範轉移。透過結合層次化壓縮、動態選擇與滑動窗口，以及硬體對齊實現，它使長文本建模變得實用且高效。該機制證明，演算法與硬體優化的精密協同設計，能在不犧牲模型品質的前提下，於速度與記憶體效率方面帶來數量級改進。

引用文獻：
[1] https://arxiv.org/abs/2502.11089
[2] https://medium.com/data-science-in-your-pocket/deepseek-native-sparse-attention-advanced-attention-mechanism-for-llms-6ac68fc014ff
[3] https://arxiv.org/pdf/2502.11089
[4] https://shchegrikovich.substack.com/p/attention-vs-attention
[5] https://medium.com/@nanda.yugandhar/the-illustrated-guid-to-native-sparse-attention-b657b5e76bbc
[6] https://www.marktechpost.com/2025/02/18/deepseek-ai-introduces-nsa-a-hardware-aligned-and-natively-trainable-sparse-attention-mechanism-for-ultra-fast-long-context-training-and-inference/
[7] https://medium.com/foundation-models-deep-dive/deepseeks-nsa-for-efficient-attention-14b6f01486d5
[8] https://arxiv.org/html/2502.11089v1
[9] https://api-docs.deepseek.com/news/news250929