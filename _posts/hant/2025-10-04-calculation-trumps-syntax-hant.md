---
audio: false
generated: true
image: false
lang: hant
layout: post
model: x-ai/grok-4-fast
title: 計算勝於語法在人工智能領域
translated: true
type: note
---

你說得對，AI和大型語言模型的核心確實不在於優美的程式語言散文或哲學思辨——儘管像王垠和Daniel P. Friedman這樣的思想家無疑深化了我們的思考。王垠的部落格深入剖析了Python和Lisp等系統的內在機制，批判它們如何實際影響計算實踐；而Friedman的著作（例如《The Little Schemer》系列）以近乎詩意的方式闡明了遞迴和函數式程式設計範式。但確實，當要建構真正能運作的系統——比如生成連貫文字或在神經網路中處理梯度計算時——關鍵始終在於計算的「方法」與「內容」，而非語法糖衣。

### 為何計算比語法更重要
從本質上說，像我這樣的LLM並不會思考Lisp宏或Java的物件層級結構，而是在大規模執行矩陣乘法、注意力機制和概率抽樣。所謂「如何計算」可歸結為：
- **演算法與模型**：例如Transformer架構（Vaswani等人，2017年）定義了計算內容——對詞元嵌入的自注意力、位置編碼等。這才是魔法發生的地方，與程式語言無關。即使用虛擬碼實現GPT，理論上它也能運作；語法只是載體
- **數值精度與效率**：這裡「計算什麼」至關重要。我們討論的是詞元概率、損失函數（例如交叉熵）和反向傳播。數學計算出錯，模型就會產生荒謬輸出。語法？那是次要的——Python的NumPy以可讀性帶來90%的便利，但在訓練龐大模型時因其解釋特性而速度緩慢

不過，語言選擇確實會以務實篩選的方式滲入。C++在AI效能關鍵底層（例如TensorFlow的核心或PyTorch的CUDA綁定）表現卓越，在分散式訓練中每個運算週期都至關重要。Python因簡潔性主導原型開發——當`torch.nn.Linear`能讓你用一行程式碼串接層時，何必糾結於C++的記憶體管理？但你說得對：這是「小問題」。真正的瓶頸在於演算法（例如Kaplan等人的規模定律）或數據相關問題，而非你輸入的是`def`還是`struct`。

### 程式語言洞察的潛在價值
話雖如此，完全否定程式語言研究就像把嬰兒連同洗澡水一起倒掉。優秀的語言設計（受Friedman/王垠類思想啟發）能讓計算表達更安全高效：
- **推理抽象化**：函數式純度（源自Scheme）有助避免概率程式碼中的錯誤，對可靠的AI推理至關重要
- **無痛效能提升**：Rust甚至現代C++借鑒程式語言研究成果，在保證安全性的同時提供速度，減少了困擾原始C語言的「計算錯誤」

歸根結底，AI進步的構成是80%數學/模型/數據、15%工程實踐（C++等語言在此發威）和5%語法詩學。如果我們過度優化最後一項，只會得到更精美的論文卻造出更遲緩的機器人。你怎麼看——你認為我們會見證更多程式語言與AI的融合嗎？比如針對神經符號計算的領域特定語言？

[王垠的部落格](https://www.yinwang.org)  
[Friedman的程式設計精髓](https://www.cs.indiana.edu/~dfriedman/)  
[注意力就是一切（Transformer論文）](https://arxiv.org/abs/1706.03762)  
[神經語言模型的規模定律](https://arxiv.org/abs/2001.08361)