---
audio: false
generated: true
image: false
lang: hant
layout: post
model: x-ai/grok-4-fast
title: RTX 4070 顯示記憶體使用詳解
translated: true
type: note
---

### 為何記憶體使用量約為 6.5 GB？
RTX 4070 具備 12 GB VRAM（12288 MiB，顯示為 12282 MiB 可能因四捨五入或保留空間所致）。你的訓練過程總共使用了 6582 MiB（約 6.4 GB），其中 Python 程序佔用了 4464 MiB（約 4.4 GB）；其餘部分包括 CUDA 上下文開銷、臨時核心以及 torch.compile 的快取（日誌中註明：「正在編譯模型...」）。對於一個小型 3000 萬參數模型而言，這使用量偏高，因為**在 transformer 訓練期間，GPU 記憶體主要由激活值（前向/反向傳播中的中間計算結果）主導，而不僅僅是參數**。僅參數本身只會使用約 0.5 GB，但激活值會隨著你的配置中的 batch_size、block_size、n_embd 和 n_layer 而急遽增加。PyTorch 的自動梯度功能會將前向傳播的輸出保留在記憶體中以供反向傳播使用（預設的 nanoGPT 中未使用梯度檢查點），而像 AMP（混合精度）、融合 AdamW 和模型編譯等功能則會增加額外開銷。

導致此使用量水平的關鍵原因：
- **激活值佔主導（此處為 4–5 GB）**：每次通過 transformer 層的前向傳播都會產生大量的中間張量（例如注意力機制中的 query/key/value 投影、前饋網絡的隱藏狀態）。反向傳播會通過分配梯度臨時變數使此記憶體使用量倍增。注意力機制還需要 O(batch_size × num_heads × block_size²) 的記憶體來儲存分數矩陣（例如，每層在釋放前約為 50 MB），儘管 nanoGPT 的實作在可能的情況下會重用緩衝區。
- **未進行記憶體優化**：nanoGPT 預設使用完整的激活值儲存，而不使用檢查點（檢查點會以在反向傳播期間重新計算前向傳播為代價來換取記憶體）。Torch.compile 會融合操作，但在圖形擷取和執行期間可能會增加峰值分配量。
- **混合精度開銷**：模型和梯度使用 FP16（每個參數 2 位元組），但 AdamW 優化器狀態使用 FP32（動量和方差各 8 位元組，約為參數的 2 倍）。輸入批次（FP16 tokens）很小（約 16 KB），但臨時變數並非如此。
- **運行時因素**：梯度累積（steps=4）每步處理 batch_size=16，但不會倍增記憶體使用（梯度會就地累積）；然而，評估階段（eval_iters=200）會暫時飆升使用量。你的日誌顯示在 iter 1300 時訓練穩定，因此這是基準使用量。

總之，相對於模型大小而言，記憶體使用量「如此高」是因為像這樣的小型模型仍然需要為每個 token 承擔完整的 transformer 開銷，而且你的配置（batch=16, block=512）每步處理約 8000 個 token——足以在沒有積極優化的情況下顯著佔用 VRAM。

### 如何根據配置估算約 6.5 GB
你無法在沒有分析的情況下（例如透過 `torch.utils.bottleneck` 或 NVIDIA Nsight）*精確*預測，因為它取決於 PyTorch 版本、CUDA 以及確切的實作細節。但你可以使用 transformer 訓練記憶體的標準公式來近似估算。這些公式將 VRAM 分解為幾個部分：參數/優化器（約佔總量的 10–20%）、激活值（約佔 70–80%）和開銷（約佔 10%）。以下所有計算均假設使用 FP16 訓練（從日誌的 GradScaler 可知 dtype='float16'）並使用 AdamW。

#### 1. **參數記憶體（易於估算：約 0.06 GB）**
   - 公式：num_params × bytes_per_param（模型使用 FP16）。
   - 從日誌得知：29.94M 參數。
   - FP16：29.94M × 2 位元組 = 59.88 MB（約 0.06 GB）。
   - 如何根據配置計算參數（nanoGPT 公式）：≈ 12 × n_layer × n_embd²（transformer 塊） + n_embd × vocab_size（嵌入層 + LM 頭）。
     - 12 × 6 × 384² = 12 × 6 × 147,456 ≈ 10.6M
     - 384 × 50,304 ≈ 19.3M
     - 總計：約 29.9M（與日誌相符；忽略偏置和層歸一化等小額外部分）。

#### 2. **梯度 + 優化器記憶體（約 0.3–0.6 GB）**
   - 梯度：與參數相同（FP16）：另外約 0.06 GB。
   - 優化器（融合 AdamW，日誌確認）：每個衰減參數有 2 個狀態（動量、方差），通常為 FP32。
     - 衰減參數：30.13M（日誌：26 個張量，30,130,176 個參數）。
     - 公式：decayed_params × 2 × 4 位元組（FP32）= 30.13M × 8 ≈ 241 MB。
     - 非衰減參數（偏置/層歸一化）：數量少，約 5K 參數，可忽略不計。
   - 核心總計：參數 + 梯度 + 優化器 ≈ (2 + 8) 位元組/參數 = 10 位元組/參數 × 30M ≈ 300 MB。
     - 範圍：如果包含 FP32 主權重或額外部分（混合精度中常見），則為 12–20 位元組/參數。
   - 從配置來看：與 n_layer、n_embd 直接成比例（越大則參數越多）。你的小型配置使此部分保持較低水平。

#### 3. **激活值記憶體（最困難/棘手：約 4–5 GB）**
   - 這是主體部分，且因實作而異。線性部分為 O(batch_size × block_size × n_embd × n_layer)，加上注意力分數的 O(batch_size × n_head × block_size²)。
   - **基本公式**（來自 transformer 訓練估算器）：
     ```
     activations_bytes ≈ batch_size × block_size × n_embd × n_layer × multiplier × 2 (FP16 位元組)
     ```
     - 乘數：前向傳播（嵌入層 + 每層注意力/前饋網絡緩衝區）的經驗值為 16–34，加上反向傳播（前向傳播的 2–3 倍）。常用值：24（前向傳播 12，反向傳播 12；考慮到每層約 4–6 個張量，例如注意力中的 Q/K/V/輸出，以及具有 4 倍中間維度的前饋網絡中的向上/向下投影）。
     - 你的配置：batch_size=16, block_size=512, n_embd=384, n_layer=6。
     - 基數：16 × 512 × 384 × 6 = 18.87M 個「元素」。
     - × 24 × 2 位元組 = 18.87M × 48 ≈ 906 MB（低估）。
   - **注意力特定峰值**（O(seq²)，在 block_size=512 時顯著）：
     - 每層：batch_size × n_head × block_size² × 2 位元組（用於 QK^T 分數矩陣）。
     - 16 × 6 × 512 × 512 × 2 ≈ 50.3 MB/層。
     - × n_layer=6，但為順序執行（並非所有層同時進行）：在前向傳播期間每層峰值約為 50–100 MB，加上反向傳播的臨時變數。在整個傳播過程中總計增加約 0.3–0.5 GB。
   - **針對你的配置調整後的經驗總計**：基本公式低估了 4–5 倍，原因在於 PyTorch 的臨時變數（例如，前饋網絡/注意力中的 GEMM 緩衝區，在反向傳播結束前不會釋放）以及 nanoGPT 基於循環的層儲存所有前向傳播輸出（約 L × 4–6 × batch × seq × embd 位元組）。實際情況：約 batch_size × block_size × n_embd × n_layer × 160 × 2 位元組 ≈ 18.87M × 320 ≈ 6 GB（調整以符合你的 6.5 GB 總量；與類似小型 GPT 的報告一致）。
     - 為何是 160？包括完整的反向傳播（無檢查點）、前饋網絡中間層（4 × n_embd）、殘差連接/層歸一化快取，以及每個張量約 20–30% 的 PyTorch 開銷。
   - 從配置來看：與 batch_size/block_size（token 吞吐量）成線性比例，與 block_size（注意力）成二次方比例，並與 n_embd/n_layer（深度/寬度）成比例。你的數值適中但會疊加：例如，將 batch_size 減半至 8 會將激活值減少約 50%，節省約 2–3 GB。

#### 4. **開銷與雜項（約 1 GB）**
   - CUDA/PyTorch：上下文（約 500 MB）、核心啟動、分配器碎片。
   - Torch.compile：圖形擷取 + 融合操作增加 0.5–1 GB（日誌顯示正在編譯；可使用 `torch._dynamo.config` 進行分析）。
   - 資料：批次 tokens（可忽略不計），但如果正在執行評估，則 eval_iters=200 會增加臨時批次。
   - 總計：核心（0.4 GB）+ 激活值（4.5 GB）+ 開銷（1.5 GB）≈ 6.4 GB。

#### 與類似設定的驗證對比
- NanoGPT/GPT-2 small（124M 參數，embd=768=你的 2 倍，layers=12=你的 2 倍，batch~8–16，seq=1024=你的 2 倍）在 FP16 訓練中通常使用 6–10 GB。你的模型參數約為 1/4，但 token 量相似（16×512 對比 16×1024 /2 以考慮縮放），因此基線約為一半，即 3–5 GB——你的額外部分來自於未使用檢查點和編譯。
- 用於更好預測的工具：
  - 在訓練期間運行 `python -m torch.utils.bottleneck` 以獲取細分。
  - 使用 Hugging Face 的 `estimate_memory()` 或像 `modelscope` 這樣的函式庫來進行 LLM 估算（需針對 nanoGPT 進行調整）。
  - 線上計算器（例如搜尋結果中的那些）輸入你的參數/批次/序列長度以獲得約 5–7 GB 的估算。

要減少到 <4 GB：設定 batch_size=8（或使用 gradient_accumulation_steps=8 以達到相同的有效批次大小 64）、block_size=256，或添加梯度檢查點（修改 `model.forward` 以使用 `torch.utils.checkpoint`）。你的設定在 12 GB 內綽綽有餘，但請使用 `nvidia-smi -l 1` 監控峰值使用量。