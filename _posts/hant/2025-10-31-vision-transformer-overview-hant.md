---
audio: false
generated: true
image: false
lang: hant
layout: post
model: x-ai/grok-4-fast
title: 視覺變換器概覽
translated: true
type: note
---

### Vision Transformer (ViT) 簡介

Vision Transformer (ViT) 是一種神經網絡架構，將原本為自然語言處理（NLP）設計的 Transformer 模型應用於電腦視覺任務，特別是影像分類。該架構由 Alexey Dosovitskiy 與 Google 團隊於 2020 年在論文《An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale》中提出，ViT 將影像視為一系列圖塊（patch）序列進行處理，而非使用傳統的卷積神經網絡（CNN）。這種方法讓 ViT 能夠充分利用 Transformer 在處理長距離依賴關係和並行計算方面的優勢。

在大型數據集（如 ImageNet）上，ViT 展現出與 CNN 相當甚至更優異的性能，尤其是在大規模數據（例如 JFT-300M）上進行預訓練時。其變體如 DeiT（數據高效影像 Transformer）進一步提升了在較小數據集上的效率。如今，受 ViT 啟發的模型已廣泛應用於 DALL-E、Stable Diffusion 等現代分類器中。

### ViT 運作原理：整體架構與工作流程

ViT 的核心概念是將影像「分詞化」為固定大小的圖塊序列，類似於將文本分解為單詞或詞元。該序列隨後由標準的 Transformer 編碼器處理（與生成式文本模型不同，無需解碼器）。以下是其運作步驟的詳細分解：

1. **影像預處理與圖塊提取**：
   - 輸入大小為 \\(H \times W \times C\\) 的影像（例如 RGB 影像為 224 × 224 × 3）。
   - 將影像分割為固定大小 \\(P \times P\\) 的非重疊圖塊（例如 16 × 16 像素），得到 \\(N = \frac{HW}{P^2}\\) 個圖塊（例如 224×224 影像以 16×16 圖塊分割可得 196 個圖塊）。
   - 每個圖塊被展平為長度 \\(P^2 \cdot C\\) 的一維向量（例如 16×16×3 對應 768 維）。
   - 為何使用圖塊？原始像素會產生過長的序列（例如高解析度影像可達數百萬像素），圖塊作為「視覺詞彙」可有效降低維度。

2. **圖塊嵌入**：
   - 對每個展平的圖塊向量應用可學習的線性投影（全連接層），將其映射至固定嵌入維度 \\(D\\)（例如 768，與 BERT 等 Transformer 模型一致）。
   - 產生 \\(N\\) 個大小為 \\(D\\) 的嵌入向量。
   - 可選擇在序列前端添加特殊的 [CLS] 詞元嵌入（大小為 \\(D\\) 的可學習向量），類似於 BERT 用於分類任務。

3. **位置嵌入**：
   - 向圖塊嵌入添加可學習的一維位置嵌入，以編碼空間資訊（若無此步驟，Transformer 本身不具有順序感知能力）。
   - 完整輸入序列現為：\\([ \text{[CLS]}, \text{patch}_1, \text{patch}_2, \dots, \text{patch}_N ] + \text{positions}\\)，形狀為 \\((N+1) \times D\\) 的矩陣。

4. **Transformer 編碼器塊**：
   - 將序列輸入至 \\(L\\) 個堆疊的 Transformer 編碼器層（例如 12 層）。
   - 每層包含：
     - **多頭自注意力機制（MSA）**：計算所有圖塊對（包括 [CLS]）之間的注意力分數，使模型能夠捕捉全局關係（例如「貓耳與 100 個圖塊外的鬍鬚相關」），不同於 CNN 的局部感受野。
       - 公式：Attention(Q, K, V) = \\(\text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right) V\\)，其中 Q、K、V 為輸入的投影。
     - **多層感知器（MLP）**：前饋網絡（兩個線性層與 GELU 激活函數），逐位置應用。
     - 層歸一化與殘差連接：輸入 + MSA → 歸一化 → MLP → 歸一化 + 輸入。
   - 輸出：精煉後的嵌入序列，形狀仍為 \\((N+1) \times D\\)。

5. **分類頭**：
   - 對於影像分類，提取 [CLS] 詞元的輸出（或取所有圖塊嵌入的平均值）。
   - 將其通過簡單的 MLP 頭（例如一或兩個線性層）輸出類別 logits。
   - 訓練時使用帶標籤數據的交叉熵損失。預訓練通常涉及掩碼圖塊預測或其他自監督任務。

**關鍵超參數**（以原始 ViT-Base 模型為例）：
- 圖塊大小 \\(P\\)：16
- 嵌入維度 \\(D\\)：768
- 層數 \\(L\\)：12
- 注意力頭數：12
- 參數量：約 86M

ViT 具有良好的擴展性：更大模型（如 ViT-Large，\\(D=1024\\)，\\(L=24\\)）表現更佳，但需要更多數據與計算資源。

**訓練與推論**：
- **訓練**：在帶標籤數據上進行端到端訓練；大規模預訓練（數十億影像）能顯著提升性能。
- **推論**：通過編碼器進行前向傳遞（由於注意力機制，時間複雜度約為 O(N²)，但可透過 FlashAttention 等優化提升效率）。
- 與 CNN 不同，ViT 不具有平移不變性等歸納偏置——所有特性均從數據中學習。

### 與文本 Transformer 的比較：相似與相異之處

ViT 本質上與文本 Transformer（如 BERT）的編碼器部分*架構相同*，但針對二維視覺數據進行了調整。以下為並列比較：

| 面向              | 文本 Transformer（例如 BERT）                  | Vision Transformer (ViT)                       |
|---------------------|------------------------------------------------|------------------------------------------------|
| **輸入表徵** | 嵌入為向量的詞元序列（單詞/子詞）。 | 嵌入為向量的影像圖塊序列。圖塊如同「視覺詞元」。 |
| **序列長度** | 可變（例如一句話 512 個詞元）。   | 根據影像大小/圖塊大小固定（例如包含 [CLS] 共 197）。 |
| **位置編碼** | 一維（絕對或相對）用於詞序。     | 一維（可學習）用於圖塊順序（例如行優先展平）。無內建二維結構。 |
| **核心機制**  | 對詞元進行自注意力以建模依賴關係。 | 對圖塊進行自注意力——數學原理相同，但關注的是空間「關係」而非語法關係。 |
| **輸出/任務**    | 編碼器用於分類/掩碼語言模型；解碼器用於生成。 | 僅編碼器用於分類；可擴展至檢測/分割任務。 |
| **優勢**       | 處理長距離文本依賴關係。         | 影像中的全局上下文理解（例如整體場景理解）。 |
| **劣勢**      | 需要大量文本語料庫。                      | 數據需求高；若無 CNN 預訓練，在小數據集上表現不佳。 |
| **預測方式**| 解碼器中的下一詞元預測（自回歸）。 | 本質上無「下一項」預測——以整體方式對影像進行分類。 |

本質上，ViT 是一種「即插即用」的替換：將詞元嵌入替換為圖塊嵌入，即可得到視覺模型。兩者均依賴注意力機制來衡量序列中的關係，但文本本質上是順序/線性的，而影像則是空間性的（ViT 透過注意力學習此特性）。

### 釐清 ViT 中的「下一詞元」與「下一像素」

不，ViT *並不會*像文本 Transformer 在自回歸生成（例如 GPT）中預測「下一詞元」那樣預測「下一像素」。原因如下：

- **文本 Transformer（自回歸）**：在如 GPT 的模型中，解碼器以序列方式生成——每次產生一個詞元，並以所有先前詞元為條件。對於影像，某些生成模型（例如 PixelRNN）會逐像素生成，但效率低下。

- **ViT 的處理方式**：ViT 是*非自回歸*且整體性的。它透過編碼器*並行*處理*整個*影像（所有圖塊）。在推論過程中，沒有序列式的「下一項」預測——注意力機制同時計算*所有*圖塊之間的關係。對於生成任務（例如修補），ViT 變體可能會使用解碼器以自回歸方式預測被掩碼的圖塊，但基礎 ViT 僅用於分類，而非生成。

- **為何不處理像素？** 將單個像素視為詞元會導致序列長度爆炸性增長（例如 224×224×3 = 約 15 萬個詞元），使得二次方注意力計算在計算上不可行（O(N²) = 數萬億次運算）。圖塊將約 256 個像素（16×16）分組為一個「詞元」，使 N 保持在可管理範圍（約 200）。

若需生成式視覺模型（如下一像素或下一圖塊預測），可參考 VQ-VAE + Transformer 或 Parti 等基於 ViT 理念的模型。

更多細節請查閱原始論文：[An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale](https://arxiv.org/abs/2010.11929)。如需程式碼範例（例如 PyTorch）或與特定變體的比較，請告知！