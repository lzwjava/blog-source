---
audio: false
generated: true
image: false
lang: hant
layout: post
model: x-ai/grok-4-fast
title: 大型語言模型推理技術
translated: true
type: note
---

### 概述
Denny Zhou（可能原意為「Danny Zhou」）在史丹佛大學 CS25：Transformers United 課程（第五版）中發表了一場名為「大型語言模型推理」的講座。他全面概述了大型語言模型（LLM）中的推理機制，重點介紹了實用技巧、理論基礎及當前挑戰。以下根據其簡報及附註，整理出結構化重點摘要。

### LLM 推理的定義
- LLM 中的推理本質上是關於在輸入提示與最終輸出之間**生成中間詞元**（或步驟），而非直接跳至答案。此過程讓模型能分解複雜問題。
- 它無需完全模擬人類推理——目標是有效解決問題。例如，透過逐步串接詞尾來解答「『artificial intelligence』的最後兩個字母是什麼？」得出「le」，展示了中間步驟如何輔助計算。
- 理論支持：對於可透過大小為 *T* 的布林電路解決的問題，固定大小的 transformer 可透過產生 *O(T)* 個中間詞元來處理，無需大幅擴展模型規模。

### 動機
- 預訓練的 LLM 本質上具備推理能力，無需特殊提示或微調；認為它們無法推理的迷思已被破除——問題出在解碼方法未能呈現推理輸出。
- 此方法符合「苦澀教訓」：利用計算（透過詞元生成）勝過依賴類人捷徑，透過下一個詞元預測實現湧現的類人行為。
- 專注於針對最終目標指標（如正確性）進行優化，使用模型生成的數據而非昂貴的人工標註。

### 核心概念
- **思維鏈解碼**：生成多個候選回應，並選擇對最終答案信心最高者。推理路徑通常比直接猜測信心更高（例如，計算場景中的蘋果數量）。
- **透過長度而非深度擴展**：訓練模型為序列問題生成更長序列（*O(T)* 個詞元），使其在無需擴大模型規模下具備任意強大的能力。
- **聚合勝於單次輸出**：生成並合併多個回應（例如透過多數決）勝過單一輸出；檢索類似問題 + 推理優於單獨推理。
- 範例：Gemini 2.0 的「思考模式」透過優先處理運算（例如 45 × 45 = 2025）解決如用數字 1-10 組成 2025 的謎題。

### 關鍵技術
- **提示**：使用少量示例或如「讓我們一步步思考」的短語來引導中間步驟（例如用於數學文字題）。零樣本提示有效但可靠性較低。
- **監督式微調**：在人工標註的逐步解答上訓練，以提高推理路徑的生成可能性。
- **自我改進**：從模型輸出中篩選正確的推理解答，生成自身的訓練數據。
- **強化學習微調**：迭代獎勵正確完整回應（推理 + 答案）並懲罰錯誤答案，使用驗證器。對於可驗證任務泛化效果最佳；感謝 Jonathan Lai 等團隊成員。
- **自我一致性**：採樣多條推理路徑後聚合（例如最頻繁答案）。針對開放式任務的通用變體讓模型自行選擇。
- **檢索 + 推理**：引入相關示例以輔助推理（例如回憶距離公式用於面積查詢）。
- **其他增強技巧**：「退一步思考」以進行抽象化；邊際化以修正概率解碼偏差。

### 限制
- **提示**：簡單但脆弱——需要任務特定示例；通用提示表現不佳。
- **監督式微調**：對分布外問題泛化能力差（例如，儘管經過訓練，仍無法處理新穎的「strawberry」字母計數問題）。
- **強化學習**：依賴可靠的驗證器，但並非所有任務都具備（例如創意寫作）。
- **一般挑戰**：LLM 是概率預測器，因此貪婪解碼偏好流暢但錯誤的答案。聚合有助於改善，但無法解決不可驗證的領域。
- 解碼不對齊：即使強大模型也可能在未干預下未將推理輸出排名最高。

### 未來方向
- 超越基準測試，推進至真實世界應用，尤其是不可驗證任務（例如在無完美指標下優化翻譯）。
- 擴展聚合與檢索規模以獲更大收益。
- 改進解碼（例如透過邊際化）以更好地對齊概率輸出與正確性。
- 建立能跨階段「記憶」成功推理路徑的系統。

本講座強調強化學習微調為當前主力，並呈現將推理視為可擴展生成問題的趨勢。

### 參考資料
- [LLM 推理簡報](https://dennyzhou.github.io/LLM-Reasoning-Stanford-CS-25.pdf)
- [Denny Zhou 的 X 貼文關於重點](https://x.com/denny_zhou/status/1948499173986201915)
- [講座影片於 YouTube](https://www.youtube.com/watch?v=ebnX5Ur1hBk)
- [CS25 課程頁面](https://web.stanford.edu/class/cs25/)