---
audio: false
generated: true
lang: hant
layout: post
title: Apache Hive
translated: true
type: note
---

Apache Hive 是一個建基於 Hadoop 的強大開源數據倉庫系統。它提供名為 Hive 查詢語言（HQL）的類 SQL 介面，用於查詢和分析存儲在分散式儲存系統（如 Hadoop 分散式檔案系統 HDFS 或雲端物件儲存）中的大型數據集。Hive 會將這些 HQL 查詢轉換為 MapReduce 或其他執行框架（如 Tez 或 Spark），從而實現可擴展的數據處理。

以下是 Apache Hive 的完整指南：

**1. Hive 簡介：**

* **目的：** Hive 通過提供熟悉的類 SQL 介面，簡化了查詢和分析海量數據集的過程。它抽象了底層分散式處理框架的複雜性。
* **讀時模式：** 與傳統關係數據庫在寫入時強制執行模式不同，Hive 基於「讀時模式」原則運作。這意味著您可以在查詢數據時才定義其結構，為處理多樣化且不斷演變的數據集提供了靈活性。
* **數據倉庫系統：** Hive 專為線上分析處理（OLAP）工作負載而設計，專注於數據匯總、聚合和分析，而非交易操作（OLTP）。
* **可擴展性與容錯能力：** 建基於 Hadoop，Hive 繼承了其可擴展性和容錯能力，使其能夠在大型商用硬件集群上處理 PB 級數據。

**2. Hive 架構與組件：**

* **Hive 客戶端：** 這些是用戶與 Hive 互動的介面。常見的客戶端包括：
    * **Beeline：** 用於執行 HQL 查詢的命令列介面（CLI）。建議優先使用它來取代舊版的 Hive CLI，特別是在使用 HiveServer2 時。
    * **HiveServer2：** 一個伺服器，允許多個客戶端（JDBC、ODBC、Thrift）同時連接並執行查詢。它比其前身 HiveServer1 提供更好的安全性並支援更多進階功能。
    * **WebHCat：** 用於存取 Hive 元儲存庫並執行 Hive 查詢的 REST API。
* **Hive 服務：** 這些是實現 Hive 功能的核心組件：
    * **元儲存庫：** 一個中央儲存庫，用於儲存 Hive 表的元數據，例如其模式（欄位名稱和數據類型）、在 HDFS 中的位置以及其他屬性。它通常使用關係數據庫（例如 MySQL、PostgreSQL）來持久化這些元數據。
    * **驅動程式：** 接收來自客戶端的 HQL 查詢，進行解析，並啟動編譯和執行過程。
    * **編譯器：** 分析 HQL 查詢，執行語意檢查，並生成執行計劃（一個有向無環圖的任務集合）。
    * **優化器：** 通過應用各種轉換（例如重新排序連接、選擇適當的連接策略等）來優化執行計劃以提升效能。基於成本的優化（CBO）會使用有關數據的統計資訊來做出更明智的優化決策。
    * **執行引擎：** 執行執行計劃中的任務。預設情況下，Hive 使用 MapReduce，但它也可以利用其他引擎，如 Tez 或 Spark，這些引擎通常能帶來顯著的效能提升。
    * **Thrift 伺服器：** 使用 Apache Thrift 框架實現 Hive 客戶端與 Hive 伺服器之間的通訊。
* **處理框架與資源管理：** Hive 依賴分散式處理框架（通常是 MapReduce、Tez 或 Spark）和資源管理系統（如 Hadoop 中的 YARN）在集群上執行查詢。
* **分散式儲存：** Hive 主要使用 HDFS 來儲存表的實際數據。它也可以與其他儲存系統互動，如 Amazon S3、Azure Blob Storage 和 Alluxio。

**3. Hive 查詢語言（HQL）：**

* **類 SQL 語法：** HQL 的語法與標準 SQL 非常相似，這使得熟悉關係數據庫的用戶更容易學習和使用 Hive。
* **數據定義語言（DDL）：** HQL 提供用於定義和管理數據庫物件的指令：
    * `CREATE DATABASE`：建立一個新的數據庫（表的命名空間）。
    * `DROP DATABASE`：刪除一個數據庫及其所有表。
    * `CREATE TABLE`：定義一個新表，指定其模式、儲存格式和位置。您可以建立**受控表**（Hive 控制數據生命週期）或**外部表**（數據由外部管理，Hive 僅管理元數據）。
    * `DROP TABLE`：刪除一個表及其關聯的數據（對於受控表）或僅刪除元數據（對於外部表）。
    * `ALTER TABLE`：修改現有表的模式或屬性（例如，新增/刪除欄位、重新命名表、更改儲存格式）。
    * `CREATE VIEW`：基於查詢結果建立虛擬表。
* **數據操作語言（DML）：** HQL 包含將數據載入表中和查詢數據的指令：
    * `LOAD DATA INPATH`：將數據從指定來源（本地檔案系統或 HDFS）複製到 Hive 表中。
    * `INSERT INTO`：將新行插入現有表中（通常是 `SELECT` 查詢的結果）。
    * `SELECT`：根據指定條件從一個或多個表中檢索數據。它支援各種子句，如 `WHERE`、`GROUP BY`、`HAVING`、`ORDER BY`、`SORT BY`、`CLUSTER BY` 和 `DISTRIBUTE BY`。
    * **連接：** Hive 支援不同類型的連接（INNER JOIN、LEFT OUTER JOIN、RIGHT OUTER JOIN、FULL OUTER JOIN）以合併來自多個表的數據。對於較小的表，Map 端連接可以顯著提升效能。
* **函數：** Hive 提供豐富的內建函數用於數據操作、聚合等。您還可以建立**用戶定義函數（UDF）**、**用戶定義聚合函數（UDAF）** 和**用戶定義表生成函數（UDTF）** 來擴展 Hive 的功能。

**4. Hive 數據類型與格式：**

* **基本數據類型：**
    * 數值：`TINYINT`、`SMALLINT`、`INT`、`BIGINT`、`FLOAT`、`DOUBLE`、`DECIMAL`。
    * 字串：`STRING`、`VARCHAR`、`CHAR`。
    * 布林值：`BOOLEAN`。
    * 日期與時間：`TIMESTAMP`、`DATE`、`INTERVAL`（在後續版本中可用）。
    * 二進位：`BINARY`。
* **複雜數據類型：**
    * `ARRAY`：相同類型元素的有序列表（例如 `ARRAY<STRING>`）。
    * `MAP`：鍵值對的集合，其中鍵為基本類型，值可以是任何類型（例如 `MAP<STRING, INT>`）。
    * `STRUCT`：具有固定命名欄位的記錄類型，每個欄位都有自己的類型（例如 `STRUCT<first_name:STRING, last_name:STRING, age:INT>`）。
    * `UNION`：可以容納多個指定數據類型之一的值的類型。
* **數據格式：** Hive 支援多種數據儲存格式：
    * **文字檔案：** 帶有分隔符的純文字數據（例如 CSV、TSV）。使用 `ROW FORMAT DELIMITED FIELDS TERMINATED BY ...` 定義。
    * **序列檔案：** 一種以鍵值對儲存數據的二進位檔案格式。
    * **RCFile（記錄列式檔案）：** 一種列式儲存格式，可提高讀取密集型工作負載的查詢效能。
    * **ORC（優化行列式）：** 一種高度優化的列式儲存格式，與 RCFile 相比，提供更好的壓縮和查詢效能。它通常是推薦的格式。
    * **Parquet：** 另一種流行的列式儲存格式，以其高效的數據壓縮和編碼方案而聞名，使其適用於分析查詢。
    * **Avro：** 一種基於行的儲存格式，其模式以 JSON 定義，提供模式演變能力。
    * **JSON：** 以 JavaScript 物件表示法格式儲存的數據。

**5. Hive 安裝與配置：**

* **先決條件：** 通常，您需要一個運作中的 Hadoop 集群（HDFS 和 YARN）以及安裝了 Java 開發工具包（JDK）。
* **安裝方法：**
    * **從 Tarball 安裝：** 下載預先建置的二進位套件，解壓縮並配置環境變數（`HIVE_HOME`、`PATH`）。
    * **從原始碼安裝：** 下載原始碼並使用 Apache Maven 建置 Hive。
* **配置：** 主要的配置檔案是 `hive-site.xml`，位於 `conf` 目錄中。關鍵配置屬性包括：
    * `javax.jdo.option.ConnectionURL`、`javax.jdo.option.ConnectionDriverName`、`javax.jdo.option.ConnectionUserName`、`javax.jdo.option.ConnectionPassword`：配置與 Hive 元儲存庫數據庫的連接。
    * `hive.metastore.warehouse.dir`：指定 HDFS 中受控表數據的預設位置。
    * `hive.exec.engine`：設定要使用的執行引擎（例如，`mr` 對應 MapReduce，`tez`，`spark`）。
    * `hive.server2.thrift.http.port`（用於 HTTP 模式）或 `hive.server2.thrift.port`（用於二進位模式）：配置 HiveServer2 的端口。
    * `hive.metastore.uris`：如果以遠程元儲存庫模式運行，則指定元儲存庫伺服器的一個或多個 URI。
* **設定元儲存庫：** 您需要在配置的數據庫中初始化元儲存庫模式。這通常使用 Hive 提供的 `schematool` 指令完成。

**6. Hive 效能調優與優化：**

* **執行引擎選擇：** 與 MapReduce 相比，使用 Tez 或 Spark 作為執行引擎可以顯著提升效能，特別是對於複雜查詢。
* **數據格式優化：** 選擇列式格式如 ORC 或 Parquet 可以帶來更好的壓縮比和更快的查詢執行速度，因為減少了 I/O。
* **分區：** 根據經常查詢的欄位（例如日期、區域）將表劃分為更小、更易管理的部分，允許 Hive 在查詢執行期間修剪不必要的數據，從而提高效能。支援靜態和動態分區。
* **分桶：** 根據欄位的哈希值進一步將分區劃分為桶，可以提高連接和抽樣的效率。
* **索引：** 在經常篩選的欄位上建立索引可以加速查詢執行。Hive 支援不同類型的索引，例如緊湊索引和點陣圖索引。
* **基於成本的優化（CBO）：** 啟用 CBO 允許 Hive 根據數據統計資訊生成更有效的執行計劃。使用 `ANALYZE TABLE` 指令來收集統計資訊。
* **向量化：** 啟用向量化查詢執行可以批次處理數據，提高掃描、聚合和篩選等操作的效能。
* **Map 端連接：** 對於涉及小表的連接，Hive 可以在 Map 端執行連接，避免 Shuffle 階段並提高效能。配置 `hive.auto.convert.join` 及相關屬性。
* **並行執行：** 通過將 `hive.exec.parallel` 設定為 `true`，允許 Hive 並行執行獨立的任務。
* **連接優化：** Hive 會自動優化連接的順序。您也可以提供提示來影響連接策略。
* **避免不必要的數據檢索：** 使用帶有特定欄位的 `SELECT` 而不是 `SELECT *` 以減少處理的數據量。使用 `LIMIT` 來限制返回的行數以進行抽樣或測試。
* **偏斜數據處理：** 如果數據在連接或聚合鍵中分佈不均（偏斜），可能導致效能瓶頸。Hive 提供了處理偏斜連接和聚合的機制。
* **資源調優：** 調整分配給 Hive 及底層執行引擎的資源（例如容器的記憶體）會影響效能。

**7. Hive 使用案例與範例：**

* **數據倉庫：** 建構可擴展的數據倉庫，用於儲存和分析大量結構化和半結構化數據。
* **商業智能（BI）：** 執行數據匯總、報告和分析，以獲取見解用於業務決策。Hive 與各種 BI 工具整合，如 Tableau、Power BI 和 Looker。
* **ETL（提取、轉換、載入）：** 轉換和準備大型數據集，用於下游分析或載入其他系統。
* **日誌分析：** 分析 Web 伺服器日誌、應用程式日誌和其他機器生成的數據，以識別趨勢、模式和異常。
* **點擊流分析：** 分析網站或應用程式上的用戶互動，以了解用戶行為。
* **金融分析：** 分析大規模金融數據，用於欺詐檢測、風險管理和其他目的。
* **機器學習數據預處理：** 為訓練機器學習模型準備和轉換大型數據集。

**HQL 查詢範例：**

```sql
-- 建立一個名為 'mydatabase' 的數據庫
CREATE DATABASE IF NOT EXISTS mydatabase;

-- 使用 'mydatabase'
USE mydatabase;

-- 建立一個名為 'users' 的外部表
CREATE EXTERNAL TABLE IF NOT EXISTS users (
    user_id INT,
    username STRING,
    age INT,
    country STRING
)
ROW FORMAT DELIMITED FIELDS TERMINATED BY ','
LOCATION '/hdfs/user/hive/warehouse/users';

-- 從 HDFS 路徑載入數據到 'users' 表
LOAD DATA INPATH '/hdfs/raw_data/user_data.csv' INTO TABLE users;

-- 查詢來自特定國家的用戶
SELECT user_id, username, age
FROM users
WHERE country = 'China';

-- 按國家分組用戶並計算每個國家的用戶數量
SELECT country, COUNT(*) AS user_count
FROM users
GROUP BY country
ORDER BY user_count DESC;

-- 建立一個按 order_date 分區的表 'orders'
CREATE TABLE IF NOT EXISTS orders (
    order_id INT,
    user_id INT,
    product STRING,
    amount DOUBLE
)
PARTITIONED BY (order_date DATE)
ROW FORMAT DELIMITED FIELDS TERMINATED BY ',';

-- 將數據載入到 'orders' 表的特定分區中
LOAD DATA INPATH '/hdfs/raw_data/orders_2025-03-31.csv' INTO TABLE orders PARTITION (order_date='2025-03-31');

-- 查詢特定日期的訂單
SELECT order_id, user_id, product, amount
FROM orders
WHERE order_date = '2025-03-31';
```

本指南提供了 Apache Hive 的全面概述。通過理解其架構、查詢語言、數據處理能力和優化技術，您可以在大數據專案中有效地利用 Hive 進行大規模數據分析。請記住查閱官方 Apache Hive 文檔以獲取最新的資訊和進階功能。