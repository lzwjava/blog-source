---
audio: false
generated: true
lang: hant
layout: post
title: 神經網絡中的激活函數
translated: true
type: note
---

**Sigmoid**、**ReLU** 和 **Leaky ReLU** 都是神經網絡中使用的激活函數。它們各自具有獨特特性，並根據待解決問題的類型適用於不同任務。以下是它們的差異分析：

### 1. **Sigmoid**：
   - **公式**：  
     \\[
     \sigma(x) = \frac{1}{1 + e^{-x}}
     \\]
   - **輸出範圍**：(0, 1)
   - **優點**：
     - 梯度平滑，有助於優化過程
     - 輸出值限制在 0 到 1 之間，適用於概率或二元分類任務
   - **缺點**：
     - **梯度消失問題**：當輸入值極大或極小時，梯度會變得非常小（接近零），這可能減慢訓練速度，尤其在深度網絡中
     - 輸出非零中心化，可能導致梯度更新偏向單一方向
   - **應用場景**：常用於二元分類任務的輸出層（例如邏輯回歸）

### 2. **ReLU（線性整流單元）**：
   - **公式**：  
     \\[
     f(x) = \max(0, x)
     \\]
   - **輸出範圍**：[0, ∞)
   - **優點**：
     - **運算快速簡便**：計算簡單且實際效率高
     - 通過良好梯度傳播解決梯度消失問題
     - 促進稀疏性（可使多個神經元進入非活躍狀態）
   - **缺點**：
     - **死亡 ReLU 問題**：若神經元輸出始終為零（即輸入為負值），訓練過程中可能「死亡」，導致部分神經元永久失活
   - **應用場景**：廣泛用於深度網絡的隱藏層，特別在卷積神經網絡與深度神經網絡中

### 3. **Leaky ReLU**：
   - **公式**：  
     \\[
     f(x) = \max(\alpha x, x)
     \\]
     其中 \\( \alpha \\) 為微小常數（例如 0.01）
   - **輸出範圍**：(-∞, ∞)
   - **優點**：
     - 通過對負值輸入允許微小負斜率，避免**死亡 ReLU 問題**
     - 與 ReLU 類似，有助緩解梯度消失問題並促進稀疏性
   - **缺點**：
     - 仍存在負值輸出的可能性，但比標準 ReLU 的問題更輕微
     - \\( \alpha \\) 值的選擇需謹慎，有時需要調參
   - **應用場景**：用於可能出現死亡 ReLU 問題的網絡中，替代標準 ReLU

---

### **對比總結**：

- **Sigmoid**：  
  最適用於二元分類任務的輸出層，但存在梯度消失問題
  
- **ReLU**：  
  隱藏層的通用激活函數選擇。能加速訓練並在大型網絡中表現良好，但可能存在神經元死亡問題

- **Leaky ReLU**：  
  通過對負值輸入提供非零梯度，解決死亡 ReLU 問題的改良版本。廣泛應用於深度模型

### **實際應用**：
- **Sigmoid** 常用於需要輸出範圍在 (0, 1) 的場景，例如二元分類任務的最終層
- **ReLU** 因其簡潔性與處理梯度消失的能力，常作為隱藏層的默認選擇
- **Leaky ReLU** 在擔心訓練過程中出現神經元「死亡」時，可替代 ReLU 使用

根據所構建神經網絡的結構與目標，這些激活函數各有其適用場景！