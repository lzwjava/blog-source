---
audio: false
generated: true
image: false
lang: hant
layout: post
model: x-ai/grok-4-fast
title: Goodfellow Bengio Courville 深度學習
translated: true
type: note
---

**Ian Goodfellow**  
**Yoshua Bengio**  
**Aaron Courville**

## 目錄

- 網站 viii  
- 致謝 ix  
- 符號說明 xiii  

### 1 引言 1

- 1.1 本書適合哪些讀者？ 8  
- 1.2 深度學習的歷史趨勢 12  

## I 應用數學與機器學習基礎 27

### 2 線性代數 29

- 2.1 純量、向量、矩陣和張量 29  
- 2.2 矩陣和向量相乘 32  
- 2.3 單位矩陣和逆矩陣 34  
- 2.4 線性相依性和生成空間 35  
- 2.5 範數 37  
- 2.6 特殊類型的矩陣和向量 38  
- 2.7 特徵分解 40  
- 2.8 奇異值分解 42  
- 2.9 Moore-Penrose 偽逆 43  
- 2.10 跡運算 44  
- 2.11 行列式 45  
- 2.12 範例：主成分分析 45  

### 3 機率與資訊理論 51

- 3.1 為什麼需要機率？ 52  
- 3.2 隨機變數 54  
- 3.3 機率分佈 54  
- 3.4 邊緣機率 56  
- 3.5 條件機率 57  
- 3.6 條件機率的鏈式法則 57  
- 3.7 獨立性和條件獨立性 58  
- 3.8 期望、變異數和共變異數 58  
- 3.9 常見機率分佈 60  
- 3.10 常用函數的有用性質 65  
- 3.11 貝葉斯規則 68  
- 3.12 連續變數的技術細節 69  
- 3.13 資訊理論 71  
- 3.14 結構化概率模型 73  

### 4 數值計算 78

- 4.1 溢位和下溢 78  
- 4.2 病態條件 80  
- 4.3 基於梯度的優化 80  
- 4.4 約束優化 91  
- 4.5 範例：線性最小平方 94  

### 5 機器學習基礎 96

- 5.1 學習算法 97  
- 5.2 容量、過擬合和欠擬合 108  
- 5.3 超參數和驗證集 118  
- 5.4 估計器、偏差和變異數 120  
- 5.5 最大似然估計 129  
- 5.6 貝葉斯統計 133  
- 5.7 監督學習算法 137  
- 5.8 無監督學習算法 142  
- 5.9 隨機梯度下降 149  
- 5.10 構建機器學習算法 151  
- 5.11 推動深度學習的挑戰 152  

## II 深度網路：現代實踐 162

### 6 深度前饋網路 164

- 6.1 範例：學習 XOR 167  
- 6.2 基於梯度的學習 172  
- 6.3 隱藏單元 187  
- 6.4 架構設計 193  
- 6.5 反向傳播和其他微分算法 200  
- 6.6 歷史註記 220  

### 7 深度學習的正則化 224

- 7.1 參數範數懲罰 226  
- 7.2 作為約束優化的範數懲罰 233  
- 7.3 正則化和欠約束問題 235  
- 7.4 資料集增強 236  
- 7.5 噪聲魯棒性 238  
- 7.6 半監督學習 240  
- 7.7 多任務學習 241  
- 7.8 早停 241  
- 7.9 參數綁定和參數共享 249  
- 7.10 稀疏表示 251  
- 7.11 套袋法和其他集成方法 253  
- 7.12 Dropout 255  
- 7.13 對抗訓練 265  
- 7.14 切線距離、切線傳播和流形切線分類器 267  

### 8 深度模型訓練的優化 271

- 8.1 學習與純優化的不同 272  
- 8.2 神經網路優化的挑戰 279  
- 8.3 基本算法 290  
- 8.4 參數初始化策略 296  
- 8.5 自適應學習率算法 302  
- 8.6 近似二階方法 307  
- 8.7 優化策略和元算法 313  

### 9 卷積網路 326

- 9.1 卷積運算 327  
- 9.2 動機 329  
- 9.3 池化 335  
- 9.4 卷積和池化作為無限強先驗 339  
- 9.5 基本卷積函數的變體 342  
- 9.6 結構化輸出 352  
- 9.7 資料類型 354  
- 9.8 高效卷積算法 356  
- 9.9 隨機或無監督特徵 356  
- 9.10 卷積網路的神經科學基礎 358  
- 9.11 卷積網路與深度學習歷史 365  

### 10 序列建模：循環和遞歸網路 367

- 10.1 展開計算圖 369  
- 10.2 循環神經網路 372  
- 10.3 雙向 RNN 388  
- 10.4 編碼器-解碼器序列到序列架構 390  
- 10.5 深度循環網路 392  
- 10.6 遞歸神經網路 394  
- 10.7 長期依賴的挑戰 396  
- 10.8 迴聲狀態網路 399  
- 10.9 滲漏單元和其他多時間尺度策略 402  
- 10.10 長短期記憶和其他門控 RNN 404  
- 10.11 長期依賴的優化 408  
- 10.12 顯式記憶 412  

### 11 實踐方法論 416

- 11.1 效能指標  
- 11.2 默認基準模型  
- 11.3 決定是否收集更多數據  
- 11.4 選擇超參數  
- 11.5 除錯策略  
- 11.6 範例：多位數字識別  

## III 深度學習研究 482

### 12 線性因子模型 485

- 12.1 概率 PCA 和因子分析  
- 12.2 獨立成分分析 (ICA)  
- 12.3 慢特徵分析  
- 12.4 稀疏編碼  
- 12.5 PCA 的流形解釋  

### 13 自編碼器 500

- 13.1 欠完備自編碼器  
- 13.2 正則化自編碼器  
- 13.3 表示能力、層大小和深度  
- 13.4 隨機編碼器和解碼器  
- 13.5 去噪自編碼器  
- 13.6 用自編碼器學習流形  
- 13.7 收縮自編碼器  
- 13.8 預測稀疏分解  
- 13.9 自編碼器的應用  

### 14 表示學習 525

- 14.1 貪心逐層無監督預訓練  
- 14.2 遷移學習和領域自適應  
- 14.3 因果因子的半監督解耦  
- 14.4 分散式表示  
- 14.5 深度的指數增益  
- 14.6 提供線索以發現潛在原因  

### 15 深度學習的結構化概率模型 540

- 15.1 非結構化建模的挑戰  
- 15.2 使用圖描述模型結構  
- 15.3 從圖模型採樣  
- 15.4 結構化建模的優勢  
- 15.5 學習依賴關係  
- 15.6 推斷和近似推斷  
- 15.7 深度學習的結構化概率模型方法  

### 16 蒙地卡羅方法 557

- 16.1 採樣和蒙地卡羅方法  
- 16.2 重要性採樣  
- 16.3 馬可夫鏈蒙地卡羅方法  
- 16.4 吉布斯採樣  
- 16.5 分離模態間混合的挑戰  

### 17 面對配分函數 567

- 17.1 對數似然梯度  
- 17.2 隨機最大似然和對比散度  
- 17.3 偽似然  
- 17.4 得分匹配和比率匹配  
- 17.5 去噪得分匹配  
- 17.6 噪聲對比估計  
- 17.7 估計配分函數  

### 18 近似推斷 579

- 18.1 推斷作為優化  
- 18.2 期望最大化  
- 18.3 MAP 推斷和稀疏編碼  
- 18.4 變分推斷和學習  
- 18.5 學習的近似推斷  

### 19 深度生成模型 594

- 19.1 玻爾茲曼機  
- 19.2 受限玻爾茲曼機  
- 19.3 深度信念網路  
- 19.4 深度玻爾茲曼機  
- 19.5 實值資料的玻爾茲曼機  
- 19.6 卷積玻爾茲曼機  
- 19.7 結構化或序列輸出的玻爾茲曼機  
- 19.8 其他玻爾茲曼機  
- 19.9 透過隨機操作的反向傳播  
- 19.10 有向生成網路  
- 19.11 從自編碼器抽取樣本  
- 19.12 生成隨機網路  
- 19.13 其他生成方案  
- 19.14 評估生成模型  
- 19.15 結論  

[深度學習目錄](https://www.deeplearningbook.org/contents/toc.html)