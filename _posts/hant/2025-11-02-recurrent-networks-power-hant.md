---
audio: false
generated: true
image: false
lang: hant
layout: post
model: x-ai/grok-4-fast
title: 循環神經網絡的驚人威力
translated: true
type: note
---

這篇由 Andrej Karpathy 於 2015 年發表的部落格文章，是介紹循環神經網絡（RNNs）的經典之作，重點闡述了它們在處理文本生成等序列任務時令人驚奇的強大能力。Karpathy 分享了他訓練 RNNs 進行圖像描述等任務後的熱情，這些任務只需極少調試就能產出連貫的輸出。他發布了使用 LSTM（一種 RNN）的字符級語言模型的開源代碼，並通過各種文本生成實驗展示了其「魔力」。以下是關鍵章節的結構化摘要。

## 引言
Karpathy 將 RNNs 描述為對序列數據「異常有效」，並將其與處理固定大小輸入/輸出的傳統前饋神經網絡進行對比。他訓練簡單的 RNNs 於文本語料庫上以預測和生成字符，並質疑它們為何能如此出色地捕捉語言模式。文章包含 GitHub 上的代碼，用於重現演示。

## 關鍵概念：RNNs 如何運作
RNNs 擅長處理序列（例如句子、影片），通過維護一個內部「狀態」（隱藏向量）來跨時間步傳遞信息。與靜態網絡不同，它們重複應用相同的轉換：

- **輸入/輸出類型**：固定輸入到序列輸出（例如圖像到描述）；序列到固定輸出（例如句子到情感分析）；序列到序列（例如翻譯）。
- **核心機制**：在每個時間步，新狀態 \\( h_t = \tanh(W_{hh} h_{t-1} + W_{xh} x_t) \\)，其中 \\( x_t \\) 是輸入，輸出 \\( y_t \\) 從狀態衍生。通過時間反向傳播（BPTT）進行訓練。
- **深度與變體**：堆疊層以增加深度；使用 LSTM 以比普通 RNNs 更好地處理長期依賴。
- **哲學註記**：RNNs 是圖靈完備的，本質上是「學習程序」而不僅僅是函數。

一個簡單的 Python 代碼片段說明了步進函數：
```python
def step(self, x):
    self.h = np.tanh(np.dot(self.W_hh, self.h) + np.dot(self.W_xh, x))
    y = np.dot(self.W_hy, self.h)
    return y
```

## 字符級語言建模
核心示例：訓練模型於文本上以預測下一個字符（獨熱編碼），在詞彙表（例如英文的 65 個字符）上建立概率分佈。生成是通過從預測中採樣並反饋來實現的。它通過循環連接學習上下文——例如，在 "hel" 後預測 'l' 與在 "he" 後預測 'l'。使用小批量隨機梯度下降和 RMSProp 等優化器進行訓練。

## 演示：RNN 生成的文本
所有演示均使用作者的 char-rnn 代碼於單個文本文件，顯示了從胡言亂語到連貫輸出的進展。

- **Paul Graham 散文**（~1MB）：模仿創業建議風格。示例：「投資者們驚訝的是他們不打算籌集資金……不要在最初成員工作時就期待孩子們在一個糟糕的成功初創公司之前看起來會怎樣。」
- **莎士比亞**（4.4MB）：產生類似戲劇的對話。示例：「潘達魯斯：唉，我想他將會到來，那一天當細雨將被達成，卻從未被餵養……」
- **維基百科**（96MB）：生成帶有 Markdown、連結和列表的文章式文本。示例：「自然主義和對大多數阿拉伯國家首都的決定是基於愛爾蘭語言，由 [[John Clair]] 奠定……」
- **代數幾何 LaTeX**（16MB）：輸出近乎可編譯的數學證明。示例：「\begin{proof} 我們可以假設 $\mathcal{I}$ 是 $\mathcal{C}$ 上的一個阿貝爾層……」
- **Linux 內核 C 代碼**（474MB）：逼真的函數，包含註釋和語法。示例：「static int indicate_policy(void) { ... if (ss->segment < mem_total) unblock_graph_and_set_blocked(); ... }」
- **嬰兒名字**（8K 個名字）：新穎的發明，如 "Rudi Levette" 或 "Hany Mareanne」。

「溫度」控制採樣：低溫產生重複但安全的文本；高溫產生有創意但不穩定的文本。

## 挑戰與限制
- 長期依賴：遺忘早期上下文（例如不匹配的 LaTeX 標籤）。
- 記憶而非泛化：擅長語法/結構但會虛構內容。
- 計算密集：需要 GPU；沒有 LSTM 的普通 RNN 不穩定。
- 未來改進：外部記憶（神經圖靈機）、注意力機制。

## 結論
RNNs 在自然語言處理（翻譯、語音）、視覺（圖像描述）及其他領域表現出色，從無意義的輸出演進到結構化的生成。Karpathy 可視化了激活（例如檢測引號的神經元），並戲謔地提到一個在文章本身訓練的 RNN，輸出了元文本如「我已經用 RNN 並且有效……」。他推薦了進一步閱讀和代碼資源，預測 RNNs 將在更智能的人工智能中扮演角色。

[循環神經網絡的異常有效性](http://karpathy.github.io/2015/05/21/rnn-effectiveness/)