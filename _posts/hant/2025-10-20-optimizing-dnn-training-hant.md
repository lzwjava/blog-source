---
audio: false
generated: true
image: false
lang: hant
layout: post
model: x-ai/grok-4-fast
title: 優化深度神經網絡訓練
translated: true
type: note
---

### 第八章：深度模型訓練的優化

本章節源自Ian Goodfellow、Yoshua Bengio與Aaron Courville合著的《深度學習》，重點闡述專為訓練深度神經網絡設計的梯度式優化技術。內容奠基於第四章的基礎概念，著重通過最小化成本函數 \\( J(\theta) \\) 來尋找最佳參數 \\( \theta \\)，其中 \\( J(\theta) \\) 通常結合訓練數據的損失項與正則化項。目標在於逼近真實風險 \\( J^*(\theta) = \mathbb{E}_{(x,y) \sim p_{data}} L(f(x;\theta), y) \\)，但實務上需透過訓練集的經驗風險實現。

#### 學習與純優化的差異
機器學習的優化並非直接最小化成本函數，而是間接提升對未見數據（如測試集）的表現。關鍵差異包括：
- **間接目標**：成本函數 \\( J(\theta) \\) 僅是難解指標（如0-1損失）的代理指標。常採用替代損失函數（如分類任務中的負對數似然），因真實損失往往缺乏有效梯度。
- **可分解性**：\\( J(\theta) \\) 對樣本取平均，實現經驗風險最小化：\\( J(\theta) \approx \frac{1}{m} \sum_{i=1}^m L(f(x^{(i)};\theta), y^{(i)}) \\)。
- **過擬合風險**：高容量模型可能記憶訓練數據，因此需依驗證集表現進行早停，即使訓練損失持續下降。
- **批次策略**：
  - **批次法**：使用全數據集計算精確梯度（確定性但大數據時緩慢）。
  - **隨機梯度下降**：使用單一樣本（更新快速但噪聲大）。
  - **小批次法**：兩者折衷，深度學習中常用（批次大小32–256）。小批次噪聲具正則化效果；數據打亂可避免偏差。

在線學習（流數據）無需重複數據即可逼近真實風險梯度。

#### 深度學習優化的挑戰
訓練深度模型需耗費大量計算資源（叢集上運算數日至數月），且比傳統優化更困難，原因包括：
- **難解性**：經驗風險最小化中存在不可微損失與過擬合問題。
- **規模性**：大數據集使全批次梯度不可行；採樣引入方差（誤差縮放比例為 \\( 1/\sqrt{n} \\)）。
- **數據問題**：冗餘性、相關性（通過打亂修正）及重採樣偏差。
- **硬體限制**：批次大小受記憶體限制；異步並行可加速但可能導致不一致。
- 神經網絡特有障礙（後續詳述）：病態條件、局部極小值、平穩區與梯度消失/爆炸。

一階方法（僅用梯度）比二階方法（基於Hessian矩陣）更能耐受噪聲，後者會放大小批次誤差。

#### 優化演算法
本章回顧用於最小化 \\( J(\theta) \\) 的演算法，從經典SGD延伸至變體：
- **隨機梯度下降**：核心小批次更新式 \\( \theta \leftarrow \theta - \epsilon \hat{g} \\)，其中 \\( \hat{g} \\) 為小批次梯度估計值，\\( \epsilon \\) 為學習率。因噪聲有助逃離不良局部極小值，收斂速度優於批次法。
- **動量法與變體**：引入速度項以加速穿越平坦區並抑制震盪。
- **自適應方法**：調整各參數學習率（如AdaGrad、RMSProp、Adam），處理稀疏梯度與尺度差異。
- **二階近似法**：利用曲率資訊（如共軛梯度法、自然梯度法）改善條件數，但計算成本高。
- **座標下降法等**：適用特定架構。

實務技巧包括學習率調度、初始化策略（如Xavier初始化）與方法組合（如預條件處理）。

#### 其他重要內容
- **參數初始化**：對避免梯度消失至關重要；討論對稱/正交初始化與分層預訓練。
- **權衡取捨**：平衡收斂速度、穩定性與泛化能力；強調基於驗證集的調參。
- **進階主題**：涵蓋異步環境中的延遲梯度、分散式優化及與演化方法的關聯。

總體而言，本章強調沒有單一演算法能適用所有場景——成功依賴針對問題的細部調校，其中SGD及其擴展版本為深度學習的核心工具。

[深度學習書籍 - 第八章：優化](https://www.deeplearningbook.org/contents/optimization.html)