---
audio: false
generated: true
image: false
lang: hant
layout: post
model: google/gemini-3-pro-preview
title: GPT-2 文本編碼解析
translated: true
type: note
---

這是一組關於大型語言模型（LLM）（如 GPT）如何處理文字的絕佳問題。您正確指出了兩個主要步驟：**編碼**（tokenization）和**嵌入**（embedding）。

以下是關於 GPT-2 的編碼方式、其與 Word2Vec 的比較，以及為何 LLM 需要先對文字進行編碼的解析。

---

## 🔢 GPT-2 編碼：從文字到數字

GPT-2 使用一種稱為 **Byte Pair Encoding (BPE)** 的標記化方法，具體來說是一個位元組層級的版本。其目的是將原始文字轉換成一串數字，這些數字稱為 **token ID**，以供模型處理。

### 運作原理
1.  **初始詞彙表：** 此過程始於一個基礎詞彙表，包含訓練資料（大量文字語料庫）中出現的所有單一位元組（256 個字元）。這保證了**任何文字**（無論任何語言/文字系統）都可以透過將其分解到位元組層級來進行編碼，即使它包含全新或罕見的字元。
2.  **迭代合併（訓練階段）：**
    * 標記化器會反覆掃描整個訓練文字，以找出**最常出現的相鄰位元組/標記對**。
    * 將此對合併成**一個新的標記**，並將這個新標記加入到詞彙表中。
    * 此步驟會重複數千次（GPT-2 的詞彙表有 50,257 個標記），直到達到所需的詞彙表大小為止。
3.  **標記化（使用階段）：** 當您給模型一個新句子時，標記化器會使用學習到的標記詞彙表及其合併規則。它會將文字分解成其詞彙表中所能找到的**最長可能子詞單元**。

### 結果：子詞單元
這種子詞方法在以下兩者之間取得了平衡：
* **字元層級：** 序列過長，模型難以捕捉意義。
* **單詞層級：** 詞彙量過多，且無法處理詞彙表外（OOV）的單詞（例如拼寫錯誤或新名稱）。

BPE 建立的標記可以是：
* **完整的常見單詞**（例如 "the", "a", "is"）
* **常見的詞部件（子詞）**（例如 "ing", "tion", "un"）
* **罕見詞**會被分解成較小的已知子詞（例如 "unbelievable" $\rightarrow$ "un" + "believ" + "able"）。這防止了 OOV 詞的出現，並且很有效率。

---

## 🆚 編碼 vs. Word2Vec：主要差異

您說得對，兩者都是在文字上訓練，並將文字轉換為數值格式，但它們扮演不同的角色並產生不同的輸出：

| 特性 | GPT-2 編碼 (BPE 標記化) | Word2Vec / GloVe |
| :--- | :--- | :--- |
| **輸出類型** | **Token ID**（一個標記的唯一**整數**） | **詞嵌入**（浮點數的密集**向量**） |
| **目的** | **標記化：** 將文字分解成易於管理的子詞單元，並將其映射到一個**唯一的整數 ID**。這是 LLM 的第一步。 | **嵌入：** 將單詞的**意義**表示為一個靜態向量，使得相似單詞具有相似向量。 |
| **上下文相關** | **否**（"bank" 的 ID 在無論上下文為何時都是相同的）。 | **否**（"bank" 的向量無論是指河岸還是金融銀行都是相同的）。 |
| **處理 OOV**| **優秀。** 它可以將任何未知單詞分解成已知的子詞/位元組。 | **差。** 通常會分配一個 "未知" 向量，或者需要像 FastText 這樣的字元層級擴展。 |

---

## 🧠 如果 LLM 已有嵌入層，為何還要先編碼文字？

這是關於 LLM 如何運作的根本問題！您說得對，LLM 有一個**嵌入層**，但這個過程有兩個不同的步驟：

### 1. 編碼（標記化：文字 $\rightarrow$ ID）
Transformer 架構（如 GPT）是一種神經網絡，它**只能**處理數字。它無法直接處理字串 "cat"。
* **文字**（字串）$\rightarrow$ **標記化器** $\rightarrow$ **Token ID**（整數）
* *範例：* "Hello world" $\rightarrow$ `[15496, 995]`（兩個整數）

Token ID 充當**查找鍵**。

### 2. 嵌入（嵌入層：ID $\rightarrow$ 向量）
LLM 的**嵌入區塊**本質上是一個巨大的、可訓練的查找表（一個矩陣），其中 Token ID 被用作索引（行號）。

* **Token ID**（整數）$\rightarrow$ **嵌入層查找** $\rightarrow$ **嵌入向量**（浮點數向量）
* *範例：* `995`（" world" 的 ID）$\rightarrow$ 在嵌入矩陣中查找第 995 行 $\rightarrow$ `[0.45, -0.12, 0.99, ...]`（一個由，例如，768 個數字組成的向量）。

### 為何不直接使用隨機數字作為輸入？

如果您在沒有標記化的情況下，用隨機數字初始化您的輸入，您將會遇到嚴重的問題：

1.  **任意性/意義喪失：** 電腦是純粹的計算機器。如果您只是將 "the" 隨機分配為數字 5，將 "cat" 隨機分配為數字 42，這些數字本身並沒有模型可以開始利用的內在聯繫或意義。
2.  **詞彙管理：** 您將如何表示 "apple" 與 "apples"？或者拼寫錯誤的 "applle"？
    * 透過**標記化**，它們會變成不同的 Token ID，模型會學習它們獨特（但相關）的意義。
    * 沒有標記化，您就沒有一個標準化、通用的方法來將每個可能的字元串映射到模型能夠重複識別的輸入。

Token ID（整數）是一個**離散的、唯一的識別碼**，對應一個已學習的文字單元。**嵌入向量**則是該識別碼的**連續的、有意義的數值表示**，這是在大規模訓練過程中學習得來的。模型的訓練過程會調整嵌入矩陣中的值，使得相似的標記（如 "cat" 和 "dog"）最終擁有相似的向量。



您想看看關於 Byte Pair Encoding (BPE) 如何從樣本文字建立標記的更詳細、逐步的範例嗎？