---
audio: false
generated: true
image: false
lang: hant
layout: post
model: x-ai/grok-4-fast
title: SLOs-Serve 與 vLLM 功能比較
translated: true
type: note
---

### 比較：SLOs-Serve 對比 vLLM

SLOs-Serve 透過將 vLLM 整合為核心批次處理引擎，並增加多 SLO（服務等級目標）保證層級進行擴展，使其更適合具有多樣化、延遲敏感工作負載的生產環境。vLLM 主要專注於單一 SLO 或吞吐量最大化場景的高吞吐量推理，並使用如 PagedAttention 等技術實現記憶體效率。以下是根據 SLOs-Serve 論文和 vLLM 設計的關鍵面向所進行的結構化比較。

| 面向                  | SLOs-Serve                                                                 | vLLM                                                                 |
|-------------------------|----------------------------------------------------------------------------|----------------------------------------------------------------------|
| **主要焦點**      | 針對多階段 LLM 應用的多 SLO 服務（例如，推理中的緊湊 TTFT 用於預填充，編碼中的緊湊 TPOT 用於解碼）。處理具有階段特定保證的突發性混合工作負載。 | 針對連續解碼的高吞吐量批次處理，透過 PagedAttention 優化記憶體受限工作負載。假設統一 SLO 或優先考慮總體吞吐量。 |
| **SLO 處理**       | 明確的多 SLO 支援：每階段（預填充/解碼）和每應用 SLO（例如，編碼 50ms TPOT 對比聊天 100ms）。使用軟性准入控制來拒絕/延遲違規請求。 | 無原生多 SLO 支援；依賴靜態配置（例如，最大批次大小）。在競爭情況下常見 SLO 違規（例如，突發時延遲飆升 >2 倍）。 |
| **排程器**          | 基於動態規劃（DP）：針對每個 SLO 層級優化預填充預算、批次大小和推測長度。使用 Roofline 模型預測執行時間（R² > 0.8 準確度）。 | 連續批次處理排程器：貪婪地將請求打包成動態批次，專注於解碼密集型工作負載。無 SLO 感知規劃。 |
| **預填充優化**| 分塊預填充與自適應推測（每個 SLO 1-10 個詞元）。分配「預填充預算」以與解碼平衡。 | 每個請求的單次預填充；支援分塊但無 SLO 適應。在混合負載中容易出現隊頭阻塞。 |
| **解碼優化**| SLO 自適應批次大小（高達 512+ 詞元）和針對 TPOT 目標量身定制的推測解碼。 | 高效的連續解碼與前瞻批次處理；高吞吐量（例如，比 Hugging Face 快 10-20 倍）但忽略每個請求的截止時間。 |
| **資源管理**| 透過 Ray 進行多副本路由；透過盡力而為佇列和搶佔實現突發恢復能力。處理分離式設定。 | 單節點或基本分散式（透過 Ray 整合）；無主動路由或 SLO 優先分配。 |
| **吞吐量與容量**| 相比 vLLM 平均容量增益 2.2 倍（跨 6 個場景的幾何平均數：聊天機器人、編碼等）。例如，在推理突發中為 2.4 倍。隨副本數量呈超線性擴展。 | 吞吐量基準：在解碼密集型追蹤中比替代方案快達 24 倍，但在 SLO 限制下性能下降（例如，混合工作負載中容量損失 50%）。 |
| **開銷**           | 每次排程 <10ms；由於 DP 效率（O(n) 狀態）而極小。             | 低（批次處理 <1ms）；但缺乏 SLO 邏輯，因此無額外開銷。      |
| **使用案例**          | 具有嚴格 SLA 的生產應用：工具調用代理、推理鏈、突發流量（例如 Azure 追蹤）。 | 研究/開發吞吐量：簡單服務、長上下文生成、統一負載。 |
| **限制**        | 假設已知 SLO/截止時間；未來需擴展以處理未知長度。基於 vLLM 構建，因此繼承其解碼優勢。 | 難以處理多階段/可變 SLO；在共享 GPU 設定中尾部延遲高（評估中 p99 >2 倍目標）。 |
| **實現**     | 在 vLLM + Ray 上的開源原型；支援 A100/H100 GPU 上的 OPT/ToolLlama 模型。 | 成熟的開源（GitHub）；廣泛採用，支援 100+ 模型，易於部署。 |

總而言之，vLLM 在面向吞吐量的服務原始效率方面表現出色，但在複雜的實際場景中對 SLO 合規性不足——這導致 SLOs-Serve 透過針對性優化實現了 2 倍以上的增益。如果您的工作負載純粹是吞吐量最大化且無需延遲保證，vLLM 仍然更簡單且部署更快。

[SLOs-Serve: Optimized Serving of Multi-SLO LLMs](https://arxiv.org/abs/2504.08784)  
[vLLM: Easy, Fast, and Cheap LLM Serving with PagedAttention](https://arxiv.org/abs/2309.06180)