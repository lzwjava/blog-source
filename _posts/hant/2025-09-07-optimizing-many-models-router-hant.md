---
audio: false
date: 2025-09-07
generated: true
image: false
lang: hant
layout: post
title: 優化多模型代碼路由器
translated: true
type: note
---

簡短回答：是的——你的「多模型路由器」確實可能比像 Codex（OpenAI 的程式碼助手）這樣緊密整合的技術堆疊表現更差，因為後者是在單一模型家族加上深度 IDE 上下文上進行端到端調優的。以下是原因及快速修復方法。

### 為什麼 Codex 感覺更好用

* **單任務專用模型。** 持續使用單一模型能保留其潛在計劃和編碼風格。在任務中途切換模型會重置該計劃，並經常引入衝突的慣例。
* **垂直整合。** Codex（OpenAI）控制了從模型 → 工具調用 → 程式庫索引 → 測試 → 行內編輯的整個流程。這種閉環為其提供了更豐富的信號（符號圖、文件熱力圖、失敗測試），且減少了膠水代碼的延遲。
* **提示與策略對齊。** 他們的提示詞、程式碼格式化工具以及「生成最小化、可編譯的差異」的啟發式方法，都是為 GPT-5 家族模型共同設計的。通用路由器很容易為某些模型錯誤指定溫度、停止序列或修補程式格式。
* **延遲/吞吐量。** 每個額外的跳轉（代理、OpenRouter 中介軟體、模型協商）都會增加抖動。編碼工作流程對反饋延遲非常敏感；每輪互動增加 300–800 毫秒的延遲會明顯破壞「流暢感」。
* **上下文質量。** 能夠計算程式庫映射（文件、符號、近期變更的拓撲結構）的 IDE 整合，勝過「僅僅傾倒長上下文」。缺乏結構的長上下文會浪費令牌並稀釋注意力。

### 你的配置中可能存在的問題

* **模型泛濫。** 你混用了通用模型、編碼模型和思考模型。「思考」變體（例如 `claude-3.7-sonnet:thinking`、`deepseek-r1`）對於推導證明很好，但對於程式碼編輯來說速度較慢且更冗長。
* **預設路由不匹配。** `default: "openrouter,x-ai/grok-code-fast-1"` 看起來你想使用 Grok Code Fast，但它並未列在你的 `models` 陣列中。這可能導致靜默回退和不一致性。
* **意圖未分類。** 所有任務共用一個「預設」路由，意味著小編輯、重度重構和長上下文讀取都必須經過同一路徑。
* **溫度/格式漂移。** 如果你沒有針對每個模型強制執行低溫度和嚴格的修補程式格式，不同供應商的輸出結果會差異巨大。

### 讓你的路由器擁有「Codex 般」的體驗

1.  **為每個任務選擇並堅持使用一個主要模型。** 選擇一個強大的編碼模型作為預設（例如 `openai/gpt-5` 或 `x-ai/grok-code-fast-1` 或 `qwen/qwen3-coder`），僅在明確原因下（如需要極長上下文或複雜數學運算）才切換。
2.  **按意圖分片（而非按品牌）。**

    *   *小編輯 / 快速修復：* 快速模型（GPT-5-mini 或 Gemini-Flash）。
    *   *重構 / 多文件變更：* GPT-5（或 Claude Sonnet 3.7 非思考模式）。
    *   *超長上下文讀取：* Kimi-K2。
    *   *編碼前的困難推理：* 使用 DeepSeek-R1 制定大綱 → 交給編碼模型產生修補程式。
3.  **強制執行嚴格的修補程式契約。** 始終要求輸出統一的 diff 或明確的 "ApplyPatch" JSON，包含文件路徑和程式碼區塊。自動拒絕任何其他格式並重新提示。
4.  **降低程式碼的隨機性。** `temperature: 0–0.2`，不使用頻率/存在懲罰，並將 `top_p` 限制在 ~0.9。
5.  **提供程式庫映射，而不僅僅是令牌。** 輸入一個緊湊的符號索引：入口點、關鍵類/函數、測試目標、近期差異。這比傾倒 20 萬個令牌的原始文件更好。
6.  **保持工具循環緊湊。** 一個模型提議修補程式 → 你執行建構/測試/檢查工具 → 將失敗結果反饋給*同一個*模型進行迭代。避免在循環中途切換模型。
7.  **精簡模型列表。** 最多保留 3–5 個。每增加一個模型都會增加變異性和路由錯誤。
8.  **從熱路徑中移除「思考」模型。** 僅使用它們來產生簡短的設計計劃，然後切換到編碼模型進行實際編輯。

### 更精簡的配置（即用想法；請根據你的框架調整名稱）

```json
{
  "PROXY_URL": "http://127.0.0.1:7890",
  "LOG": true,
  "Providers": [
    {
      "name": "openrouter",
      "api_base_url": "https://openrouter.ai/api/v1/chat/completions",
      "api_key": "",
      "models": [
        "openai/gpt-5",
        "openai/gpt-5-mini",
        "x-ai/grok-code-fast-1",
        "moonshotai/kimi-k2",
        "anthropic/claude-3.7-sonnet",
        "qwen/qwen3-coder"
      ],
      "transformer": {"use": ["openrouter"]}
    }
  ],
  "Router": {
    "default": "openrouter,openai/gpt-5",
    "quickEdit": "openrouter,openai/gpt-5-mini",
    "refactor": "openrouter,openai/gpt-5",
    "reasonPlan": "openrouter,anthropic/claude-3.7-sonnet",
    "longContext": "openrouter,moonshotai/kimi-k2",
    "longContextThreshold": 220000,
    "webSearch": "openrouter,qwen/qwen3-coder"
  }
}
```

### 穩定輸出的提示詞框架（無表格）

*   **系統提示：**「你是一位資深軟體工程師。產生最小化、可編譯的修補程式。保持風格與程式庫一致。如果不確定，提出一個簡短的澄清問題，否則繼續進行。」
*   **規則：**「僅輸出統一的 diff（git 格式）。絕不包含評論。如果文件不存在，則創建它並包含在 diff 中。保留許可證標頭。」
*   **上下文區塊：** 程式庫映射（文件、關鍵符號）、當前錯誤、測試目標。
*   **工具循環：** 在產生 diff 後，自動執行 `建構 + 測試 + 檢查`；僅將失敗結果返回給同一個模型。

### 實用規範

*   精簡你的模型列表，並將預設路由固定到一個確實存在於 `models` 中的模型。
*   為每條路由設定溫度；程式碼相關任務保持在 0–0.2。
*   在不同回合間快取程式庫映射；除非文件變更，否則不要重新發送巨大的上下文。
*   避免在任務中途切換模型。
*   優先使用「diff」而非「替換整個文件」，以減少幻覺導入。

如果你願意，貼上你路由器的提示詞和溫度設定，我可以為每條路由進行緊縮調整，讓 Claude Code 和你的技術堆疊能像 Codex 一樣表現出更一致的可信度。