---
audio: false
generated: true
lang: hant
layout: post
title: DeepSeek V3
translated: true
type: note
---

### 概述與關鍵亮點
1. 模型名稱：DeepSeek-V3，這是一個混合專家模型，擁有 6710 億參數，每個 Token 激活 370 億參數。
2. 訓練數據集：預訓練使用了 14.8 萬億個多元化、高品質的 Token。
3. 核心創新：結合了多頭潛在注意力與 DeepSeekMoE 架構，並採用無輔助損失的負載平衡以提升效率。
4. 訓練效率：僅使用 278.8 萬 H800 GPU 小時即完成完整訓練。
5. 成本效益：訓練成本估計為 557.6 萬美元，假設每 GPU 小時成本為 2 美元。

---

### 架構創新
6. 基於 Transformer 的框架：保留 Transformer 架構以確保可擴展性和靈活性。
7. 多頭潛在注意力：通過壓縮鍵值快取來減少推理記憶體佔用，且不損失性能。
8. DeepSeekMoE：結合共享專家與路由專家，實現成本效益高的訓練與高計算效率。
9. 無輔助損失的負載平衡：引入偏置項以保持專家負載平衡，且不影響性能。
10. 多 Token 預測：每個位置順序預測多個 Token，提升數據效率與表示預規劃能力。

---

### 訓練框架
11. FP8 混合精度訓練：利用細粒度量化與低精度存儲來優化記憶體與計算。
12. DualPipe 算法：重疊計算與通信階段，減少管道氣泡並提升並行度。
13. 高效的跨節點通信：採用優化的 all-to-all 操作核心，充分利用 NVLink 與 InfiniBand 帶寬。
14. 低精度優化器狀態：以 BF16 格式存儲優化器狀態，減少記憶體消耗且不影響性能。
15. 記憶體優化技術：在反向傳播期間重新計算某些操作以節省記憶體。

---

### 預訓練細節
16. 穩定的訓練過程：預訓練期間未出現不可恢復的損失尖峰或回滾。
17. 上下文長度擴展：分兩個階段將上下文長度從 32K 擴展至 128K。
18. 訓練成本：預訓練耗費 266.4 萬 GPU 小時，上下文擴展耗費 11.9 萬 GPU 小時，後訓練耗費 0.5 萬 GPU 小時。
19. Token 效率：通過最小化每萬億 Token 的 GPU 小時來確保訓練效率。
20. 高品質數據：預訓練數據集經過精心策劃，確保多樣性與相關性。

---

### 後訓練增強
21. 監督微調：使模型輸出與人類偏好對齊。
22. 強化學習：採用群組相對策略優化進行微調。
23. 知識蒸餾：整合 DeepSeek-R1 模型的推理能力。
24. 輸出風格控制：平衡準確性與生成長度及風格。
25. 性能優化：後訓練進一步提升基準測試結果。

---

### 基準測試表現
26. MMLU：得分 88.5，超越其他開源模型。
27. GPQA：得分 59.1，與 GPT-4o 和 Claude-3.5-Sonnet 相當。
28. 數學基準測試：在數學推理任務中表現達到最先進水平。
29. 編程競賽：在 LiveCodeBench 等編程基準測試中表現優異。
30. 事實知識：在英文與中文事實性基準測試中展現卓越結果。

---

### 推理與部署
31. 預填充階段：結合張量並行、序列並行與專家並行以提升效率。
32. 解碼階段：採用 EP320 與 IBGDA 實現低延遲通信。
33. 動態冗餘：動態調整專家負載以優化資源利用率。
34. 階段分離：預填充與解碼階段分離以提升吞吐量。
35. 硬件利用率：針對配備 NVLink 與 InfiniBand 互連的 H800 GPU 進行優化。

---

### 負載平衡與解碼創新
36. 基於偏置的路由：引入偏置項以動態確保專家負載平衡。
37. 推測解碼：利用 MTP 模組提升生成延遲。
38. 冗餘專家：複製高負載專家以平衡 GPU 工作負載。
39. 節點限制路由：將 Token 路由限制在最多 4 個節點以減少通信開銷。
40. 無 Token 丟棄：確保訓練與推理期間所有 Token 均被保留。

---

### 技術細節
41. 集群配置：在配備 2048 個 NVIDIA H800 GPU 的集群上進行訓練。
42. 管道並行：採用 16 路並行方案以實現可擴展性。
43. 記憶體佔用：通過優化記憶體使用避免昂貴的張量並行。
44. 自定義核心：開發專用通信核心以高效處理跨節點操作。
45. 混合精度優化：結合 FP8 與 BF16 格式以實現最佳訓練動態。

---

### 評估與結果
46. 全面基準測試：在教育、編程與推理等多個領域進行評估。
47. 開源領導地位：成為同類別中最強大的開源基礎模型。
48. 與閉源模型比較：性能與 GPT-4o 和 Claude-3.5-Sonnet 相當。
49. 中文知識優勢：在中文事實性基準測試中超越領先模型。
50. 長上下文處理：在需要處理長上下文的任務中表現卓越。

---

### 未來方向
51. 動態冗餘探索：研究更具適應性的冗餘策略。
52. 推測解碼擴展：探索 MTP 在推理加速中的進一步應用。
53. 硬件協同設計：適應下一代 GPU 以提升性能。
54. 更廣泛的基準測試覆蓋：將評估擴展至更多樣化的任務。
55. 可持續性：通過算法與硬件優化進一步降低訓練成本。

---

本文件提供了 DeepSeek-V3 的全面總結，涵蓋其架構、訓練方法、基準測試表現及未來展望。如需對特定部分進行進一步闡述或補充其他要點，請隨時告知！