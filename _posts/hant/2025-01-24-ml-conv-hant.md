---
audio: false
generated: false
lang: hant
layout: post
title: 機器學習 - 對話
translated: true
type: note
---

A：喂，最近成日聽到人講機器學習（ML）、深度學習（DL）同 GPT，你可唔可以同我解釋下？

B：冇問題！我哋從基礎開始講。機器學習係電腦科學嘅一個領域，系統透過數據學習來提升表現，而唔需要明確編程。你可以想像成教電腦識別模式。

A：明白。咁深度學習呢？

B：深度學習係機器學習嘅一個分支。佢使用神經網絡——基本上係受人類大腦啟發嘅計算模型——透過多層結構處理數據。這些層次幫助模型理解複雜模式，例如識別圖像中嘅人臉或者理解語音。

A：神經網絡聽落好勁。佢哋點運作㗎？

B：想像一個由互連節點組成嘅網絡，就好似神經元咁。每個節點處理一小段資訊然後傳遞出去。「深度」學習嘅「深度」正係指擁有眾多層次，讓模型能夠學習更細膩嘅模式。

A：咁 GPT 呢？我聽講好厲害。

B：GPT 真係好強大！佢係 Generative Pre-trained Transformer 嘅縮寫，係 OpenAI 開發嘅大型語言模型系列。GPT 可以生成類人文字、回答問題，甚至撰寫文章。

A：真犀利。佢點運作㗎？

B：GPT 使用稱為 Transformer 嘅架構，依賴自注意力機制。即係話模型能夠專注於輸入文字嘅不同部分來更好理解上下文。佢先透過海量文本數據進行預訓練，再針對特定任務進行微調。

A：GPT 同 ChatGPT 有咩分別？

B：ChatGPT 係 GPT 嘅對話優化版本，專為與用戶互動而設計，能夠遵循指令並生成自然流暢嘅回應。

A：原來如此。咩係「預訓練」同「微調」？

B：預訓練就好似俾模型接受通識教育，從巨量數據集學習語言規律。微調就似專業培訓——將模型調整至適應特定任務，例如回答客戶問題或總結文本。

A：咁就清晰喇。你頭先講嘅「Transformer」係咩來㗎？

B：Transformer 係一種神經網絡架構，出自著名論文《Attention Is All You Need》。佢透過自注意力機制革新咗自然語言處理，讓模型能夠衡量句子中不同詞語嘅重要性。

A：自注意力？即係點？

B：呢個係讓模型聚焦於輸入最相關部分嘅方法。例如喺「隻貓坐喺地蓆上面」呢句當中，模型可能會更關注「貓」同「地蓆」來理解兩者關係。

A：厲害！咁 GPT 點生成文字㗎？

B：GPT 使用因果語言建模技術，根據前面所有字詞預測序列中下一個字。例如如果你輸入「個天係」，佢可能會預測下個字係「藍色」。

A：聽落簡單，但實情應該好複雜。

B：無錯！精妙之處在於規模。GPT 模型擁有數十億個參數，就好似訓練過程中模型調節嘅旋鈕同刻度，參數越多就能捕捉越複雜嘅模式。

A：有冇咩要注意㗎？

B：呢類模型需要巨量數據同運算資源來訓練，仲要小心調整避免產生偏頗或有害內容。所以先會有人類反饋強化學習（RLHF）呢類技術——用來將模型輸出與人類價值觀對齊。

A：RLHF？

B：係呀，呢個係透過人類反饋訓練模型嘅方法。例如讓人類對不同回應評分，模型就會學習優先選擇人類更鍾意嘅答案。

A：真係有趣。咁未來發展會點？

B：未來令人振奮！我哋見到更強大嘅模型、處理多模態數據（例如文字同圖像）嘅更好方法，同埋推動AI更符合道德同公平。但亦都有挑戰，例如要確保呢啲模型唔會損害社會或環境。

A：聽落有好多要考慮㗎。

B：絕對係！不過而家亦係探索AI嘅黃金時代，解決現實問題同開創可能性嘅潛力真係無限。

A：多謝你解釋得咁清楚！

B：唔使客氣！有任何問題隨時再問——AI 呢個領域博大精深，永遠都有新嘢可以學。