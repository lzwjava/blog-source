---
audio: false
generated: false
image: false
lang: hant
layout: post
title: é›²è¨ˆç®—å’Œå¤§æ•¸æ“šå…¥é–€
translated: true
type: post
---

é€™ç¯€èª²åŒ…å«ä»¥ä¸‹è©±é¡Œï¼š

* Spark
* Hadoop
* Kubernetes
* Docker
* Flink
* MongoDB

èªªèµ·é›²è¨ˆç®—ï¼Œä¼¼ä¹é›¢ä¸é–‹å¾ˆå¤šçš„å·¥å…·ï¼ŒHadoopã€Hiveã€Hbaseã€ZooKeeperã€Dockerã€Kubernetesã€Sparkã€Kafkaã€MongoDBã€Flinkã€Druidã€Prestoã€Kylinã€Elastic Searchã€‚éƒ½æœ‰è½éå—ã€‚é€™äº›å·¥å…·æœ‰äº›æˆ‘æ˜¯å¾`å¤§æ•¸æ“šå·¥ç¨‹å¸«`ã€`åˆ†ä½ˆå¼å¾Œç«¯å·¥ç¨‹å¸«`çš„è·ä½æè¿°ä¸Šæ‰¾åˆ°çš„ã€‚é€™äº›éƒ½æ˜¯é«˜è–ªè·ä½ã€‚æˆ‘å€‘è©¦è‘—æŠŠä»–å€‘éƒ½å®‰è£ä¸Šï¼Œè©¦è‘—æŠŠç©å…©ä¸‹ã€‚
## åˆæ¢ Spark

å®˜ç¶²èªªï¼Œ`Spark`ç”¨ä¾†è™•ç†å¤§è¦æ¨¡æ•¸æ“šçš„åˆ†æå¼•æ“ã€‚`spark`å°±æ˜¯ä¸€å¥—åº«ã€‚å®ƒä¼¼ä¹ä¸åƒ`Redis`é‚£æ¨£åˆ†æˆæœå‹™ç«¯å’Œå®¢æˆ¶ç«¯ã€‚`spark`å°±æ˜¯åªåœ¨å®¢æˆ¶ç«¯ä½¿ç”¨çš„ã€‚å¾å®˜ç¶²ä¸‹è¼‰äº†æœ€æ–°çš„ç‰ˆæœ¬ï¼Œ`spark-3.1.1-bin-hadoop3.2.tar`ã€‚

```shell
$ tree . -L 1
.
â”œâ”€â”€ LICENSE
â”œâ”€â”€ NOTICE
â”œâ”€â”€ R
â”œâ”€â”€ README.md
â”œâ”€â”€ RELEASE
â”œâ”€â”€ bin
â”œâ”€â”€ conf
â”œâ”€â”€ data
â”œâ”€â”€ examples
â”œâ”€â”€ jars
â”œâ”€â”€ kubernetes
â”œâ”€â”€ licenses
â”œâ”€â”€ python
â”œâ”€â”€ sbin
â””â”€â”€ yarn

11 directories, 4 files
```

ä¼¼ä¹å°±æ˜¯å„èªè¨€ç·¨å¯«çš„ä¸€äº›åˆ†æåº«ã€‚

åŒæ™‚å®˜ç¶²èªªå¯ä»¥åœ¨Pythonä¸Šç›´æ¥è£ä¾è³´åº«ã€‚`pip install pyspark`

```shell
$ pip install pyspark
Collecting pyspark
  Downloading pyspark-3.1.1.tar.gz (212.3 MB)
     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 212.3 MB 14 kB/s
Collecting py4j==0.10.9
  Downloading py4j-0.10.9-py2.py3-none-any.whl (198 kB)
     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 198 kB 145 kB/s
Building wheels for collected packages: pyspark
  Building wheel for pyspark (setup.py) ... done
  Created wheel for pyspark: filename=pyspark-3.1.1-py2.py3-none-any.whl size=212767604 sha256=0b8079e82f3a5bcadad99179902d8c8ff9f8eccad928a469c11b97abdc960b72
  Stored in directory: /Users/lzw/Library/Caches/pip/wheels/23/bf/e9/9f3500437422e2ab82246f25a51ee480a44d4efc6c27e50d33
Successfully built pyspark
Installing collected packages: py4j, pyspark
Successfully installed py4j-0.10.9 pyspark-3.1.1
```

è£ä¸Šäº†ã€‚

é€™æœƒçœ‹å®˜ç¶²ï¼Œæœ‰äº›ä¾‹å­

```shell
./bin/run-example SparkPi 10
```

å“¦ï¼ŒåŸä¾†å¯ä»¥é‹è¡Œå‰›å‰›ä¸‹è¼‰çš„å®‰è£åŒ…è£¡çš„ç¨‹åºã€‚ä½†å‡ºéŒ¯äº†ã€‚

```shell
$ ./bin/run-example SparkPi 10
21/03/11 00:06:15 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
21/03/11 00:06:16 INFO ResourceUtils: No custom resources configured for spark.driver.
21/03/11 00:06:16 WARN Utils: Service 'sparkDriver' could not bind on a random free port. You may check whether configuring an appropriate binding address.
```

> Spark is a fast and general processing engine compatible with Hadoop data. It can run in Hadoop clusters through YARN or Spark's standalone mode, and it can process data in HDFS, HBase, Cassandra, Hive, and any Hadoop InputFormat. It is designed to perform both batch processing (similar to MapReduce) and new workloads like streaming, interactive queries, and machine learning.

å‡ºç¾äº†å¥½å¹¾æ¬¡`hadoop`ã€‚è°·æ­Œäº†`spark depends hadoop `ä¹‹å¾Œï¼Œæ‰¾åˆ°é€™æ¨£ä¸€æ®µè©±ã€‚çœ‹ä¾†é€™ä¾è³´æ–¼`Hadoop`æ ¼å¼çš„æ•¸æ“šã€‚è®“æˆ‘å€‘å…ˆç ”ç©¶ `Hadoop`ã€‚

## Hadoop

ç°¡å–®çœ‹äº†å®˜ç¶²å¾Œã€‚ä¾†å®‰è£ä¸€ä¸‹ã€‚

```shell
brew install hadoop
```

å®‰è£çš„éç¨‹ä¸­ï¼Œä¾†äº†è§£ä¸€ä¸‹ã€‚

> The Apache Hadoop software library is a framework that allows for the distributed processing of large data sets across clusters of computers using simple programming models. It is designed to scale up from single servers to thousands of machines, each offering local computation and storage. Rather than rely on hardware to deliver high-availability, the library itself is designed to detect and handle failures at the application layer, so delivering a highly-available service on top of a cluster of computers, each of which may be prone to failures.

å°±æ˜¯èªª Hadoop æ˜¯ä¸€å¥—æ¡†æ¶ï¼Œä¾†è™•ç†åˆ†ä½ˆå¼çš„æ•¸æ“šé›†ã€‚é€™äº›æ•¸æ“šé›†å¯èƒ½åˆ†éƒ¨åœ¨å¾ˆå¤šè¨ˆç®—æ©Ÿä¸Šã€‚ç”¨å¾ˆç°¡å–®çš„ç·¨ç¨‹æ¨¡å‹ä¾†è™•ç†ã€‚å®ƒæ˜¯è¨­è¨ˆä¾†å¾å–®ä¸€æœå‹™å™¨æ“´å±•åˆ°åƒå°æ©Ÿå™¨çš„ã€‚èˆ‡å…¶ä¾è³´æ–¼ç¡¬ä»¶çš„é«˜å¯ç”¨ï¼Œé€™å€‹åº«å‰‡è¨­è¨ˆä¾†åœ¨æ‡‰ç”¨å±¤å°±èƒ½æª¢æŸ¥å’Œè™•ç†éŒ¯èª¤ã€‚å› æ­¤èƒ½å°‡é«˜å¯ç”¨çš„æœå‹™éƒ¨ç½²åˆ°é›†ç¾¤ä¸­ï¼Œé›–ç„¶é›†ç¾¤ä¸­çš„æ¯å°é›»è…¦éƒ½å¯èƒ½å°è‡´å¤±æ•—ã€‚

```shell
$ brew install hadoop
Error:
  homebrew-core is a shallow clone.
  homebrew-cask is a shallow clone.
To `brew update`, first run:
  git -C /usr/local/Homebrew/Library/Taps/homebrew/homebrew-core fetch --unshallow
  git -C /usr/local/Homebrew/Library/Taps/homebrew/homebrew-cask fetch --unshallow
These commands may take a few minutes to run due to the large size of the repositories.
This restriction has been made on GitHub's request because updating shallow
clones is an extremely expensive operation due to the tree layout and traffic of
Homebrew/homebrew-core and Homebrew/homebrew-cask. We don't do this for you
automatically to avoid repeatedly performing an expensive unshallow operation in
CI systems (which should instead be fixed to not use shallow clones). Sorry for
the inconvenience!
==> Downloading https://homebrew.bintray.com/bottles/openjdk-15.0.1.big_sur.bottle.tar.gz
Already downloaded: /Users/lzw/Library/Caches/Homebrew/downloads/d1e3ece4af1d225bc2607eaa4ce85a873d2c6d43757ae4415d195751bc431962--openjdk-15.0.1.big_sur.bottle.tar.gz
==> Downloading https://www.apache.org/dyn/closer.lua?path=hadoop/common/hadoop-3.3.0/hadoop-3.3.0.tar.gz
Already downloaded: /Users/lzw/Library/Caches/Homebrew/downloads/764c6a0ea7352bb8bb505989feee1b36dc628c2dcd6b93fef1ca829d191b4e1e--hadoop-3.3.0.tar.gz
==> Installing dependencies for hadoop: openjdk
==> Installing hadoop dependency: openjdk
==> Pouring openjdk-15.0.1.big_sur.bottle.tar.gz
==> Caveats
For the system Java wrappers to find this JDK, symlink it with
  sudo ln -sfn /usr/local/opt/openjdk/libexec/openjdk.jdk /Library/Java/JavaVirtualMachines/openjdk.jdk

openjdk is keg-only, which means it was not symlinked into /usr/local,
because it shadows the macOS `java` wrapper.

If you need to have openjdk first in your PATH run:
  echo 'export PATH="/usr/local/opt/openjdk/bin:$PATH"' >> /Users/lzw/.bash_profile

For compilers to find openjdk you may need to set:
  export CPPFLAGS="-I/usr/local/opt/openjdk/include"

==> Summary
ğŸº  /usr/local/Cellar/openjdk/15.0.1: 614 files, 324.9MB
==> Installing hadoop
ğŸº  /usr/local/Cellar/hadoop/3.3.0: 21,819 files, 954.7MB, built in 2 minutes 15 seconds
==> Upgrading 1 dependent:
maven 3.3.3 -> 3.6.3_1
==> Upgrading maven 3.3.3 -> 3.6.3_1
==> Downloading https://www.apache.org/dyn/closer.lua?path=maven/maven-3/3.6.3/binaries/apache-maven-3.6.3-bin.tar.gz
==> Downloading from https://mirror.olnevhost.net/pub/apache/maven/maven-3/3.6.3/binaries/apache-maven-3.6.3-bin.tar.gz
######################################################################## 100.0%
Error: The `brew link` step did not complete successfully
The formula built, but is not symlinked into /usr/local
Could not symlink bin/mvn
Target /usr/local/bin/mvn
is a symlink belonging to maven. You can unlink it:
  brew unlink maven

To force the link and overwrite all conflicting files:
  brew link --overwrite maven

To list all files that would be deleted:
  brew link --overwrite --dry-run maven

Possible conflicting files are:
/usr/local/bin/mvn -> /usr/local/Cellar/maven/3.3.3/bin/mvn
/usr/local/bin/mvnDebug -> /usr/local/Cellar/maven/3.3.3/bin/mvnDebug
/usr/local/bin/mvnyjp -> /usr/local/Cellar/maven/3.3.3/bin/mvnyjp
==> Summary
ğŸº  /usr/local/Cellar/maven/3.6.3_1: 87 files, 10.7MB, built in 7 seconds
Removing: /usr/local/Cellar/maven/3.3.3... (92 files, 9MB)
==> Checking for dependents of upgraded formulae...
==> No broken dependents found!
==> Caveats
==> openjdk
For the system Java wrappers to find this JDK, symlink it with
  sudo ln -sfn /usr/local/opt/openjdk/libexec/openjdk.jdk /Library/Java/JavaVirtualMachines/openjdk.jdk

openjdk is keg-only, which means it was not symlinked into /usr/local,
because it shadows the macOS `java` wrapper.

If you need to have openjdk first in your PATH run:
  echo 'export PATH="/usr/local/opt/openjdk/bin:$PATH"' >> /Users/lzw/.bash_profile

For compilers to find openjdk you may need to set:
  export CPPFLAGS="-I/usr/local/opt/openjdk/include"
```

æ³¨æ„åˆ°`brew`çš„è¼¸å‡ºæ—¥èªŒä¸­`maven`æ²’æœ‰å¾ˆå¥½åœ°è¢«éˆæ¥ã€‚æ¥ä¸‹ä¾†ï¼Œé€²è¡Œå¼·åˆ¶éˆæ¥åˆ°`3.6.3_1`ç‰ˆæœ¬ã€‚

```shell
  brew link --overwrite maven
```

`Hadoop`å°±å®‰è£æˆåŠŸäº†ã€‚

> ## Modules
>
> The project includes these modules:
>
> - **Hadoop Common**: The common utilities that support the other Hadoop modules.
> - **Hadoop Distributed File System (HDFSâ„¢)**: A distributed file system that provides high-throughput access to application data.
> - **Hadoop YARN**: A framework for job scheduling and cluster resource management.
> - **Hadoop MapReduce**: A YARN-based system for parallel processing of large data sets.
> - **Hadoop Ozone**: An object store for Hadoop.

èªªæœ‰é€™äº›æ¨¡å¡Šã€‚é€™æœƒæ•²å…¥`hadoop`å‡ºç¾äº†ï¼š

```shell
$ hadoop
Usage: hadoop [OPTIONS] SUBCOMMAND [SUBCOMMAND OPTIONS]
 or    hadoop [OPTIONS] CLASSNAME [CLASSNAME OPTIONS]
  where CLASSNAME is a user-provided Java class

  OPTIONS is none or any of:

--config dir                     Hadoop config directory
--debug                          turn on shell script debug mode
--help                           usage information
buildpaths                       attempt to add class files from build tree
hostnames list[,of,host,names]   hosts to use in slave mode
hosts filename                   list of hosts to use in slave mode
loglevel level                   set the log4j level for this command
workers                          turn on worker mode

  SUBCOMMAND is one of:
    Admin Commands:

daemonlog     get/set the log level for each daemon

    Client Commands:

archive       create a Hadoop archive
checknative   check native Hadoop and compression libraries availability
classpath     prints the class path needed to get the Hadoop jar and the required libraries
conftest      validate configuration XML files
credential    interact with credential providers
distch        distributed metadata changer
distcp        copy file or directories recursively
dtutil        operations related to delegation tokens
envvars       display computed Hadoop environment variables
fs            run a generic filesystem user client
gridmix       submit a mix of synthetic job, modeling a profiled from production load
jar <jar>     run a jar file. NOTE: please use "yarn jar" to launch YARN applications, not this command.
jnipath       prints the java.library.path
kdiag         Diagnose Kerberos Problems
kerbname      show auth_to_local principal conversion
key           manage keys via the KeyProvider
rumenfolder   scale a rumen input trace
rumentrace    convert logs into a rumen trace
s3guard       manage metadata on S3
trace         view and modify Hadoop tracing settings
version       print the version

    Daemon Commands:

kms           run KMS, the Key Management Server
registrydns   run the registry DNS server

SUBCOMMAND may print help when invoked w/o parameters or with -h.
```

å®˜ç¶²çµ¦äº†äº›ä¾‹å­ã€‚

```shell
  $ mkdir input
  $ cp etc/hadoop/*.xml input
  $ bin/hadoop jar share/hadoop/mapreduce/hadoop-mapreduce-examples-3.2.2.jar grep input output 'dfs[a-z.]+'
  $ cat output/*
```

æ³¨æ„åˆ°æœ‰`share/hadoop/mapreduce/hadoop-mapreduce-examples-3.2.2.jar`ã€‚é€™æ„å‘³è‘—ä¹Ÿè¨±æœ‰äº›æ¨£ä¾‹æ–‡ä»¶æˆ‘å€‘æ²’æœ‰å¾—åˆ°ã€‚çŒœæ¸¬ç”¨`Homebrew`å®‰è£æœƒæ²’æœ‰é€™äº›æ–‡ä»¶ã€‚æˆ‘å€‘å¾å®˜ç¶²ä¸‹è¼‰äº†å®‰è£æ–‡ä»¶åŒ…ã€‚

```shell
$ tree . -L 1
.
â”œâ”€â”€ LICENSE-binary
â”œâ”€â”€ LICENSE.txt
â”œâ”€â”€ NOTICE-binary
â”œâ”€â”€ NOTICE.txt
â”œâ”€â”€ README.txt
â”œâ”€â”€ bin
â”œâ”€â”€ etc
â”œâ”€â”€ include
â”œâ”€â”€ lib
â”œâ”€â”€ libexec
â”œâ”€â”€ licenses-binary
â”œâ”€â”€ sbin
â””â”€â”€ share
```

å‡ºç¾äº†`share`ç›®éŒ„ã€‚ç„¶è€Œ`Homebrew`çœŸçš„æ²’æœ‰é™„åŠ çš„é€™äº›æ–‡ä»¶å—ã€‚æ‰¾åˆ°`Homebrew`å®‰è£çš„ç›®éŒ„ã€‚

```shell
$ type hadoop
hadoop is /usr/local/bin/hadoop
$ ls -alrt /usr/local/bin/hadoop
lrwxr-xr-x  1 lzw  admin  33 Mar 11 00:48 /usr/local/bin/hadoop -> ../Cellar/hadoop/3.3.0/bin/hadoop
$ cd /usr/local/Cellar/hadoop/3.3.0
```

é€™æ˜¯åœ¨`/usr/local/Cellar/hadoop/3.3.0/libexec/share/hadoop`ä¸‹æ‰“å°çš„ç›®éŒ„æ¨¹

```shell
$ tree . -L 2
.
â”œâ”€â”€ client
â”‚Â Â  â”œâ”€â”€ hadoop-client-api-3.3.0.jar
â”‚Â Â  â”œâ”€â”€ hadoop-client-minicluster-3.3.0.jar
â”‚Â Â  â””â”€â”€ hadoop-client-runtime-3.3.0.jar
â”œâ”€â”€ common
â”‚Â Â  â”œâ”€â”€ hadoop-common-3.3.0-tests.jar
â”‚Â Â  â”œâ”€â”€ hadoop-common-3.3.0.jar
â”‚Â Â  â”œâ”€â”€ hadoop-kms-3.3.0.jar
â”‚Â Â  â”œâ”€â”€ hadoop-nfs-3.3.0.jar
â”‚Â Â  â”œâ”€â”€ hadoop-registry-3.3.0.jar
â”‚Â Â  â”œâ”€â”€ jdiff
â”‚Â Â  â”œâ”€â”€ lib
â”‚Â Â  â”œâ”€â”€ sources
â”‚Â Â  â””â”€â”€ webapps
â”œâ”€â”€ hdfs
â”‚Â Â  â”œâ”€â”€ hadoop-hdfs-3.3.0-tests.jar
â”‚Â Â  â”œâ”€â”€ hadoop-hdfs-3.3.0.jar
â”‚Â Â  â”œâ”€â”€ hadoop-hdfs-client-3.3.0-tests.jar
â”‚Â Â  â”œâ”€â”€ hadoop-hdfs-client-3.3.0.jar
â”‚Â Â  â”œâ”€â”€ hadoop-hdfs-httpfs-3.3.0.jar
â”‚Â Â  â”œâ”€â”€ hadoop-hdfs-native-client-3.3.0-tests.jar
â”‚Â Â  â”œâ”€â”€ hadoop-hdfs-native-client-3.3.0.jar
â”‚Â Â  â”œâ”€â”€ hadoop-hdfs-nfs-3.3.0.jar
â”‚Â Â  â”œâ”€â”€ hadoop-hdfs-rbf-3.3.0-tests.jar
â”‚Â Â  â”œâ”€â”€ hadoop-hdfs-rbf-3.3.0.jar
â”‚Â Â  â”œâ”€â”€ jdiff
â”‚Â Â  â”œâ”€â”€ lib
â”‚Â Â  â”œâ”€â”€ sources
â”‚Â Â  â””â”€â”€ webapps
â”œâ”€â”€ mapreduce
â”‚Â Â  â”œâ”€â”€ hadoop-mapreduce-client-app-3.3.0.jar
â”‚Â Â  â”œâ”€â”€ hadoop-mapreduce-client-common-3.3.0.jar
â”‚Â Â  â”œâ”€â”€ hadoop-mapreduce-client-core-3.3.0.jar
â”‚Â Â  â”œâ”€â”€ hadoop-mapreduce-client-hs-3.3.0.jar
â”‚Â Â  â”œâ”€â”€ hadoop-mapreduce-client-hs-plugins-3.3.0.jar
â”‚Â Â  â”œâ”€â”€ hadoop-mapreduce-client-jobclient-3.3.0-tests.jar
â”‚Â Â  â”œâ”€â”€ hadoop-mapreduce-client-jobclient-3.3.0.jar
â”‚Â Â  â”œâ”€â”€ hadoop-mapreduce-client-nativetask-3.3.0.jar
â”‚Â Â  â”œâ”€â”€ hadoop-mapreduce-client-shuffle-3.3.0.jar
â”‚Â Â  â”œâ”€â”€ hadoop-mapreduce-client-uploader-3.3.0.jar
â”‚Â Â  â”œâ”€â”€ hadoop-mapreduce-examples-3.3.0.jar
â”‚Â Â  â”œâ”€â”€ jdiff
â”‚Â Â  â”œâ”€â”€ lib-examples
â”‚Â Â  â””â”€â”€ sources
â”œâ”€â”€ tools
â”‚Â Â  â”œâ”€â”€ dynamometer
â”‚Â Â  â”œâ”€â”€ lib
â”‚Â Â  â”œâ”€â”€ resource