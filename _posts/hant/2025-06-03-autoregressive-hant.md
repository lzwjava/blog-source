---
audio: false
generated: true
lang: hant
layout: post
title: 自迴歸模型生成
translated: true
type: note
---

在 GPT（生成式預訓練轉換器）這類模型的語境中，「自回歸」一詞指的是模型生成輸出序列的方法。具體而言，這表示模型會逐個生成序列中的每個項目，並將其先前的輸出作為生成下一個項目的額外輸入上下文。

以下是此類模型中自回歸過程的運作原理詳解：

1. **序列化生成**：模型以序列化方式生成文本。它從初始輸入（通常稱為提示）開始，每次生成一個詞元（可以是單詞、單詞的一部分或字符）。

2. **對先前詞元的依賴性**：模型在生成每個新詞元時，會考慮所有先前生成的詞元。這意味著每個步驟的輸出都取決於之前所有步驟的輸出。

3. **反饋循環**：模型採用反饋循環機制，將時間步 \\( t \\) 的輸出作為時間步 \\( t+1 \\) 的輸入。這種機制使模型能夠在較長序列中保持連貫性和上下文關聯。

4. **概率分佈**：在每個步驟中，模型會生成可能的下一個詞元的概率分佈，並根據此分佈選擇一個詞元（通常採用採樣或選擇最可能詞元等技術）。

總而言之，在 GPT 及類似模型的語境中，「自回歸」意味著模型通過逐步生成序列的方式，使每個步驟都依賴於先前步驟，從而產生連貫且符合語境的文本。