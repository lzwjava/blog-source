---
audio: false
generated: true
lang: hant
layout: post
title: 餘弦相似性
translated: true
type: note
---

餘弦相似度是機器學習中廣泛使用的度量標準，用於衡量高維空間中兩個向量之間的相似性。它在自然語言處理（NLP）、資訊檢索和推薦系統等領域特別受歡迎，因為它能夠捕捉向量之間的方向（或角度），而不是它們的大小。這使得它在比較文本文件、用戶偏好或嵌入等對象時具有魯棒性，其中向量的方向比其長度更重要。

### 什麼是餘弦相似度？

餘弦相似度通過計算兩個向量之間角度的餘弦值來量化它們的相似程度。數學上，它定義為：

\\[
\text{餘弦相似度} = \cos(\theta) = \frac{A \cdot B}{\|A\| \|B\|}
\\]

其中：
- \\( A \\) 和 \\( B \\) 是兩個向量（例如，代表文件、嵌入或特徵集）。
- \\( A \cdot B \\) 是向量的點積，計算為 \\( \sum_{i=1}^n A_i B_i \\)。
- \\( \|A\| \\) 和 \\( \|B\| \\) 是向量 \\( A \\) 和 \\( B \\) 的歐幾里得範數（大小），分別計算為 \\( \sqrt{\sum_{i=1}^n A_i^2} \\) 和 \\( \sqrt{\sum_{i=1}^n B_i^2} \\)。
- \\( \theta \\) 是向量之間的角度。

結果範圍從：
- **1**：向量方向完全相同（角度 = 0°）。
- **0**：向量正交（角度 = 90°），表示沒有相似性。
- **-1**：向量方向相反（角度 = 180°），表示最大程度的不相似。

### 關鍵特性

1. **範圍**：餘弦相似度值介於 -1 和 1 之間，易於解釋。
2. **大小獨立性**：由於向量被其大小歸一化，餘弦相似度關注的是方向，而不是長度。這在比較不同長度的文件或具有不同尺度的嵌入時非常有用。
3. **非負特徵**：在許多應用中（例如，具有詞頻的文本數據），向量具有非負分量，因此相似度通常範圍從 0 到 1。
4. **計算效率**：點積和範數計算簡單直接，使得餘弦相似度在處理高維數據時計算效率高。

### 在機器學習中的應用

餘弦相似度因其多功能性而被應用於各種機器學習任務中：

1. **文本分析和自然語言處理**：
   - **文件相似度**：在聚類或搜尋引擎等任務中，文件被表示為向量（例如，TF-IDF 或詞嵌入如 Word2Vec、GloVe 或 BERT）。餘弦相似度根據內容衡量兩個文件的相似程度。
   - **情感分析**：比較文本片段的情感向量。
   - **抄襲檢測**：通過比較文本的向量表示來識別文本之間的相似性。

2. **推薦系統**：
   - 餘弦相似度用於比較用戶或物品檔案（例如，在協同過濾中）。例如，它可以根據用戶的評分或行為來衡量兩個用戶偏好的相似程度。
   - 它在基於內容的過濾中非常有效，其中物品（例如，電影、產品）被表示為特徵向量。

3. **圖像和音頻處理**：
   - 在電腦視覺中，餘弦相似度比較從圖像中提取的特徵向量（例如，來自 CNN）以衡量視覺相似性。
   - 在音頻處理中，它用於比較聲譜圖或聲音片段的嵌入。

4. **聚類和分類**：
   - 在聚類算法中（例如，使用文本數據的 K-means），餘弦相似度作為距離度量來分組相似的物品。
   - 在分類任務中，它用於比較輸入向量與類別原型。

5. **異常檢測**：
   - 餘弦相似度可以通過將數據點與質心或預期模式進行比較來識別異常值。低相似度表示潛在的異常。

### 示例：文本分析中的餘弦相似度

假設我們有兩個文件表示為 TF-IDF 向量：
- 文件 1：\\( A = [2, 1, 0, 3] \\)（例如，四個詞的詞頻）。
- 文件 2：\\( B = [1, 1, 1, 0] \\)。

**步驟 1：計算點積**：
\\[
A \cdot B = (2 \cdot 1) + (1 \cdot 1) + (0 \cdot 1) + (3 \cdot 0) = 2 + 1 + 0 + 0 = 3
\\]

**步驟 2：計算範數**：
\\[
\|A\| = \sqrt{2^2 + 1^2 + 0^2 + 3^2} = \sqrt{4 + 1 + 0 + 9} = \sqrt{14} \approx 3.742
\\]
\\[
\|B\| = \sqrt{1^2 + 1^2 + 1^2 + 0^2} = \sqrt{1 + 1 + 1 + 0} = \sqrt{3} \approx 1.732
\\]

**步驟 3：計算餘弦相似度**：
\\[
\cos(\theta) = \frac{A \cdot B}{\|A\| \|B\|} = \frac{3}{3.742 \cdot 1.732} \approx \frac{3}{6.483} \approx 0.462
\\]

餘弦相似度約為 0.462，表示文件之間具有中等程度的相似性。

### 餘弦相似度的優點

- **尺度不變性**：它不受向量大小的影響，使其非常適合文本數據，其中文件長度各不相同。
- **處理高維數據**：在稀疏的高維空間中（例如，具有數千個特徵的文本數據）非常有效。
- **直觀解釋**：餘弦值直接與角度相關，提供了清晰的相似度度量。

### 局限性

- **忽略大小**：在某些情況下，大小差異很重要（例如，在比較絕對數量時）。
- **假設線性關係**：餘弦相似度假設相似性最好通過角度距離來捕捉，但這可能並不總是成立。
- **稀疏數據敏感性**：在非常稀疏的向量中，餘弦相似度的區分度可能較低，因為許多維度對點積的貢獻很小。

### 餘弦相似度與其他度量的比較

- **歐幾里得距離**：測量直線距離，對大小敏感，與餘弦相似度不同。當方向比絕對差異更重要時，餘弦相似度是首選。
- **Jaccard 相似度**：用於集合（例如，二進制數據），關注共享元素而不是向量方向。
- **皮爾遜相關**：測量線性相關，考慮均值中心化的數據，而餘弦相似度適用於原始向量。

### 實際實現

餘弦相似度在許多機器學習庫中都有實現：
- **Python**：`scikit-learn` 在 `sklearn.metrics.pairwise` 中提供 `cosine_similarity`。
  ```python
  from sklearn.metrics.pairwise import cosine_similarity
  import numpy as np

  A = np.array([[2, 1, 0, 3]])
  B = np.array([[1, 1, 1, 0]])
  similarity = cosine_similarity(A, B)
  print(similarity)  # 輸出：[[0.46225063]]
  ```
- **TensorFlow/PyTorch**：用於深度學習模型中比較嵌入。
- **Spark**：用於大規模分佈式相似度計算。

### 高級應用

1. **詞嵌入**：餘弦相似度是比較詞或句子嵌入（例如，BERT、Sentence-BERT）以衡量語義相似度的標準方法。
2. **基於圖的模型**：用於圖神經網絡中的節點相似度。
3. **個性化搜尋**：像 Elasticsearch 這樣的搜尋引擎使用餘弦相似度來根據文件與查詢的相關性進行排名。

### 結論

餘弦相似度是一種強大、直觀且計算效率高的度量標準，用於衡量機器學習中的相似性。它對向量方向的關注使其非常適合文本、嵌入和推薦系統，儘管在大小重要的任務中不太適用。通過理解其優勢和局限性，從業者可以有效地將其應用於從自然語言處理到電腦視覺的各種應用中。如果您想深入了解特定用例或實現，請