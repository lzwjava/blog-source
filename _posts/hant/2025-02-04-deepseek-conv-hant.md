---
audio: false
generated: false
lang: hant
layout: post
title: Deepseek - 對話
translated: true
type: note
---

A：我最近在閱讀 DeepSeek-V3 的技術報告，對這個模型的規模印象深刻。6710 億參數，但每個 token 只激活 370 億？這真是龐大的 MoE 架構。它是如何運作的？

B：沒錯，這確實是一項壯舉！DeepSeek-V3 基於混合專家（MoE）框架構建，這使得每個 token 僅激活參數的一個子集。具體來說，它使用了 256 個路由專家，但每個 token 只激活 8 個。與密集模型相比，這使其效率極高，因為在密集模型中，所有參數對每個 token 都是活躍的。

A：這很合理。但它如何決定激活哪些專家？是隨機的，還是有某種路由機制？

B：問得好！路由基於 token 對專家的親和度分數。每個 token 都會被分配一個針對每個專家的分數，然後激活分數最高的前 K 個專家。DeepSeek-V3 使用 sigmoid 函數計算這些分數，這有助於平衡專家之間的負載。

A：啊，所以它不是隨機的——而是在訓練過程中學習的。但這不會導致專家使用不平衡嗎？我聽說這是 MoE 模型的常見問題。

B：確實如此！專家使用不平衡可能是個問題，但 DeepSeek-V3 引入了一種無輔助損失的策略來處理這個問題。它沒有添加單獨的損失項來鼓勵負載平衡，而是動態調整每個專家的偏置項。如果某個專家過載，其偏置會降低；如果負載不足，則偏置增加。這在保持負載平衡的同時，不會降低模型性能。

A：這很聰明。所以，沒有輔助損失意味著對主要訓練目標的干擾更少。但這與使用輔助損失的傳統 MoE 模型相比如何？

B：對。傳統的 MoE 模型通常使用輔助損失來鼓勵負載平衡，但這些損失有時會損害性能。DeepSeek-V3 的無輔助損失方法避免了這種權衡。事實上，消融研究表明，它在依賴輔助損失的模型上始終表現更佳，尤其是在編程和數學等任務上。

A：有趣。說到編程和數學，我注意到 DeepSeek-V3 在 HumanEval 和 MATH 等基準測試中表現異常出色。這裡面的秘訣是什麼？

B：其中很大一部分是多 token 預測（MTP）目標。DeepSeek-V3 不僅預測下一個 token，還在每個位置預測多個未來的 token。這使訓練信號更加密集，並幫助模型提前規劃，這對於需要順序推理的任務（如編程和數學）尤其有用。

A：等等，所以它一次預測多個 token？在推理過程中這如何運作？它仍然使用 MTP，還是僅用於訓練？

B：在推理過程中，MTP 模塊可以被丟棄，模型表現得像標準的自回歸模型。但這裡很酷的部分是：MTP 模塊還可以重新用於推測解碼，通過並行預測多個 token 然後驗證它們，從而加速生成。

A：這招很巧妙。所以，就是在訓練時獲得 MTP 的好處，然後用它來加速推理。但注意力機制呢？我看到了一些關於多頭潛在注意力（MLA）的內容。這又是如何融入的？

B：MLA 是另一個關鍵創新。它通過壓縮鍵值（KV）快取來減少記憶體佔用。它不使用完整的注意力鍵和值，而是使用低秩聯合壓縮來表示它們。這在推理過程中顯著減少了 KV 快取的大小，同時保持與標準多頭注意力相當的性能。

A：這對效率來說是個巨大的勝利。但壓縮不會導致一些信息損失嗎？它如何保持性能？

B：說得好。壓縮旨在通過專注於捕捉鍵和值基本特徵的潛在向量來保留最重要的信息。該模型還使用旋轉位置嵌入（RoPE）來保持位置信息，這有助於減輕壓縮帶來的任何損失。

A：明白了。所以，MLA 是關於效率而不犧牲太多性能。但訓練呢？訓練這麼大的模型一定非常昂貴。DeepSeek-V3 是如何控制成本的？

B：訓練效率是一個主要關注點。DeepSeek-V3 使用 FP8 混合精度框架，減少了記憶體使用並加速了計算。它還採用 DualPipe 算法進行管道並行，最大限度地減少管道氣泡並重疊計算與通信。這些優化使得模型能夠在僅使用 278.8 萬 H800 GPU 小時的情況下，在 14.8 萬億個 token 上進行訓練。

A：這令人印象深刻。但 FP8 訓練可能很棘手——他們如何處理精度問題？我聽說低精度訓練可能導致不穩定。

B：你說得對。由於動態範圍有限，FP8 訓練具有挑戰性。DeepSeek-V3 通過細粒度量化來解決這個問題，其中激活和權重被分組到更小的切片或塊中，並獨立縮放。這減少了異常值的影響並保持訓練穩定。他們還對關鍵操作使用高精度累積以保持準確性。

A：這很合理。所以，這是在效率和精度之間取得平衡。但數據呢？14.8 萬億個 token 是一個巨大的數據集。它是在什麼樣的數據上訓練的？

B：數據集多樣且高質量，重點關注英文和中文文本。它還包含大量數學和編程數據，這有助於模型在這些領域表現出色。數據管道經過優化，以在保持多樣性的同時最大限度地減少冗餘，並且他們使用文檔打包等技術來確保數據完整性。

A：這解釋了在編程和數學任務上的強大表現。但多語言性能呢？它能很好地處理其他語言嗎？

B：是的，DeepSeek-V3 是在多語言語料庫上訓練的，並且在包括非英語任務的 MMMLU 等基準測試中表現良好。它在中文方面尤其強大，在 C-Eval 和 CMMLU 等中文基準測試中超越了 Qwen2.5 等模型。

A：這令人印象深刻。但長上下文任務呢？我看到它支持高達 128K 個 token。它如何處理如此長的輸入？

B：DeepSeek-V3 分兩個階段擴展其上下文長度：首先擴展到 32K 個 token，然後使用 YaRN 技術擴展到 128K 個 token。這使其能夠有效處理長上下文任務，如文檔摘要和檢索。它在評估長上下文理解的 'Needle In A Haystack' 測試中也表現良好。

A：這相對於以前的模型是一個巨大的改進。但部署呢？他們如何處理這麼大模型的推理？

B：推理在 H800 集群上處理，GPU 使用 NVLink 和 InfiniBand 互連。部署策略將預填充和解碼階段分開，以確保高吞吐量和低延遲。他們還使用冗餘專家來平衡推理過程中的負載，這有助於保持效率。

A：這有很多優化。但有哪些限制？這麼大的模型肯定有一些權衡。

B：一個限制是部署單元的大小。DeepSeek-V3 需要相對較大的集群才能進行高效推理，這對較小的團隊來說可能是一個挑戰。生成速度也有改進的空間，儘管帶有 MTP 的推測解碼有所幫助。

A：很公平。但總的來說，這似乎是一個巨大的進步。DeepSeek-V3 的下一步是什麼？他們在探索哪些未來方向？

B：他們正在研究幾個領域，例如改進架構以支持無限上下文長度，探索額外的訓練信號來源，以及增強模型的推理能力。他們還在研究更全面的評估方法，以更好地評估模型性能。

A：聽起來他們不會很快放慢腳步。感謝你帶我了解這一切——DeepSeek-V3 無疑是開源 LLM 領域的遊戲規則改變者。

B：絕對是！看到開源模型發展到如此程度真是令人興奮。DeepSeek-V3 正在突破界限，我迫不及待想看看他們下一步會做什麼。

A：你提到 DeepSeek-V3 使用 FP8 混合精度訓練。我很好奇——這與 BF16 或 FP16 相比如何？FP8 對於訓練這麼大的模型真的足夠穩定嗎？

B：這是個好問題。由於動態範圍有限，FP8 確實更具挑戰性，但 DeepSeek-V3 使用細粒度量化策略來緩解這個問題。例如，激活被分組為 1x128 的切片，權重被分組為 128x128 的塊。每個組獨立縮放，這有助於處理異常值並保持訓練穩定。

A：有趣。所以，這不僅僅是全面的 FP8 量化——它更加細緻。但這不會因為管理所有這些組和縮放因子而引入額外開銷嗎？

B：確實會，但與好處相比，開銷是最小的。關鍵在於 FP8 減少了記憶體使用並加速了計算，這對於訓練如此大的模型至關重要。他們還對關鍵操作（如矩陣乘法）使用高精度累積，以確保數值穩定性。

A：明白了。所以，這是在精度和效率之間的權衡，但他們設法取得了良好的平衡。DualPipe 算法呢？它是如何工作的？

B：DualPipe 旨在最小化管道並行中的管道氣泡。它通過將每個工作塊劃分為四個組件來重疊計算和通信：注意力、全對所有分發、MLP 和全對所有合併。在反向傳播過程中，它進一步將計算拆分為「輸入反向傳播」和「權重反向傳播」，這允許更高效的重疊。

A：聽起來很複雜，但很有道理。所以，它基本上是通過重疊計算來隱藏通信開銷。這與其他管道並行方法（如 1F1B 或 Zero Bubble）相比如何？

B：與 1F1B 和 Zero Bubble 相比，DualPipe 的管道氣泡更少。它還允許雙向調度，其中微批次從管道的兩端輸入。這進一步減少了空閒時間並提高了整體效率。事實上，DualPipe 實現了接近零的全對所有通信開銷，這對於擴展 MoE 模型至關重要。

A：這令人印象深刻。但記憶體使用呢？DualPipe 是否需要比其他方法更多的記憶體？

B：它確實需要稍多的記憶體，因為它保留了模型參數的兩個副本，但增加是可控的。記憶體佔用通過 RMSNorm 和 MLA 上投影的重計算等技術進行了優化，這消除了存儲中間激活的需要。

A：啊，所以他們是用一點記憶體換取更好的效率。這似乎是公平的權衡。說到記憶體，他們如何處理這麼大上下文長度的 KV 快取？128K 個 token 一定需要巨大的快取。

B：這就是 MLA 真正發揮作用的地方。通過壓縮 KV 快取，他們顯著減小了其大小。他們不存儲完整的注意力鍵和值，而是存儲壓縮的潛在向量，這些向量要小得多。這使得 DeepSeek-V3 能夠處理長上下文而不會遇到記憶體瓶頸。

A：這是一個聰明的解決方案。但注意力的質量呢？壓縮是否影響模型關注正確 token 的能力？

B：壓縮旨在保留最重要的信息，因此對注意力質量的影響是最小的。他們還使用 RoPE（旋轉位置嵌入）來保持位置信息，這有助於模型理解 token 的相對位置，即使鍵和值被壓縮。

A：有道理。所以，MLA 是雙贏的——它減少了記憶體使用，而沒有犧牲太多性能。但訓練數據呢？你提到是 14.8 萬億個 token。他們如何確保如此大規模數據集的質量和多樣性？

B：數據集經過精心策劃，以包含高質量和多樣化的 token。他們優化數據管道，以在保持多樣性的同時最大限度地減少冗餘，並且他們使用文檔打包等技術來確保數據完整性。語料庫包括英文和中文文本的混合，並強調數學和編程樣本。

A：這解釋了在編程和數學任務上的強大表現。但多語言任務呢？它能很好地處理其他語言嗎？

B：是的，DeepSeek-V3 是在多語言語料庫上訓練的，並且在包括非英語任務的 MMMLU 等基準測試中表現良好。它在中文方面尤其強大，在 C-Eval 和 CMMLU 等中文基準測試中超越了 Qwen2.5 等模型。

A：這令人印象深刻。但長上下文任務呢？我看到它支持高達 128K 個 token。它如何處理如此長的輸入？

B：DeepSeek-V3 分兩個階段擴展其上下文長度：首先擴展到 32K 個 token，然後使用 YaRN 技術擴展到 128K 個 token。這使其能夠有效處理長上下文任務，如文檔摘要和檢索。它在評估長上下文理解的 'Needle In A Haystack' 測試中也表現良好。

A：這相對於以前的模型是一個巨大的改進。但部署呢？他們如何處理這麼大模型的推理？

B：推理在 H800 集群上處理，GPU 使用 NVLink 和 InfiniBand 互連。部署策略將預填充和解碼階段分開，以確保高吞吐量和低延遲。他們還使用冗餘專家來平衡推理過程中的負載，這有助於保持效率。

A：這有很多優化。但有哪些限制？這麼大的模型肯定有一些權衡。

B：一個限制是部署單元的大小。DeepSeek-V3 需要相對較大的集群才能進行高效推理，這對較小的團隊來說可能是一個挑戰。生成速度也有改進的空間，儘管帶有 MTP 的推測解碼有所幫助。

A：很公平。但總的來說，這似乎是一個巨大的進步。DeepSeek-V3 的下一步是什麼？他們在探索哪些未來方向？

B：他們正在研究幾個領域，例如改進架構以支持無限上下文長度，探索額外的訓練信號來源，以及增強模型的推理能力。他們還在研究更全面的評估方法，以更好地評估模型性能。

A：聽起來他們不會很快放慢腳步。感謝你帶我了解這一切——DeepSeek-V3 無疑是開源 LLM 領域的遊戲規則改變者。

B：絕對是！看到開源模型發展到如此程度真是令人興奮。DeepSeek-V3 正在突破界限，我迫不及待想看看他們下一步會做什麼。