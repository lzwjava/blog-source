---
audio: false
generated: true
lang: hant
layout: post
title: 變壓器中的K、Q、V機制
translated: true
type: note
---

**鍵 (K)、查詢 (Q) 與值 (V)** 機制是基於 Transformer 的大型語言模型 (LLM) 中**自注意力**機制的核心組件。此機制由開創性論文《Attention is All You Need》（Vaswani 等人，2017 年）提出，使模型在處理或生成文本時能夠權衡序列中不同詞語的重要性。以下，我將全面介紹 **K、Q、V** 機制在 Transformer LLM 中的運作方式，涵蓋其直覺理解、數學公式、在自注意力中的實現，以及其在整體架構中的角色。

---

### 1. **K、Q、V 在自注意力中的直覺理解**
自注意力機制讓 Transformer 模型能夠透過關注序列中每個詞語（或詞元）的相關部分來處理輸入序列。**K、Q、V** 組件是此過程的基礎，使模型能夠動態決定輸入序列中哪些部分彼此最相關。

- **查詢 (Q)：** 代表一個詞元對序列中其他詞元提出的「問題」。對於每個詞元，查詢向量編碼了該詞元希望從序列其他部分獲取什麼資訊。
- **鍵 (K)：** 代表序列中每個詞元的「描述」。鍵向量編碼了一個詞元能向其他詞元提供什麼資訊。
- **值 (V)：** 代表詞元實際承載的內容或資訊。一旦模型透過 Q 和 K 的互動確定了哪些詞元是相關的，它就會檢索相應的值向量來構建輸出。

**Q** 與 **K** 之間的互動決定了每個詞元應對其他每個詞元投入多少注意力，而 **V** 向量則根據此注意力進行加權和組合，以產生每個詞元的輸出。

可以將其想像成圖書館檢索：
- **查詢**：你的檢索查詢（例如 "machine learning"）。
- **鍵**：圖書館中書籍的標題或元數據，你將其與查詢比對以找到相關書籍。
- **值**：在識別出相關書籍後，你檢索到的書籍實際內容。

---

### 2. **K、Q、V 在自注意力中的運作方式**
自注意力機制計算 **值** 向量的加權總和，其中權重由 **查詢** 和 **鍵** 向量之間的相似度決定。以下是此過程的逐步分解：

#### 步驟 1：輸入表示
- Transformer 層的輸入是一個詞元序列（例如單詞或子詞），每個詞元表示為一個高維嵌入向量（例如維度 \\( d_{\text{model}} = 512 \\)）。
- 對於一個長度為 \\( n \\) 的詞元序列，輸入是一個矩陣 \\( X \in \mathbb{R}^{n \times d_{\text{model}}} \\)，其中每一行是一個詞元的嵌入。

#### 步驟 2：線性轉換生成 K、Q、V
- 對每個詞元，計算三個向量：**查詢 (Q)**、**鍵 (K)** 和 **值 (V)**。這些是透過對輸入嵌入應用學習到的線性轉換來獲得的：
  \\[
  Q = X W_Q, \quad K = X W_K, \quad V = X W_V
  \\]
  - \\( W_Q, W_K, W_V \in \mathbb{R}^{d_{\text{model}} \times d_k} \\) 是學習到的權重矩陣。
  - 通常，\\( d_k = d_v \\)，且它們常設定為 \\( d_{\text{model}} / h \\)（其中 \\( h \\) 是注意力頭的數量，稍後解釋）。
  - 結果是：
    - \\( Q \in \mathbb{R}^{n \times d_k} \\)：所有詞元的查詢矩陣。
    - \\( K \in \mathbb{R}^{n \times d_k} \\)：所有詞元的鍵矩陣。
    - \\( V \in \mathbb{R}^{n \times d_v} \\)：所有詞元的值矩陣。

#### 步驟 3：計算注意力分數
- 注意力機制透過計算一個詞元的查詢向量與所有詞元的鍵向量之間的**點積**，來計算每個詞元應對其他每個詞元投入多少注意力：
  \\[
  \text{Attention Scores} = Q K^T
  \\]
  - 這產生一個矩陣 \\( \in \mathbb{R}^{n \times n} \\)，其中每個元素 \\( (i, j) \\) 代表詞元 \\( i \\) 的查詢與詞元 \\( j \\) 的鍵之間的未歸一化相似度。
- 為了穩定梯度並防止數值過大，分數會透過鍵維度的平方根進行縮放：
  \\[
  \text{Scaled Scores} = \frac{Q K^T}{\sqrt{d_k}}
  \\]
  - 這稱為**縮放點積注意力**。

#### 步驟 4：應用 Softmax 獲取注意力權重
- 縮放後的分數通過 **softmax** 函數，將其轉換為機率（注意力權重），使得每個詞元的權重總和為 1：
  \\[
  \text{Attention Weights} = \text{softmax}\left( \frac{Q K^T}{\sqrt{d_k}} \right)
  \\]
  - 結果是一個矩陣 \\( \in \mathbb{R}^{n \times n} \\)，其中每一行代表一個詞元對序列中所有詞元的注意力分佈。
  - 高的注意力權重表示相應的詞元彼此高度相關。

#### 步驟 5：計算輸出
- 注意力權重用於計算 **值** 向量的加權總和：
  \\[
  \text{Attention Output} = \text{softmax}\left( \frac{Q K^T}{\sqrt{d_k}} \right) V
  \\]
  - 輸出是一個矩陣 \\( \in \mathbb{R}^{n \times d_v} \\)，其中每一行是詞元的新表示，融合了基於相關性從所有其他詞元獲得的資訊。

#### 步驟 6：多頭注意力
- 實際上，Transformers 使用 **多頭注意力**，上述過程會並行執行多次（使用不同的 \\( W_Q, W_K, W_V \\)）以捕捉不同類型的關係：
  - 輸入被分割成 \\( h \\) 個頭，每個頭具有較小的維度為 \\( d_k = d_{\text{model}} / h \\) 的 \\( Q, K, V \\) 向量。
  - 每個頭計算自己的注意力輸出。
  - 所有頭的輸出被拼接起來，並通過一個最終的線性轉換：
    \\[
    \text{MultiHead}(Q, K, V) = \text{Concat}(\text{head}_1, \text{head}_2, \dots, \text{head}_h) W_O
    \\]
    其中 \\( W_O \in \mathbb{R}^{h \cdot d_v \times d_{\text{model}}} \\) 是一個學習到的輸出投影矩陣。

---

### 3. **K、Q、V 在 Transformer LLM 中的角色**
**K、Q、V** 機制在 Transformer 架構的不同部分中使用，取決於注意力的類型：

- **編碼器中的自注意力（例如 BERT）：**
  - 所有詞元都關注輸入序列中的所有其他詞元（雙向注意力）。
  - \\( Q, K, V \\) 全部源自相同的輸入序列 \\( X \\)。
  - 這使得模型能夠捕捉前後詞元的上下文，對於文本分類或問答等任務非常有用。

- **解碼器中的自注意力（例如 GPT）：**
  - 在像 GPT 這樣的自回歸模型中，解碼器使用**掩碼自注意力**來防止關注未來的詞元（因為模型是順序生成文本的）。
  - 掩碼確保對於每個詞元 \\( i \\)，在 softmax 之前，對於詞元 \\( j > i \\) 的注意力分數被設定為 \\(-\infty\\)，實際上使它們的權重為零。
  - \\( Q, K, V \\) 仍然源自輸入序列，但注意力是因果性的（僅關注先前的詞元）。

- **編碼器-解碼器模型中的交叉注意力（例如 T5）：**
  - 在編碼器-解碼器架構中，解碼器使用交叉注意力來關注編碼器的輸出。
  - 這裡，\\( Q \\) 源自解碼器的輸入，而 \\( K \\) 和 \\( V \\) 則來自編碼器的輸出，允許解碼器在生成輸出時專注於輸入序列的相關部分。

---

### 4. **為何 K、Q、V 如此有效**
**K、Q、V** 機制之所以強大，有幾個原因：
- **動態上下文化**：它允許每個詞元根據其他詞元的內容來收集資訊，而不是依賴固定模式（例如在 RNN 或 CNN 中）。
- **並行化**：與循環神經網絡不同，自注意力同時處理所有詞元，使其在現代硬體（如 GPU）上非常高效。
- **靈活性**：多頭注意力使模型能夠透過學習 \\( Q, K, V \\) 的不同投影來捕捉多樣化的關係（例如句法、語義）。
- **可擴展性**：該機制能很好地適應長序列（儘管計算成本隨序列長度呈二次方增長，可透過稀疏注意力或高效 Transformer 等技術緩解）。

---

### 5. **數學總結**
縮放點積注意力的公式為：
\\[
\text{Attention}(Q, K, V) = \text{softmax}\left( \frac{Q K^T}{\sqrt{d_k}} \right) V
\\]
對於多頭注意力：
\\[
\text{MultiHead}(Q, K, V) = \text{Concat}(\text{head}_1, \dots, \text{head}_h) W_O
\\]
其中：
\\[
\text{head}_i = \text{Attention}(Q W_{Q_i}, K W_{K_i}, V W_{V_i})
\\]

---

### 6. **實際範例**
考慮句子：*"The cat sat on the mat."*
- **輸入**：每個單詞被轉換為嵌入向量（例如透過詞嵌入層）。
- **Q、K、V 計算**：對於每個詞元，模型使用學習到的權重計算查詢、鍵和值向量。
- **注意力分數**：對於單詞 "cat"，其查詢向量可能會對 "sat" 和 "mat" 分配高的注意力分數，因為它們在上下文中有關聯，而對 "the" 給予較低的分數。
- **輸出**：注意力機制為 "cat" 產生一個新的表示，其中融合了來自 "sat" 和 "mat" 的資訊，並根據它們的注意力分數進行加權。

---

### 7. **擴展與優化**
- **高效注意力**：為了處理長序列，像 Performer、Linformer 或 Longformer 這樣的變體降低了注意力的計算複雜度。
- **位置編碼**：由於自注意力是置換不變的，Transformers 會向輸入嵌入添加位置編碼以捕捉詞序，這間接影響了 \\( Q, K, V \\)。
- **層堆疊**：Transformers 堆疊多層自注意力，讓 \\( Q, K, V \\) 能夠在越來越抽象的表示上運作。

---

### 8. **結論**
**K、Q、V** 機制是 Transformer 自注意力的核心，使 LLM 能夠高效且有效地建模文本中的複雜關係。透過基於查詢和鍵計算注意力分數，並從值向量聚合資訊，Transformers 在語言建模、翻譯等任務中實現了最先進的性能。理解此機制是掌握現代 LLM 如何處理和生成類人文本的關鍵。

如果你想更深入了解任何特定方面（例如多頭注意力、程式碼實現或在特定模型中的應用），請告訴我！