---
audio: false
generated: true
lang: hant
layout: post
title: 變壓器
translated: true
type: note
---

**Transformer** 是一種革命性的神經網絡架構，已成為大多數現代大型語言模型（LLM）的基礎。它由 Vaswani 等人在 2017 年的開創性論文《Attention Is All You Need》中提出，從此從根本上改變了自然語言處理（NLP）領域。

與先前主導的架構（如循環神經網絡 RNN）逐步處理序列數據不同，Transformer **並行**處理整個輸入序列。這一關鍵差異使得訓練速度顯著提升，並能夠捕捉文本中的長距離依賴關係。

以下深入探討 Transformer 架構在 LLM 背景下的關鍵組件和概念：

**1. 核心思想：注意力機制**

Transformer 的核心創新在於**注意力機制**，特別是**自注意力**。該機制讓模型在處理特定詞語（或標記）時，能夠權衡輸入序列中不同詞語的重要性。自注意力使模型能夠考慮整個上下文來理解詞語的含義和關係，而不僅僅依賴於緊鄰的前文（如 RNN 那樣）。

可以這樣理解：當你閱讀一個句子時，你並不會孤立地處理每個詞語。你的大腦會同時考慮所有詞語，以理解整體含義以及每個詞語在其中的作用。自注意力機制模仿了這種行為。

**自注意力工作原理（簡化版）：**

對於輸入序列中的每個詞語，Transformer 會計算三個向量：

*   **查詢向量（Q）：** 代表當前詞語在其他詞語中「尋找」什麼。
*   **鍵向量（K）：** 代表其他每個詞語「包含」什麼信息。
*   **值向量（V）：** 代表其他每個詞語所持有的、可能相關的實際信息。

然後，自注意力機制執行以下步驟：

1.  **計算注意力分數：** 計算一個詞語的查詢向量與序列中其他每個詞語的鍵向量的點積。這些分數表明其他每個詞語的信息與當前詞語的相關程度。
2.  **縮放分數：** 將分數除以鍵向量維度的平方根（`sqrt(d_k)`）。這種縮放有助於在訓練期間穩定梯度。
3.  **應用 Softmax：** 將縮放後的分數通過 softmax 函數，將其歸一化為 0 到 1 之間的概率。這些概率代表了**注意力權重**——當前詞語應對其他每個詞語投入多少「注意力」。
4.  **計算加權值：** 將每個詞語的值向量乘以其對應的注意力權重。
5.  **求和加權值：** 將加權後的值向量求和，產生當前詞語的**輸出向量**。這個輸出向量現在包含了輸入序列中所有其他相關詞語的信息，並根據其重要性進行了加權。

**2. 多頭注意力**

為了進一步增強模型捕捉不同類型關係的能力，Transformer 採用了**多頭注意力**。它並非只執行一次自注意力機制，而是使用不同的查詢、鍵和值權重矩陣集，並行地多次執行該機制。每個「頭」學習專注於詞語之間關係的不同方面（例如，語法依賴關係、語義連接）。然後，所有注意力頭的輸出會被拼接起來，並進行線性變換，以產生多頭注意力層的最終輸出。

**3. 位置編碼**

由於 Transformer 並行處理所有詞語，它失去了序列中詞語**順序**的信息。為了解決這個問題，**位置編碼**被添加到輸入嵌入中。這些編碼是代表每個詞語在序列中位置的向量。它們通常是固定的模式（例如，正弦函數）或學習得到的嵌入。通過添加位置編碼，Transformer 能夠理解語言的順序特性。

**4. 編碼器和解碼器堆疊**

Transformer 架構通常包含兩個主要部分：一個**編碼器**和一個**解碼器**，兩者都由多個相同的層堆疊而成。

*   **編碼器：** 編碼器的角色是處理輸入序列並創建其豐富的表示。每個編碼器層通常包含：
    *   一個**多頭自注意力**子層。
    *   一個**前饋神經網絡**子層。
    *   圍繞每個子層的**殘差連接**，其後是**層歸一化**。殘差連接有助於訓練期間的梯度流動，而層歸一化則穩定激活值。

*   **解碼器：** 解碼器的角色是生成輸出序列（例如，在機器翻譯或文本生成中）。每個解碼器層通常包含：
    *   一個**掩碼多頭自注意力**子層。「掩碼」防止解碼器在訓練期間看到目標序列中未來的標記，確保它僅使用先前生成的標記來預測下一個標記。
    *   一個**多頭注意力**子層，用於關注編碼器的輸出。這使得解碼器在生成輸出時能夠專注於輸入序列的相關部分。
    *   一個**前饋神經網絡**子層。
    *   與編碼器類似的**殘差連接**和**層歸一化**。

**5. 前饋網絡**

每個編碼器和解碼器層都包含一個前饋神經網絡（FFN）。該網絡獨立應用於每個標記，有助於進一步處理由注意力機制學習到的表示。它通常由兩個線性變換組成，中間有一個非線性激活函數（例如 ReLU）。

**Transformer 如何在 LLM 中使用：**

LLM 主要基於**僅解碼器**的 Transformer 架構（如 GPT 模型）或**編碼器-解碼器**架構（如 T5）。

*   **僅解碼器模型：** 這些模型被訓練來根據前面的標記預測序列中的下一個標記。它們堆疊多個解碼器層。輸入提示通過這些層，最終層預測詞彙表上下一個標記的概率分佈。通過從這個分佈中自回歸地採樣，模型可以生成連貫且與上下文相關的文本。

*   **編碼器-解碼器模型：** 這些模型接收輸入序列並生成輸出序列。它們使用完整的編碼器-解碼器架構。編碼器處理輸入，解碼器使用編碼器的輸出生成目標序列。這些模型非常適合翻譯、摘要和問答等任務。

**深入理解其重要性：**

Transformer 架構對 LLM 的影響是深遠的：

*   **處理長距離依賴：** 自注意力機制允許模型直接連接序列中相距較遠的詞語，克服了 RNN 在捕捉長距離上下文方面的限制。
*   **並行處理：** 並行處理整個序列顯著減少了訓練時間，並允許使用更大的數據集和模型。
*   **可擴展性：** Transformer 架構能夠隨著數據和模型規模的增加而良好地擴展，從而催生了具有數十億甚至數萬億參數的極其強大的 LLM。
*   **上下文理解：** 注意力機制使模型能夠以更細緻的方式理解詞語的上下文，從而實現更好的語言理解和生成。
*   **創新的基礎：** Transformer 架構已成為眾多進步和變體的基礎，造就了現代 LLM 豐富多樣的格局。

總之，Transformer 架構以其核心的自注意力機制、並行處理能力以及編碼器-解碼器結構（或僅解碼器的變體），徹底改變了 NLP 領域，並且是現代大型語言模型卓越能力背後的驅動力。