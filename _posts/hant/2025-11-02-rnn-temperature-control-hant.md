---
audio: false
generated: true
image: false
lang: hant
layout: post
model: x-ai/grok-4-fast
title: 控制循環神經網絡文本生成溫度
translated: true
type: note
---

### 理解 RNN 採樣中的溫度參數

在循環神經網絡（RNN）的文本生成任務中——例如 Andrej Karpathy 在其著名字符級語言模型博文中探討的案例——溫度是一個超參數，用於調整生成文本的「隨機性」或「可預測性」。這個參數在採樣階段發揮作用，當 RNN 根據先前的字符（或標記）預測下一個字符時。若缺乏溫度控制，生成結果可能過於僵化（總是選擇最可能的下一個字符，導致單調循環）或過度隨機（純粹隨機）。溫度參數通過柔化模型對可能下一個字符的概率分佈來取得平衡。

#### 背後的簡易數學原理
RNN 會為每個可能的下一個字符輸出 *logits*（原始未歸一化分數）。這些分數通過 softmax 函數轉換為概率：

\\[
p_i = \frac{\exp(\text{logit}_i / T)}{\sum_j \exp(\text{logit}_j / T)}
\\]

- \\(T\\) 即溫度值（通常介於 0.1 至 2.0 之間）
- 當 \\(T = 1\\) 時，即標準 softmax：概率分佈反映模型的「自然」置信度
- 接著從此分佈中*採樣*下一個字符（例如通過多項式採樣），而非總是選擇最高概率字符（貪婪解碼）

此採樣過程會迭代進行：將選定的字符反饋為輸入，預測下一個字符，如此循環以生成序列。

#### 低溫模式：重複性高但穩健
- **效果**：\\(T < 1\\)（如 0.5 或接近 0）會*銳化*概率分佈。高置信度預測的概率進一步提升，低概率值則被壓縮趨近於零
- **輸出**：文本保持「穩健」與連貫，但會迅速出現重複。模型會固守最高概率路徑，如同陷入循環
- **Karpathy 博文示例**（生成 Paul Graham 風格隨筆）：在極低溫度下，會產生如下文本：  
  > 「關鍵在於它們都是同一回事 初創公司就是這樣 關鍵在於它們都是同一回事 初創公司就是這樣 關鍵在於它們都是同一回事 初創公司就是這樣」

  雖然置信度高且語法正確，但缺乏創造力——如同訓練資料的無限迴聲

#### 高溫模式：創意迸發但失序
- **效果**：\\(T > 1\\)（如 1.5 或 2.0）會*平坦化*概率分佈。概率分佈趨向均勻，使弱勢字符（低概率字符）獲得更多機會
- **輸出**：文本更具多樣性與創造性，但可能偏離常軌，出現無意義內容、拼寫錯誤或突兀轉折。模型會探索罕見路徑，雖能激發原創性，卻犧牲了連貫度
- **Karpathy 示例**：較高溫度會產出用詞新穎的多元隨筆，但「代價是更多錯誤（如拼寫錯誤等）」。這好比 RNN 在自由發散思考——適合詩歌或科幻創作，但對事實性寫作則存在風險

#### 溫度參數在 RNN 中的重要性
RNN（或 Karpathy 實驗中的 LSTM）具有自回歸特性：每個預測都基於前序結果，因此採樣中的微小偏差會在長序列中不斷疊加。溫度參數讓您能調控探索與利用的平衡：
- 低 \\(T\\) 適用於可靠、符合風格的文本（如產品描述）
- 提高溫度可用於腦力激盪或藝術創作（如超現實故事）
- 程式碼實驗：Karpathy 的基礎 RNN 程式庫包含溫度調節滑桿——可對莎士比亞文本樣本進行測試，觀察從平淡十四行詩到混沌詩篇的轉變

本質而言，溫度參數並未「學習」新知識；它僅是在推論階段調節 RNN 的創造力旋鈕。低溫保持冷靜可預測，高溫則釋放混沌能量。

[循環神經網絡的驚人效力](http://karpathy.github.io/2015/05/21/rnn-effectiveness/)