---
audio: false
generated: true
lang: hant
layout: post
title: 卷積神經網絡
translated: true
type: note
---

### 什麼是 CNN（卷積神經網絡）？

**卷積神經網絡（CNN）** 是一種深度神經網絡，主要用於圖像相關任務，例如分類、物件檢測和分割。CNN 的設計目的是從輸入圖像中自動且自適應地學習空間層次特徵。與傳統神經網絡中的全連接層相比，CNN 在捕捉局部依賴關係和減少參數數量方面非常有效。

### CNN 的關鍵組件：
1. **卷積層**：  
   該層對輸入數據應用卷積運算，有助於從輸入圖像中提取特徵（例如邊緣、紋理、圖案）。卷積運算使用在輸入圖像上滑動的濾波器（也稱為卷積核）。

2. **池化層**：  
   池化層用於對特徵圖進行下採樣，減少其空間維度，使網絡計算效率更高，同時有助於實現平移不變性（即使圖像中的物件移動位置，也能識別出來的能力）。

3. **全連接層**：  
   在卷積層和池化層之後，全連接層用於對先前層提取的特徵進行分類。最終輸出層通常使用 softmax 或 sigmoid 激活函數進行分類任務。

4. **激活函數（ReLU）**：  
   在每個卷積層或全連接層之後，通常會使用像 **ReLU**（線性整流單元）這樣的激活函數，為模型引入非線性，使其能夠學習更複雜的模式。

### CNN 架構示例：
- **輸入層**：一張圖像或一批圖像。
- **卷積層 1**：應用一組卷積濾波器（卷積核）。
- **ReLU 激活**：應用 ReLU 以引入非線性。
- **池化層 1**：最大池化或平均池化。
- **卷積層 2**：應用額外的卷積運算。
- **全連接層**：將輸出展平並輸入到全連接層進行分類。
- **輸出層**：使用 softmax 或 sigmoid 激活函數得到最終分類結果。

---

### 從零開始實現 CNN（不使用 TensorFlow/PyTorch 等框架）

以下是使用 **NumPy** 實現的一個簡單 CNN。這將讓您基本了解 CNN 中操作（卷積、ReLU、池化等）的工作原理。

我們將實現一個基本的 CNN，包括：
1. 一個卷積層
2. 一個 ReLU 激活層
3. 一個池化層
4. 一個全連接層

我們將專注於一個非常簡化版本的 CNN，其中不包含批次歸一化、丟棄層等高級功能。

### 步驟 1：卷積層

我們將實現**卷積**操作，該操作涉及在輸入圖像上滑動濾波器（卷積核）。

```python
import numpy as np

def convolve2d(input_image, kernel):
    kernel_height, kernel_width = kernel.shape
    image_height, image_width = input_image.shape
    
    # 卷積後的輸出維度
    output_height = image_height - kernel_height + 1
    output_width = image_width - kernel_width + 1
    
    output = np.zeros((output_height, output_width))
    
    # 在輸入圖像上滑動卷積核
    for i in range(output_height):
        for j in range(output_width):
            region = input_image[i:i+kernel_height, j:j+kernel_width]
            output[i, j] = np.sum(region * kernel)  # 逐元素相乘並求和
    return output
```

### 步驟 2：ReLU 激活

ReLU 逐元素應用於卷積輸出。

```python
def relu(input_image):
    return np.maximum(0, input_image)  # ReLU 操作
```

### 步驟 3：池化層（最大池化）

我們將實現一個簡單的**最大池化**層，使用 2x2 窗口和步長 2。

```python
def max_pooling(input_image, pool_size=2, stride=2):
    image_height, image_width = input_image.shape
    output_height = (image_height - pool_size) // stride + 1
    output_width = (image_width - pool_size) // stride + 1
    
    output = np.zeros((output_height, output_width))
    
    # 應用最大池化
    for i in range(0, image_height - pool_size + 1, stride):
        for j in range(0, image_width - pool_size + 1, stride):
            region = input_image[i:i+pool_size, j:j+pool_size]
            output[i // stride, j // stride] = np.max(region)
    
    return output
```

### 步驟 4：全連接層

全連接層只是一個密集層，它接收先前層的輸出並計算加權和。

```python
def fully_connected(input_image, weights, bias):
    # 將輸入圖像展平（如果是多維的）
    flattened_input = input_image.flatten()
    
    # 計算全連接層的輸出
    output = np.dot(flattened_input, weights) + bias
    return output
```

### 步驟 5：整合所有組件

現在，讓我們定義一個簡單示例，我們將創建一個 CNN，該網絡接收一張圖像，應用卷積、ReLU、池化，然後通過全連接層進行預測。

```python
# 示例圖像 (5x5)
image = np.array([[1, 2, 3, 0, 1],
                  [4, 5, 6, 1, 2],
                  [7, 8, 9, 2, 3],
                  [1, 2, 3, 4, 5],
                  [6, 7, 8, 5, 6]])

# 定義一個簡單的卷積核 (3x3)
kernel = np.array([[1, 0, -1],
                   [1, 0, -1],
                   [1, 0, -1]])

# 卷積操作
conv_output = convolve2d(image, kernel)
print("卷積輸出：")
print(conv_output)

# 應用 ReLU 激活
relu_output = relu(conv_output)
print("ReLU 輸出：")
print(relu_output)

# 應用最大池化
pool_output = max_pooling(relu_output)
print("最大池化輸出：")
print(pool_output)

# 全連接層（展平輸出，一維權重和偏置）
weights = np.random.randn(pool_output.size)  # 隨機權重
bias = np.random.randn()  # 隨機偏置
fc_output = fully_connected(pool_output, weights, bias)
print("全連接輸出：")
print(fc_output)
```

### 過程解釋：

1. **輸入圖像**：我們創建一個簡單的 5x5 圖像。
2. **卷積**：我們對圖像應用一個 3x3 濾波器，得到一個較小的輸出。
3. **ReLU**：我們應用 ReLU 函數以引入非線性。
4. **池化**：我們使用 2x2 窗口和步長 2 對圖像進行下採樣（最大池化）。
5. **全連接**：我們將池化輸出展平，並應用帶有隨機權重和偏置的全連接層。

這是一個非常簡化的 CNN，但它應該能讓您了解卷積、ReLU、池化和全連接層的工作原理。

### 結論

此實現提供了一個基本框架，用於理解 CNN 在底層的工作原理。在實際應用中，像 **TensorFlow**、**Keras** 和 **PyTorch** 這樣的框架處理了許多優化、效率改進和附加功能（例如批次歸一化、丟棄層），這些使得 CNN 更加強大且易於在大型數據集上訓練。

如果您想進一步了解如何擴展此實現，或者需要有關 CNN 其他部分的幫助，請告訴我！