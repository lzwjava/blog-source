---
audio: false
generated: true
image: false
lang: hant
layout: post
model: x-ai/grok-4-fast
title: å¾ç¶­åŸºç™¾ç§‘æ•¸æ“šåº«æå–æ–‡æœ¬
translated: true
type: note
---

ä»¥ä¸‹æ˜¯å¾ç¶­åŸºç™¾ç§‘ `.bz2` å‚™ä»½æª”æ¡ˆï¼ˆä¾‹å¦‚ä½ çš„ `enwiki-latest-pages-articles1.xml-p1p41242.bz2`ï¼‰ä¸­æå–ç´”æ–‡å­—æœ€å¸¸è¦‹ä¸”æœ‰æ•ˆçš„æ–¹æ³•ã€‚

### æ¨è–¦å·¥å…·èˆ‡æ–¹æ³•ï¼ˆå¾æœ€ç°¡å–®åˆ°æœ€é«˜æ•ˆï¼‰

#### 1. ä½¿ç”¨ wikiextractorï¼ˆé©åˆå¤§å¤šæ•¸äºº â€“ å¿«é€Ÿä¸”ä¹¾æ·¨ï¼‰
é€™æ˜¯ NLP/ML é ˜åŸŸå¹¾ä¹äººäººä½¿ç”¨çš„æ¨™æº–å·¥å…·ï¼Œç”¨æ–¼è™•ç†ç¶­åŸºç™¾ç§‘å‚™ä»½æª”æ¡ˆã€‚

```bash
# å®‰è£ï¼ˆåªéœ€ä¸€æ¬¡ï¼‰
pip install wikiextractor

# æå–æ–‡å­—ï¼ˆç§»é™¤æ¨¡æ¿ã€è¡¨æ ¼ç­‰ï¼Œæä¾›æ¯ç¯‡æ–‡ç« çš„ä¹¾æ·¨æ–‡å­—ï¼‰
wikiextractor enwiki-latest-pages-articles1.xml-p1p41242.bz2 --output wiki_text
```

- è¼¸å‡ºï¼šå»ºç«‹ä¸€å€‹ `wiki_text/` è³‡æ–™å¤¾ï¼Œå…§å«å­è³‡æ–™å¤¾ AAã€AB ç­‰ï¼ŒåŒ…å«ä¹¾æ·¨ JSON/æ–‡å­—çš„ `.bz2` æª”æ¡ˆã€‚
- è‹¥è¦å–å¾—ç´”æ–‡å­—ï¼ˆæ¯ç¯‡æ–‡ç« ä¸€è¡Œï¼‰ï¼š
```bash
wikiextractor enwiki-latest-pages-articles1.xml-p1p41242.bz2 --processes 8 -o wiki_text --json
# ç„¶å¾Œå¦‚æœ‰éœ€è¦ï¼Œå°‡ JSON è½‰æ›ç‚ºç´”æ–‡å­—
find wiki_text -name '*' -exec cat {} \; | grep '"text":' | sed 's/.*"text": "//;s/"$//' > all_text.txt
```

#### 2. ä½¿ç”¨ gensim çš„ WikiCorpusï¼ˆå¦‚æœä½ åœ¨ Python ä¸­é€²è¡Œ NLP è™•ç†ï¼Œé€™å·¥å…·å¾ˆæ£’ï¼‰

```python
from gensim.corpora import WikiCorpus

# é€™æœƒè‡ªå‹•è™•ç† .bz2 ä¸¦æå–æ–‡å­— + åˆ†è©
wiki = WikiCorpus("enwiki-latest-pages-articles1.xml-p1p41242.bz2", dictionary=False)

# éæ­·æ–‡ç« 
with open("output.txt", "w", encoding="utf-8") as f:
    for i, text in enumerate(wiki.get_texts()):
        f.write(" ".join(text) + "\n")
        if i % 10000 == 0:
            print(f"Processed {i} articles")
```

#### 3. ç°¡å–®çš„ bzip2 + grep/awkï¼ˆå¿«é€Ÿä½†ç²—ç³™ï¼Œä¸é€²è¡Œæ¸…ç†ï¼‰

```bash
# åƒ…è§£å£“ç¸®ä¸¦æŸ¥çœ‹åŸå§‹ XML
bzip2 -dc enwiki-latest-pages-articles1.xml-p1p41242.bz2 | less

# åƒ…æå– <text> å…§å®¹ï¼ˆéå¸¸ç²—ç•¥ï¼Œä¿ç•™ wiki æ¨™è¨˜ï¼‰
bzip2 -dc enwiki-latest-pages-articles1.xml-p1p41242.bz2 | \
  sed -n '/<text /,/<\/text>/p' | \
  sed 's/<text[^>]*>//g;s/<\/text>//g' > raw_text_with_markup.txt
```

#### 4. å¦‚æœä½ æœ‰å®Œæ•´å‚™ä»½æª”æ¡ˆä¸”å¸Œæœ›å¿«é€Ÿè™•ç†ï¼ˆä¸¦è¡Œè™•ç†ï¼‰
ä½ çš„æª”æ¡ˆåªæ˜¯ç¬¬ä¸€å€‹å€å¡Š (p1p41242)ï¼Œä½†å¦‚æœä½ ä¹‹å¾Œæœ‰æ‰€æœ‰ç´„ 30+ å€‹å€å¡Šï¼š

```bash
# ä¸¦è¡Œæå–æ‰€æœ‰æª”æ¡ˆ
ls *.bz2 | xargs -P 8 -I {} wikiextractor {} --output wiki_extracted
```

### ç¸½çµ â€“ æˆ‘ç›®å‰çµ¦ä½ çš„å»ºè­°ï¼š

```bash
pip install wikiextractor
wikiextractor enwiki-latest-pages-articles1.xml-p1p41242.bz2 -o extracted_wiki --bytes 1M --json
```

é€™å°‡åœ¨å¹¾åˆ†é˜å…§ç‚ºä½ æä¾›ä¹¾æ·¨ã€å¯è®€çš„æ–‡ç« ï¼ˆä½ çš„æª”æ¡ˆåƒ…ç´„ 2-3 GB å£“ç¸®ï¼Œå› æ­¤éå¸¸æ˜“æ–¼è™•ç†ï¼‰ã€‚

å¦‚æœä½ éœ€è¦ä»¥ä¸‹ä»»ä½•ä¸€é …ï¼Œè«‹å‘Šè¨´æˆ‘ï¼š
- åƒ…ä¸»è¦å‘½åç©ºé–“ï¼ˆçœŸå¯¦æ–‡ç« ï¼Œéè¨è«–é é¢ï¼‰
- ç´”æ–‡å­—è€Œé JSON
- åˆ†è©è¼¸å‡º
- æ”¹ç‚ºä¸­æ–‡ç¶­åŸºç™¾ç§‘ (zhwiki) â€“ è™•ç†éç¨‹å®Œå…¨ç›¸åŒ

ç¥ä½ æå–é †åˆ©ï¼ğŸš€