---
audio: false
generated: true
image: false
lang: hant
layout: post
model: x-ai/grok-4.1-fast
title: 體現智能概述
translated: true
type: note
---

### 體現智能：概覽

體現智能，亦稱體現式人工智慧，指的是透過軀體（例如機械人）在現實世界中具身化的人工智慧系統，這些系統透過與環境的直接互動來學習、推理和行動。有別於處理孤立抽象數據（例如像 GPT 這樣的語言模型）的傳統人工智慧，體現式人工智慧強調真正的智能源於感知-運動迴路：感知世界、對其採取行動，並根據反饋進行適應。這種範式借鑒了認知科學，認為認知植根於物理體現，而非純粹的計算。

關鍵原則包括：
- **多模態感知**：整合視覺、觸覺、本體感覺，有時還包括語言或聲音。
- **互動驅動學習**：智能體透過在現實世界或高擬真模擬環境中的試錯來改進（模擬到現實的轉移）。
- **泛化與適應性**：處理非結構化、動態的環境，執行長時程任務，應對多模態（例如結合視覺和語言），並對干擾具有魯棒性。

截至 2025 年，體現式人工智慧因基礎模型（大型預訓練視覺語言模型）、擴散技術以及像 Open X-Embodiment 這樣的大規模數據集而蓬勃發展。它推動了人形機械人、操作、導航和人機互動領域的進步。在實時性能、安全性、模擬到現實的差距以及擴展到開放世界任務方面仍然存在挑戰。領先的研究包括 Google 的 RT 系列、OpenVLA 和基於擴散策略的策略，旨在實現通用機械人。

### 關鍵技術：Diffusion Policy、RT-2 和 ACT

這三項技術代表了透過模仿學習（根據人類或專家示範進行訓練，無需明確獎勵）來學習機械人策略（從觀測到行動的映射）的最先進方法。

#### ACT（基於 Transformer 的行動分塊）
- **起源**：由 Tony Zhao 等人（Covariant.ai，前加州大學柏克萊分校）於 2023 年提出，作為用於低成本雙臂操作的 ALOHA 系統的一部分。
- **核心思想**：一種基於 Transformer 的策略，可預測**塊狀**的未來行動（例如，一次預測 100 步），而不是每個時間步預測一個行動。這減少了時間誤差（長時間範圍內的錯誤累積），並實現了平滑、高頻率的控制（例如 50Hz）。
- **架構**：使用變分自編碼器或 Transformer 骨幹。輸入：多視角 RGB 影像 + 本體感覺（關節狀態）。輸出：分塊的關節位置/速度。
- **優勢**：
  - 極高的樣本效率（從約 50 次示範中即可學習複雜任務）。
  - 可在消費級硬件上實現實時運行。
  - 在精確、靈巧的任務（例如穿針引線、摺疊衣物）上表現出色，且使用低成本機械人。
- **局限性**：主要基於模仿；在沒有擴展的情況下，對語言指令或網絡級泛化的內在支持較少。
- **現實影響**：驅動著如 ALOHA（移動操作器）等系統，並已被廣泛應用於雙臂任務。

#### Diffusion Policy
- **起源**：Cheng Chi 等人（哥倫比亞大學、豐田研究所、麻省理工學院）於 2023 年發表的論文。後續工作如 3D Diffusion Policy 和 ScaleDP（2025 年參數高達 10 億）對其進行了擴展。
- **核心思想**：將機械人行動視為來自擴散模型的生成樣本（靈感來自像 Stable Diffusion 這樣的影像生成器）。從帶噪聲的行動開始，根據觀測條件迭代地去噪，以產生高質量、多模態的行動序列。
- **架構**：條件去噪擴散模型（通常使用 Transformer）。學習「評分函數」（行動分佈的梯度）。推論使用滾動時域控制：規劃一個序列，執行第一個行動，重新規劃。
- **優勢**：
  - 自然地處理**多模態**行為（例如，抓取物體有多種有效方式——擴散模型能連貫地採樣其中一種，而不會取平均）。
  - 對高維度行動和帶噪聲的示範具有魯棒性。
  - 在基準測試中達到最先進水平（2023 年比先前方法提升 46% 以上；2025 年仍具競爭力）。
  - 像 3D Diffusion Policy 這樣的擴展使用點雲來實現更好的 3D 理解。
- **局限性**：推論速度較慢（10–100 個去噪步驟），儘管優化（例如減少步驟數、蒸餾）使其可實現實時運行。
- **現實影響**：廣泛用於視覺運動操作；已整合到如 PoCo（策略組合）和規模化模型等系統中。

#### RT-2（Robotics Transformer 2）
- **起源**：由 Google DeepMind 於 2023 年提出（基於 RT-1）。屬於視覺-語言-行動系列的一部分。
- **核心思想**：在大型預訓練視覺語言模型（例如 PaLM-E 或 PaLI-X，參數高達 550 億）上對機械人軌跡進行協同微調。行動被標記為文本字符串，使模型能夠直接輸出行動，同時利用網絡規模的知識（影像 + 文本）。
- **架構**：接收影像 + 語言指令 → 標記化行動的 Transformer。從網絡預訓練中湧現出技能（例如符號推理、思維鏈）。
- **優勢**：
  - **語義泛化**：理解新穎指令（例如「撿起滅絕動物」→ 抓起恐龍玩具），無需進行機械人特定訓練。
  - 將網絡知識遷移到機械人技術（例如，從網絡影像中識別垃圾）。
  - 在湧現技能上比先前的機械人模型提升高達 3 倍。
- **局限性**：模型龐大 → 計算需求更高；與 ACT/Diffusion 相比，在低層級靈巧控制方面精確度較低（更擅長高層級推理）。
- **現實影響**：為 Google 的機械人車隊數據收集提供動力；已演進為 RT-X 並與後續系統整合。

### 對比表格

| 方面                  | ACT                                      | Diffusion Policy                          | RT-2                                      |
|-------------------------|------------------------------------------|-------------------------------------------|-------------------------------------------|
| **主要方法**     | Transformer + 行動分塊（確定性/回歸） | 去噪擴散（生成式）         | VLA（在 LLM/VLM 中將行動標記化）       |
| **輸入**              | 多視角影像 + 本體感覺      | 影像/點雲 + 本體感覺     | 影像 + 語言指令           |
| **輸出**             | 分塊關節行動                    | 去噪後的行動序列                | 標記化的行動字符串                  |
| **關鍵優勢**       | 樣本效率、精確度、實時性  | 多模態性、魯棒性、表現力| 語義推理、從網絡數據泛化 |
| **推論速度**    | 快（單次通過）                       | 較慢（迭代去噪）             | 中等（Transformer 自回歸）      |
| **數據效率**    | 非常高（約 50 次示範/任務）               | 高                                     | 中等（受益於網絡預訓練）  |
| **最適用於**           | 精確的靈巧操作           | 複雜、多模態任務                | 語言引導、新穎/湧現任務    |
| **相對於基線的典型成功率提升** | 在真實機械人基準測試中表現強勁          | +46%（2023 年基準測試）                   | 在湧現技能上提升 2–3 倍                  |
| **2025 年現狀**        | 低成本硬件的基準           | 規模擴展至數十億參數             | 成為 OpenVLA/RT-X 後繼者的基礎   |

### 它們如何融入體現智能

- **ACT 和 Diffusion Policy**：純視覺運動策略——擅長低層級感知運動技能，是體現式人工智慧的「身體」。它們使機械人能夠根據原始像素在物理世界中流暢地行動。
- **RT-2（及 VLA 模型）**：通往高層認知的橋樑——加入語言以實現類人互動和推理，推動邁向通用體現智能體。
- **混合是未來趨勢**：2025 年的趨勢是將它們結合（例如，OpenVLA = 類 RT-2 + 擴散頭；OCTO = 小型高效模型 + 擴散元素）。最終目標：能夠多模態感知、使用 LLM/世界模型進行推理，並透過擴散/分塊精確行動的機械人。

這些方法已經使靈巧機械人技術大眾化：過去需要數年工程才能完成的任務，現在只需數十次人類示範即可成功。隨著數據集（例如 Open X-Embodiment）的增長和硬件（如 Figure 01、Tesla Optimus 等人形機械人）的改進，體現式人工智慧正迅速接近實現多功能、實用的現實世界機械人。

[Diffusion Policy 項目頁面](https://diffusion-policy.cs.columbia.edu/)  
[RT-2 公告](https://deepmind.google/discover/blog/rt-2-new-model-translates-vision-and-language-into-action/)  
[ALOHA / ACT 論文](https://tonyzhaozh.github.io/aloha/)  
[Open X-Embodiment 數據集](https://robotics-transformer-x.github.io/)