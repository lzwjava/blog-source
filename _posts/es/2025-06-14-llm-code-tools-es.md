---
audio: false
generated: true
image: false
lang: es
layout: post
model: openai/gpt-5
title: Modelos de Lenguaje Grandes, Razonamiento de C√≥digo y Uso de Herramientas
translated: true
type: note
---

¬°Excelente pregunta! B√°sicamente est√°s preguntando c√≥mo algo como **Claude de Anthropic (y otros LLMs similares)** maneja internamente:

1.  **La ejecuci√≥n de c√≥digo o el razonamiento relacionado con c√≥digo**
2.  **La validaci√≥n de esquemas JSON/verificaci√≥n de salidas estructuradas**
3.  **El uso de herramientas y el enrutamiento entre ellas**

Perm√≠teme desglosarlo en detalle.

---

### 1. Razonamiento y ejecuci√≥n de c√≥digo
Claude (y otros LLMs de vanguardia) en realidad no *ejecutan* c√≥digo directamente dentro del modelo; **simulan el razonamiento de c√≥digo** prediciendo tokens de c√≥digo o pasos l√≥gicos.

- En el **modo de texto normal**, Claude puede "escribir" c√≥digo continuando un patr√≥n, pero no lo ejecuta internamente.
- Cuando se usa con un **backend (como Anthropic's Workbench, o el Code Interpreter de OpenAI)**, el sistema puede capturar esos fragmentos de c√≥digo y luego *ejecutarlos* realmente en un entorno aislado (sandbox).
- Los resultados pueden luego ser alimentados de vuelta al modelo como contexto, para que pueda encadenar razonamientos como:
  ‚Üí Generar c√≥digo ‚Üí Ejecutar externamente ‚Üí Recibir salida ‚Üí Decidir el siguiente paso.

As√≠ que la inteligencia est√° en el bucle: el LLM genera ‚Üí un ejecutor externo ejecuta ‚Üí el LLM interpreta los resultados.

---

### 2. Verificaci√≥n de esquemas JSON
Muchas APIs (Claude, OpenAI, etc.) ahora admiten un **modo de salida estructurada**, donde puedes adjuntar un esquema JSON. As√≠ es como funciona internamente:

- Pasas una **definici√≥n de esquema JSON** con tu solicitud.
- Se gu√≠a al LLM para que genere **solo JSON v√°lido** que se ajuste al esquema.
- Si intenta generar JSON inv√°lido, el sistema lo detecta (usando un validador como [jsonschema](https://pypi.org/project/jsonschema/)) y le pide al modelo que "repare" la salida.
- Internamente, esto a menudo es un mecanismo de **decodificaci√≥n restringida** o **decodificaci√≥n basada en gram√°tica**: b√°sicamente, restringe la generaci√≥n de tokens para que solo se permitan los `"siguientes tokens v√°lidos en el esquema"`.

Esto garantiza que la salida sea JSON sint√°ctica y sem√°nticamente correcto, lo cual es cr√≠tico para la automatizaci√≥n del uso de herramientas.

---

### 3. Uso de herramientas y enrutamiento
El uso de herramientas se maneja mediante algo como una **capa de enrutador** entre el LLM y un entorno de ejecuci√≥n de herramientas:

- **L√≥gica de enrutamiento**: Se le pide al modelo que *decida qu√© herramienta llamar* (o que responda directamente).
- **Aplicaci√≥n de esquemas**: Cada herramienta tiene un esquema de entrada. Se requiere que el LLM produzca JSON v√°lido para esa herramienta.
- **Adaptadores de ejecuci√≥n**: La capa de orquestaci√≥n (fuera del modelo) toma el JSON del LLM, lo valida contra el esquema, ejecuta la herramienta y luego alimenta los resultados de vuelta como contexto.
- **Razonamiento encadenado (estilo ReAct)**: El LLM "piensa en voz alta" (puede producir pasos de razonamiento ocultos para el usuario), luego emite llamadas a herramientas cuando es necesario, llegando progresivamente a una respuesta final.

Por ejemplo:
```json
{
  "tool": "web_search",
  "query": "latest research on quantum error correction"
}
```
‚Üí El enrutador ve `"tool": "web_search"`, valida el esquema, ejecuta la b√∫squeda web, coloca los resultados de vuelta en el contexto del modelo, y el modelo contin√∫a razonando.

Claude de Anthropic, los GPTs de OpenAI, LangChain y otros, todos admiten este patr√≥n.

---

### üîë Resumen
- **C√≥digo en Claude**: No ejecuta c√≥digo internamente; genera c√≥digo ‚Üí un sandbox lo ejecuta ‚Üí se devuelven los resultados ‚Üí razonamiento iterativo.
- **Verificaci√≥n de esquemas JSON**: Utiliza decodificaci√≥n restringida / validaci√≥n de esquemas para garantizar una salida estructurada v√°lida.
- **Enrutamiento de uso de herramientas**: El LLM selecciona llamadas a herramientas en JSON, el motor de orquestaci√≥n las valida y ejecuta, y alimenta los resultados de vuelta en un bucle.

---

üëâ ¬øTe gustar√≠a que **dibuje un diagrama** que muestre el flujo (LLM ‚Üî enrutador de herramientas ‚Üî validador de esquemas ‚Üî motor de ejecuci√≥n) para que sea m√°s claro visualmente?