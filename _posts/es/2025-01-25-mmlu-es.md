---
audio: true
lang: es
layout: post
title: Benchmark MMLU
translated: true
---

## Prólogo

Esta publicación evalúa un modelo de lenguaje utilizando el benchmark MMLU (Massive Multitask Language Understanding).

El benchmark MMLU es una prueba exhaustiva de la capacidad de un modelo para realizar diversas tareas en una amplia gama de materias. Consiste en preguntas de opción múltiple que cubren áreas diversas como matemáticas, historia, derecho y medicina.

**Enlaces al conjunto de datos:**

*   [Papers with Code](https://paperswithcode.com/dataset/mmlu)
*   [Hugging Face Datasets](https://huggingface.co/datasets/cais/mmlu)

## llama-server

Para ejecutar el llama-server:

```bash
build/bin/llama-server -m models/7B/mistral-7b-instruct-v0.2.Q4_K_M.gguf --port 8080
```

## Benchmark MMLU

Este script evalúa el benchmark MMLU utilizando tres backends diferentes: `ollama`, `llama-server` y `deepseek`.

Para ejecutar el código del benchmark MMLU:

```python
import torch
from datasets import load_dataset
import requests
import json
from tqdm import tqdm
import argparse
import os
from openai import OpenAI
from dotenv import load_dotenv

load_dotenv()

# Configurar el análisis de argumentos
parser = argparse.ArgumentParser(description="Evalúa el conjunto de datos MMLU con diferentes backends.")
parser.add_argument("--type", type=str, default="ollama", choices=["ollama", "llama", "deepseek"], help="Tipo de backend: ollama, llama o deepseek")
args = parser.parse_args()

# Cargar el conjunto de datos MMLU
subject = "college_computer_science"  # Elige tu materia
dataset = load_dataset("cais/mmlu", subject, split="test", cache_dir="./.cache")

# Formatear el prompt sin ejemplos few-shot
def format_mmlu_prompt(example):
    prompt = "Las siguientes son preguntas de opción múltiple sobre {}".format(subject.replace("_", " "))
    prompt += ". Por favor, responde con la letra de la opción correcta (A, B, C o D) solamente."
    prompt += " Responde solo con la letra. No es necesario incluir una explicación."
    
    # Añadir la pregunta actual
    prompt += f"Pregunta: {example['question']}\n"
    prompt += "Opciones:\nA. {}\nB. {}\nC. {}\nD. {}\n".format(*example['choices'])
    return prompt

# Bucle de evaluación
correct = 0
total = 0

# Inicializar el cliente de DeepSeek si es necesario
if args.type == "deepseek":
    api_key = os.environ.get("DEEPSEEK_API_KEY")
    if not api_key:
        print("Error: La variable de entorno DEEPSEEK_API_KEY no está configurada.")
        exit()
    client = OpenAI(api_key=api_key, base_url="https://api.deepseek.com")


for i, example in tqdm(enumerate(dataset), total=len(dataset), desc="Evaluando"):
    prompt = format_mmlu_prompt(example)
    
    # Enviar solicitud al backend
    if args.type == "ollama":
        url = "http://localhost:11434/v1/chat/completions"
        data = {
            "messages": [{"role": "user", "content": prompt}],
            "model": "mistral:7b"
        }
        headers = {"Content-Type": "application/json"}
        print(f"Entrada a la API: {data}")
        response = requests.post(url, headers=headers, data=json.dumps(data))
        if response.status_code == 200:
            output_text = response.json()["choices"][0]["message"]["content"]
            predicted_answer = output_text.strip()[0] if len(output_text.strip()) > 0 else ""
            print(f"Salida de la API: {output_text}")
        else:
            predicted_answer = ""
            print(f"Error: {response.status_code} - {response.text}")
    elif args.type == "llama":
        url = "http://localhost:8080/v1/chat/completions"
        data = {
            "messages": [{"role": "user", "content": prompt}]
        }
        headers = {"Content-Type": "application/json"}
        print(f"Entrada a la API: {data}")
        response = requests.post(url, headers=headers, data=json.dumps(data))
        if response.status_code == 200:
            output_text = response.json()["choices"][0]["message"]["content"]
            predicted_answer = output_text.strip()[0] if len(output_text.strip()) > 0 else ""
            print(f"Salida de la API: {output_text}")
        else:
            predicted_answer = ""
            print(f"Error: {response.status_code} - {response.text}")
    elif args.type == "deepseek":
        try:
            response = client.chat.completions.create(
                model="deepseek-chat",
                messages=[
                    {"role": "user", "content": prompt}
                ],
                max_tokens=100
            )
            if response and response.choices:
                output_text = response.choices[0].message.content.strip()
                predicted_answer = output_text.strip()[0] if len(output_text.strip()) > 0 else ""
                print(f"Salida de la API: {output_text}")
            else:
                predicted_answer = ""
                print("Error: No hay respuesta de la API.")
        except Exception as e:
            predicted_answer = ""
            print(f"Error durante la llamada a la API: {e}")
    else:
        raise ValueError("Tipo de backend no válido")
    
    # Comparar con la respuesta correcta
    
    answer_map = {0: "A", 1: "B", 2: "C", 3: "D"}
    ground_truth_answer = answer_map.get(example["answer"], "")
    is_correct = predicted_answer.upper() == ground_truth_answer
    if is_correct:
        correct += 1
    total += 1
    
    print(f"Pregunta: {example['question']}")
    print(f"Opciones: A. {example['choices'][0]}, B. {example['choices'][1]}, C. {example['choices'][2]}, D. {example['choices'][3]}")
    print(f"Respuesta predicha: {predicted_answer}, Respuesta correcta: {ground_truth_answer}, Correcta: {is_correct}")
    print("-" * 30)

    if (i+1) % 10 == 0:
        accuracy = correct / total
        print(f"Procesadas {i+1}/{len(dataset)}. Precisión actual: {accuracy:.2%} ({correct}/{total})")


# Calcular la precisión
accuracy = correct / total
print(f"Materia: {subject}")
print(f"Precisión: {accuracy:.2%} ({correct}/{total})")
```

## Resultados

### Evaluación Zero-Shot

| Modelo                     | Método                      | Materia                        | Precisión   |
|---------------------------|--------------------------|--------------------------------|------------|
| mistral-7b-instruct-v0.2, Q4_K_M | macOS m2, 16GB, llama-server | MMLU college_computer_science | 40.00% (40/100) |
| Mistral-7B-Instruct-v0.3, Q4_0  | macOS m2, 16GB, ollama      | MMLU college_computer_science | 40.00% (40/100) |
| deepseek v3               | API, 2025.1.25           | MMLU college_computer_science | 78.00% (78/100) |
| gemini-1.5-flash          | API, 2025.1.25           | MMLU college_computer_science | 78.00% (78/100) |