---
audio: true
lang: es
layout: post
title: Evaluación MMLU
translated: true
---

## Prólogo

Este post evalúa un modelo de lenguaje utilizando el benchmark MMLU (Massive Multitask Language Understanding).

El benchmark MMLU es una prueba exhaustiva de la capacidad de un modelo para realizar diversas tareas en una amplia gama de materias. Consiste en preguntas de opción múltiple que cubren áreas diversas como matemáticas, historia, derecho y medicina.

**Enlaces al conjunto de datos:**

*   [Papers with Code](https://paperswithcode.com/dataset/mmlu)
*   [Hugging Face Datasets](https://huggingface.co/datasets/cais/mmlu)

## llama-server

Para ejecutar el llama-server:

```bash
build/bin/llama-server -m models/7B/mistral-7b-instruct-v0.2.Q4_K_M.gguf --port 8080
```

## Benchmark MMLU

Para ejecutar el código del benchmark MMLU:

```python
import torch
from datasets import load_dataset
import requests
import json
from tqdm import tqdm

# Cargar el conjunto de datos MMLU
subject = "college_computer_science"  # Elige tu materia
dataset = load_dataset("cais/mmlu", subject, split="test")

# Formatear el prompt sin ejemplos de pocos disparos
def format_mmlu_prompt(example):
    prompt = "Las siguientes son preguntas de opción múltiple sobre {}".format(subject.replace("_", " "))
    prompt += ". Por favor, responde con la letra de la opción correcta (A, B, C o D) solamente."
    prompt += " Responde solo con la letra. No se necesita explicación."
    
    # Agregar la pregunta actual
    prompt += f"Pregunta: {example['question']}\n"
    prompt += "Opciones:\nA. {}\nB. {}\nC. {}\nD. {}\n".format(*example['choices'])
    return prompt

# Bucle de evaluación
correct = 0
total = 0

for i, example in tqdm(enumerate(dataset), total=len(dataset), desc="Evaluando"):
    prompt = format_mmlu_prompt(example)
    
    # Enviar solicitud al llama-server
    url = "http://localhost:8080/v1/chat/completions"
    headers = {"Content-Type": "application/json"}
    data = {
        "messages": [{"role": "user", "content": prompt}]
    }
    
    print(f"Entrada a la API: {data}")
    response = requests.post(url, headers=headers, data=json.dumps(data))
    
    if response.status_code == 200:
        output_text = response.json()["choices"][0]["message"]["content"]
        predicted_answer = output_text.strip()[0] if len(output_text.strip()) > 0 else ""
        print(f"Salida de la API: {output_text}")
    else:
        predicted_answer = ""
        print(f"Error: {response.status_code} - {response.text}")
    
    # Comparar con la verdad fundamental
    
    answer_map = {0: "A", 1: "B", 2: "C", 3: "D"}
    ground_truth_answer = answer_map.get(example["answer"], "")
    is_correct = predicted_answer.upper() == ground_truth_answer
    if is_correct:
        correct += 1
    total += 1
    
    print(f"Pregunta: {example['question']}")
    print(f"Opciones: A. {example['choices'][0]}, B. {example['choices'][1]}, C. {example['choices'][2]}, D. {example['choices'][3]}")
    print(f"Respuesta Predicha: {predicted_answer}, Verdad Fundamental: {ground_truth_answer}, Correcta: {is_correct}")
    print("-" * 30)

    if (i+1) % 10 == 0:
        accuracy = correct / total
        print(f"Procesados {i+1}/{len(dataset)}. Precisión Actual: {accuracy:.2%} ({correct}/{total})")


# Calcular la precisión
accuracy = correct / total
print(f"Materia: {subject}")
print(f"Precisión: {accuracy:.2%} ({correct}/{total})")
```

Registro:

```bash
% python scripts/mmlu.py

Evaluando:   9%| 9/100 [01:31<15:19, 10.10s/it]Procesados 10/100. Precisión Actual: 0.00% (0/10)
Evaluando:  19%| 19/100 [03:14<12:47,  9.47s/it]Procesados 20/100. Precisión Actual: 0.00% (0/20)
Evaluando:  26%| 26/100 [04:30<13:44, 11.14s/it]

...

Procesados 100/100. Precisión Actual: 40.00% (40/100)
Evaluando: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [15:15<00:00,  9.16s/it]
Materia: college_computer_science
Precisión: 40.00% (40/100)
```

## ollama

```bash
Procesados 100/100. Precisión Actual: 40.00% (40/100)
Evaluando: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [02:18<00:00,  1.39s/it]
Materia: college_computer_science
Precisión: 40.00% (40/100)
```

El programa ollama parece ser mucho más rápido que llama-server. 

Para ollama, se utilizan parámetros como:

```bash
/Applications/Ollama.app/Contents/Resources/ollama runner --model /Users/lzwjava/.ollama/models/blobs/sha256-ff82381e2bea77d91c1b824c7afb83f6fb73e9f7de9dda631bcdbca564aa5435 --ctx-size 8192 --batch-size 512 --n-gpu-layers 33 --threads 4 --parallel 4 --port 51151
```

## Deepseek

Benchmark de Deepseek.

```python
import torch
from datasets import load_dataset
import requests
import json
from tqdm import tqdm
import os
from openai import OpenAI
from dotenv import load_dotenv

load_dotenv()

# Cargar el conjunto de datos MMLU
subject = "college_computer_science"  # Elige tu materia
dataset = load_dataset("cais/mmlu", subject, split="test")

# Formatear el prompt sin ejemplos de pocos disparos
def format_mmlu_prompt(example):
    prompt = "Las siguientes son preguntas de opción múltiple sobre {}".format(subject.replace("_", " "))
    prompt += ". Por favor, responde con la letra de la opción correcta (A, B, C o D) solamente."
    prompt += " Responde solo con la letra. No se necesita explicación."
    
    # Agregar la pregunta actual
    prompt += f"Pregunta: {example['question']}\n"
    prompt += "Opciones:\nA. {}\nB. {}\nC. {}\nD. {}\n".format(*example['choices'])
    return prompt

# Bucle de evaluación
correct = 0
total = 0

# Inicializar el cliente de DeepSeek
api_key = os.environ.get("DEEPSEEK_API_KEY")
if not api_key:
    print("Error: No se ha configurado la variable de entorno DEEPSEEK_API_KEY.")
    exit()
client = OpenAI(api_key=api_key, base_url="https://api.deepseek.com")


for i, example in tqdm(enumerate(dataset), total=len(dataset), desc="Evaluando"):
    prompt = format_mmlu_prompt(example)
    
    # Enviar solicitud a la API de DeepSeek
    try:
        response = client.chat.completions.create(
            model="deepseek-chat",
            messages=[
                {"role": "user", "content": prompt}
            ],
            max_tokens=100
        )
        if response and response.choices:
            output_text = response.choices[0].message.content.strip()
            predicted_answer = output_text.strip()[0] if len(output_text.strip()) > 0 else ""
            print(f"Salida de la API: {output_text}")
        else:
            predicted_answer = ""
            print("Error: No hay respuesta de la API.")
    except Exception as e:
        predicted_answer = ""
        print(f"Error durante la llamada a la API: {e}")
    
    # Comparar con la verdad fundamental
    
    answer_map = {0: "A", 1: "B", 2: "C", 3: "D"}
    ground_truth_answer = answer_map.get(example["answer"], "")
    is_correct = predicted_answer.upper() == ground_truth_answer
    if is_correct:
        correct += 1
    total += 1
    
    print(f"Pregunta: {example['question']}")
    print(f"Opciones: A. {example['choices'][0]}, B. {example['choices'][1]}, C. {example['choices'][2]}, D. {example['choices'][3]}")
    print(f"Respuesta Predicha: {predicted_answer}, Verdad Fundamental: {ground_truth_answer}, Correcta: {is_correct}")
    print("-" * 30)

    if (i+1) % 10 == 0:
        accuracy = correct / total
        print(f"Procesados {i+1}/{len(dataset)}. Precisión Actual: {accuracy:.2%} ({correct}/{total})")


# Calcular la precisión
accuracy = correct / total
print(f"Materia: {subject}")
print(f"Precisión: {accuracy:.2%} ({correct}/{total})")
```

Resultado:

```bash
Procesados 100/100. Precisión Actual: 78.00% (78/100)
Evaluando: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [03:44<00:00,  2.24s/it]
Materia: college_computer_science
Precisión: 78.00% (78/100)
```