---
audio: true
lang: es
layout: post
title: '# Benchmark MMLU'
translated: true
---

## Prólogo

Este post evalúa un modelo de lenguaje utilizando el benchmark MMLU (Massive Multitask Language Understanding).

El benchmark MMLU es una prueba exhaustiva de la capacidad de un modelo para realizar diversas tareas en una amplia gama de materias. Consiste en preguntas de opción múltiple que cubren áreas diversas como matemáticas, historia, derecho y medicina.

**Enlaces al conjunto de datos:**

*   [Papers with Code](https://paperswithcode.com/dataset/mmlu)
*   [Hugging Face Datasets](https://huggingface.co/datasets/cais/mmlu)

## llama-server

Para ejecutar el llama-server:

```bash
build/bin/llama-server -m models/7B/mistral-7b-instruct-v0.2.Q4_K_M.gguf --port 8080
```

## Benchmark MMLU

Este script evalúa el benchmark MMLU utilizando tres backends diferentes: `ollama`, `llama-server` y `deepseek`.

Para ejecutar el código del benchmark MMLU:

```python
import torch
from datasets import load_dataset
import requests
import json
from tqdm import tqdm
import argparse
import os
from openai import OpenAI
from dotenv import load_dotenv

load_dotenv()

# Configurar el análisis de argumentos
parser = argparse.ArgumentParser(description="Evalúa el conjunto de datos MMLU con diferentes backends.")
parser.add_argument("--type", type=str, default="ollama", choices=["ollama", "llama", "deepseek"], help="Tipo de backend: ollama, llama o deepseek")
args = parser.parse_args()

# Cargar el conjunto de datos MMLU
subject = "college_computer_science"  # Elige tu materia
dataset = load_dataset("cais/mmlu", subject, split="test", cache_dir="./.cache")

# Formatear el prompt sin ejemplos few-shot
def format_mmlu_prompt(example):
    prompt = "Las siguientes son preguntas de opción múltiple sobre {}".format(subject.replace("_", " "))
    prompt += ". Por favor, responde con la letra de la opción correcta (A, B, C o D) solamente."
    prompt += " Responde solo la letra. No es necesaria una explicación."
    
    # Agregar la pregunta actual
    prompt += f"Pregunta: {example['question']}\n"
    prompt += "Opciones:\nA. {}\nB. {}\nC. {}\nD. {}\n".format(*example['choices'])
    return prompt

# Bucle de evaluación
correct = 0
total = 0

# Inicializar el cliente de DeepSeek si es necesario
if args.type == "deepseek":
    api_key = os.environ.get("DEEPSEEK_API_KEY")
    if not api_key:
        print("Error: La variable de entorno DEEPSEEK_API_KEY no está configurada.")
        exit()
    client = OpenAI(api_key=api_key, base_url="https://api.deepseek.com")


for i, example in tqdm(enumerate(dataset), total=len(dataset), desc="Evaluando"):
    prompt = format_mmlu_prompt(example)
    
    # Enviar solicitud al backend
    if args.type == "ollama":
        url = "http://localhost:11434/v1/chat/completions"
        data = {
            "messages": [{"role": "user", "content": prompt}],
            "model": "mistral:7b"
        }
        headers = {"Content-Type": "application/json"}
        print(f"Entrada a la API: {data}")
        response = requests.post(url, headers=headers, data=json.dumps(data))
        if response.status_code == 200:
            output_text = response.json()["choices"][0]["message"]["content"]
            predicted_answer = output_text.strip()[0] if len(output_text.strip()) > 0 else ""
            print(f"Salida de la API: {output_text}")
        else:
            predicted_answer = ""
            print(f"Error: {response.status_code} - {response.text}")
    elif args.type == "llama":
        url = "http://localhost:8080/v1/chat/completions"
        data = {
            "messages": [{"role": "user", "content": prompt}]
        }
        headers = {"Content-Type": "application/json"}
        print(f"Entrada a la API: {data}")
        response = requests.post(url, headers=headers, data=json.dumps(data))
        if response.status_code == 200:
            output_text = response.json()["choices"][0]["message"]["content"]
            predicted_answer = output_text.strip()[0] if len(output_text.strip()) > 0 else ""
            print(f"Salida de la API: {output_text}")
        else:
            predicted_answer = ""
            print(f"Error: {response.status_code} - {response.text}")
    elif args.type == "deepseek":
        try:
            response = client.chat.completions.create(
                model="deepseek-chat",
                messages=[
                    {"role": "user", "content": prompt}
                ],
                max_tokens=100
            )
            if response and response.choices:
                output_text = response.choices[0].message.content.strip()
                predicted_answer = output_text.strip()[0] if len(output_text.strip()) > 0 else ""
                print(f"Salida de la API: {output_text}")
            else:
                predicted_answer = ""
                print("Error: No hay respuesta de la API.")
        except Exception as e:
            predicted_answer = ""
            print(f"Error durante la llamada a la API: {e}")
    else:
        raise ValueError("Tipo de backend no válido")
    
    # Comparar con la verdad fundamental
    
    answer_map = {0: "A", 1: "B", 2: "C", 3: "D"}
    ground_truth_answer = answer_map.get(example["answer"], "")
    is_correct = predicted_answer.upper() == ground_truth_answer
    if is_correct:
        correct += 1
    total += 1
    
    print(f"Pregunta: {example['question']}")
    print(f"Opciones: A. {example['choices'][0]}, B. {example['choices'][1]}, C. {example['choices'][2]}, D. {example['choices'][3]}")
    print(f"Respuesta predicha: {predicted_answer}, Verdad fundamental: {ground_truth_answer}, Correcta: {is_correct}")
    print("-" * 30)

    if (i+1) % 10 == 0:
        accuracy = correct / total
        print(f"Procesadas {i+1}/{len(dataset)}. Precisión actual: {accuracy:.2%} ({correct}/{total})")


# Calcular la precisión
accuracy = correct / total
print(f"Materia: {subject}")
print(f"Precisión: {accuracy:.2%} ({correct}/{total})")
```

## Resultados

### Evaluación Zero-Shot

| Modelo                     | Método                      | Materia                        | Precisión   |
|---------------------------|--------------------------|--------------------------------|------------|
| mistral-7b-instruct-v0.2, Q4_K_M | macOS m2, 16GB, llama-server | MMLU college_computer_science | 40.00% (40/100) |
| Mistral-7B-Instruct-v0.3, Q4_0  | macOS m2, 16GB, ollama      | MMLU college_computer_science | 40.00% (40/100) |
| deepseek v3               | API, 2025.1.25           | MMLU college_computer_science | 78.00% (78/100) |
| gemini-1.5-flash          | API, 2025.1.25           | MMLU college_computer_science | 72.00% (72/100) |

### Figura

Creemos una figura basada en la tabla anterior.

```python
import matplotlib.pyplot as plt
import os

# Datos de muestra (reemplaza con tus datos reales)
models = ['mistral-7b-instruct-v0.2 (llama.cpp)', 'Mistral-7B-Instruct-v0.3 (ollama)', 'deepseek v3 (API)', 'gemini-1.5-flash (API)']
accuracy = [40.00, 40.00, 78.00, 72.00]

# Crear el gráfico de barras
plt.figure(figsize=(10, 6))
plt.bar(models, accuracy, color=['skyblue', 'lightcoral', 'lightgreen', 'gold'])
plt.xlabel('Modelo')
plt.ylabel('Precisión (%)')
plt.title('Precisión del Benchmark MMLU')
plt.ylim(0, 100)  # Establecer límite del eje y de 0 a 100 para porcentaje
plt.xticks(rotation=45, ha="right")  # Rotar las etiquetas del eje x para mejor legibilidad
plt.tight_layout()

# Agregar valores de precisión encima de las barras
for i, val in enumerate(accuracy):
    plt.text(i, val + 1, f'{val:.2f}%', ha='center', va='bottom')

# Guardar el gráfico como un archivo JPG en el directorio actual
plt.savefig(os.path.join(os.path.dirname(__file__), 'mmlu_accuracy_chart.jpg'))
plt.show()
```

{: .centered }
![](assets/images/mmlu/mmlu_accuracy_chart.jpg)
*Precisión del Benchmark MMLU*{: .caption }