---
audio: true
lang: es
layout: post
title: 'MMLU Benchmark


  EVALUACIÓN DE MMLU'
translated: true
---

## Introducción

Esta publicación evalúa un modelo de lenguaje utilizando el benchmark MMLU (Massive Multitask Language Understanding).

El benchmark MMLU es una prueba exhaustiva de la capacidad de un modelo para realizar diversas tareas en una amplia gama de temas. Consiste en preguntas de opción múltiple que abarcan áreas diversas como matemáticas, historia, derecho y medicina.

**Enlaces al Conjunto de Datos:**

*   [Papers with Code](https://paperswithcode.com/dataset/mmlu)
*   [Hugging Face Datasets](https://huggingface.co/datasets/cais/mmlu)

## llama-server

Para ejecutar el servidor llama-server:

```bash
build/bin/llama-server -m models/7B/mistral-7b-instruct-v0.2.Q4_K_M.gguf --port 8080
```

## Benchmark MMLU

Este script evalúa el benchmark MMLU utilizando tres backends diferentes: `ollama`, `llama-server` y `deepseek`.

Para ejecutar el código del benchmark MMLU:

```python
import torch
from datasets import load_dataset
import requests
import json
from tqdm import tqdm
import argparse
import os
from openai import OpenAI
from dotenv import load_dotenv
import time

load_dotenv()

# Configuración del análisis de argumentos
parser = argparse.ArgumentParser(description="Evaluar el conjunto de datos MMLU con diferentes backends.")
parser.add_argument("--type", type=str, default="ollama", choices=["ollama", "llama", "deepseek", "gemini", "deepseek-r1"], help="Tipo de backend: ollama, llama, deepseek, o gemini")
args = parser.parse_args()

# Cargar el conjunto de datos MMLU
subject = "college_computer_science"  # Elegir el tema
dataset = load_dataset("cais/mmlu", subject, split="test")

# Formatear el prompt sin ejemplos de pocos disparos
def format_mmlu_prompt(example):
    prompt = "Las siguientes son preguntas de opción múltiple sobre {}".format(subject.replace("_", " "))
    prompt += ". Por favor, responda con la letra de la opción correcta (A, B, C o D) solo."
    prompt += " Responder solo con la letra. No se necesita explicación."

    # Agregar la pregunta actual
    prompt += f"Pregunta: {example['question']}\n"
    prompt += "Opciones:\nA. {}\nB. {}\nC. {}\nD. {}\n".format(*example['choices'])
    return prompt

# Inicializar el cliente de DeepSeek si es necesario
def initialize_deepseek_client():
    api_key = os.environ.get("DEEPSEEK_API_KEY")
    if not api_key:
        print("Error: la variable de entorno DEEPSEEK_API_KEY no está configurada.")
        exit()
    return OpenAI(api_key=api_key, base_url="https://api.deepseek.com")

def call_gemini_api(prompt, retries=3, backoff_factor=1):
    gemini_api_key = os.environ.get("GEMINI_API_KEY")
    if not gemini_api_key:
        print("Error: la variable de entorno GEMINI_API_KEY no está configurada.")
        exit()
    url = f"https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash:generateContent"
    params = {"key": gemini_api_key}
    payload = {"contents": [{"parts": [{"text": prompt}]}]}

    for attempt in range(retries):
        response = requests.post(url, json=payload, params=params)
        if response.status_code != 429:
            return response.json()
        time.sleep(backoff_factor * (2 ** attempt))  # Retraso exponencial
    return None

import re

def process_ollama_response(response):
    if response.status_code == 200:
        output_text = response.json()["choices"][0]["message"]["content"]
        match = re.search(r"Answer:\s*([A-D])", output_text, re.IGNORECASE)
        if match:
            predicted_answer = match.group(1).upper()
        else:
            predicted_answer = output_text.strip()[0] if len(output_text.strip()) > 0 else ""
        print(f"Salida de la API: {output_text}")
        return predicted_answer
    else:
        print(f"Error: {response.status_code} - {response.text}")
        return ""

def process_llama_response(response):
    if response.status_code == 200:
        output_text = response.json()["choices"][0]["message"]["content"]
        predicted_answer = output_text.strip()[0] if len(output_text.strip()) > 0 else ""
        print(f"Salida de la API: {output_text}")
        return predicted_answer
    else:
        print(f"Error: {response.status_code} - {response.text}")
        return ""

def process_deepseek_response(client, prompt):
    try:
        response = client.chat.completions.create(
            model="deepseek-chat",
            messages=[
                {"role": "user", "content": prompt}
            ],
            max_tokens=100
        )
        if response and response.choices:
            output_text = response.choices[0].message.content.strip()
            predicted_answer = output_text.strip()[0] if len(output_text.strip()) > 0 else ""
            print(f"Salida de la API: {output_text}")
            return predicted_answer
        else:
            print("Error: No hubo respuesta de la API.")
            return ""
    except Exception as e:
        print(f"Error durante la llamada a la API: {e}")
        return ""

def process_deepseek_r1_response(client, prompt, retries=3, backoff_factor=1):
    for attempt in range(retries):
        try:
            response = client.chat.completions.create(
                model="deepseek-reasoner",
                messages=[
                    {"role": "user", "content": prompt}
                ],
                max_tokens=100
            )
            if response and response.choices:
                output_text = response.choices[0].message.content.strip()
                predicted_answer = output_text.strip()[0] if len(output_text.strip()) > 0 else ""
                print(f"Salida de la API: {output_text}")
                return predicted_answer
            else:
                print("Error: No hubo respuesta de la API.")
                return ""
        except Exception as e:
            if "502" in str(e):
                print(f"Error de pasarela incorrecta (502) durante la llamada a la API, reintentar en {backoff_factor * (2 ** attempt)} segundos...")
                time.sleep(backoff_factor * (2 ** attempt))
            else:
                print(f"Error durante la llamada a la API: {e}")
                return ""
    print("Se alcanzó el número máximo de reintentos, devolviendo respuesta vacía.")
    return ""

def process_gemini_response(prompt):
    json_response = call_gemini_api(prompt)
    if not json_response:
        print("No hubo respuesta de la API de Gemini después de los reintentos.")
        return ""
    if 'candidates' not in json_response or not json_response['candidates']:
        print("No se encontraron candidatos en la respuesta, reintentar...")
        json_response = call_gemini_api(prompt)
        if not json_response or 'candidates' not in json_response or not json_response['candidates']:
            print("No se encontraron candidatos en la respuesta después del reintento.")
            return ""

    first_candidate = json_response['candidates'][0]
    if 'content' in first_candidate and 'parts' in first_candidate['content']:
        first_part = first_candidate['content']['parts'][0]
        if 'text' in first_part:
            output_text = first_part['text']
            predicted_answer = output_text.strip()[0] if len(output_text.strip()) > 0 else ""
            print(f"Salida de la API: {output_text}")
            return predicted_answer
        else:
            print("No se encontró texto en la respuesta")
            return ""
    else:
        print("Formato de respuesta inesperado: contenido o partes faltantes")
        return ""

def evaluate_model(args, dataset):
    correct = 0
    total = 0
    client = None
    if args.type == "deepseek" or args.type == "deepseek-r1":
        client = initialize_deepseek_client()

    for i, example in tqdm(enumerate(dataset), total=len(dataset), desc="Evaluando"):
        prompt = format_mmlu_prompt(example)
        predicted_answer = ""

        if args.type == "ollama":
            url = "http://localhost:11434/v1/chat/completions"
            data = {
                "messages": [{"role": "user", "content": prompt}],
                "model": "mistral:7b"
            }
            headers = {"Content-Type": "application/json"}
            print(f"Entrada a la API: {data}")
            response = requests.post(url, headers=headers, data=json.dumps(data))
            predicted_answer = process_ollama_response(response)

        elif args.type == "llama":
            url = "http://localhost:8080/v1/chat/completions"
            data = {
                "messages": [{"role": "user", "content": prompt}]
            }
            headers = {"Content-Type": "application/json"}
            print(f"Entrada a la API: {data}")
            response = requests.post(url, headers=headers, data=json.dumps(data))
            predicted_answer = process_llama_response(response)

        elif args.type == "deepseek":
            predicted_answer = process_deepseek_response(client, prompt)

        elif args.type == "deepseek-r1":
            predicted_answer = process_deepseek_r1_response(client, prompt)

        elif args.type == "gemini":
            predicted_answer = process_gemini_response(prompt)
        else:
            raise ValueError("Tipo de backend no válido")

        answer_map = {0: "A", 1: "B", 2: "C", 3: "D"}
        ground_truth_answer = answer_map.get(example["answer"], "")
        is_correct = predicted_answer.upper() == ground_truth_answer
        if is_correct:
            correct += 1
        total += 1

        print(f"Pregunta: {example['question']}")
        print(f"Opciones: A. {example['choices'][0]}, B. {example['choices'][1]}, C. {example['choices'][2]}, D. {example['choices'][3]}")
        print(f"Respuesta Predicha: {predicted_answer}, Verdadera: {ground_truth_answer}, Correcta: {is_correct}")
        print("-" * 30)

        if (i+1) % 10 == 0:
            accuracy = correct / total
            print(f"Procesado {i+1}/{len(dataset)}. Exactitud actual: {accuracy:.2%} ({correct}/{total})")

    return correct, total

# Bucle de evaluación
correct, total = evaluate_model(args, dataset)

# Calcular exactitud
accuracy = correct / total
print(f"Tema: {subject}")
print(f"Exactitud: {accuracy:.2%} ({correct}/{total})")

## Resultados

### Evaluación de Zero-Shot

| Modelo                     | Formato                      | Tema                        | Exactitud   |
|---------------------------|--------------------------|--------------------------------|------------|
| mistral-7b-instruct-v0.2, Q4_K_M | macOS m2, 16GB, llama-server | MMLU college_computer_science | 40.00% (40/100) |
| Mistral-7B-Instruct-v0.3, Q4_0  | macOS m2, 16GB, ollama      | MMLU college_computer_science | 40.00% (40/100) |
| deepseek v3 (API)               | API, 2025.1.25           | MMLU college_computer_science | 78.00% (78/100) |
| gemini-1.5-flash (API)          | API, 2025.1.25           | MMLU college_computer_science | 72.00% (72/100) |
| deepseek r1 (API)               | API, 2025.1.26           | MMLU college_computer_science | 87.14% (61/70) |
| Mistral Small Latest (API) | API, 2025.01.31 | MMLU college_computer_science | 65.00% (65/100) |
| Mistral Large Latest (API) | API, 2025.01.31 | MMLU college_computer_science | 73.00% (73/100) |
| Mistral Small 2501 (API)   | API, 2025.01.31 | MMLU college_computer_science | 66.00% (66/100) |

### Gráfico

Vamos a crear un gráfico basado en la tabla anterior.

```python
import matplotlib.pyplot as plt
import os

# Datos de muestra (reemplazar con los datos reales)
models = ['mistral-7b-instruct-v0.2 (llama.cpp)', 'Mistral-7B-Instruct-v0.3 (ollama)', 'deepseek v3 (API)', 'gemini-1.5-flash (API)', 'deepseek r1 (API)']
accuracy = [40.00, 40.00, 78.00, 72.00, 87.14]
subject = "college_computer_science"

# Crear el gráfico de barras
plt.figure(figsize=(10, 6))
plt.bar(models, accuracy, color=['skyblue', 'lightcoral', 'lightgreen', 'gold', 'lightcoral'])
plt.xlabel('Modelo')
plt.ylabel('Exactitud (%)')
plt.title(f'Exactitud del Benchmark MMLU para {subject}')
plt.ylim(0, 100)  # Establecer límite del eje y en 0-100 para porcentaje
plt.xticks(rotation=45, ha="right")  # Rotar etiquetas del eje x para mejor legibilidad
plt.tight_layout()

# Agregar valores de exactitud encima de las barras
for i, val in enumerate(accuracy):
    plt.text(i, val + 1, f'{val:.2f}%', ha='center', va='bottom')

# Guardar el gráfico como un archivo JPG en el directorio actual
plt.savefig(os.path.join(os.path.dirname(__file__), f'mmlu_accuracy_chart.jpg'))
plt.show()

```

{: .centered }
![](assets/images/mmlu/mmlu_accuracy_chart.jpg)
*Exactitud del Benchmark MMLU*