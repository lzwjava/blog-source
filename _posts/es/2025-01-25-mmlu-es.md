---
audio: true
lang: es
layout: post
title: Prueba de referencia MMLU
translated: true
---

Esta publicación evalúa un modelo de lenguaje en el benchmark MMLU (Massive Multitask Language Understanding).

El benchmark MMLU es una prueba exhaustiva de la capacidad de un modelo para realizar diversas tareas en una amplia gama de materias. Consiste en preguntas de opción múltiple que cubren áreas diversas como matemáticas, historia, derecho y medicina.

**Enlaces al conjunto de datos:**

*   [Papers with Code](https://paperswithcode.com/dataset/mmlu)
*   [Hugging Face Datasets](https://huggingface.co/datasets/cais/mmlu)

```python
import torch
from datasets import load_dataset
import requests
import json
from tqdm import tqdm

# Cargar el conjunto de datos MMLU
subject = "college_computer_science"  # Elige tu materia
dataset = load_dataset("cais/mmlu", subject, split="test")

# Formatear el prompt sin ejemplos few-shot
def format_mmlu_prompt(example):
    prompt = "Las siguientes son preguntas de opción múltiple sobre {}".format(subject.replace("_", " "))
    prompt += ". Por favor, responde con la letra de la opción correcta (A, B, C o D) solamente."
    prompt += " Responde solo la letra. No es necesaria una explicación."
    
    # Añadir la pregunta actual
    prompt += f"Pregunta: {example['question']}\n"
    prompt += "Opciones:\nA. {}\nB. {}\nC. {}\nD. {}\n".format(*example['choices'])
    return prompt

# Bucle de evaluación
correct = 0
total = 0

for i, example in tqdm(enumerate(dataset), total=len(dataset), desc="Evaluando"):
    prompt = format_mmlu_prompt(example)
    
    # Enviar solicitud a llama-server
    url = "http://localhost:8080/v1/chat/completions"
    headers = {"Content-Type": "application/json"}
    data = {
        "messages": [{"role": "user", "content": prompt}]
    }
    
    print(f"Entrada a la API: {data}")
    response = requests.post(url, headers=headers, data=json.dumps(data))
    
    if response.status_code == 200:
        output_text = response.json()["choices"][0]["message"]["content"]
        predicted_answer = output_text.strip()[0] if len(output_text.strip()) > 0 else ""
        print(f"Salida de la API: {output_text}")
    else:
        predicted_answer = ""
        print(f"Error: {response.status_code} - {response.text}")
    
    # Comparar con la respuesta correcta
    
    answer_map = {0: "A", 1: "B", 2: "C", 3: "D"}
    ground_truth_answer = answer_map.get(example["answer"], "")
    is_correct = predicted_answer.upper() == ground_truth_answer
    if is_correct:
        correct += 1
    total += 1
    
    print(f"Pregunta: {example['question']}")
    print(f"Opciones: A. {example['choices'][0]}, B. {example['choices'][1]}, C. {example['choices'][2]}, D. {example['choices'][3]}")
    print(f"Respuesta predicha: {predicted_answer}, Respuesta correcta: {ground_truth_answer}, Correcta: {is_correct}")
    print("-" * 30)

    if (i+1) % 10 == 0:
        accuracy = correct / total
        print(f"Procesadas {i+1}/{len(dataset)}. Precisión actual: {accuracy:.2%} ({correct}/{total})")


# Calcular la precisión
accuracy = correct / total
print(f"Materia: {subject}")
print(f"Precisión: {accuracy:.2%} ({correct}/{total})")
```

Registro:

```bash
% python scripts/mmlu.py

Evaluando:   9%| 9/100 [01:31<15:19, 10.10s/it]Procesadas 10/100. Precisión actual: 0.00% (0/10)
Evaluando:  19%| 19/100 [03:14<12:47,  9.47s/it]Procesadas 20/100. Precisión actual: 0.00% (0/20)
Evaluando:  26%| 26/100 [04:30<13:44, 11.14s/it]

...

Procesadas 100/100. Precisión actual: 40.00% (40/100)
Evaluando: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [15:15<00:00,  9.16s/it]
Materia: college_computer_science
Precisión: 40.00% (40/100)
```