---
audio: false
lang: es
layout: post
title: Pensamientos de IA
translated: true
---

- Satya Nadella mencionó el paradox de Jevons. Vale la pena aprenderlo.

- Yin Wang: No hay "inteligencia" en la inteligencia artificial, no hay "neuronal" en la red neuronal, no hay "aprendizaje" en el aprendizaje automático y no hay "profundidad" en el aprendizaje profundo. Lo que realmente funciona en este campo se llama "cálculo". Por eso prefiero llamar a este campo "computación diferenciable" y el proceso de construir modelos se llama "programación diferenciable".

- Yin Wang: El aprendizaje automático es realmente útil, incluso podríamos decir que es una teoría hermosa, porque es simplemente cálculo después de un cambio de imagen. Es la antigua y grande teoría de Newton, Leibniz, en una forma más simple, elegante y poderosa. El aprendizaje automático es básicamente el uso del cálculo para derivar e ajustar algunas funciones, y el aprendizaje profundo es el ajuste de funciones más complejas.

- En la actualidad, los grandes modelos de lenguaje no pueden filtrar por lenguaje de archivo como YAML o Python. Sin embargo, una parte significativa de la información del mundo real está organizada de esta manera. Esto significa que podríamos entrenar grandes modelos de lenguaje utilizando archivos.

- Para entrenar grandes modelos de lenguaje, podríamos desarrollar un sistema que encuentre coincidencias exactas. Tal vez podamos combinar el algoritmo de búsqueda KMP (Knuth-Morris-Pratt) con la arquitectura transformador para mejorar las capacidades de búsqueda.

- No hay secretos tecnológicos. El código abierto revelará todos los secretos que se guardan celosamente.

- [Nuevas Plataformas Impulsadas por Flujos de Trabajo de IA](./ai-workflow-en)

- [La Próxima Dirección de los Editores de Código de IA](./ai-code-en)

- [Cómo Vivo Bien en la Era de la IA y Blockchain](./ai-blockchain-en)