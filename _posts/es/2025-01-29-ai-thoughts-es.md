---
audio: true
lang: es
layout: post
title: Pensamientos de IA
translated: true
---

- Satya Nadella mencionó el paradox de Jevons. Vale la pena aprenderlo.

- Yin Wang: No hay "inteligencia" en la inteligencia artificial, no "neuronal" en la red neuronal, no "aprendizaje" en el aprendizaje automático, y no "profundidad" en el aprendizaje profundo. Lo que realmente funciona en este campo se llama "cálculo". Por eso prefiero llamar a este campo "cómputo diferenciable", y el proceso de construir modelos se llama "programación diferenciable".

- Yin Wang: El aprendizaje automático es realmente útil, incluso podría decirse que es una teoría hermosa, porque no es más que cálculo disfrazado. Es la antigua y gran teoría de Newton y Leibniz, pero en una forma más simple, elegante y poderosa. El aprendizaje automático es, en esencia, el uso del cálculo para derivar y ajustar algunas funciones, y el aprendizaje profundo es el ajuste de funciones más complejas.

- Actualmente, los modelos de lenguaje grandes no pueden filtrar por lenguaje de archivo como YAML o Python. Sin embargo, una parte significativa de la información del mundo real está organizada de esta manera. Esto significa que podríamos entrenar modelos de lenguaje grandes utilizando archivos.

- Para entrenar modelos de lenguaje grandes, podríamos desarrollar un sistema que encuentre coincidencias exactas. Tal vez podamos combinar el algoritmo de búsqueda KMP (Knuth-Morris-Pratt) con la arquitectura transformer para mejorar las capacidades de búsqueda.

- No hay secretos tecnológicos. El software de código abierto revelará todos los secretos celosamente guardados.