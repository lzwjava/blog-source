---
audio: false
generated: true
image: false
lang: es
layout: post
model: x-ai/grok-4-fast
title: Lista de Lectura sobre IA de Ilya Sutskever
translated: true
type: note
---

Ilya Sutskever, cofundador de OpenAI, compartió una lista de lectura recomendada con John Carmack alrededor de 2020 que incluía aproximadamente 30 artículos clave, publicaciones de blog, cursos y recursos sobre aprendizaje profundo e IA. Según los informes, dijo que dominarlos cubriría "el 90% de lo que importa hoy" en el campo. Si bien la lista original completa de 30 ítems no está disponible públicamente (algunos elementos podrían haberse perdido debido a la eliminación de correos electrónicos), una versión ampliamente difundida incluye 27 elementos fundamentales que abarcan redes convolucionales, redes recurrentes, transformers, teoría de la información y más. A continuación se presenta esa lista seleccionada, agrupada por categoría para mayor claridad. Cada entrada incluye el título, autores, año y tipo.

### Redes Neuronales Convolucionales
1. **CS231n: Convolutional Neural Networks for Visual Recognition** - Fei-Fei Li, Andrej Karpathy, Justin Johnson - 2017 - Curso de Stanford
2. **ImageNet Classification with Deep Convolutional Neural Networks (AlexNet)** - Alex Krizhevsky, Ilya Sutskever, Geoffrey E. Hinton - 2012 - Artículo
3. **Deep Residual Learning for Image Recognition (ResNet)** - Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun - 2015 - Artículo
4. **Identity Mappings in Deep Residual Networks** - Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun - 2016 - Artículo
5. **Multi-Scale Context Aggregation by Dilated Convolutions** - Fisher Yu, Vladlen Koltun - 2015 - Artículo

### Redes Neuronales Recurrentes
6. **Understanding LSTM Networks** - Christopher Olah - 2015 - Publicación de Blog
7. **The Unreasonable Effectiveness of Recurrent Neural Networks** - Andrej Karpathy - 2015 - Publicación de Blog
8. **Recurrent Neural Network Regularization** - Wojciech Zaremba, Ilya Sutskever, Oriol Vinyals - 2014 - Artículo
9. **Neural Turing Machines** - Alex Graves, Greg Wayne, Ivo Danihelka - 2014 - Artículo
10. **Deep Speech 2: End-to-End Speech Recognition in English and Mandarin** - Dario Amodei et al. - 2016 - Artículo
11. **Neural Machine Translation by Jointly Learning to Align and Translate (RNNsearch)** - Dzmitry Bahdanau, Kyunghyun Cho, Yoshua Bengio - 2015 - Artículo
12. **Pointer Networks** - Oriol Vinyals, Meire Fortunato, Navdeep Jaitly - 2015 - Artículo
13. **Order Matters: Sequence to Sequence for Sets (Set2Set)** - Oriol Vinyals, Samy Bengio, Manjunath Kudlur - 2016 - Artículo
14. **A Simple Neural Network Module for Relational Reasoning (Relation Networks)** - Adam Santoro, David Raposo, David G. Barrett, Mateusz Malinowski, Razvan Pascanu, Peter Battaglia, Timothy Lillicrap - 2017 - Artículo
15. **Relational Recurrent Neural Networks** - Adam Santoro, Ryan Faulkner, David Raposo, Jack Rae, Mike Chrzanowski, Theophane Weber, Daan Wierstra, Oriol Vinyals, Razvan Pascanu, Timothy Lillicrap - 2018 - Artículo

### Transformers
16. **Attention Is All You Need** - Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin - 2017 - Artículo
17. **The Annotated Transformer** - Sasha Rush et al. - 2017 (anotado en 2020) - Publicación de Blog
18. **Scaling Laws for Neural Language Models** - Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B. Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, Dario Amodei - 2020 - Artículo

### Teoría de la Información
19. **A Tutorial Introduction to the Minimum Description Length Principle** - Peter Grünwald - 2004 - Capítulo de Libro
20. **Kolmogorov Complexity and Algorithmic Randomness (Chapter 14)** - Alexander Shen, Vladimir A. Uspensky, Nikolay Vereshchagin - 2017 - Capítulo de Libro
21. **The First Law of Complexodynamics** - Scott Aaronson - 2011 - Publicación de Blog
22. **Quantifying the Rise and Fall of Complexity in Closed Systems: The Coffee Automaton** - Scott Aaronson, Sean M. Carroll, Lauren Ouellette - 2014 - Artículo
23. **Machine Super Intelligence** - Shane Legg - 2008 - Tesis

### Misceláneos
24. **Keeping Neural Networks Simple by Minimizing the Description Length of the Weights** - Geoffrey E. Hinton, Drew van Camp - 1993 - Artículo
25. **Variational Lossy Autoencoder** - Xi Chen, Diederik P. Kingma, Tim Salimans, Yan Duan, Prafulla Dhariwal, John Schulman, Ilya Sutskever, Pieter Abbeel - 2017 - Artículo
26. **GPipe: Easy Scaling with Micro-Batch Pipeline Parallelism** - Yanping Huang, Youlong Cheng, Ankur Bapna, Orhan Firat, Mia Xu Chen, Dehao Chen, HyoukJoong Lee, Jiquan Ngiam, Quoc V. Le, Yonghui Wu, Zhifeng Chen - 2018 - Artículo
27. **Neural Message Passing for Quantum Chemistry** - Justin Gilmer, Samuel S. Schoenholz, Patrick F. Riley, Oriol Vinyals, George E. Dahl - 2017 - Artículo

La especulación sobre los 3 elementos "faltantes" a menudo apunta a artículos de meta-aprendizaje (por ejemplo, *Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks* de Chelsea Finn et al., 2017) o trabajos de aprendizaje por refuerzo sobre auto-juego, basándose en las charlas contemporáneas de Sutskever, pero esto no está confirmado.

[Deep learning reading list from Ilya Sutskever](https://github.com/dzyim/ilya-sutskever-recommended-reading)
[Summary of Ilya Sutskever's AI Reading List](https://tensorlabbet.com/2024/09/24/ai-reading-list/)
[The Lost Reading Items of Ilya Sutskever's AI Reading List](https://tensorlabbet.com/2024/11/11/lost-reading-items/)