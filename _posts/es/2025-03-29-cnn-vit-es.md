---
audio: false
generated: true
lang: es
layout: post
title: Procesamiento de Im치genes con CNN y Transformadores de Visi칩n (ViT)
translated: true
type: note
---

A continuaci칩n se muestran ejemplos de c칩digo que demuestran c칩mo se procesan im치genes usando **Convolutional Neural Networks (CNNs)** y **Vision Transformers (ViT)** en Python con **PyTorch**.

---

## **1. Procesamiento de Im치genes con una CNN (Convolutional Neural Network)**
Las CNNs son ampliamente utilizadas para clasificaci칩n de im치genes, detecci칩n de objetos y extracci칩n de caracter칤sticas.

### **Ejemplo: Usando una CNN Pre-entrenada (ResNet)**
```python
import torch
import torchvision.models as models
import torchvision.transforms as transforms
from PIL import Image

# Cargar un modelo ResNet pre-entrenado
model = models.resnet18(pretrained=True)
model.eval()  # Poner en modo evaluaci칩n

# Definir el preprocesamiento de la imagen
preprocess = transforms.Compose([
    transforms.Resize(256),
    transforms.CenterCrop(224),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
])

# Cargar y preprocesar una imagen
image = Image.open("example.jpg")  # Reemplazar con la ruta de tu imagen
input_tensor = preprocess(image)
input_batch = input_tensor.unsqueeze(0)  # A침adir dimensi칩n de lote

# Mover a GPU si est치 disponible
if torch.cuda.is_available():
    input_batch = input_batch.to('cuda')
    model.to('cuda')

# Extraer caracter칤sticas (antes de la capa de clasificaci칩n final)
with torch.no_grad():
    features = model(input_batch)

print("Forma del vector de caracter칤sticas:", features.shape)  # ej., torch.Size([1, 1000])
```
**Explicaci칩n**:
1. **ResNet18** es una arquitectura CNN pre-entrenada en ImageNet.
2. La imagen se preprocesa (se redimensiona, se normaliza).
3. El modelo convierte la imagen en un **vector de caracter칤sticas** (ej., 1000-dimensional para ResNet18).

---

## **2. Procesamiento de Im치genes con un Vision Transformer (ViT)**
Los ViT tratan las im치genes como secuencias de parches y utilizan mecanismos de self-attention (como en PLN).

### **Ejemplo: Usando un ViT Pre-entrenado (Hugging Face)**
```python
from transformers import ViTFeatureExtractor, ViTModel
from PIL import Image
import torch

# Cargar un Vision Transformer (ViT) pre-entrenado
model_name = "google/vit-base-patch16-224-in21k"
feature_extractor = ViTFeatureExtractor.from_pretrained(model_name)
model = ViTModel.from_pretrained(model_name)

# Cargar una imagen
image = Image.open("example.jpg")  # Reemplazar con la ruta de tu imagen

# Preprocesar la imagen (convertir a parches)
inputs = feature_extractor(images=image, return_tensors="pt")

# Extraer caracter칤sticas (token CLS o embeddings de parches)
with torch.no_grad():
    outputs = model(**inputs)

# Obtener el vector de caracter칤sticas (token CLS)
features = outputs.last_hidden_state[:, 0, :]  # Forma: [1, 768]

print("Forma del vector de caracter칤sticas:", features.shape)  # ej., torch.Size([1, 768])
```
**Explicaci칩n**:
1. **ViT** divide la imagen en **parches de 16x16** y los procesa como tokens en PLN.
2. El `token CLS` (primer token) representa el vector de caracter칤sticas de toda la imagen.
3. La salida es un **vector de 768 dimensiones** (para `vit-base`).

---

## **3. Comparaci칩n de la Extracci칩n de Caracter칤sticas: CNN vs. ViT**

| Modelo | Enfoque | Tama침o del Vector de Caracter칤sticas | Librer칤as |
|-------|----------|---------------------|-----------|
| **CNN (ResNet18)** | Capas convolucionales + pooling | 1000 (clases de ImageNet) | `torchvision` |
| **ViT (Google ViT-Base)** | Embeddings de parches + Transformer | 768 (dimensi칩n oculta) | `transformers` |

---

## **4. Aplicaciones de los Vectores de Caracter칤sticas de Imagen**
- **B칰squeda de Im치genes**: Comparar vectores de caracter칤sticas (ej., similitud del coseno).
- **Transfer Learning**: Usar caracter칤sticas pre-entrenadas para tareas personalizadas.
- **Detecci칩n de Objetos**: Extraer regiones de inter칠s (ROI) de las caracter칤sticas.

```python
# Ejemplo: Calcular la similitud entre dos im치genes
from sklearn.metrics.pairwise import cosine_similarity

# Suponer que features1 y features2 son vectores extra칤dos
similarity = cosine_similarity(features1, features2)
print("Puntuaci칩n de similitud:", similarity[0][0])
```

---

## **Puntos Clave**
1. **CNNs** (ej., ResNet) usan **capas convolucionales** para extraer caracter칤sticas jer치rquicas.
2. **ViTs** (ej., ViT de Google) tratan las im치genes como **secuencias de parches** y usan **self-attention**.
3. Ambos convierten im치genes en **vectores de caracter칤sticas** para tareas posteriores.

쯊e gustar칤a un ejemplo sobre **entrenar una CNN/ViT personalizada** desde cero? 춰H치zmelo saber! 游