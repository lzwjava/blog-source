---
audio: false
generated: true
lang: es
layout: post
title: Uso de Apache Kafka en Java
translated: true
type: note
---

El uso de Apache Kafka en Java implica configurar un cliente Kafka en tu aplicación Java para producir y consumir mensajes. Kafka es una plataforma de streaming distribuida, y sus bibliotecas cliente para Java facilitan la integración. A continuación, te guiaré a través de los pasos básicos para comenzar.

Primero, necesitarás configurar tu entorno. Asegúrate de tener Kafka instalado y ejecutándose en tu sistema o en un servidor. Puedes descargarlo desde el sitio web oficial de Apache Kafka e iniciar el servidor de ZooKeeper y Kafka usando los scripts proporcionados. Para simplificar, asumiré que estás ejecutando Kafka localmente con la configuración predeterminada (por ejemplo, `localhost:9092` como el servidor bootstrap).

A continuación, agrega la dependencia del cliente Kafka a tu proyecto Java. Si estás usando Maven, incluye esto en tu `pom.xml`:

```xml
<dependency>
    <groupId>org.apache.kafka</groupId>
    <artifactId>kafka-clients</artifactId>
    <version>3.6.0</version> <!-- Usa la última versión -->
</dependency>
```

Ahora, escribamos algo de código. Te mostraré cómo crear un productor y un consumidor simples.

### Ejemplo de Productor Kafka
El productor envía mensajes a un topic de Kafka. Aquí tienes un ejemplo básico:

```java
import org.apache.kafka.clients.producer.*;
import java.util.Properties;

public class SimpleProducer {
    public static void main(String[] args) {
        // Configurar las propiedades del productor
        Properties props = new Properties();
        props.put("bootstrap.servers", "localhost:9092"); // Dirección del servidor Kafka
        props.put("key.serializer", "org.apache.kafka.common.serialization.StringSerializer");
        props.put("value.serializer", "org.apache.kafka.common.serialization.StringSerializer");

        // Crear una instancia del productor
        try (Producer<String, String> producer = new KafkaProducer<>(props)) {
            // Enviar un mensaje a un topic llamado "test-topic"
            String topic = "test-topic";
            for (int i = 0; i < 10; i++) {
                String key = "key" + i;
                String value = "Hola, Kafka " + i;
                ProducerRecord<String, String> record = new ProducerRecord<>(topic, key, value);

                producer.send(record, (metadata, exception) -> {
                    if (exception == null) {
                        System.out.println("Mensaje enviado: " + value + " a la partición " + metadata.partition());
                    } else {
                        exception.printStackTrace();
                    }
                });
            }
        }
    }
}
```

En este código:
- `bootstrap.servers` especifica dónde se está ejecutando Kafka.
- Los serializadores definen cómo las claves y los valores (ambos cadenas aquí) se convierten a bytes.
- `ProducerRecord` representa el mensaje, y `send()` lo envía de forma asíncrona con un callback para manejar el éxito o el fallo.

### Ejemplo de Consumidor Kafka
El consumidor se suscribe a un topic y lee mensajes. Aquí tienes un ejemplo:

```java
import org.apache.kafka.clients.consumer.*;
import java.util.Collections;
import java.util.Properties;

public class SimpleConsumer {
    public static void main(String[] args) {
        // Configurar las propiedades del consumidor
        Properties props = new Properties();
        props.put("bootstrap.servers", "localhost:9092");
        props.put("group.id", "test-group"); // ID del grupo de consumidores
        props.put("key.deserializer", "org.apache.kafka.common.serialization.StringDeserializer");
        props.put("value.deserializer", "org.apache.kafka.common.serialization.StringDeserializer");
        props.put("auto.offset.reset", "earliest"); // Comenzar desde el inicio del topic

        // Crear una instancia del consumidor
        try (KafkaConsumer<String, String> consumer = new KafkaConsumer<>(props)) {
            // Suscribirse al topic
            consumer.subscribe(Collections.singletonList("test-topic"));

            // Poll para obtener mensajes
            while (true) {
                ConsumerRecords<String, String> records = consumer.poll(1000); // Timeout en ms
                for (ConsumerRecord<String, String> record : records) {
                    System.out.printf("Recibido: clave=%s, valor=%s, partición=%d, offset=%d%n",
                            record.key(), record.value(), record.partition(), record.offset());
                }
            }
        }
    }
}
```

Aquí:
- `group.id` identifica el grupo de consumidores para la coordinación.
- Los deserializadores convierten los datos de bytes de vuelta a cadenas.
- `poll()` obtiene mensajes en un bucle, y `auto.offset.reset` asegura que comiences desde el mensaje más antiguo si no hay un offset comprometido.

### Ejecutando el Código
1. **Iniciar Kafka**: Asegúrate de que ZooKeeper y Kafka estén ejecutándose (por ejemplo, `bin/zookeeper-server-start.sh config/zookeeper.properties` y `bin/kafka-server-start.sh config/server.properties` en sistemas tipo Unix).
2. **Crear un Topic**: Usa la herramienta de línea de comandos de Kafka para crear `test-topic` si no existe: `bin/kafka-topics.sh --create --topic test-topic --bootstrap-server localhost:9092 --partitions 1 --replication-factor 1`.
3. **Ejecutar el Productor**: Ejecuta la clase `SimpleProducer` para enviar mensajes.
4. **Ejecutar el Consumidor**: Ejecuta la clase `SimpleConsumer` para ver los mensajes siendo consumidos.

### Consejos
- **Manejo de Errores**: Añade bloques try-catch o logging para el código de producción.
- **Configuración**: Ajusta propiedades como `acks` (para la fiabilidad del productor) o `max.poll.records` (para el tamaño del lote del consumidor) según tus necesidades.
- **Bibliotecas**: Para aplicaciones más complejas, considera usar Spring Kafka, que simplifica la configuración y el uso.

Esta es una configuración básica para que puedas empezar. ¡Avísame si quieres profundizar en características específicas como particionado, manejo de errores o integración con una aplicación más grande!