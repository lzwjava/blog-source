---
audio: false
generated: true
lang: es
layout: post
title: Longitudes de Contexto del Modelo Mistral AI 2025
translated: true
type: note
---

A junio de 2025, Mistral AI ha lanzado varios modelos con diferentes longitudes de contexto. Aquí hay un resumen de algunos de los modelos clave y sus ventanas de contexto:

*   **Magistral Small (magistral-small-2506):** Este modelo de pesos abiertos tiene una ventana de contexto de 128k, aunque el rendimiento podría degradarse más allá de 40k, por lo que la longitud máxima recomendada es de 40k tokens.
*   **Magistral Medium (magistral-medium-2506):** Este modelo de razonamiento solo disponible por API también tiene una ventana de contexto recomendada de 40k.
*   **Mistral Small 3.1 (mistral-small-2503) / Mistral Small 3.2 (mistral-small-2506):** Estos modelos cuentan con una ventana de contexto de hasta 128,000 tokens.
*   **Codestral (codestral-2501):** Este modelo enfocado en código tiene una ventana de contexto significativa de 256,000 tokens.
*   **Mistral Large 2 (mistral-large-2407 / mistral-large-2411):** El modelo grande insignia de Mistral tiene una ventana de contexto de 128k.
*   **Mistral NeMo (open-mistral-nemo-2407):** Este modelo de 12B, desarrollado en colaboración con NVIDIA, ofrece una gran ventana de contexto de hasta 128,000 tokens.
*   **Mistral Medium 3 (mistral-medium-2505):** Este modelo multimodal también admite una ventana de contexto de 128k.

Es importante señalar que, aunque algunos modelos anuncian ventanas de contexto muy grandes (por ejemplo, 128k), el rendimiento práctico podría estar optimizado para un rango ligeramente menor (por ejemplo, 40k para los modelos Magistral).