---
audio: false
generated: true
lang: es
layout: post
title: Los secretos oscuros del equipo Pangu de Huawei
translated: true
type: note
---

https://github.com/HW-whistleblower/True-Story-of-Pangu

---

**Nota del editor:** Lo siguiente es una traducción de una extensa publicación de un individuo anónimo que afirma ser un empleado del Laboratorio Arca de Noé de Huawei. Esta persona alega problemas significativos dentro del equipo de desarrollo del modelo grande Pangu, incluyendo supuesta infracción de propiedad intelectual, luchas internas de poder y un entorno laboral perjudicial. La publicación original fue escrita en chino y compartida en línea el 6 de julio de 2025. Esta traducción pretende transmitir con precisión el contenido y el tono del original.

---

## La tragedia de Pangu: El desconsuelo y la oscuridad del viaje de desarrollo del modelo grande Pangu del Laboratorio Arca de Noé de Huawei

Hola a todos,

Soy un empleado del equipo del modelo grande Pangu en el Laboratorio Arca de Noé de Huawei.

Primero, para verificar mi identidad, enumeraré algunos detalles:

El actual jefe de Noah es Wang Yunhe, anteriormente Ministro del Departamento de Aplicación de Algoritmos, luego renombrado como Director del Laboratorio de Modelos Pequeños. El antiguo jefe de Noah era Yao Jun (a quien todos se refieren como el Profesor Yao). Varios directores de laboratorio: Tang Ruiming (Hermano Ming, Equipo Ming, ya renunció), Shang Lifeng, Zhang Wei (Hermano Wei), Hao Jianye (Profesor Hao), Liu Wulong (referido como Wu Long Suo), etc. Muchos otros miembros centrales y expertos han dejado la empresa sucesivamente.

Pertenecemos a la organización "Sìyě" (Cuatro Campos). Sìyě tiene muchas brigadas, y el modelo lingüístico grande fundamental es la Cuarta Brigada. El modelo pequeño de Wang Yunhe es la Decimosexta Brigada. Participamos en reuniones en Suzhou, con varios hitos mensuales. Durante la "reunión de ataque" en Suzhou, se asignaban tareas y se debían alcanzar los objetivos antes de la fecha límite. La reunión de Suzhou reunía al personal de varias ubicaciones en el Instituto de Investigación de Suzhou, donde normalmente se alojaban en hoteles, como los de Luzhi, separados de sus familias e hijos.

Durante la reunión de Suzhou, los sábados eran días laborables por defecto, lo cual era muy duro, pero había merienda por la tarde los sábados, y una vez incluso hubo langostinos. Las estaciones de trabajo en el Instituto de Investigación de Suzhou se reubicaron una vez, de un edificio a otro. Los edificios del Instituto de Investigación de Suzhou tienen decoración de estilo europeo, con grandes pendientes en la entrada y un paisaje hermoso en el interior. Ir a la reunión de Suzhou solía durar al menos una semana, o incluso más, con algunas personas incapaces de regresar a casa durante uno o dos meses.

Una vez se rumoreó que Noah era orientado a la investigación, pero después de unirse, debido a trabajar en proyectos de modelos grandes en Sìyě, los miembros del proyecto se convirtieron completamente en orientados a la entrega, y estaban llenos de reuniones regulares, revisiones e informes. Muchas veces, incluso realizar experimentos requería aprobación. El equipo necesitaba interactuar con varias líneas de negocio como Terminal Xiaoyi, Huawei Cloud e ICT, lo que generaba una presión de entrega considerable.

El nombre en clave interno temprano para el modelo Pangu desarrollado por Noah era "Pangu Zhizi". Inicialmente, era solo una versión web que requería solicitud interna para uso de prueba. Más tarde, debido a la presión, se integró en Welink para pruebas beta públicas.

Recientemente, el revuelo respecto a las acusaciones de que el modelo grande Pangu plagió a Qwen ha sido ferviente. Como miembro del equipo Pangu, he estado dando vueltas en la cama por la noche, incapaz de dormir. La marca Pangu se ha visto tan afectada. Por un lado, egoístamente me preocupa mi desarrollo profesional y siento que mi trabajo duro pasado fue en vano. Por otro lado, siento un gran alivio de que alguien haya comenzado a exponer estos asuntos. Durante innumerables días y noches, hemos rechinado los dientes por las acciones de ciertos individuos dentro de la empresa que repetidamente obtuvieron innumerables beneficios mediante el fraude, pero nos sentimos impotentes. Esta opresión y humillación han erosionado gradualmente mi afecto por Huawei, haciendo que mi tiempo aquí sea cada vez más confuso y perdido, a menudo dudando de mi vida y mi valor propio.

Admito que soy un cobarde. Como un pequeño trabajador, no me atrevo a oponerme a individuos poderosos como Wang Yunhe dentro de la empresa, ni me atrevo a oponerme a un gigante como Huawei. Temo perder mi trabajo, ya que también tengo familia e hijos, así que realmente admiro a los denunciantes desde lo más profundo de mi corazón. Sin embargo, al ver los intentos internos de encubrir los hechos y engañar al público, simplemente no lo puedo tolerar más. También deseo ser valiente una vez y seguir mi verdadero yo. Incluso si me daño a mí mismo ochocientos, espero dañar al enemigo mil. He decidido divulgar aquí lo que he visto y oído (parcialmente de relatos verbales de colegas) respecto a la "historia legendaria" del modelo grande Pangu:

Huawei indeed entrena principalmente modelos grandes en tarjetas Ascend (el Laboratorio de Modelos Pequeños tiene muchas tarjetas Nvidia, que usaban para entrenar antes, y luego las transfirieron a Ascend). Una vez estuve impresionado por la determinación de Huawei de "construir una segunda opción mundial", y yo mismo una vez tuve un profundo afecto por Huawei. Acompañamos a Ascend paso a paso, desde estar lleno de errores hasta poder entrenar modelos, pagando una enorme cantidad de esfuerzo y costo.

Inicialmente, nuestra capacidad de computación era muy limitada, y entrenábamos modelos en el 910A. En ese momento, solo admitía FP16, y la estabilidad del entrenamiento era muy inferior a BF16. El MoE de Pangu comenzó muy temprano; en 2023, se centró principalmente en entrenar modelos MoE de 38B y posteriores modelos densos de 71B. El modelo denso de 71B se expandió en el primer modelo denso de 135B de primera generación, y luego los modelos principales se entrenaron gradualmente en el 910B.

Tanto el modelo de 71B como el de 135B tenían un defecto importante: el **tokenizer**. El tokenizer utilizado en ese momento tenía una eficiencia de codificación extremadamente baja. Cada símbolo, número, espacio e incluso carácter chino ocupaba un token. Es concebible que esto desperdiciaría enormemente la capacidad de computación y resultaría en un rendimiento del modelo muy pobre. En este momento, el Laboratorio de Modelos Pequeños casualmente tenía un vocabulario autoentrenado. El Profesor Yao sospechaba que el tokenizer del modelo podría no ser bueno (aunque en retrospectiva, su sospecha era sin duda correcta). Así que decidió cambiar los tokenizers para el 71B y el 135B, porque el Laboratorio de Modelos Pequeños lo había intentado antes. El equipo unió dos tokenizers y comenzó el reemplazo del tokenizer. El reemplazo del modelo de 71B falló, mientras que el 135B, utilizando una estrategia de inicialización de embedding más refinada, reemplazó exitosamente el vocabulario después de al menos 1T de datos de fine-tuning, pero como se esperaba, el efecto no mejoró.

Concurrentemente, otras empresas nacionales como Alibaba y Zhipu estaban entrenando en GPUs y ya habían encontrado los métodos correctos, y la brecha entre Pangu y sus competidores se hizo cada vez más amplia. Un modelo denso interno de 230B entrenado desde cero también falló debido a varias razones, llevando al proyecto a una situación casi desesperada. Frente a la presión de varios hitos y fuertes dudas internas sobre Pangu, la moral del equipo cayó en picado a un mínimo histórico. El equipo hizo muchos esfuerzos y luchas cuando la capacidad de computación era extremadamente limitada. Por ejemplo, el equipo descubrió accidentalmente que el MoE de 38B en ese momento no tenía el efecto MoE esperado. Así que eliminaron los parámetros MoE y lo restauraron a un modelo denso de 13B. Dado que el MoE de 38B se originó a partir de un Pangu Alpha 13B muy temprano, con una arquitectura relativamente obsoleta, el equipo realizó una serie de operaciones, como cambiar la codificación posicional absoluta a RoPE, eliminar el bias y cambiar a RMSNorm. Al mismo tiempo, dados algunos fallos del tokenizer y la experiencia de cambiar vocabularios, el vocabulario de este modelo también fue reemplazado por el vocabulario utilizado por el modelo 7B del Laboratorio de Modelos Pequeños de Wang Yunhe. Más tarde, este modelo de 13B fue expandido y fine-tuneado, convirtiéndose en el modelo denso de 38B de segunda generación (durante varios meses, este modelo fue el principal modelo Pangu de gama media), que una vez tuvo cierta competitividad. Sin embargo, debido a la arquitectura obsoleta del modelo más grande de 135B y el daño significativo causado por el cambio de vocabulario (análisis posteriores encontraron que el vocabulario unido cambiado en ese momento tenía errores más graves), incluso después del fine-tuning, había una gran brecha con los modelos nacionales líderes como Qwen en ese momento. En este punto, las dudas internas y la presión del liderazgo también crecieron, y el estado del equipo estaba casi en la desesperación.

Bajo estas circunstancias, Wang Yunhe y su Laboratorio de Modelos Pequeños intervinieron. Afirmaron que su modelo era heredado y modificado a partir de los parámetros antiguos del 135B, y después de entrenar solo unos pocos cientos de B de datos, todos los indicadores mejoraron en aproximadamente diez puntos en promedio. En realidad, esta fue su primera obra maestra de **"encapsulamiento" (套壳, es decir, usar secretamente o renombrar el trabajo de otro)** aplicada a un modelo grande. Los líderes de Huawei, siendo legos gestionando expertos, no tenían concepto de tales absurdos; solo pensarían que debe haber alguna innovación algorítmica. A través del análisis interno, en realidad usaron **Qwen 1.5 110B** para fine-tuning, agregando capas, expandiendo dimensiones FFN e incorporando algunos mecanismos del artículo Pi de Pangu para lograr aproximadamente 135B parámetros. De hecho, el antiguo 135B tenía 107 capas, mientras que este modelo tenía solo 82 capas, y varias configuraciones eran diferentes. Muchas distribuciones de parámetros del nuevo y oscuro modelo 135B, después del entrenamiento, eran casi idénticas a Qwen 110B. Incluso el nombre de la clase del código del modelo en ese momento era Qwen, mostrando que fueron demasiado perezosos para cambiarlo. Posteriormente, este modelo se convirtió en el llamado 135B V2. Y este modelo luego fue proporcionado a muchos usuarios downstream, incluidos incluso clientes externos.

Este incidente tuvo un impacto enorme en nuestros colegas honestos y trabajadores. Muchos conocedores internos, incluidos aquellos en Terminal y Huawei Cloud, en realidad sabían sobre esto. Todos lo llamábamos en broma modelo "Qiangu" (Mil Antiguo) en lugar de modelo Pangu. En ese momento, los miembros del equipo querían informarlo a BCG, ya que esto ya era un gran fraude empresarial. Sin embargo, más tarde se dijo que los líderes lo detuvieron, porque los líderes de nivel superior (como el Profesor Yao, y posiblemente el Sr. Xiong y el Sr. Cha) también se enteraron más tarde pero no intervinieron, ya que obtener buenos resultados mediante "encapsulamiento" también les beneficiaba a ellos. Este incidente hizo que varios de los colegas más fuertes del equipo se desilusionaran, y la renuncia se convirtió en un tema frecuente de conversación.

En este punto, Pangu pareció tener un punto de inflexión. Dado que los modelos Pangu mencionados anteriormente básicamente fueron fine-tuneados y modificados, Noah no dominaba la tecnología de entrenar desde cero en ese momento, y mucho menos entrenar en Ascend NPU. Con los fuertes esfuerzos de los miembros centrales del equipo en ese momento, Pangu comenzó el entrenamiento del modelo de tercera generación. Después de enormes esfuerzos, en términos de arquitectura de datos y algoritmos de entrenamiento, gradualmente se alineó con la industria, y las dificultades involucradas no tenían nada que ver con el Laboratorio de Modelos Pequeños.

Inicialmente, los miembros del equipo no tenían confianza y solo comenzaron a entrenar un modelo de 13B. Sin embargo, más tarde encontraron que el efecto no era malo, así que este modelo posteriormente se expandió en parámetros una vez más, convirtiéndose en el 38B de tercera generación, con nombre en código 38B V3. Muchos hermanos en las líneas de productos deben estar muy familiarizados con este modelo. En ese momento, el tokenizer de este modelo era una extensión basada en el vocabulario de LLaMA (que también es una práctica común en la industria). En ese momento, el laboratorio de Wang Yunhe desarrolló otro vocabulario (que es el vocabulario de la serie Pangu posterior). En ese momento, incluso se obligó a competir a los dos vocabularios, y en última instancia no hubo una conclusión clara sobre cuál era mejor o peor. Así que el liderazgo decidió inmediatamente que el vocabulario debería unificarse y usarse el de Wang Yunhe. Por lo tanto, el posterior 135B V3 (que es conocido externamente como Pangu Ultra) entrenado desde cero adoptó este tokenizer. Esto también explica la confusión de muchos hermanos que usaron nuestros modelos, por qué dos modelos de diferentes niveles en la misma generación V3 usarían tokenizers diferentes.

Genuinamente sentimos que **135B V3 es el orgullo de nuestro equipo de la Cuarta Brigada en ese momento**. Este es el primer modelo a nivel de miles de millones verdaderamente autodesarrollado de pila completa por Huawei, genuinamente entrenado desde cero, y su rendimiento era comparable al de los competidores en 2024. Mientras escribo esto, se me llenan los ojos de lágrimas; fue increíblemente difícil. En ese momento, para garantizar un entrenamiento estable, el equipo realizó una gran cantidad de experimentos comparativos y retrocedió y reinició múltiples veces de manera oportuna cuando los gradientes del modelo mostraban anomalías. Este modelo realmente logró lo que el informe técnico posterior afirmaba: sin picos de pérdida durante todo el proceso de entrenamiento. Superamos innumerables dificultades; lo logramos. Estamos dispuestos a apostar nuestras vidas y honor en la autenticidad del entrenamiento de este modelo. ¿Cuántas noches en vela pasamos por su entrenamiento? Cuando nos criticaban como inútiles en el foro interno "Xinsheng" (Voz del Empleado), ¿cuánta indignación y agravios sentimos? Lo soportamos.

Nosotros, este grupo de personas, estamos realmente quemando nuestra juventud para pulir nuestra base de computación nacional... Viviendo en tierra extraña, renunciamos a nuestras familias, nuestras vacaciones, nuestra salud, nuestro entretenimiento, derramando sangre y sudor. Las dificultades y penalidades no pueden resumirse en unos pocos trazos. En varias reuniones de movilización, cuando se gritaban los lemas "Pangu ganará, Huawei ganará", estábamos realmente profundamente conmovidos en nuestros corazones.

Sin embargo, todos los frutos de nuestro trabajo duro a menudo fueron tomados sin esfuerzo por el Laboratorio de Modelos Pequeños. Datos, tomados directamente. Código, tomado directamente, e incluso exigían que lo adaptáramos para que fuera ejecutable con un clic. En ese entonces bromeábamos llamando al Laboratorio de Modelos Pequeños el "Laboratorio de Clic del Ratón". Nosotros pusimos el trabajo duro, y ellos cosecharon la gloria. Realmente encarna el dicho: "Tú cargas con la carga pesada porque alguien más está viviendo cómodamente". Bajo tales circunstancias, cada vez más compañeros ya no pudieron perseverar y optaron por irse. Al ver a esos excelentes colegas irse uno por uno, me sentí tanto emocionado como triste. En un entorno tan similar al combate, éramos más como camaradas que colegas. También tenían innumerables aspectos técnicos dignos de aprender, verdaderamente excelentes mentores. Al verlos ir a muchos equipos destacados como ByteDance Seed, DeepSeek, Moonshot AI, Tencent y Kuaishou, me alegro genuinamente y les deseo lo mejor, escapando de este lugar arduo pero sucio. Todavía recuerdo vívidamente lo que dijo un ex colega: "Venir aquí es una desgracia en mi carrera técnica; quedarme aquí otro día es una pérdida de vida". Aunque las palabras fueron duras, me dejaron sin palabras. Me preocupaba mi insuficiente acumulación técnica y mi incapacidad para adaptarme al entorno de alta rotación de las empresas de internet, lo que me hizo querer renunciar repetidamente pero nunca dar ese paso.

Además del modelo denso, Pangu posteriormente inició la exploración de MoE. Inicialmente, se entrenó un modelo MoE de 224B. Paralelamente, el Laboratorio de Modelos Pequeños también lanzó su segunda operación importante de "encapsulamiento" (los interludios menores pueden incluir otros modelos, como modelos matemáticos), a saber, el ampliamente difundido Pangu Pro MoE 72B. Este modelo afirmaba internamente ser una expansión del 7B del Laboratorio de Modelos Pequeños (incluso así, esto contradice el informe técnico, y mucho menos ser un "encapsulamiento" de Qwen 2.5 14B para fine-tuning). Recuerdo que solo habían entrenado durante unos días cuando su evaluación interna inmediatamente se puso al día con el entonces 38B V3. Muchos hermanos en el Laboratorio de Sistemas de IA sabían sobre su operación de "encapsulamiento" porque necesitaban adaptar el modelo, pero debido a varias razones, no pudieron hacer justicia. De hecho, para este modelo que fue fine-tuneado durante mucho tiempo después, ya estoy muy sorprendido de que HonestAGI pudiera analizar este nivel de similitud, porque el poder de computación gastado para fine-tunear y "lavar" los parámetros de este modelo fue lo suficientemente largo como para entrenar un modelo del mismo nivel desde cero. Escuché de colegas que usaron muchos métodos para "lavar" la marca de agua de Qwen, incluso incluyendo entrenar intencionalmente con datos sucios. Esto también proporciona un ejemplo sin precedentes y especial para que la comunidad académica estudie el linaje de modelos. En el futuro, se pueden mostrar nuevos métodos de linaje usando este.

A finales de 2024 y principios de 2025, después del lanzamiento de DeepSeek V3 y R1, debido a su asombroso nivel técnico, el equipo sufrió un gran impacto y enfrentó un mayor escrutinio. Para mantenerse al día con la tendencia, Pangu imitó el tamaño del modelo de DeepSeek y comenzó a entrenar un MoE de 718B. En este momento, el Laboratorio de Modelos Pequeños actuó nuevamente. Eligieron "encapsular" DeepSeek V3 para fine-tuning. Entrenaron congelando los parámetros cargados de DeepSeek. Incluso el directorio para cargar checkpoints era "deepseekv3", sin siquiera cambiarlo, ¡qué arrogancia! En contraste, algunos colegas con genuinas creencias técnicas estaban entrenando otro MoE de 718B desde cero. Sin embargo, surgieron varios problemas. Pero obviamente, ¿cómo podría este modelo ser mejor que simplemente "encapsular" uno? Si no fuera por la insistencia del líder del equipo, se habría detenido hace mucho tiempo.

La pesada carga de la gestión de procesos de Huawei frenó severamente el ritmo de desarrollo de los modelos grandes, como el control de versiones, el linaje de modelos, varios requisitos de procedimiento y la trazabilidad. Irónicamente, los modelos del Laboratorio de Modelos Pequeños parecían estar exentos de estas restricciones de proceso: podían "encapsular" cuando quisieran, fine-tunear cuando quisieran, y la capacidad de computación se tomaba continuamente sin cuestionar. Este fuerte contraste, casi mágico, ilustra el estado actual de la gestión de procesos: "Solo se permite a los oficiales prender fuego, pero no se permite a los plebeyos encender lámparas". ¿Qué ridículo? ¿Qué trágico? ¿Qué odioso? ¿Qué vergonzoso!

Después de que salió el incidente de HonestAGI, se celebraron constantemente discusiones y análisis internos sobre cómo hacer relaciones públicas y "responder". De hecho, el análisis original podría no haber sido lo suficientemente fuerte, dando a Wang Yunhe y al Laboratorio de Modelos Pequeños la oportunidad de buscar evasivas y distorsionar los hechos. Por esto, me he sentido enfermo del estómago estos últimos dos días, dudando constantemente del significado de mi vida y de la injusticia del cielo. Ya no voy a seguir el juego; estoy renunciando. Al mismo tiempo, también estoy solicitando que me eliminen de la lista de autores de algunos informes técnicos de Pangu. Ser nombrado en esos informes técnicos es una mancha que nunca podré borrar de mi vida. En ese momento, no esperaba que fueran tan rampantes como para atreverse a hacer código abierto. No esperaba que se atrevieran a engañar al mundo tan descaradamente y promocionarlo ampliamente. En ese momento, tal vez tuve una mentalidad de侥幸 (complacencia) y no me negué a ser nombrado. Creo que muchos compañeros trabajadores también fueron simplemente forzados a subir a un "barco pirata" o no estaban al tanto. Pero este asunto es irreversible. Espero que en el resto de mi vida, pueda perseverar en hacer un trabajo verdaderamente significativo y expiar mi debilidad e indecisión en ese entonces.

Escribiendo esto tarde en la noche, ya estoy en lágrimas, sollozando incontrolablemente. Todavía recuerdo cuando algunos excelentes colegas renunciaron, forcé una sonrisa y les pregunté si querían escribir una larga publicación en "Xinsheng", exponiendo la situación actual. Dijeron: "No, es una pérdida de tiempo, y también temo que exponerlo empeore aún más sus vidas". En ese momento, de repente me desilusioné porque los camaradas que una vez lucharon juntos por ideales habían perdido completamente la esperanza en Huawei. En ese entonces, todos bromeábamos diciendo que estábamos usando las "armas cortas y el trigo" del Partido Comunista de antaño, pero la organización tenía un estilo comparable al del Kuomintang de aquellos días.

Hubo un tiempo en que estaba orgulloso de que derrotamos a las armas extranjeras con armas cortas y trigo.
Ahora, estoy cansado. Quiero rendirme.

En realidad, incluso hoy, todavía espero sinceramente que Huawei pueda aprender de sus lecciones, hacer un buen trabajo con Pangu, hacer de Pangu un modelo de clase mundial y llevar a Ascend al nivel de Nvidia. La "mala hierba que ahoga la buena" interna ha causado que Noah e incluso Huawei pierdan rápidamente una gran cantidad de excelentes talentos en modelos grandes en un corto período. Creo que ahora están brillando en varios equipos como DeepSeek, cumpliendo sus ambiciones y talentos, y contribuyendo a la feroz competencia de IA entre China y EE. UU. A menudo suspiro pensando que Huawei no carece de talento, sino que simplemente no sabe cómo retenerlo. Si a estas personas se les da el entorno correcto, los recursos correctos, menos ataduras y menos luchas políticas, ¿por qué Pangu no tendría éxito?

Finalmente: Juro por mi vida, carácter y honor que todo el contenido que he escrito anteriormente es verdadero (al menos dentro de mi conocimiento limitado). No tengo el alto nivel de experiencia técnica ni la oportunidad de realizar un análisis detallado y exhaustivo, ni me atrevo a usar registros internos directamente como evidencia, temiendo que me atrapen debido a la seguridad de la información. Sin embargo, creo que muchos de mis antiguos camaradas testificarán por mí. Hermanos dentro de Huawei, incluidos los hermanos de la línea de productos a los que una vez servimos, creo que los innumerables detalles en este artículo coincidirán con sus impresiones y confirmarán mis declaraciones. Es posible que también hayan sido engañados, pero estas crueles verdades no serán enterradas. Las huellas de nuestra lucha no deben ser distorsionadas o enterradas.

Habiendo escrito tanto, ciertas personas definitivamente querrán encontrarme y silenciarme. La empresa también podría querer silenciarme o responsabilizarme. Si eso realmente sucede, mi seguridad personal, e incluso la seguridad de mi familia, podrían verse amenazadas. Para autoprotegerme, informaré mi seguridad a todos diariamente en el futuro cercano.

Si desaparezco, considérenlo un sacrificio por la verdad y los ideales, por el mejor desarrollo de la capacidad de computación y la IA en Huawei e incluso en China. Estoy dispuesto a ser enterrado en el lugar donde una vez luché.

Adiós, Noah.
Escrito en Shenzhen el 6 de julio de 2025, en la madrugada.