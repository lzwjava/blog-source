---
audio: false
generated: true
lang: es
layout: post
title: Red Neuronal Convolucional
translated: true
type: note
---

### ¿Qué es una CNN (Convolutional Neural Network)?

Una **Red Neuronal Convolucional (CNN)** es un tipo de red neuronal profunda que se utiliza principalmente para tareas relacionadas con imágenes, como clasificación, detección de objetos y segmentación. Las CNN están diseñadas para aprender automáticamente y de forma adaptativa las jerarquías espaciales de características a partir de imágenes de entrada. Son muy eficaces para capturar dependencias locales y reducir el número de parámetros en comparación con las capas totalmente conectadas tradicionales en las redes neuronales.

### Componentes Clave de una CNN:
1. **Capa Convolucional**:  
   Esta capa aplica operaciones de convolución a los datos de entrada, lo que ayuda a extraer características de la imagen de entrada (por ejemplo, bordes, texturas, patrones). La operación de convolución utiliza filtros (también llamados kernels) que se deslizan sobre la imagen de entrada.

2. **Capa de Pooling**:  
   Las capas de pooling se utilizan para reducir el tamaño de los mapas de características, disminuyendo sus dimensiones espaciales y haciendo que la red sea más eficiente computacionalmente, además de ayudar con la invariancia a la traslación (la capacidad de reconocer objetos incluso si se mueven dentro de la imagen).

3. **Capa Totalmente Conectada**:  
   Después de las capas de convolución y pooling, se utilizan capas totalmente conectadas para clasificar las características extraídas de las capas anteriores. La capa de salida final suele utilizar una función de activación softmax o sigmoide para tareas de clasificación.

4. **Función de Activación (ReLU)**:  
   Después de cada capa convolucional o totalmente conectada, a menudo se utiliza una función de activación como **ReLU** (Unidad Lineal Rectificada) para introducir no linealidad en el modelo, permitiéndole aprender patrones más complejos.

### Ejemplo de Arquitectura de una CNN:
- **Capa de Entrada**: Una imagen o un lote de imágenes.
- **Capa Convolucional 1**: Aplicar un conjunto de filtros convolucionales (kernels).
- **Activación ReLU**: Aplicar ReLU para introducir no linealidad.
- **Capa de Pooling 1**: Max pooling o average pooling.
- **Capa Convolucional 2**: Aplicar convoluciones adicionales.
- **Capa Totalmente Conectada**: Aplanar la salida y alimentarla a las capas totalmente conectadas para la clasificación.
- **Capa de Salida**: Activación softmax o sigmoide para el resultado de clasificación final.

---

### Implementación de una CNN desde Cero (sin usar frameworks como TensorFlow/PyTorch)

Aquí hay una implementación simple de una CNN usando **NumPy**. Esto te dará una idea básica de cómo funcionan las operaciones (convolución, ReLU, pooling, etc.) en una CNN.

Implementaremos una CNN básica con:
1. Una capa de convolución
2. Una capa de activación ReLU
3. Una capa de pooling
4. Una capa totalmente conectada

Nos centraremos en una versión muy simplificada de una CNN, donde no tenemos características avanzadas como normalización por lotes, dropout, etc.

### Paso 1: Capa Convolucional

Implementaremos la operación de **convolución**, que implica deslizar un filtro (kernel) sobre la imagen de entrada.

```python
import numpy as np

def convolve2d(input_image, kernel):
    kernel_height, kernel_width = kernel.shape
    image_height, image_width = input_image.shape
    
    # Dimensiones de salida después de la convolución
    output_height = image_height - kernel_height + 1
    output_width = image_width - kernel_width + 1
    
    output = np.zeros((output_height, output_width))
    
    # Deslizar el kernel sobre la imagen de entrada
    for i in range(output_height):
        for j in range(output_width):
            region = input_image[i:i+kernel_height, j:j+kernel_width]
            output[i, j] = np.sum(region * kernel)  # Multiplicación elemento a elemento y suma
    return output
```

### Paso 2: Activación ReLU

ReLU se aplica elemento a elemento en la salida de la convolución.

```python
def relu(input_image):
    return np.maximum(0, input_image)  # Operación ReLU
```

### Paso 3: Capa de Pooling (Max Pooling)

Implementaremos una capa simple de **max pooling** con una ventana de 2x2 y un stride de 2.

```python
def max_pooling(input_image, pool_size=2, stride=2):
    image_height, image_width = input_image.shape
    output_height = (image_height - pool_size) // stride + 1
    output_width = (image_width - pool_size) // stride + 1
    
    output = np.zeros((output_height, output_width))
    
    # Aplicar max pooling
    for i in range(0, image_height - pool_size + 1, stride):
        for j in range(0, image_width - pool_size + 1, stride):
            region = input_image[i:i+pool_size, j:j+pool_size]
            output[i // stride, j // stride] = np.max(region)
    
    return output
```

### Paso 4: Capa Totalmente Conectada

La capa totalmente conectada es simplemente una capa densa que toma la salida de las capas anteriores y calcula una suma ponderada.

```python
def fully_connected(input_image, weights, bias):
    # Aplanar la imagen de entrada (si es multidimensional)
    flattened_input = input_image.flatten()
    
    # Calcular la salida de la capa totalmente conectada
    output = np.dot(flattened_input, weights) + bias
    return output
```

### Paso 5: Integrando Todo

Ahora, definamos un ejemplo simple donde crearemos una CNN que toma una imagen, aplica convolución, ReLU, pooling y luego una capa totalmente conectada para hacer una predicción.

```python
# Imagen de ejemplo (5x5)
image = np.array([[1, 2, 3, 0, 1],
                  [4, 5, 6, 1, 2],
                  [7, 8, 9, 2, 3],
                  [1, 2, 3, 4, 5],
                  [6, 7, 8, 5, 6]])

# Definir un kernel simple (3x3)
kernel = np.array([[1, 0, -1],
                   [1, 0, -1],
                   [1, 0, -1]])

# Operación de convolución
conv_output = convolve2d(image, kernel)
print("Salida de la Convolución:")
print(conv_output)

# Aplicar activación ReLU
relu_output = relu(conv_output)
print("Salida de ReLU:")
print(relu_output)

# Aplicar Max Pooling
pool_output = max_pooling(relu_output)
print("Salida de Max Pooling:")
print(pool_output)

# Capa totalmente conectada (salida aplanada, pesos y bias 1D)
weights = np.random.randn(pool_output.size)  # Pesos aleatorios
bias = np.random.randn()  # Bias aleatorio
fc_output = fully_connected(pool_output, weights, bias)
print("Salida de la Capa Totalmente Conectada:")
print(fc_output)
```

### Explicación del Proceso:

1. **Imagen de Entrada**: Creamos una imagen simple de 5x5.
2. **Convolución**: Aplicamos un filtro de 3x3 a la imagen, resultando en una salida más pequeña.
3. **ReLU**: Aplicamos la función ReLU para introducir no linealidad.
4. **Pooling**: Reducimos el tamaño de la imagen usando max pooling con una ventana de 2x2 y un stride de 2.
5. **Totalmente Conectada**: Aplanamos la salida del pooling y aplicamos una capa totalmente conectada con pesos y bias aleatorios.

Esta es una CNN muy simplificada, pero debería darte una idea de cómo funcionan las capas de convolución, ReLU, pooling y totalmente conectadas.

### Conclusión

Esta implementación proporciona un marco básico para entender cómo funciona una CNN a bajo nivel. En la práctica, frameworks como **TensorFlow**, **Keras** y **PyTorch** manejan muchas de las optimizaciones, mejoras de eficiencia y características adicionales (por ejemplo, normalización por lotes, dropout) que hacen que las CNN sean más potentes y fáciles de entrenar en grandes conjuntos de datos.

¡Házme saber si quieres más detalles sobre cómo extender esto o si necesitas ayuda con otra parte de las CNN!