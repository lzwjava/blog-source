---
audio: false
generated: true
image: false
lang: en
layout: post
model: x-ai/grok-4-fast
title: Inverting a 3x3 Matrix
translated: false
type: note
---

Yes, you're on the right track! You're describing the standard method for finding the inverse of a 3×3 matrix \\( A \\) (often denoted \\( A^{-1} \\)) using the **cofactor matrix** (or more precisely, the **adjugate matrix**, which is the transpose of the cofactor matrix). The "divider 1/value * matrix" part refers to scaling by \\( 1/\det(A) \\), where \\( \det(A) \\) is the determinant of \\( A \\). And "rorate" might mean "transpose" (rotating rows to columns)—that's a key step too.

I'll explain the method step by step, why it works, and how to compute it for a general 3×3 matrix. This comes from the general formula for matrix inverses: \\( A^{-1} = \frac{1}{\det(A)} \adj(A) \\), where \\( \adj(A) \\) is the adjugate.

### Step-by-Step Method for a 3×3 Matrix
Let’s say your matrix is:
\\[
A = \begin{pmatrix}
a & b & c \\
d & e & f \\
g & h & i
\end{pmatrix}
\\]

1. **Compute the Determinant \\( \det(A) \\)** (the "value" you divide by):
   \\[
   \det(A) = a(ei - fh) - b(di - fg) + c(dh - eg)
   \\]
   If \\( \det(A) = 0 \\), the matrix has no inverse (it's singular). Otherwise, proceed.

2. **Find the Cofactor Matrix \\( C \\)**:
   - The cofactor \\( C_{ij} \\) for element \\( a_{ij} \\) is \\( (-1)^{i+j} \\) times the determinant of the 2×2 submatrix obtained by deleting row \\( i \\) and column \\( j \\).
   - You don't need to compute *all* cofactors from scratch—you can compute them along one row (or column) and use properties to fill the rest, but for 3×3, it's quick to do all nine.
   
   The cofactors are:
   \\[
   C_{11} = (-1)^{2} \det\begin{pmatrix} e & f \\ h & i \end{pmatrix} = ei - fh
   \\]
   \\[
   C_{12} = (-1)^{3} \det\begin{pmatrix} d & f \\ g & i \end{pmatrix} = -(di - fg)
   \\]
   \\[
   C_{13} = (-1)^{4} \det\begin{pmatrix} d & e \\ g & h \end{pmatrix} = dh - eg
   \\]
   \\[
   C_{21} = (-1)^{3} \det\begin{pmatrix} b & c \\ h & i \end{pmatrix} = -(bi - ch)
   \\]
   \\[
   C_{22} = (-1)^{4} \det\begin{pmatrix} a & c \\ g & i \end{pmatrix} = ai - cg
   \\]
   \\[
   C_{23} = (-1)^{5} \det\begin{pmatrix} a & b \\ g & h \end{pmatrix} = -(ah - bg)
   \\]
   \\[
   C_{31} = (-1)^{4} \det\begin{pmatrix} b & c \\ e & f \end{pmatrix} = bf - ce
   \\]
   \\[
   C_{32} = (-1)^{5} \det\begin{pmatrix} a & c \\ d & f \end{pmatrix} = -(af - cd)
   \\]
   \\[
   C_{33} = (-1)^{6} \det\begin{pmatrix} a & b \\ d & e \end{pmatrix} = ae - bd
   \\]
   
   So the cofactor matrix is:
   \\[
   C = \begin{pmatrix}
   ei-fh & -(di-fg) & dh-eg \\
   -(bi-ch) & ai-cg & -(ah-bg) \\
   bf-ce & -(af-cd) & ae-bd
   \end{pmatrix}
   \\]

3. **Transpose to Get the Adjugate \\( \adj(A) = C^T \\)** (this is the "rorate" step—swap rows and columns):
   \\[
   \adj(A) = \begin{pmatrix}
   ei-fh & -(bi-ch) & bf-ce \\
   -(di-fg) & ai-cg & -(af-cd) \\
   dh-eg & -(ah-bg) & ae-bd
   \end{pmatrix}
   \\]

4. **Scale by \\( 1/\det(A) \\)** to Get the Inverse:
   \\[
   A^{-1} = \frac{1}{\det(A)} \adj(A)
   \\]
   Each entry of \\( \adj(A) \\) gets multiplied by \\( 1/\det(A) \\).

### Example
Let \\( A = \begin{pmatrix} 1 & 2 & 3 \\ 0 & 1 & 4 \\ 5 & 6 & 0 \end{pmatrix} \\).

- \\( \det(A) = 1(1\cdot0 - 4\cdot6) - 2(0\cdot0 - 4\cdot5) + 3(0\cdot6 - 1\cdot5) = 1(-24) - 2(-20) + 3(-5) = -24 + 40 - 15 = 1 \\).
- Cofactors (quickly): \\( C = \begin{pmatrix} -24 & 20 & -5 \\ -15 & -15 & 5 \\ -14 & -10 & 1 \end{pmatrix} \\) (I'll skip full calc here, but you can verify).
- \\( \adj(A) = C^T = \begin{pmatrix} -24 & -15 & -14 \\ 20 & -15 & -10 \\ -5 & 5 & 1 \end{pmatrix} \\).
- \\( A^{-1} = \frac{1}{1} \adj(A) = \begin{pmatrix} -24 & -15 & -14 \\ 20 & -15 & -10 \\ -5 & 5 & 1 \end{pmatrix} \\).

You can check: \\( A \cdot A^{-1} = I \\) (the identity matrix).

### Why Does This Work?
The key reason is a property of determinants and cofactors: for any square matrix \\( A \\),
\\[
A \cdot \adj(A) = \det(A) \cdot I
\\]
(where \\( I \\) is the identity matrix). This comes from the **Laplace expansion** of the determinant along rows/columns. Each entry in \\( A \cdot \adj(A) \\) expands to a sum that equals \\( \det(A) \\) on the diagonal and 0 elsewhere (because off-diagonal terms cancel via cofactor signs).

To get the inverse, divide both sides by \\( \det(A) \\) (assuming it's nonzero):
\\[
A \cdot \left( \frac{1}{\det(A)} \adj(A) \right) = I \implies A^{-1} = \frac{1}{\det(A)} \adj(A)
\\]
Similarly, \\( \adj(A) \cdot A = \det(A) \cdot I \\), so it works from both sides.

This method generalizes to any \\( n \times n \\) matrix, but for 3×3, it's efficient. For larger matrices, Gaussian elimination is often faster computationally.

If you have a specific matrix you'd like me to invert or clarify a step, just share it!