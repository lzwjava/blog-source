---
audio: false
generated: true
image: false
lang: de
layout: post
title: Behebung des 500-Fehlers im MCP-Server
translated: true
type: note
---

Der Fehler, auf den Sie stoßen, ist ein 500 Internal Server Error von einem lokalen API-Endpunkt unter `http://127.0.0.1:3456`, der offenbar ein Model Context Protocol (MCP) Server ist. Dieser wird häufig verwendet, um lokale Large Language Models (LLMs) – wie die von Ollama verwalteten – mit Anwendungen wie Claude Desktop, VS Code Extensions (z.B. Copilot) oder IDEs wie Cursor für KI-gestützte Coding-Aufgaben zu verbinden. Der zugrundeliegende JavaScript/TypeScript-Fehler – "Cannot read properties of undefined (reading 'includes')" – deutet darauf hin, dass der Servercode versucht, auf die Methode `.includes()` einer Variable zuzugreifen, die undefined oder null ist, wahrscheinlich während der Request-Verarbeitung, Response-Behandlung oder der Interaktion mit Ollama.

Dieses Problem tritt oft auf, wenn die API aufgerufen wird, um Code zu analysieren oder zu beheben (in diesem Fall Ihr `recommend_posts.py` Skript), der Server jedoch aufgrund eines Konfigurationsproblems, fehlender Abhängigkeiten oder einer unerwarteten Antwort vom Backend-LLM versagt.

### Schritte zur Fehlerbehebung
1. **Überprüfen Sie, ob Ollama läuft und korrekt konfiguriert ist**:
   - Ollama (die lokale LLM-Engine) ist typischerweise das Backend für MCP-Server. Stellen Sie sicher, dass es installiert ist und auf seinem Standardport (11434) läuft.
   - Testen Sie es, indem Sie `curl http://localhost:11434/api/tags` in Ihrem Terminal ausführen. Dies sollte die installierten Modelle auflisten. Falls es fehlschlägt oder eine leere Liste zurückgibt, installieren Sie ein Modell mit `ollama pull <modell-name>` (z.B. `ollama pull llama3`).
   - Wenn Ollama nicht antwortet, starten Sie es mit `ollama serve` und stellen Sie sicher, dass es keine Port-Konflikte gibt.

2. **Starten Sie den MCP-Server neu**:
   - Der MCP-Server auf Port 3456 könnte sich in einem fehlerhaften Zustand befinden. Beenden Sie den Prozess: `kill -9 $(lsof -t -i:3456)`.
   - Starten Sie ihn gemäß Ihrer Einrichtung neu (z.B. wenn Sie ein Tool wie `ollama-mcp` verwenden, führen Sie den Startbefehl aus dessen Dokumentation aus). Prüfen Sie die Start-Logs auf Hinweise für eine erfolgreiche Verbindung zu Ollama.

3. **Prüfen Sie auf Port-Konflikte oder Störungen durch Claude Desktop**:
   - Claude Desktop (falls installiert) verwendet oft Port 3456 für Authentifizierung oder MCP. Wenn es läuft, schließen Sie die App oder beenden Sie deren Prozess wie oben beschrieben.
   - Wenn Sie Cursor oder VS Code verwenden, bestätigen Sie, dass Ihre settings.json die korrekte API-Base-URL enthält und keine Tippfehler vorhanden sind. Wechseln Sie temporär zu einem anderen Port, indem Sie eine Environment-Variable wie `PORT=4567` setzen, wenn Sie den MCP-Server starten, und passen Sie dann Ihre API-Base-URL entsprechend an.

4. **Aktualisieren Sie die Software und prüfen Sie die Logs**:
   - Aktualisieren Sie Ollama: `ollama update`.
   - Wenn Sie eine spezifische MCP-Bridge verwenden (z.B. von GitHub Repos wie emgeee/mcp-ollama oder patruff/ollama-mcp-bridge), pullen Sie die neueste Version und bauen Sie sie neu/installieren Sie sie neu.
   - Führen Sie den MCP-Server mit verbose-Logging aus (fügen Sie Flags wie `--debug` hinzu, falls unterstützt) und untersuchen Sie die Ausgabe nach Hinweisen, was undefined ist (z.B. eine fehlende Antwort von Ollama oder ein ungültiger Request-Payload).
   - Prüfen Sie in Cursor oder Ihrer IDE die Developer-Konsole (Strg+Umschalt+I in Cursor) für zusätzliche Fehlerdetails.

5. **Testen Sie die API direkt**:
   - Simulieren Sie eine einfache Anfrage an die API mit curl: `curl -X POST http://127.0.0.1:3456/v1/chat/completions -H "Content-Type: application/json" -d '{"model": "your-model-name", "messages": [{"role": "user", "content": "Hello"}]}'`.
   - Wenn derselbe 500-Fehler zurückgegeben wird, liegt das Problem serverseitig. Wenn es funktioniert, könnte das Problem spezifisch dafür sein, wie Ihre IDE Requests formatiert (z.B. fehlerhafter Payload beim Einbinden von Dateiinhalten).

6. **Workarounds**:
   - Wechseln Sie zu einem direkten Ollama OpenAI-kompatiblen Endpunkt: Setzen Sie Ihre API-Base auf `http://127.0.0.1:11434/v1` (Ollama unterstützt dies) und umgehen Sie MCP, falls es nicht für erweiterte Kontext-Features benötigt wird.
   - Verwenden Sie eine andere MCP-Implementierung oder ein anderes Tool. Versuchen Sie beispielsweise einen minimalen Ollama-Setup ohne MCP für Code-Korrekturen.
   - Wenn Sie Cursor verwenden, fallen Sie temporär auf cloud-basierte Modelle zurück (z.B. Claude über die offizielle API), indem Sie die benutzerdefinierte Base-URL entfernen.
   - Installieren Sie Abhängigkeiten neu: Wenn Ihr MCP-Server Node.js verwendet, führen Sie `npm install` in dessen Verzeichnis aus, um fehlende Module zu beheben.

Falls diese Schritte das Problem nicht lösen, könnte der Fehler von einem Bug in Ihrer spezifischen MCP-Bridge-Version stammen, insbesondere wenn die Verarbeitung von Dateiinhalten oder Post-Empfehlungen ihn auslöst. Durchsuchen Sie GitHub Issues für Ihr exaktes MCP-Repo mit der Fehlermeldung, oder eröffnen Sie ein neues Issue mit den Details Ihres Setups.

[Failed to register Ollama model: TypeError: Cannot read properties of undefined (reading 'includes')](https://github.com/microsoft/vscode-copilot-release/issues/8461)  
[Ollama MCP Server on GitHub](https://github.com/emgeee/mcp-ollama)  
[Model Context Protocol Documentation](https://modelcontextprotocol.io/introduction)  
[Ollama Documentation](https://github.com/ollama/ollama)