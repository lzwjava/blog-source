---
audio: false
generated: true
lang: de
layout: post
title: Faltendes Neuronales Netzwerk
translated: true
type: note
---

### Was ist CNN (Convolutional Neural Network)?

Ein **Convolutional Neural Network (CNN)** ist eine Art von tiefem neuronalem Netzwerk, das primär für bildbezogene Aufgaben wie Klassifizierung, Objekterkennung und Segmentierung verwendet wird. CNNs sind darauf ausgelegt, räumliche Hierarchien von Merkmalen aus Eingabebildern automatisch und adaptiv zu lernen. Sie sind sehr effektiv darin, lokale Abhängigkeiten zu erfassen und die Anzahl der Parameter im Vergleich zu traditionellen vollständig verbundenen Schichten in neuronalen Netzwerken zu reduzieren.

### Wichtige Komponenten eines CNN:
1. **Faltungsschicht (Convolutional Layer)**:  
   Diese Schicht wendet Faltungsoperationen auf die Eingabedaten an, was hilft, Merkmale aus dem Eingabebild zu extrahieren (z. B. Kanten, Texturen, Muster). Die Faltungsoperation verwendet Filter (auch Kernel genannt), die über das Eingabebild gleiten.

2. **Pooling-Schicht**:  
   Pooling-Schichten werden verwendet, um die Merkmalskarten herunterzurechnen, ihre räumlichen Dimensionen zu reduzieren und das Netzwerk rechnerisch effizienter zu machen. Sie tragen auch zur Translationsinvarianz bei (die Fähigkeit, Objekte zu erkennen, auch wenn sie im Bild verschoben wurden).

3. **Vollständig verbundene Schicht (Fully Connected Layer)**:  
   Nach den Faltungs- und Pooling-Schichten werden vollständig verbundene Schichten verwendet, um die von den vorherigen Schichten extrahierten Merkmale zu klassifizieren. Die endgültige Ausgabeschicht verwendet typischerweise eine Softmax- oder Sigmoid-Aktivierungsfunktion für Klassifizierungsaufgaben.

4. **Aktivierungsfunktion (ReLU)**:  
   Nach jeder Faltungs- oder vollständig verbundenen Schicht wird oft eine Aktivierungsfunktion wie **ReLU** (Rectified Linear Unit) verwendet, um Nichtlinearität in das Modell einzuführen, damit es komplexere Muster lernen kann.

### Beispiel einer CNN-Architektur:
- **Eingabeschicht**: Ein Bild oder ein Stapel von Bildern.
- **Faltungsschicht 1**: Anwenden eines Satzes von Faltungsfiltern (Kernels).
- **ReLU-Aktivierung**: Anwenden von ReLU, um Nichtlinearität einzuführen.
- **Pooling-Schicht 1**: Max-Pooling oder Average-Pooling.
- **Faltungsschicht 2**: Anwenden zusätzlicher Faltungen.
- **Vollständig verbundene Schicht**: Die Ausgabe glätten und in vollständig verbundene Schichten zur Klassifizierung einspeisen.
- **Ausgabeschicht**: Softmax- oder Sigmoid-Aktivierung für das endgültige Klassifizierungsergebnis.

---

### CNN-Implementierung von Grund auf (ohne Frameworks wie TensorFlow/PyTorch)

Hier ist eine einfache Implementierung eines CNN mit **NumPy**. Dies gibt Ihnen eine grundlegende Vorstellung davon, wie die Operationen (Faltung, ReLU, Pooling usw.) in einem CNN funktionieren.

Wir implementieren ein einfaches CNN mit:
1. Einer Faltungsschicht
2. Einer ReLU-Aktivierungsschicht
3. Einer Pooling-Schicht
4. Einer vollständig verbundenen Schicht

Wir konzentrieren uns auf eine stark vereinfachte Version eines CNN, ohne erweiterte Funktionen wie Batch-Normalisierung, Dropout usw.

### Schritt 1: Faltungsschicht

Wir werden die **Faltungsoperation** implementieren, bei der ein Filter (Kernel) über das Eingabebild gleitet.

```python
import numpy as np

def convolve2d(input_image, kernel):
    kernel_height, kernel_width = kernel.shape
    image_height, image_width = input_image.shape
    
    # Ausgabedimensionen nach der Faltung
    output_height = image_height - kernel_height + 1
    output_width = image_width - kernel_width + 1
    
    output = np.zeros((output_height, output_width))
    
    # Den Kernel über das Eingabebild schieben
    for i in range(output_height):
        for j in range(output_width):
            region = input_image[i:i+kernel_height, j:j+kernel_width]
            output[i, j] = np.sum(region * kernel)  # Elementweise Multiplikation und Summe
    return output
```

### Schritt 2: ReLU-Aktivierung

ReLU wird elementweise auf die Faltungsausgabe angewendet.

```python
def relu(input_image):
    return np.maximum(0, input_image)  # ReLU-Operation
```

### Schritt 3: Pooling-Schicht (Max Pooling)

Wir implementieren eine einfache **Max-Pooling**-Schicht mit einem 2x2-Fenster und Stride 2.

```python
def max_pooling(input_image, pool_size=2, stride=2):
    image_height, image_width = input_image.shape
    output_height = (image_height - pool_size) // stride + 1
    output_width = (image_width - pool_size) // stride + 1
    
    output = np.zeros((output_height, output_width))
    
    # Max-Pooling anwenden
    for i in range(0, image_height - pool_size + 1, stride):
        for j in range(0, image_width - pool_size + 1, stride):
            region = input_image[i:i+pool_size, j:j+pool_size]
            output[i // stride, j // stride] = np.max(region)
    
    return output
```

### Schritt 4: Vollständig verbundene Schicht

Die vollständig verbundene Schicht ist einfach eine dichte Schicht, die die Ausgabe der vorherigen Schichten nimmt und eine gewichtete Summe berechnet.

```python
def fully_connected(input_image, weights, bias):
    # Das Eingabebild glätten (falls mehrdimensional)
    flattened_input = input_image.flatten()
    
    # Die Ausgabe der vollständig verbundenen Schicht berechnen
    output = np.dot(flattened_input, weights) + bias
    return output
```

### Schritt 5: Alles zusammenfügen

Definieren wir nun ein einfaches Beispiel, in dem wir ein CNN erstellen, das ein Bild nimmt, Faltung, ReLU, Pooling und dann eine vollständig verbundene Schicht anwendet, um eine Vorhersage zu treffen.

```python
# Beispielbild (5x5)
image = np.array([[1, 2, 3, 0, 1],
                  [4, 5, 6, 1, 2],
                  [7, 8, 9, 2, 3],
                  [1, 2, 3, 4, 5],
                  [6, 7, 8, 5, 6]])

# Einen einfachen Kernel definieren (3x3)
kernel = np.array([[1, 0, -1],
                   [1, 0, -1],
                   [1, 0, -1]])

# Faltungsoperation
conv_output = convolve2d(image, kernel)
print("Convolution Output:")
print(conv_output)

# ReLU-Aktivierung anwenden
relu_output = relu(conv_output)
print("ReLU Output:")
print(relu_output)

# Max-Pooling anwenden
pool_output = max_pooling(relu_output)
print("Max Pooling Output:")
print(pool_output)

# Vollständig verbundene Schicht (geglättete Ausgabe, 1D-Gewichte und Bias)
weights = np.random.randn(pool_output.size)  # Zufällige Gewichte
bias = np.random.randn()  # Zufälliger Bias
fc_output = fully_connected(pool_output, weights, bias)
print("Fully Connected Output:")
print(fc_output)
```

### Erklärung des Prozesses:

1. **Eingabebild**: Wir erstellen ein einfaches 5x5-Bild.
2. **Faltung**: Wir wenden einen 3x3-Filter auf das Bild an, was zu einer kleineren Ausgabe führt.
3. **ReLU**: Wir wenden die ReLU-Funktion an, um Nichtlinearität einzuführen.
4. **Pooling**: Wir rechnen das Bild mit Max-Pooling und einem 2x2-Fenster und Stride 2 herunter.
5. **Vollständig verbunden**: Wir glätten die gepoolte Ausgabe und wenden eine vollständig verbundene Schicht mit zufälligen Gewichten und Bias an.

Dies ist ein sehr vereinfachtes CNN, aber es sollte Ihnen ein Gefühl dafür vermitteln, wie Faltungs-, ReLU-, Pooling- und vollständig verbundene Schichten funktionieren.

### Schlussfolgerung

Diese Implementierung bietet einen grundlegenden Rahmen, um zu verstehen, wie ein CNN auf niedriger Ebene funktioniert. In der Praxis übernehmen Frameworks wie **TensorFlow**, **Keras** und **PyTorch** viele der Optimierungen, Effizienzverbesserungen und zusätzlichen Funktionen (z. B. Batch-Normalisierung, Dropout), die CNNs leistungsfähiger und einfacher für das Training auf großen Datensätzen machen.

Lassen Sie mich wissen, wenn Sie weitere Details zur Erweiterung benötigen oder Hilfe bei einem anderen Teil von CNNs brauchen!