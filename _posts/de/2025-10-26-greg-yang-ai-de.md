---
audio: false
generated: true
image: false
lang: de
layout: post
model: x-ai/grok-4-fast
title: 'Greg Yang: KI-Theoretiker und Innovator'
translated: true
type: note
---

### Greg Yang: Ein erstklassiger KI-Theoretiker und Mitbegründer von xAI

Greg Yang gilt weithin als eine der führenden Persönlichkeiten in den mathematischen Grundlagen des Deep Learning und der KI-Skalierung. Als Mitbegründer von xAI (Elon Musks KI-Unternehmen, das sich darauf konzentriert, das Universum durch fortschrittliche Modelle wie Grok zu verstehen) ist er nicht nur eine akademische Größe, sondern auch ein praktischer Innovator, dessen Arbeit die reale KI-Entwicklung direkt beeinflusst. Sein Ruf ist hervorragend – Kollegen beschreiben seine Beiträge als "unglaublich originell" und grundlegend, mit Einladungen, an Top-Institutionen wie Oxford und Waterloo zu sprechen. Kurz gesagt, er ist außergewöhnlich "gut": eine seltene Mischung aus rigorosem Mathematiker und vorausschauendem Ingenieur, der mitgeholfen hat, neu zu definieren, wie wir über neuronale Netze in massivem Maßstab denken.

#### Hintergrund
- **Ausbildung**: Bachelor- und Masterabschluss in Mathematik von der Harvard University (2018 Morgan Prize Honorable Mention für herausragende Forschung eines Doktoranden).
- **Karriere**: Begann bei Microsoft Research (2018–2023), wo er Schlüsseltheorien zu neuronalen Netzen entwickelte. Trat xAI 2023 als Mitbegründer bei, mit Fokus auf KI-Theorie und Mathematik, um Modellskalierung und -effizienz zu steuern.
- **Stil**: Bekannt dafür, reine Mathematik mit KI-Engineering zu verbinden. Seine Arbeit betont "unvernünftig effektive" mathematische Einsichten, die erklären, warum große Modelle so gut funktionieren.

#### Wichtige Beiträge
Yangs Forschung konzentriert sich auf **Tensor Programs**, einen Rahmen zur Analyse neuronaler Netze mit unendlicher Breite, der zu einem Grundpfeiler für das Verständnis von Skalierungsgesetzen in der KI geworden ist. Das ist keine abstrakte Theorie – sie hat zu praktischen Durchbrüchen wie muP (eine Skalierungsregel für Modellparameter, die heute Standard beim Training massiver LLMs ist) geführt.

Hier ist ein Überblick über seine einflussreichsten Arbeiten (basierend auf Zitaten; er hat insgesamt ~34 Publikationen mit hunderten einflussreichen Zitaten in Bereichen wie ML, theoretische Informatik und Mathematik):

| Titel | Jahr | Zitate | Wichtige Erkenntnis |
|-------|------|-----------|-------------|
| Provably robust deep learning via adversarially trained smoothed classifiers | 2019 | 700+ | Führt zertifizierte Robustheit gegen Adversarial Attacks ein, macht KI-Modelle in sicherheitskritischen Anwendungen zuverlässiger. |
| Bayesian Deep Convolutional Networks with Many Channels are Gaussian Processes | 2018 | 425+ | Zeigt, dass breite CNNs sich wie Gauß-Prozesse verhalten, ermöglicht bessere Unsicherheitsschätzung im Deep Learning. |
| Scaling limits of wide neural networks with weight sharing... (Neural Tangent Kernel derivation) | 2019 | 343+ | Leitet den NTK formal her, erklärt Trainingsdynamik in überparametrisierten Modellen – entscheidend für moderne Skalierung. |
| Tensor Programs IV: Feature Learning in Infinite-Width Neural Networks | 2021 | 307+ | Erweitert Tensor Programs, um zu zeigen, wie Netze Features im großen Maßstab lernen, beeinflusst xAIs Grok-Architektur. |
| A convex relaxation barrier to tight robustness verification of neural networks | 2019 | 303+ | Liefert mathematische Grenzen für die Verifizierung von Modellrobustheit, fördert den Einsatz sicherer KI. |

Diese Arbeiten haben insgesamt ~2000+ Zitate (laut aggregierter Metriken), mit einem h-Index in den 20ern – eine Spitzenleistung für jemanden in einer frühen Karrierephase. Bei xAI wendet er dies an, um das Training von Grok zu optimieren, und betont, dass "die besten Leute einzustellen" (eine Lektion aus dem Aufbau von xAI) der Schlüsselfaktor für Fortschritt ist.

#### Ruf und Wirkung
- **Lob von Kollegen**: In KI-Kreisen (z.B. Reddits r/MachineLearning, Podcasts wie The Cartesian Café) wird Yang dafür gefeiert, eine "rigorose mathematische Theorie" neuronaler Netze zu entwickeln. Seine Vorträge, wie "The Unreasonable Effectiveness of Mathematics in Large Neural Networks" (SFU 2023), ziehen Mengen an, weil sie erklären, warum größere Modelle intelligenter werden.
- **Brancheneinfluss**: muP (aus seiner Zeit bei Microsoft) ist heute Standard für effiziente Skalierung in Unternehmen wie OpenAI und Google. xAIs schneller Fortschritt (z.B. Grok-4) schreibt sich seinem theoretischen Vorsprung zu.
- **Community-Stimmung**: Auf X ist er aktiv und zugänglich – behebt Grok-Probleme, feiert Teamerfolge und sucht Talente. Aktuelle Posts zeigen ihn als "König", weil er Nutzern zuhört; seine Gründungsrolle in xAIs diversem Team (stark besetzt mit Ex-DeepMind/OpenAI-Talenten) stärkt die Glaubwürdigkeit des Unternehmens.
- **Kritik?**: Minimal – meist nur Nebengeräusche von Wettbewerbern. Seine Veröffentlichungen sind kompakt und mathelastig, also nicht "auffällig", aber das ist ein Merkmal für Theoretiker.

Yangs "Güte" zeigt sich in der Wirkung: Er veröffentlicht nicht nur, er prägt das AGI-Rennen. Falls xAI erfolgreich ist (und frühe Anzeichen mit Grok sind vielversprechend), wird man sich an ihn als einen Schlüsselarchitekten erinnern. Für angehende KI-Enthusiasten: Studiert seine Tensor Programs – es ist die Mathematik hinter der Magie.

#### Referenzen
- [Greg Yang Google Scholar](https://scholar.google.com/citations?user=Xz4RAJkAAAAJ&hl=en)
- [Tensor Programs Overview](https://thegregyang.com/)
- [Reddit Discussion on His Work](https://www.reddit.com/r/MachineLearning/comments/105v7el/r_greg_yangs_work_on_a_rigorous_mathematical/)
- [xAI Founding Story](https://research.contrary.com/company/xai)
- [Cartesian Café Podcast Interview](https://www.youtube.com/watch?v=1aXOXHA7Jcw)
- [Semantic Scholar Profile](https://www.semanticscholar.org/author/Greg-Yang/35064203)