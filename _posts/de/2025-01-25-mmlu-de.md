---
audio: true
lang: de
layout: post
title: MMLU-Benchmark
translated: true
---

## Vorwort

Dieser Beitrag bewertet ein Sprachmodell mithilfe des MMLU-Benchmarks (Massive Multitask Language Understanding).

Der MMLU-Benchmark ist ein umfassender Test der Fähigkeit eines Modells, verschiedene Aufgaben über eine breite Palette von Themen hinweg auszuführen. Er besteht aus Multiple-Choice-Fragen, die unterschiedliche Bereiche wie Mathematik, Geschichte, Recht und Medizin abdecken.

**Dataset-Links:**

*   [Papers with Code](https://paperswithcode.com/dataset/mmlu)
*   [Hugging Face Datasets](https://huggingface.co/datasets/cais/mmlu)

## llama-server

Um den llama-server zu starten:

```bash
build/bin/llama-server -m models/7B/mistral-7b-instruct-v0.2.Q4_K_M.gguf --port 8080
```

## MMLU-Benchmark

Um den MMLU-Benchmark-Code auszuführen:

```python
import torch
from datasets import load_dataset
import requests
import json
from tqdm import tqdm

# MMLU-Dataset laden
subject = "college_computer_science"  # Wählen Sie Ihr Fach
dataset = load_dataset("cais/mmlu", subject, split="test")

# Prompt ohne Few-Shot-Beispiele formatieren
def format_mmlu_prompt(example):
    prompt = "Die folgenden sind Multiple-Choice-Fragen über {}".format(subject.replace("_", " "))
    prompt += ". Bitte antworten Sie nur mit dem Buchstaben der richtigen Wahl (A, B, C oder D)."
    prompt += " Antworten Sie nur mit dem Buchstaben. Keine Erklärung ist erforderlich."
    
    # Aktuelle Frage hinzufügen
    prompt += f"Frage: {example['question']}\n"
    prompt += "Auswahlmöglichkeiten:\nA. {}\nB. {}\nC. {}\nD. {}\n".format(*example['choices'])
    return prompt

# Evaluierungsschleife
correct = 0
total = 0

for i, example in tqdm(enumerate(dataset), total=len(dataset), desc="Evaluating"):
    prompt = format_mmlu_prompt(example)
    
    # Anfrage an llama-server senden
    url = "http://localhost:8080/v1/chat/completions"
    headers = {"Content-Type": "application/json"}
    data = {
        "messages": [{"role": "user", "content": prompt}]
    }
    
    print(f"Input to API: {data}")
    response = requests.post(url, headers=headers, data=json.dumps(data))
    
    if response.status_code == 200:
        output_text = response.json()["choices"][0]["message"]["content"]
        predicted_answer = output_text.strip()[0] if len(output_text.strip()) > 0 else ""
        print(f"Output from API: {output_text}")
    else:
        predicted_answer = ""
        print(f"Error: {response.status_code} - {response.text}")
    
    # Mit der richtigen Antwort vergleichen
    
    answer_map = {0: "A", 1: "B", 2: "C", 3: "D"}
    ground_truth_answer = answer_map.get(example["answer"], "")
    is_correct = predicted_answer.upper() == ground_truth_answer
    if is_correct:
        correct += 1
    total += 1
    
    print(f"Frage: {example['question']}")
    print(f"Auswahlmöglichkeiten: A. {example['choices'][0]}, B. {example['choices'][1]}, C. {example['choices'][2]}, D. {example['choices'][3]}")
    print(f"Vorhergesagte Antwort: {predicted_answer}, Richtige Antwort: {ground_truth_answer}, Korrekt: {is_correct}")
    print("-" * 30)

    if (i+1) % 10 == 0:
        accuracy = correct / total
        print(f"Verarbeitet {i+1}/{len(dataset)}. Aktuelle Genauigkeit: {accuracy:.2%} ({correct}/{total})")


# Genauigkeit berechnen
accuracy = correct / total
print(f"Fach: {subject}")
print(f"Genauigkeit: {accuracy:.2%} ({correct}/{total})")
```

Log:

```bash
% python scripts/mmlu.py

Evaluating:   9%| 9/100 [01:31<15:19, 10.10s/it]Processed 10/100. Current Accuracy: 0.00% (0/10)
Evaluating:  19%| 19/100 [03:14<12:47,  9.47s/it]Processed 20/100. Current Accuracy: 0.00% (0/20)
Evaluating:  26%| 26/100 [04:30<13:44, 11.14s/it]

...

Processed 100/100. Current Accuracy: 40.00% (40/100)
Evaluating: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [15:15<00:00,  9.16s/it]
Subject: college_computer_science
Accuracy: 40.00% (40/100)
```

## ollama

```bash
Processed 100/100. Current Accuracy: 40.00% (40/100)
Evaluating: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [02:18<00:00,  1.39s/it]
Subject: college_computer_science
Accuracy: 40.00% (40/100)
```

Das ollama-Programm scheint viel schneller zu sein als llama-server.

Für ollama werden Parameter wie folgt verwendet:

```bash
/Applications/Ollama.app/Contents/Resources/ollama runner --model /Users/lzwjava/.ollama/models/blobs/sha256-ff82381e2bea77d91c1b824c7afb83f6fb73e9f7de9dda631bcdbca564aa5435 --ctx-size 8192 --batch-size 512 --n-gpu-layers 33 --threads 4 --parallel 4 --port 51151
```

## Deepseek

Benchmark für Deepseek.

```python
import torch
from datasets import load_dataset
import requests
import json
from tqdm import tqdm
import os
from openai import OpenAI
from dotenv import load_dotenv

load_dotenv()

# MMLU-Dataset laden
subject = "college_computer_science"  # Wählen Sie Ihr Fach
dataset = load_dataset("cais/mmlu", subject, split="test")

# Prompt ohne Few-Shot-Beispiele formatieren
def format_mmlu_prompt(example):
    prompt = "Die folgenden sind Multiple-Choice-Fragen über {}".format(subject.replace("_", " "))
    prompt += ". Bitte antworten Sie nur mit dem Buchstaben der richtigen Wahl (A, B, C oder D)."
    prompt += " Antworten Sie nur mit dem Buchstaben. Keine Erklärung ist erforderlich."
    
    # Aktuelle Frage hinzufügen
    prompt += f"Frage: {example['question']}\n"
    prompt += "Auswahlmöglichkeiten:\nA. {}\nB. {}\nC. {}\nD. {}\n".format(*example['choices'])
    return prompt

# Evaluierungsschleife
correct = 0
total = 0

# DeepSeek-Client initialisieren
api_key = os.environ.get("DEEPSEEK_API_KEY")
if not api_key:
    print("Fehler: DEEPSEEK_API_KEY Umgebungsvariable nicht gesetzt.")
    exit()
client = OpenAI(api_key=api_key, base_url="https://api.deepseek.com")


for i, example in tqdm(enumerate(dataset), total=len(dataset), desc="Evaluating"):
    prompt = format_mmlu_prompt(example)
    
    # Anfrage an DeepSeek API senden
    try:
        response = client.chat.completions.create(
            model="deepseek-chat",
            messages=[
                {"role": "user", "content": prompt}
            ],
            max_tokens=100
        )
        if response and response.choices:
            output_text = response.choices[0].message.content.strip()
            predicted_answer = output_text.strip()[0] if len(output_text.strip()) > 0 else ""
            print(f"Output from API: {output_text}")
        else:
            predicted_answer = ""
            print("Fehler: Keine Antwort von der API.")
    except Exception as e:
        predicted_answer = ""
        print(f"Fehler während des API-Aufrufs: {e}")
    
    # Mit der richtigen Antwort vergleichen
    
    answer_map = {0: "A", 1: "B", 2: "C", 3: "D"}
    ground_truth_answer = answer_map.get(example["answer"], "")
    is_correct = predicted_answer.upper() == ground_truth_answer
    if is_correct:
        correct += 1
    total += 1
    
    print(f"Frage: {example['question']}")
    print(f"Auswahlmöglichkeiten: A. {example['choices'][0]}, B. {example['choices'][1]}, C. {example['choices'][2]}, D. {example['choices'][3]}")
    print(f"Vorhergesagte Antwort: {predicted_answer}, Richtige Antwort: {ground_truth_answer}, Korrekt: {is_correct}")
    print("-" * 30)

    if (i+1) % 10 == 0:
        accuracy = correct / total
        print(f"Verarbeitet {i+1}/{len(dataset)}. Aktuelle Genauigkeit: {accuracy:.2%} ({correct}/{total})")


# Genauigkeit berechnen
accuracy = correct / total
print(f"Fach: {subject}")
print(f"Genauigkeit: {accuracy:.2%} ({correct}/{total})")
```

Ergebnis:

```bash
Processed 100/100. Current Accuracy: 78.00% (78/100)
Evaluating: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [03:44<00:00,  2.24s/it]
Subject: college_computer_science
Accuracy: 78.00% (78/100)
```