---
audio: false
generated: true
image: false
lang: de
layout: post
model: x-ai/grok-4-fast
title: Ilya Sutskever KI-Lektüreliste
translated: true
type: note
---

Ilya Sutskever, Mitbegründer von OpenAI, teilte John Carmack um das Jahr 2020 herum eine empfohlene Leseliste mit etwa 30 wichtigen Artikeln, Blogbeiträgen, Kursen und Ressourcen zu Deep Learning und KI. Er sagte angeblich, dass das Beherrschen dieser Inhalte "90 % dessen, was heute wichtig ist" in diesem Bereich abdecken würde. Während die vollständige ursprüngliche Liste von 30 Einträgen nicht öffentlich vollständig ist (einige Einträge gingen möglicherweise aufgrund von E-Mail-Löschungen verloren), enthält eine weit verbreitete Version 27 grundlegende Einträge, die Faltungsnetzwerke, rekurrente Netzwerke, Transformer, Informationstheorie und mehr abdecken. Nachfolgend finden Sie diese zusammengestellte Liste, zur besseren Übersicht nach Kategorien gruppiert. Jeder Eintrag enthält den Titel, die Autoren, das Jahr und den Typ.

### Faltende Neuronale Netze (Convolutional Neural Networks)
1. **CS231n: Convolutional Neural Networks for Visual Recognition** - Fei-Fei Li, Andrej Karpathy, Justin Johnson - 2017 - Stanford Kurs
2. **ImageNet Classification with Deep Convolutional Neural Networks (AlexNet)** - Alex Krizhevsky, Ilya Sutskever, Geoffrey E. Hinton - 2012 - Paper
3. **Deep Residual Learning for Image Recognition (ResNet)** - Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun - 2015 - Paper
4. **Identity Mappings in Deep Residual Networks** - Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun - 2016 - Paper
5. **Multi-Scale Context Aggregation by Dilated Convolutions** - Fisher Yu, Vladlen Koltun - 2015 - Paper

### Rekurrente Neuronale Netze (Recurrent Neural Networks)
6. **Understanding LSTM Networks** - Christopher Olah - 2015 - Blog Post
7. **The Unreasonable Effectiveness of Recurrent Neural Networks** - Andrej Karpathy - 2015 - Blog Post
8. **Recurrent Neural Network Regularization** - Wojciech Zaremba, Ilya Sutskever, Oriol Vinyals - 2014 - Paper
9. **Neural Turing Machines** - Alex Graves, Greg Wayne, Ivo Danihelka - 2014 - Paper
10. **Deep Speech 2: End-to-End Speech Recognition in English and Mandarin** - Dario Amodei et al. - 2016 - Paper
11. **Neural Machine Translation by Jointly Learning to Align and Translate (RNNsearch)** - Dzmitry Bahdanau, Kyunghyun Cho, Yoshua Bengio - 2015 - Paper
12. **Pointer Networks** - Oriol Vinyals, Meire Fortunato, Navdeep Jaitly - 2015 - Paper
13. **Order Matters: Sequence to Sequence for Sets (Set2Set)** - Oriol Vinyals, Samy Bengio, Manjunath Kudlur - 2016 - Paper
14. **A Simple Neural Network Module for Relational Reasoning (Relation Networks)** - Adam Santoro, David Raposo, David G. Barrett, Mateusz Malinowski, Razvan Pascanu, Peter Battaglia, Timothy Lillicrap - 2017 - Paper
15. **Relational Recurrent Neural Networks** - Adam Santoro, Ryan Faulkner, David Raposo, Jack Rae, Mike Chrzanowski, Theophane Weber, Daan Wierstra, Oriol Vinyals, Razvan Pascanu, Timothy Lillicrap - 2018 - Paper

### Transformer
16. **Attention Is All You Need** - Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin - 2017 - Paper
17. **The Annotated Transformer** - Sasha Rush et al. - 2017 (annotated 2020) - Blog Post
18. **Scaling Laws for Neural Language Models** - Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B. Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, Dario Amodei - 2020 - Paper

### Informationstheorie
19. **A Tutorial Introduction to the Minimum Description Length Principle** - Peter Grünwald - 2004 - Book Chapter
20. **Kolmogorov Complexity and Algorithmic Randomness (Chapter 14)** - Alexander Shen, Vladimir A. Uspensky, Nikolay Vereshchagin - 2017 - Book Chapter
21. **The First Law of Complexodynamics** - Scott Aaronson - 2011 - Blog Post
22. **Quantifying the Rise and Fall of Complexity in Closed Systems: The Coffee Automaton** - Scott Aaronson, Sean M. Carroll, Lauren Ouellette - 2014 - Paper
23. **Machine Super Intelligence** - Shane Legg - 2008 - Dissertation

### Verschiedenes
24. **Keeping Neural Networks Simple by Minimizing the Description Length of the Weights** - Geoffrey E. Hinton, Drew van Camp - 1993 - Paper
25. **Variational Lossy Autoencoder** - Xi Chen, Diederik P. Kingma, Tim Salimans, Yan Duan, Prafulla Dhariwal, John Schulman, Ilya Sutskever, Pieter Abbeel - 2017 - Paper
26. **GPipe: Easy Scaling with Micro-Batch Pipeline Parallelism** - Yanping Huang, Youlong Cheng, Ankur Bapna, Orhan Firat, Mia Xu Chen, Dehao Chen, HyoukJoong Lee, Jiquan Ngiam, Quoc V. Le, Yonghui Wu, Zhifeng Chen - 2018 - Paper
27. **Neural Message Passing for Quantum Chemistry** - Justin Gilmer, Samuel S. Schoenholz, Patrick F. Riley, Oriol Vinyals, George E. Dahl - 2017 - Paper

Spekulationen über die "fehlenden" 3 Einträge verweisen oft auf Meta-Learning-Paper (z.B. Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks von Chelsea Finn et al., 2017) oder Reinforcement-Learning-Arbeiten zu Self-Play, basierend auf Sutskever's damaligen Vorträgen, aber dies ist nicht bestätigt.

[Deep learning reading list from Ilya Sutskever](https://github.com/dzyim/ilya-sutskever-recommended-reading)
[Summary of Ilya Sutskever's AI Reading List](https://tensorlabbet.com/2024/09/24/ai-reading-list/)
[The Lost Reading Items of Ilya Sutskever's AI Reading List](https://tensorlabbet.com/2024/11/11/lost-reading-items/)