---
audio: false
generated: true
image: false
lang: de
layout: post
title: Laden von GloVe-Vektoren auf zwei Arten
translated: true
type: note
---

Sie haben zwei einfache Möglichkeiten, um GloVe-Vektoren für Wörter wie `the`, `to`, `U.S.` und für Ihren eigenen Text zu erhalten:

---

## Option A — Stanford GloVe herunterladen und mit `gensim` nachschlagen

Dies ist der gebräuchlichste Weg und funktioniert nach dem Download vollständig offline.

```python
# 1) Herunterladen & entpacken (nur beim ersten Ausführen)
import os, zipfile, urllib.request
url = "https://nlp.stanford.edu/data/glove.6B.zip"
zip_path = "glove.6B.zip"
if not os.path.exists(zip_path):
    urllib.request.urlretrieve(url, zip_path)
with zipfile.ZipFile(zip_path) as zf:
    zf.extract("glove.6B.300d.txt")  # 300-dim Set

# 2) GloVe-Format -> word2vec-Textformat konvertieren und laden
from gensim.scripts.glove2word2vec import glove2word2vec
from gensim.models import KeyedVectors

glove_path = "glove.6B.300d.txt"
w2v_path   = "glove.6B.300d.w2v.txt"
if not os.path.exists(w2v_path):
    glove2word2vec(glove_path, w2v_path)

vectors = KeyedVectors.load_word2vec_format(w2v_path, binary=False)
print(vectors.vector_size)  # 300

# 3) Einzelwort-Abfragen
print(vectors['the'][:5])   # erste 5 Dimensionen
print(vectors['to'][:5])

# HINWEIS: Der glove.6B-Wortschatz ist in Kleinbuchstaben.
# 'U.S.' -> 'u.s.' in diesem Vokabular (falls vorhanden). Sicherer: Wandeln Sie Ihre Tokens in Kleinbuchstaben um.
print(vectors['u.s.'][:5])
```

Jetzt können Sie Ihren eigenen Text einbetten:

```python
import re, numpy as np

# Einfacher Tokenizer, der Punkte in Tokens wie u.s. beibehält
TOKEN_RE = re.compile(r"[A-Za-z0-9\.\-']+")

def tokenize(text: str):
    return TOKEN_RE.findall(text.lower())

def embed_tokens(tokens, kv: KeyedVectors):
    vecs = [kv[w] for w in tokens if w in kv.key_to_index]
    return np.stack(vecs) if vecs else np.zeros((0, kv.vector_size))

def embed_sentence_avg(text: str, kv: KeyedVectors):
    V = embed_tokens(tokenize(text), kv)
    return V.mean(axis=0) if V.size else np.zeros(kv.vector_size)

# Beispiele
print(embed_sentence_avg("The quick brown fox jumps over the lazy dog.", vectors)[:10])
print(embed_tokens(tokenize("U.S. interest rates rose today."), vectors).shape)  # (n_tokens, 300)
```

**Tipps**

* `glove.6B.300d.txt` ≈ 1,1 GB im Arbeitsspeicher. Falls das zu viel ist, versuchen Sie es stattdessen mit `100d`.
* Das 6B-Set ist **in Kleinbuchstaben**; wandeln Sie Ihren Text immer in Kleinbuchstaben um, oder verwenden Sie das größere **840B**-Set, wenn Sie mehr Abdeckung benötigen (in der Praxis ebenfalls größtenteils in Kleinbuchstaben).
* OOV-Wörter (Out-of-Vocabulary) liefern nichts zurück; Sie können sie überspringen oder auf Subword-Heuristiken zurückgreifen.

---

## Option B — Lassen Sie `torchtext` GloVe für Sie herunterladen und bereitstellen

Dies vermeidet manuelles Herunterladen und Konvertieren.

```python
import torch
from torchtext.vocab import GloVe
import re

glove = GloVe(name="6B", dim=300)  # Lädt automatisch nach ~/.vector_cache herunter
stoi = glove.stoi    # Wort -> Index
vecs = glove.vectors # Tensor [vocab, 300]

def get_word_vec(word: str):
    idx = stoi.get(word.lower())
    return vecs[idx] if idx is not None else None

print(get_word_vec("the")[:5])
print(get_word_vec("to")[:5])
print(get_word_vec("U.S.")[:5])     # wird in vielen Builds zu None; versuchen Sie "u.s.":
print(get_word_vec("u.s.")[:5])

# Satzeinbettung (Durchschnitt)
TOKEN_RE = re.compile(r"[A-Za-z0-9\.\-']+")
def embed_sentence_avg(text: str):
    toks = TOKEN_RE.findall(text.lower())
    xs = [get_word_vec(t) for t in toks]
    xs = [x for x in xs if x is not None]
    return torch.stack(xs, dim=0).mean(0) if xs else torch.zeros(glove.dim)

print(embed_sentence_avg("The quick brown fox...")[:10])
```

---

## Was Sie zu den spezifischen Zeilen gefragt haben

Diese Zeilen wie:

```
the  0.418 0.24968 -0.41242 0.1217 ... 0.34527  # 300 Floats insgesamt
to   0.68047 -0.039263 -0.67412 0.056051 ... -0.067549
U.S. 0.3459  0.21513  -0.09982 0.25344 ... 0.024118
```

sind genau das, was Sie in den Klartext-`.txt`-Dateien sehen werden: Jede Zeile besteht aus `Token` gefolgt von `300` Floats. Mit `gensim` oder `torchtext` müssen Sie die Zeilen nicht selbst parsen – diese Bibliotheken übernehmen das und bieten Ihnen direkte Vektor-Abfragen.

---

## Häufige Fallstricke (und Lösungen)

* **“KeyError: ‘U.S.’”** → Verwenden Sie `u.s.` mit dem 6B-Set, oder wählen Sie ein größeres Set mit besserer Abdeckung, und wandeln Sie vor der Abfrage immer in Kleinbuchstaben um.
* **Hoher Speicher-/Zeitbedarf** → Verwenden Sie `100d` oder `200d` anstelle von `300d`, oder laden Sie nur die benötigten Wörter, indem Sie die Datei durchsuchen und einen kleineren Wortschatz aufbauen.
* **Satz-/Dokumentenvektoren** → Bilden Sie den Durchschnitt oder die Summe der Wortvektoren; für gewichtete Durchschnitte versuchen Sie es mit TF-IDF-Gewichten.

Wenn Sie möchten, teilen Sie mir Ihr genaues Textverarbeitungsziel mit (Klassifikation, Ähnlichkeit, Clustering, etc.) und ich skizziere eine minimale Pipeline unter Verwendung von GloVe von Anfang bis Ende.