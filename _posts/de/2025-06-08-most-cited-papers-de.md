---
audio: false
generated: true
lang: de
layout: post
title: 25 meistzitierte Artikel
translated: true
type: note
---

Hier finden Sie eine kurze Einführung zu jedem der genannten Artikel, die deren wichtigste Beiträge und Bedeutung in ihren jeweiligen Fachgebieten zusammenfasst. Die Einführungen sollen einen kurzen Überblick bieten und gleichzeitig das Wesentliche jeder Arbeit erfassen.

1.  **Deep Residual Learning for Image Recognition (2015)**
    Autoren: Kaiming He et al.
    Führt ResNet ein, eine tiefe Convolutional Neural Network-Architektur, die Residualverbindungen nutzt, um das Problem des verschwindenden Gradienten zu adressieren. Sie ermöglicht das Training sehr tiefer Netzwerke (Hunderte von Schichten) und erzielt dabei state-of-the-art Leistung in Bildklassifizierungsaufgaben wie ImageNet, was die Computer Vision revolutionierte.

2.  **Analysis of Relative Gene Expression Data Using Real-Time Quantitative PCR and the 2−ΔΔCT Method (2001)**
    Autoren: Kenneth J. Livak, Thomas D. Schmittgen
    Beschreibt die 2−ΔΔCT-Methode zur Analyse relativer Genexpression aus Real-Time Quantitative PCR-Daten. Dieser weit verbreitete Ansatz normalisiert Genexpressionslevel auf Referenzgene und eine Kalibratorprobe und bietet so einen robusten, zugänglichen Rahmen für die molekularbiologische Forschung.

3.  **Using Thematic Analysis in Psychology (2006)**
    Autoren: Virginia Braun, Victoria Clarke
    Stellt die thematische Analyse als flexible qualitative Forschungsmethode zur Identifizierung, Analyse und Berichterstattung von Mustern (Themen) in Daten vor. Sie bietet einen strukturierten, dennoch anpassbaren Ansatz, der in der Psychologie und den Sozialwissenschaften weit verbreitet ist für die Interpretation qualitativer Daten.

4.  **Diagnostic and Statistical Manual of Mental Disorders, DSM-5 (2013)**
    Herausgegeben von: American Psychiatric Association
    Das DSM-5 ist ein umfassendes Klassifikationssystem für psychische Störungen, das standardisierte diagnostische Kriterien für Kliniker und Forscher bereitstellt. Es aktualisiert frühere Ausgaben mit überarbeiteten Kategorien und Kriterien und dient als Eckpfeiler für die psychiatrische Diagnose und Forschung.

5.  **A Short History of SHELX (2008)**
    Autor: George M. Sheldrick
    Beschreibt die Entwicklung von SHELX, einer Programmsammlung zur Kristallstrukturbestimmung in der Röntgenkristallographie. Sie hebt die Bedeutung von SHELX für die Strukturchemie hervor und betont seine Rolle bei der Automatisierung und Verfeinerung kristallographischer Analysen.

6.  **Random Forests (2001)**
    Autor: Leo Breiman
    Führt Random Forests ein, eine Ensemble-Machine-Learning-Methode, die mehrere Entscheidungsbäume kombiniert, um die Genauigkeit von Klassifikation und Regression zu verbessern. Seine Robustheit und Vielseitigkeit haben ihn zu einem Grundpfeiler in Data Science und Predictive Modeling gemacht.

7.  **Attention Is All You Need (2017)**
    Autoren: Vaswani et al.
    Schlägt den Transformer vor, eine neuronale Netzwerkarchitektur, die vollständig auf Aufmerksamkeitsmechanismen (Attention) basiert und rekurrente sowie convolutionale Schichten eliminiert. Sie revolutionierte die natürliche Sprachverarbeitung und ermöglichte Modelle wie BERT und GPT mit überlegener Leistung in Aufgaben wie Übersetzung.

8.  **ImageNet Classification with Deep Convolutional Neural Networks (2012)**
    Autoren: Alex Krizhevsky et al.
    Stellt AlexNet vor, ein bahnbrechendes tiefes Convolutional Neural Network, das bahnbrechende Ergebnisse im ImageNet-Wettbewerb erzielte. Es popularisierte Deep Learning in der Computer Vision und demonstrierte die Leistungsfähigkeit von GPUs und großen Datensätzen.

9.  **Global Cancer Statistics 2020: GLOBOCAN Estimates of Incidence and Mortality Worldwide for 36 Cancers in 185 Countries (2021)**
    Autoren: Hyuna Sung et al.
    Bietet aktualisierte Schätzungen zur globalen Krebslast für 2020, die Inzidenz und Mortalität für 36 Krebsarten in 185 Ländern abdecken. Als Teil des GLOBOCAN-Projekts informiert es über Krebskontrollpolitiken und Forschungsprioritäten.

10. **Global Cancer Statistics 2018: GLOBOCAN Estimates of Incidence and Mortality Worldwide for 36 Cancers in 185 Countries (2018)**
    Autoren: Freddie Bray et al.
    Bietet globale Krebsstatistiken für 2018, die Inzidenz- und Mortalitätsraten für 36 Krebsarten detailliert darlegen. Dieser GLOBOCAN-Bericht dient als kritische Ressource für das Verständnis von Krebstrends und die Steuerung von Public-Health-Strategien.

11. **Preferred Reporting Items for Systematic Reviews and Meta-Analyses: The PRISMA Statement (2009)**
    Autoren: David Moher et al.
    Stellt die PRISMA-Richtlinie vor, eine 27-Punkte-Checkliste und ein Flussdiagramm für die transparente Berichterstattung von systematischen Übersichtsarbeiten und Meta-Analysen. Sie verbessert die Qualität und Reproduzierbarkeit von Evidenzsynthesen in der Gesundheitsforschung.

12. **U-Net: Convolutional Networks for Biomedical Image Segmentation (2015)**
    Autoren: Olaf Ronneberger et al.
    Beschreibt U-Net, ein Convolutional Neural Network, das für die biomedizinische Bildsegmentierung entwickelt wurde. Seine U-förmige Architektur mit Skip Connections überzeugt bei präzisen Segmentierungsaufgaben und wird häufig in der medizinischen Bildgebung eingesetzt.

13. **Electric Field Effect in Atomically Thin Carbon Films (2004)**
    Autoren: Konstantin S. Novoselov et al.
    Berichtet über die Entdeckung des elektrischen Feldeffekts in Graphen und demonstriert sein Potenzial als zweidimensionales Material. Diese grundlegende Arbeit, die mit dem Nobelpreis für Physik 2010 verbunden ist, löste umfangreiche Forschung zu den Eigenschaften und Anwendungen von Graphen aus.

14. **Fitting Linear Mixed-Effects Models Using lme4 (2015)**
    Autoren: Douglas Bates et al.
    Beschreibt detailliert das lme4-Paket in R zur Anpassung linearer gemischter Modelle (Linear Mixed-Effects Models), die sowohl feste als auch zufällige Effekte berücksichtigen. Es wird häufig in der Statistik für die Analyse hierarchischer und longitudinaler Daten verwendet.

15. **Scikit-learn: Machine Learning in Python (2011)**
    Autoren: Fabian Pedregosa et al.
    Stellt scikit-learn vor, eine Open-Source-Python-Bibliothek für maschinelles Lernen. Sie bietet zugängliche Werkzeuge für Klassifikation, Regression, Clustering und mehr und ist zu einem Standard in Data-Science-Workflows geworden.

16. **Deep Learning (2015)**
    Autoren: Yann LeCun, Yoshua Bengio, Geoffrey Hinton
    Gibt einen Überblick über Deep Learning, betont seine Grundlagen, Architekturen und Anwendungen in Bereichen wie Computer Vision und Spracherkennung. Verfasst von Pionieren, festigte es die transformative Wirkung von Deep Learning auf die KI.

17. **Common Method Biases in Behavioral Research: A Critical Review of the Literature and Recommended Remedies (2003)**
    Autoren: Philip M. Podsakoff et al.
    Untersucht Common-Method-Bias in der verhaltenswissenschaftlichen Forschung, wie z.B. Antwortverzerrungen in Umfragen, und schlägt statistische und verfahrenstechnische Abhilfemaßnahmen zu deren Minderung vor, um die Validität der Forschung zu verbessern.

18. **Moderated Estimation of Fold Change and Dispersion for RNA-seq Data with DESeq2 (2014)**
    Autoren: Michael I. Love et al.
    Stellt DESeq2 vor, ein Bioconductor-Paket für die differentielle Genexpressionsanalyse von RNA-seq-Daten. Es verbessert die Genauigkeit bei der Schätzung von Fold Changes und Dispersion und wird häufig in der Genomforschung verwendet.

19. **Hallmarks of Cancer: The Next Generation (2011)**
    Autoren: Douglas Hanahan, Robert A. Weinberg
    Aktualisiert die ursprünglichen "Hallmarks of Cancer" und skizziert zehn Schlüsselmerkmale von Krebs, wie anhaltende Proliferation und Immunevasion. Es bietet einen Rahmen zum Verständnis der Krebsbiologie und zur Steuerung der Therapieentwicklung.

20. **Measuring Inconsistency in Meta-Analyses (2003)**
    Autoren: Julian P. T. Higgins et al.
    Führt Methoden zur Quantifizierung der Heterogenität in Meta-Analysen ein, einschließlich der I²-Statistik. Es hilft Forschern, die Inkonsistenz zwischen Studien zu bewerten und verbessert die Zuverlässigkeit meta-analytischer Schlussfolgerungen.

21. **NIH Image to ImageJ: 25 Years of Image Analysis (2012)**
    Autoren: Caroline A. Schneider et al.
    Beschreibt die Entwicklung von ImageJ, einer Open-Source-Software zur Bildanalyse, ausgehend von NIH Image. Sie hebt ihre weit verbreitete Verwendung in der wissenschaftlichen Forschung zur Verarbeitung und Analyse von Bildern across disciplines hervor.

22. **Fiji: An Open-Source Platform for Biological-Image Analysis (2012)**
    Autoren: Johannes Schindelin et al.
    Stellt Fiji vor, eine auf ImageJ basierende Plattform für die biologische Bildanalyse. Sie integriert Plugins und Werkzeuge für erweiterte Bildverarbeitung und wird häufig in den Biowissenschaften eingesetzt.

23. **The PRISMA 2020 Statement: An Updated Guideline for Reporting Systematic Reviews (2021)**
    Autoren: Matthew J. Page et al.
    Aktualisiert die PRISMA-Richtlinien mit einer 27-Punkte-Checkliste und einem Flussdiagramm, um die Transparenz in systematischen Übersichtsarbeiten und Meta-Analysen zu verbessern, und spiegelt Fortschritte in der Methodik der Forschungssynthese wider.

24. **ImageNet: A Large-Scale Hierarchical Image Database (2009)**
    Autoren: Jia Deng et al.
    Beschreibt ImageNet, einen riesigen Datensatz mit Millionen von beschrifteten Bildern across thousands of categories. Er befeuerte Fortschritte in der Computer Vision, insbesondere im Deep Learning, indem er einen Benchmark für die Bildklassifizierung bereitstellte.

25. **G*Power 3: A Flexible Statistical Power Analysis Program for the Social, Behavioral, and Biomedical Sciences (2007)**
    Autoren: Franz Faul et al.
    Stellt G*Power 3 vor, ein kostenloses Software-Tool für die statistische Power-Analyse. Es unterstützt Forscher bei der Bestimmung von Stichprobengrößen und der Power für verschiedene statistische Tests und wird häufig im experimentellen Design verwendet.

Diese Artikel umfassen diverse Bereiche – maschinelles Lernen, Psychologie, Krebsforschung, Kristallographie und mehr – und leisten jeweils grundlegende Beiträge, die Forschung und Praxis weiterhin prägen. Wenn Sie eine vertiefte Betrachtung eines bestimmten Artikels wünschen, lassen Sie es mich wissen!

---

Die Nature-Analyse vom 15. April 2025 identifiziert die 25 am häufigsten zitierten Artikel des 21. Jahrhunderts, wobei drei von Informatikern der University of Toronto verfasst wurden. Nachstehend finden Sie Details zu diesen drei Artikeln sowie einen allgemeinen Überblick über die Liste, da die vollständige Liste der 25 Artikel in den verfügbaren Quellen nicht explizit detailliert ist, aber basierend auf den bereitgestellten Informationen teilweise rekonstruiert werden kann.

### Artikel von Informatikern der University of Toronto
Die drei Artikel der Informatikfakultät der University of Toronto, die in der Nature-Analyse hervorgehoben werden, sind mit Fortschritten in der künstlichen Intelligenz (KI), insbesondere Deep Learning, verbunden. Der prominenteste unter ihnen ist:

1.  **"ImageNet Classification with Deep Convolutional Neural Networks" (2012)**
    -   **Autoren**: Alex Krizhevsky, Ilya Sutskever und Geoffrey Hinton (University of Toronto)
    -   **Veröffentlicht in**: Advances in Neural Information Processing Systems (NeurIPS)
    -   **Zitationsrang**: 8. Platz auf Nature's Liste der 25 am häufigsten zitierten Artikel des 21. Jahrhunderts
    -   **Zusammenfassung**: Bekannt als das "AlexNet"-Paper, führte diese Arbeit ein tiefes Convolutional Neural Network ein, das die Bildklassifizierungsleistung auf dem ImageNet-Datensatz erheblich verbesserte. Es demonstrierte die Leistungsfähigkeit mehrschichtiger künstlicher neuronaler Netze und entfachte die Deep-Learning-Revolution. Die Wirkung des Artikels zeigt sich in seiner Rolle als Grundlage für moderne KI-Anwendungen in der Computer Vision.
    -   **Zitationen**: Während die genauen Zitationszahlen je nach Datenbank variieren, hat er Zehntausende von Zitationen angesammelt, was seinen transformativen Einfluss widerspiegelt.

Die anderen beiden von Forschern der University of Toronto verfassten Artikel werden in den bereitgestellten Quellen nicht explizit genannt, aber sie werden als eng mit dem KI-Forschungserbe der Fakultät verbunden beschrieben. Vor dem Hintergrund handelt es sich likely um Arbeiten von Geoffrey Hinton oder seinen Mitarbeitern, die sich auf Deep Learning oder verwandte KI-Methodiken konzentrieren. Dabei könnte es sich um Artikel zu neuronalen Netzwerkarchitekturen, Optimierungstechniken oder Anwendungen von Deep Learning handeln, die in hochrangigen Publikationen wie Nature, NeurIPS oder IEEE-Konferenzen veröffentlicht wurden.

### Überblick über die 25 am häufigsten zitierten Artikel
Die Nature-Analyse, basierend auf Zitationsdaten aus fünf Datenbanken, betont, dass die am häufigsten zitierten Artikel des 21. Jahrhunderts oft Methoden und Werkzeuge beschreiben rather than bahnbrechende Entdeckungen. Die Top-25-Artikel umfassen mehrere Fachgebiete mit einer starken Präsenz von KI, Forschungssoftware, statistischen Methoden und Psychologie. Wichtige Punkte zur Liste sind:

-   **Top-Artikel**: Der am häufigsten zitierte Artikel ist "Deep Residual Learning for Image Recognition" (2016) von Kaiming He, Xiangyu Zhang, Shaoqing Ren und Jian Sun, veröffentlicht in den Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Er führte ResNets ein, die entscheidend für KI-Werkzeuge wie ChatGPT und AlphaGo waren. Die Zitationszahlen reichen von 103.756 bis 254.074 across databases.
-   **Weitere bemerkenswerte Artikel**:
    -   Ein Artikel von 2006 zur thematischen Analyse in der Psychologie von Victoria Clarke belegt den dritten Platz mit Zitationen zwischen 100.000 und 230.000.
    -   Ein Artikel von 2004 zu Graphen-Experimenten, der 2010 zu einem Nobelpreis für Physik führte, gehört ebenfalls zu den Top 25 und unterstreicht die Vielfalt einflussreicher Themen.
    -   Artikel zu Forschungssoftware und statistischen Methoden sind weit verbreitet, was ihre breite Anwendung across disciplines widerspiegelt.
-   **KI-Dominanz**: KI-bezogene Artikel, insbesondere zu Deep Learning und Computer Vision, sind stark vertreten, wobei hohe Zitationszahlen durch ihre Anwendbarkeit in Bereichen wie selbstfahrenden Autos und natürlicher Sprachverarbeitung getrieben werden.

### Einschränkungen und Anmerkungen
-   Die vollständige Liste der 25 Artikel ist in den Quellen nicht provided, da Nature eine vollständige Liste in seinen Supplementary Information führt, auf die in den gegebenen Daten kein öffentlicher Zugriff besteht.
-   Die Zitationszahlen variieren erheblich between databases aufgrund von Unterschieden in der Indizierung. Die Nature-Analyse verwendet den Medianrang across databases, um die Top 25 zu bestimmen.
-   Die Beiträge der University of Toronto sind Teil eines breiteren Trends, bei dem KI-Artikel, insbesondere zu Deep Learning, aufgrund ihrer praktischen Auswirkungen und Online-Präsenz ein rasantes Zitationswachstum verzeichnen.

### So greifen Sie auf die vollständige Liste zu
Um die vollständige Liste der 25 am häufigsten zitierten Artikel zu erhalten, können Sie auf den originalen Nature-Artikel vom 15. April 2025 mit dem Titel "Exclusive: the most-cited papers of the twenty-first century" verweisen. Die Supplementary Information zum Artikel enthält die detaillierte Liste. Alternativ können Sie Einblicke gewinnen, indem Sie in Datenbanken wie Google Scholar oder Web of Science nach hochzitierten KI-Artikeln ab dem Jahr 2000 suchen.

Wenn Sie möchten, dass ich nach zusätzlichen Details zu den beiden ungenannten Artikeln der University of Toronto suche oder ein Diagramm zur Visualisierung von Zitationstrends für die bekannten Artikel erstelle, lassen Sie es mich bitte wissen!