---
audio: false
generated: true
lang: de
layout: post
title: Verwendung von Apache Kafka in Java
translated: true
type: note
---

Die Verwendung von Apache Kafka in Java erfordert die Einrichtung eines Kafka-Clients in Ihrer Java-Anwendung, um Nachrichten zu produzieren und zu konsumieren. Kafka ist eine verteilte Streaming-Plattform, und die Java-Client-Bibliotheken machen die Integration unkompliziert. Im Folgenden zeige ich Ihnen die grundlegenden Schritte für den Einstieg.

Zuerst müssen Sie Ihre Umgebung einrichten. Stellen Sie sicher, dass Kafka auf Ihrem System oder einem Server installiert und gestartet ist. Sie können es von der offiziellen Apache Kafka-Website herunterladen und den ZooKeeper- und Kafka-Server mit den bereitgestellten Skripten starten. Der Einfachheit halber gehe ich davon aus, dass Sie Kafka lokal mit Standardeinstellungen ausführen (z. B. `localhost:9092` als Bootstrap-Server).

Fügen Sie als Nächstes die Kafka-Client-Abhängigkeit zu Ihrem Java-Projekt hinzu. Wenn Sie Maven verwenden, fügen Sie dies in Ihre `pom.xml` ein:

```xml
<dependency>
    <groupId>org.apache.kafka</groupId>
    <artifactId>kafka-clients</artifactId>
    <version>3.6.0</version> <!-- Verwenden Sie die neueste Version -->
</dependency>
```

Nun schreiben wir etwas Code. Ich zeige Ihnen, wie Sie einen einfachen Producer und Consumer erstellen.

### Kafka Producer Beispiel
Der Producer sendet Nachrichten an ein Kafka-Topic. Hier ist ein grundlegendes Beispiel:

```java
import org.apache.kafka.clients.producer.*;
import java.util.Properties;

public class SimpleProducer {
    public static void main(String[] args) {
        // Producer-Eigenschaften konfigurieren
        Properties props = new Properties();
        props.put("bootstrap.servers", "localhost:9092"); // Kafka-Server-Adresse
        props.put("key.serializer", "org.apache.kafka.common.serialization.StringSerializer");
        props.put("value.serializer", "org.apache.kafka.common.serialization.StringSerializer");

        // Eine Producer-Instanz erstellen
        try (Producer<String, String> producer = new KafkaProducer<>(props)) {
            // Sende eine Nachricht an ein Topic namens "test-topic"
            String topic = "test-topic";
            for (int i = 0; i < 10; i++) {
                String key = "key" + i;
                String value = "Hello, Kafka " + i;
                ProducerRecord<String, String> record = new ProducerRecord<>(topic, key, value);

                producer.send(record, (metadata, exception) -> {
                    if (exception == null) {
                        System.out.println("Sent message: " + value + " to partition " + metadata.partition());
                    } else {
                        exception.printStackTrace();
                    }
                });
            }
        }
    }
}
```

In diesem Code:
- `bootstrap.servers` gibt an, wo Kafka läuft.
- Die Serializer definieren, wie Schlüssel und Werte (hier beide Strings) in Bytes konvertiert werden.
- `ProducerRecord` repräsentiert die Nachricht, und `send()` sendet sie asynchron mit einem Callback zur Behandlung von Erfolg oder Fehler.

### Kafka Consumer Beispiel
Der Consumer abonniert ein Topic und liest Nachrichten. Hier ist ein Beispiel:

```java
import org.apache.kafka.clients.consumer.*;
import java.util.Collections;
import java.util.Properties;

public class SimpleConsumer {
    public static void main(String[] args) {
        // Consumer-Eigenschaften konfigurieren
        Properties props = new Properties();
        props.put("bootstrap.servers", "localhost:9092");
        props.put("group.id", "test-group"); // Consumer-Gruppen-ID
        props.put("key.deserializer", "org.apache.kafka.common.serialization.StringDeserializer");
        props.put("value.deserializer", "org.apache.kafka.common.serialization.StringDeserializer");
        props.put("auto.offset.reset", "earliest"); // Vom Anfang des Topics beginnen

        // Eine Consumer-Instanz erstellen
        try (KafkaConsumer<String, String> consumer = new KafkaConsumer<>(props)) {
            // Das Topic abonnieren
            consumer.subscribe(Collections.singletonList("test-topic"));

            // Nach Nachrichten abfragen
            while (true) {
                ConsumerRecords<String, String> records = consumer.poll(1000); // Timeout in ms
                for (ConsumerRecord<String, String> record : records) {
                    System.out.printf("Received: key=%s, value=%s, partition=%d, offset=%d%n",
                            record.key(), record.value(), record.partition(), record.offset());
                }
            }
        }
    }
}
```

Hier:
- `group.id` identifiziert die Consumer-Gruppe für die Koordination.
- Deserializer konvertieren die Byte-Daten zurück in Strings.
- `poll()` ruft Nachrichten in einer Schleife ab, und `auto.offset.reset` stellt sicher, dass Sie von der frühesten Nachricht beginnen, wenn kein Offset committet wurde.

### Ausführen des Codes
1.  **Kafka starten**: Stellen Sie sicher, dass ZooKeeper und Kafka laufen (z. B. `bin/zookeeper-server-start.sh config/zookeeper.properties` und `bin/kafka-server-start.sh config/server.properties` auf Unix-ähnlichen Systemen).
2.  **Ein Topic erstellen**: Verwenden Sie das Kafka-Kommandozeilen-Tool, um `test-topic` zu erstellen, falls es nicht existiert: `bin/kafka-topics.sh --create --topic test-topic --bootstrap-server localhost:9092 --partitions 1 --replication-factor 1`.
3.  **Den Producer ausführen**: Führen Sie die `SimpleProducer`-Klasse aus, um Nachrichten zu senden.
4.  **Den Consumer ausführen**: Führen Sie die `SimpleConsumer`-Klasse aus, um zu sehen, wie die Nachrichten konsumiert werden.

### Tipps
-   **Fehlerbehandlung**: Fügen Sie für Produktionscode Try-Catch-Blöcke oder Logging hinzu.
-   **Konfiguration**: Passen Sie Eigenschaften wie `acks` (für die Producer-Zuverlässigkeit) oder `max.poll.records` (für die Consumer-Batch-Größe) basierend auf Ihren Anforderungen an.
-   **Bibliotheken**: Für komplexere Anwendungen sollten Sie Spring Kafka in Betracht ziehen, was die Konfiguration und Verwendung vereinfacht.

Dies ist ein grundlegender Aufbau, um Ihnen den Einstieg zu erleichtern. Lassen Sie mich wissen, wenn Sie tiefer in spezifische Funktionen wie Partitionierung, Fehlerbehandlung oder die Integration in eine größere Anwendung einsteigen möchten!