---
audio: false
lang: de
layout: post
title: Verwendung von Apache Kafka in Java
translated: true
---

Verwendung von Apache Kafka in Java erfordert die Einrichtung eines Kafka-Clients in Ihrer Java-Anwendung, um Nachrichten zu senden und zu empfangen. Kafka ist eine verteilte Streaming-Plattform, und deren Java-Client-Bibliotheken machen die Integration einfach. Im Folgenden werde ich Sie durch die grundlegenden Schritte führen, um loszulegen.

Zunächst müssen Sie Ihre Umgebung einrichten. Stellen Sie sicher, dass Kafka auf Ihrem System oder einem Server installiert und ausgeführt wird. Sie können es von der offiziellen Apache Kafka-Website herunterladen und den ZooKeeper und den Kafka-Server mit den bereitgestellten Skripten starten. Aus Gründen der Einfachheit gehe ich davon aus, dass Sie Kafka lokal mit den Standard-Einstellungen (z. B. `localhost:9092` als Bootstrap-Server) ausführen.

Als Nächstes fügen Sie die Kafka-Client-Abhängigkeit zu Ihrem Java-Projekt hinzu. Wenn Sie Maven verwenden, fügen Sie dies in Ihrer `pom.xml` ein:

```xml
<dependency>
    <groupId>org.apache.kafka</groupId>
    <artifactId>kafka-clients</artifactId>
    <version>3.6.0</version> <!-- Verwenden Sie die neueste Version -->
</dependency>
```

Nun schreiben wir etwas Code. Ich zeige Ihnen, wie Sie einen einfachen Producer und Consumer erstellen.

### Kafka Producer Beispiel
Der Producer sendet Nachrichten an ein Kafka-Topic. Hier ist ein grundlegendes Beispiel:

```java
import org.apache.kafka.clients.producer.*;
import java.util.Properties;

public class SimpleProducer {
    public static void main(String[] args) {
        // Konfigurieren der Producer-Eigenschaften
        Properties props = new Properties();
        props.put("bootstrap.servers", "localhost:9092"); // Kafka-Server-Adresse
        props.put("key.serializer", "org.apache.kafka.common.serialization.StringSerializer");
        props.put("value.serializer", "org.apache.kafka.common.serialization.StringSerializer");

        // Erstellen einer Producer-Instanz
        try (Producer<String, String> producer = new KafkaProducer<>(props)) {
            // Senden einer Nachricht an ein Topic namens "test-topic"
            String topic = "test-topic";
            for (int i = 0; i < 10; i++) {
                String key = "key" + i;
                String value = "Hello, Kafka " + i;
                ProducerRecord<String, String> record = new ProducerRecord<>(topic, key, value);

                producer.send(record, (metadata, exception) -> {
                    if (exception == null) {
                        System.out.println("Gesendete Nachricht: " + value + " an Partition " + metadata.partition());
                    } else {
                        exception.printStackTrace();
                    }
                });
            }
        }
    }
}
```

In diesem Code:
- `bootstrap.servers` gibt an, wo Kafka läuft.
- Die Serializer definieren, wie Schlüssel und Werte (beide Strings hier) in Bytes umgewandelt werden.
- `ProducerRecord` stellt die Nachricht dar, und `send()` versendet sie asynchron mit einem Callback, um Erfolg oder Fehler zu behandeln.

### Kafka Consumer Beispiel
Der Consumer abonniert ein Topic und liest Nachrichten. Hier ist ein Beispiel:

```java
import org.apache.kafka.clients.consumer.*;
import java.util.Collections;
import java.util.Properties;

public class SimpleConsumer {
    public static void main(String[] args) {
        // Konfigurieren der Consumer-Eigenschaften
        Properties props = new Properties();
        props.put("bootstrap.servers", "localhost:9092");
        props.put("group.id", "test-group"); // Consumer Group ID
        props.put("key.deserializer", "org.apache.kafka.common.serialization.StringDeserializer");
        props.put("value.deserializer", "org.apache.kafka.common.serialization.StringDeserializer");
        props.put("auto.offset.reset", "earliest"); // Starten Sie vom Anfang des Topics

        // Erstellen einer Consumer-Instanz
        try (KafkaConsumer<String, String> consumer = new KafkaConsumer<>(props)) {
            // Abonnieren des Topics
            consumer.subscribe(Collections.singletonList("test-topic"));

            // Nachrichten abrufen
            while (true) {
                ConsumerRecords<String, String> records = consumer.poll(1000); // Timeout in ms
                for (ConsumerRecord<String, String> record : records) {
                    System.out.printf("Empfangen: key=%s, value=%s, partition=%d, offset=%d%n",
                            record.key(), record.value(), record.partition(), record.offset());
                }
            }
        }
    }
}
```

Hier:
- `group.id` identifiziert die Consumer-Gruppe zur Koordination.
- Deserializer wandeln die Byte-Daten wieder in Strings um.
- `poll()` holt Nachrichten in einer Schleife ab, und `auto.offset.reset` stellt sicher, dass Sie vom frühesten Eintrag beginnen, wenn kein Offset festgelegt ist.

### Ausführen des Codes
1. **Starten Sie Kafka**: Stellen Sie sicher, dass ZooKeeper und Kafka laufen (z. B. `bin/zookeeper-server-start.sh config/zookeeper.properties` und `bin/kafka-server-start.sh config/server.properties` auf Unix-ähnlichen Systemen).
2. **Erstellen Sie ein Topic**: Verwenden Sie das Kafka-Befehlszeilen-Tool, um `test-topic` zu erstellen, falls es nicht existiert: `bin/kafka-topics.sh --create --topic test-topic --bootstrap-server localhost:9092 --partitions 1 --replication-factor 1`.
3. **Führen Sie den Producer aus**: Führen Sie die `SimpleProducer`-Klasse aus, um Nachrichten zu senden.
4. **Führen Sie den Consumer aus**: Führen Sie die `SimpleConsumer`-Klasse aus, um die Nachrichten zu sehen, die konsumiert werden.

### Tipps
- **Fehlerbehandlung**: Fügen Sie try-catch-Blöcke oder Protokollierung für Produktionscode hinzu.
- **Konfiguration**: Passen Sie Eigenschaften wie `acks` (für die Zuverlässigkeit des Producers) oder `max.poll.records` (für die Batch-Größe des Consumers) nach Bedarf an.
- **Bibliotheken**: Für komplexere Anwendungen sollten Sie die Verwendung von Spring Kafka in Betracht ziehen, die die Konfiguration und Nutzung vereinfacht.

Dies ist eine grundlegende Einrichtung, um Sie zum Laufen zu bringen. Lassen Sie mich wissen, wenn Sie tiefer in spezifische Funktionen wie Partitionierung, Fehlerbehandlung oder Integration in eine größere Anwendung eintauchen möchten!