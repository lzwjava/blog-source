---
audio: true
lang: de
layout: post
title: KI-Papiere
translated: true
---

Grundlegende Konzepte & Architekturen:

* Backpropagation: Der grundlegende Algorithmus zum Trainieren von neuronalen Netzen.
* Convolutional Neural Networks: Eine Schlüsselarchitektur für die Bildverarbeitung.
* Word2Vec: Arbeiten von Tomáš Mikolov zu Wort-Einbettungen.
* Sequence to Sequence Learning with Neural Networks: Eine grundlegende Arbeit zur Sequenzmodellierung.
* Attention is All You Need: Die bahnbrechende Arbeit, die die Transformer-Architektur einführte.
* ResNet, "Deep Residual Learning for Image Recognition": Führt Residualverbindungen für tiefere Netzwerke ein.

Große Sprachmodelle & verwandte Techniken:

* Chain-of-Thought Prompting Elicits Reasoning in Large Language Models: Untersucht Prompting-Techniken für verbessertes logisches Denken.
* Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks: Kombiniert Retrieval mit Generierung für verbesserte Leistung.
* GPT-4 Technischer Bericht
* Claude 3 Modellkarte
* LLaMA 3 Paper

Spezifische Modelle & Anwendungen:

* DeepSeek V3 & V2: Aktuelle Modelle von DeepSeek.
* Whisper, Robust Speech Recognition via Large-Scale Weak Supervision: Ein robustes Modell zur Spracherkennung.
* Latent Diffusion Models (Stable Diffusion) Paper: Führt latente Diffusion für die Bildgenerierung ein.
* DALL-E 3: Scaling Text-to-Image Generation: Ein leistungsstarkes Text-zu-Bild-Modell.

Benchmarks & Bewertungen:

* SWE-Bench: Can Language Models Resolve Real-World GitHub Issues? Ein Benchmark zur Bewertung von Code-Generierungsfähigkeiten.

Kuratierte Listen:

* NeurIPS Test of Time Papers: Eine Sammlung einflussreicher Arbeiten, die für ihre langfristige Wirkung anerkannt sind.
* Ilyas Top 30 AI Papers: Eine kuratierte Liste von Ilya Sutskever, verfügbar unter [https://aman.ai/primers/ai/top-30-papers/](https://aman.ai/primers/ai/top-30-papers/).