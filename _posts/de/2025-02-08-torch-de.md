---
audio: false
lang: de
layout: post
title: PyTorch
translated: true
type: note
---

### torch-Funktionen

*   `torch.Tensor`: Ein mehrdimensionales Array, das Elemente eines einzelnen Datentyps enthält.
*   `torch.tensor`: Konstruiert einen Tensor mit Daten und Eigenschaften.
*   `torch.zeros`: Gibt einen mit Nullen gefüllten Tensor zurück.
*   `torch.ones`: Gibt einen mit Einsen gefüllten Tensor zurück.
*   `torch.arange`: Gibt einen 1-D-Tensor mit gleichmäßig verteilten Werten zurück.
*   `torch.linspace`: Gibt einen 1-D-Tensor mit gleichmäßig verteilten Werten über ein bestimmtes Intervall zurück.
*   `torch.rand`: Gibt einen Tensor zurück, der mit Zufallszahlen aus einer Gleichverteilung im Intervall [0, 1) gefüllt ist.
*   `torch.randn`: Gibt einen Tensor zurück, der mit Zufallszahlen aus einer Normalverteilung mit Mittelwert 0 und Varianz 1 gefüllt ist.
*   `torch.empty`: Gibt einen Tensor mit nicht initialisierten Daten zurück.
*   `torch.full`: Erstellt einen Tensor einer bestimmten Größe, gefüllt mit einem angegebenen Wert.
*   `torch.eye`: Gibt einen 2-D-Tensor mit Einsen auf der Diagonalen und Nullen anderswo zurück.

### Tensor-Operationen

*   `torch.add`: Addiert zwei Tensoren elementweise.
*   `torch.sub`: Subtrahiert zwei Tensoren elementweise.
*   `torch.mul`: Multipliziert zwei Tensoren elementweise.
*   `torch.div`: Dividiert zwei Tensoren elementweise.
*   `torch.matmul`: Führt eine Matrixmultiplikation durch.
*   `torch.pow`: Potenziert jedes Element eines Tensors.
*   `torch.exp`: Berechnet die Exponentialfunktion jedes Elements eines Tensors.
*   `torch.log`: Berechnet den natürlichen Logarithmus jedes Elements eines Tensors.
*   `torch.sqrt`: Berechnet die Quadratwurzel jedes Elements eines Tensors.
*   `torch.abs`: Berechnet den absoluten Wert jedes Elements eines Tensors.
*   `torch.neg`: Negiert jedes Element eines Tensors.
*   `torch.round`: Rundet jedes Element eines Tensors auf die nächste Ganzzahl.
*   `torch.floor`: Gibt den abgerundeten Wert (Floor) jedes Elements eines Tensors zurück.
*   `torch.ceil`: Gibt den aufgerundeten Wert (Ceiling) jedes Elements eines Tensors zurück.
*   `torch.clamp`: Beschränkt alle Elemente der Eingabe auf den Bereich [min, max].
*   `torch.sum`: Gibt die Summe aller Elemente im Eingabe-Tensor zurück.
*   `torch.mean`: Gibt den Mittelwert aller Elemente im Eingabe-Tensor zurück.
*   `torch.std`: Gibt die Standardabweichung aller Elemente im Eingabe-Tensor zurück.
*   `torch.var`: Gibt die Varianz aller Elemente im Eingabe-Tensor zurück.
*   `torch.max`: Gibt den Maximalwert aller Elemente im Eingabe-Tensor zurück.
*   `torch.min`: Gibt den Minimalwert aller Elemente im Eingabe-Tensor zurück.
*   `torch.argmax`: Gibt den Index des Maximalwerts aller Elemente im Eingabe-Tensor zurück.
*   `torch.argmin`: Gibt den Index des Minimalwerts aller Elemente im Eingabe-Tensor zurück.
*   `torch.sort`: Sortiert die Elemente des Eingabe-Tensors entlang einer gegebenen Dimension.
*   `torch.topk`: Gibt die k größten Elemente des Eingabe-Tensors entlang einer gegebenen Dimension zurück.
*   `torch.reshape`: Gibt einen Tensor mit denselben Daten und derselben Anzahl an Elementen wie die Eingabe zurück, aber mit der angegebenen Form.
*   `torch.transpose`: Gibt eine Ansicht des Eingabe-Tensors mit vertauschten Dimensionen zurück.
*   `torch.squeeze`: Gibt einen Tensor zurück, bei dem alle Dimensionen der Größe 1 aus der Eingabe entfernt wurden.
*   `torch.unsqueeze`: Gibt einen neuen Tensor zurück, bei dem eine Dimension der Größe eins an der angegebenen Position eingefügt wurde.
*   `torch.cat`: Verkettet die gegebenen Tensoren in der gegebenen Dimension.
*   `torch.stack`: Verkettet eine Sequenz von Tensoren entlang einer neuen Dimension.
*   `torch.chunk`: Teilt einen Tensor in eine bestimmte Anzahl von Blöcken.
*   `torch.split`: Teilt einen Tensor in Blöcke einer bestimmten Größe.

### Neuronale Netzwerk-Module

*   `torch.nn.Module`: Basisklasse für alle neuronalen Netzwerk-Module.
*   `torch.nn.Linear`: Wendet eine lineare Transformation auf die eingehenden Daten an.
*   `torch.nn.Conv2d`: Wendet eine 2D-Faltung über einem Eingangssignal an, das aus mehreren Eingangsebenen besteht.
*   `torch.nn.MaxPool2d`: Wendet ein 2D-Max-Pooling über einem Eingangssignal an.
*   `torch.nn.ReLU`: Wendet die rectified linear unit Funktion elementweise an.
*   `torch.nn.Sigmoid`: Wendet die Sigmoid-Funktion elementweise an.
*   `torch.nn.Tanh`: Wendet die hyperbolische Tangens-Funktion elementweise an.
*   `torch.nn.BatchNorm2d`: Wendet Batch-Normalisierung auf einen 4D-Eingang an.
*   `torch.nn.Dropout`: Setzt während des Trainings zufällig einige Elemente des Eingabe-Tensors mit der Wahrscheinlichkeit p auf Null.
*   `torch.nn.Embedding`: Eine einfache Nachschlagetabelle, die Einbettungen (Embeddings) eines festen Wörterbuchs und einer festen Größe speichert.

### Verlustfunktionen

*   `torch.nn.MSELoss`: Erstellt ein Kriterium, das den mittleren quadratischen Fehler (quadrierte L2-Norm) zwischen jedem Element in der Eingabe und dem Ziel misst.
*   `torch.nn.CrossEntropyLoss`: Dieses Kriterium berechnet den Kreuzentropieverlust zwischen Eingabe und Ziel.
*   `torch.nn.BCELoss`: Erstellt ein Kriterium, das die binäre Kreuzentropie zwischen dem Ziel und der Ausgabe misst.
*   `torch.nn.L1Loss`: Erstellt ein Kriterium, das den mittleren absoluten Fehler (MAE) zwischen jedem Element in der Eingabe und dem Ziel misst.

### Optimierer

*   `torch.optim.SGD`: Implementiert stochastischen Gradientenabstieg (optional mit Momentum).
*   `torch.optim.Adam`: Implementiert den Adam-Algorithmus.
*   `torch.optim.RMSprop`: Implementiert den RMSprop-Algorithmus.

### Autograd

*   `torch.autograd.grad`: Berechnet und gibt die Summe der Gradienten der Ausgaben in Bezug auf die Eingaben zurück.

### Dienstprogramme

*   `torch.device`: Repräsentiert das Gerät, auf dem ein torch.Tensor allokiert ist oder allokiert wird.
*   `torch.cuda.is_available`: Gibt True zurück, wenn CUDA verfügbar ist.
*   `torch.save`: Speichert einen Tensor auf der Festplatte.
*   `torch.load`: Lädt einen Tensor von der Festplatte.

### Sonstiges

*   `torch.no_grad`: Kontext-Manager, der die Gradientenberechnung deaktiviert.
*   `torch.set_grad_enabled`: Aktiviert oder deaktiviert Grad, abhängig von seinem Argument.