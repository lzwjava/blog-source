---
audio: false
generated: true
image: false
lang: de
layout: post
title: Sichere LLM-API-Validierungsebenen
translated: true
type: note
---

Wenn Sie eine **LLM API** erstellen oder verwenden, m√∂chten Sie in der Regel Sicherheitsvorkehrungen treffen, um sicherzustellen, dass die Antworten sicher, g√ºltig und n√ºtzlich sind. Hier sind die wichtigsten Methoden, um Pr√ºfungen hinzuzuf√ºgen:

---

### 1. **Eingabevalidierung**

* **Schema-Checks**: Stellen Sie sicher, dass die JSON-Anfrage dem erwarteten Schema entspricht, bevor sie an das LLM gesendet wird.
* **Prompt-Bereinigung**: Entfernen oder maskieren Sie unerw√ºnschte Zeichen, b√∂sartige Anweisungen oder Injection-Versuche.
* **L√§ngenpr√ºfungen**: Erzwingen Sie maximale Token-/Zeichenlimits f√ºr Eingaben, um unn√∂tige Kosten oder Abschneiden zu vermeiden.

---

### 2. **Ausgabevalidierung**

* **JSON-Schema-Validierung**: Wenn das LLM JSON zur√ºckgeben soll, f√ºhren Sie es durch `json.loads()` und validieren Sie es gegen ein Schema (z.B. mit `pydantic`, `jsonschema`).
* **Regex-/Formatpr√ºfungen**: Erzwingen Sie Muster f√ºr E-Mails, URLs oder Zahlen.
* **Typ√ºberpr√ºfung**: Stellen Sie sicher, dass Felder den richtigen Typ haben (String, Integer, Liste, etc.).
* **Bereichspr√ºfungen**: Stellen Sie sicher, dass numerische oder Datumswerte innerhalb der erwarteten Grenzen liegen.

---

### 3. **Sicherheits- und Inhaltspr√ºfungen**

* **Toxizit√§ts- oder Obsz√∂nit√§tsfilter**: Lassen Sie die Ausgabe durch einen Klassifikator laufen (z.B. Perspective API, OpenAI Moderation API).
* **Richtlinienfilter**: Definieren Sie Regeln, um Antworten zu blockieren, die bestimmte Schl√ºsselw√∂rter oder Kategorien enthalten.
* **Halluzinationserkennung**: F√ºgen Sie Faktenpr√ºfungen hinzu (durch retrieval-augmentierte Pr√ºfungen, Kreuzvalidierung mit mehreren Modellen oder regelbasierte Plausibilit√§tspr√ºfungen).

---

### 4. **Gesch√§ftslogik-Beschr√§nkungen**

* **Konsistenzregeln**: Wenn die Antwort mit Systemdaten √ºbereinstimmen muss (z.B. verf√ºgbarer Kontostand, Konfigurationswerte), f√ºhren Sie eine Kreuzpr√ºfung durch, bevor Sie sie an den Benutzer zur√ºckgeben.
* **Guardrails**: Verwenden Sie Bibliotheken wie **Guardrails AI**, **LMQL** oder **Instructor**, um strukturierte Ausgaben und Beschr√§nkungen deklarativ durchzusetzen.
* **Nachbearbeitungshooks**: Versehen Sie LLM-Antworten mit benutzerdefinierter Logik (Gro√ü-/Kleinschreibung normalisieren, Standardwerte erzwingen, unerwartete Antworten ablehnen).

---

### 5. **Redundanz & Kreuzvalidierung**

* **Selbstpr√ºfungs-Prompts**: Bitten Sie das LLM, seine eigene Antwort erneut zu validieren oder zu erkl√§ren.
* **Dual-Model-Checking**: Verwenden Sie ein kleineres/schnelleres LLM, um die Ausgabe des Hauptmodells zu √ºberpr√ºfen (z.B. "Entspricht diese Antwort dem Schema X?").
* **Abstimmung/Konsens**: F√ºhren Sie mehrere Aufrufe durch und vergleichen Sie die Antworten auf Stabilit√§t.

---

### 6. **Betriebliche Sicherheitsvorkehrungen**

* **Ratenbegrenzung**: Verhindern Sie den Missbrauch der API.
* **Timeouts**: Brechen Sie Anfragen ab, die h√§ngen bleiben.
* **Protokollierung und Audit-Trails**: F√ºhren Sie Aufzeichnungen √ºber Anfragen und gefilterte Antworten.
* **Fallbacks**: Wenn die Validierung fehlschl√§gt, wiederholen Sie den Vorgang mit angepasstem Prompt oder geben Sie eine sichere Standardantwort/einen Fehler zur√ºck.

---

üëâ In der Praxis schichtet ein guter LLM-API-Stack diese Ma√ünahmen:

1. Eingabe validieren.
2. LLM aufrufen.
3. Ausgabe gegen Schema + Gesch√§ftsregeln validieren.
4. Sicherheits-/Inhaltsfilter anwenden.
5. Nur zur√ºckgeben, wenn alle Pr√ºfungen bestanden sind (andernfalls Wiederholung/Fallback).

---

M√∂chten Sie, dass ich Ihnen ein **Python-Beispiel** zeige, in dem wir einen LLM-API-Aufruf mit JSON-Schema-Validierung und Moderationspr√ºfungen umschlie√üen?