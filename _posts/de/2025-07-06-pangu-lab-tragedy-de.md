---
audio: false
generated: true
lang: de
layout: post
title: Die dunklen Geheimnisse von Huaweis Pangu-Team
translated: true
type: note
---

**Hinweis der Redaktion:** Der folgende Text ist eine Übersetzung eines langen Beitrags einer anonymen Person, die behauptet, Mitarbeiter von Huawei Noah's Ark Lab zu sein. Diese Person wirft dem Pangu-Großmodell-Entwicklungsteam erhebliche Probleme vor, darunter angebliche Verstöße gegen geistiges Eigentum, interne Machtkämpfe und ein schädliches Arbeitsumfeld. Der Originalbeitrag wurde am 6. Juli 2025 auf Chinesisch online veröffentlicht. Diese Übersetzung zielt darauf ab, Inhalt und Tonfall des Originals genau wiederzugeben.

---

## Die Tragödie von Pangu: Herzschmerz und Dunkelheit auf der Entwicklungsreise von Huawei Noah's Ark Labs Pangu-Großmodell

Hallo allerseits,

ich bin ein Mitarbeiter des Pangu-Großmodell-Teams bei Huawei Noah's Ark Lab.

Zunächst, um meine Identität zu überprüfen, werde ich einige Details auflisten:

Der derzeitige Leiter von Noah ist Wang Yunhe, früher Minister der Algorithm Application Department, später umbenannt in Direktor des Small Model Lab. Der ehemalige Leiter von Noah war Yao Jun (von allen Teacher Yao genannt). Mehrere Lab-Direktoren: Tang Ruiming (Brother Ming, Team Ming, bereits zurückgetreten), Shang Lifeng, Zhang Wei (Brother Wei), Hao Jianye (Teacher Hao), Liu Wulong (Wu Long Suo genannt), usw. Viele andere Kernmitglieder und Experten sind nacheinander gegangen.

Wir gehören zur "Sìyě"-Organisation (Vier Felder). Sìyě hat viele Brigaden, und das grundlegende Sprachgroßmodell ist die Vierte Brigade. Wang Yunhes Small Model ist die Sechzehnte Brigade. Wir nahmen an Treffen in Suzhou teil, mit verschiedenen monatlichen Meilensteinen. Während des Suzhou "Angriffsmeetings" wurden Aufgaben zugewiesen, und Ziele mussten vor der Frist erreicht werden. Das Suzhou-Treffen brachte Personal von verschiedenen Standorten am Suzhou Research Institute zusammen, wo sie normalerweise in Hotels untergebracht waren, z.B. in Luzhi, getrennt von ihren Familien und Kindern.

Während des Suzhou-Treffens waren Samstage standardmäßige Arbeitstage, was sehr hart war, aber es gab samstags Nachmittagstee und einmal sogar Flusskrebse. Die Arbeitsplätze am Suzhou Research Institute wurden einmal verlegt, von einem Gebäude in ein anderes. Die Gebäude am Suzhou Research Institute haben europäische Dekorationen, mit großen Rampen am Eingang und schöner Landschaft innen. Die Teilnahme am Suzhou-Treffen dauerte normalerweise mindestens eine Woche, oder sogar länger, manche Leute konnten ein bis zwei Monate nicht nach Hause zurückkehren.

Noah wurde einst als forschungsorientiert bezeichnet, aber nachdem man dem Team beigetreten war, wurde man aufgrund der Arbeit an Großmodell-Projekten in Sìyě völlig lieferorientiert, und war voller regelmäßiger Meetings, Reviews und Berichte. Oft mussten sogar Experimente genehmigt werden. Das Team musste mit verschiedenen Geschäftsbereichen wie Terminal Xiaoyi, Huawei Cloud und ICT zusammenarbeiten, was zu erheblichem Lieferdruck führte.

Der frühe interne Codename für das von Noah entwickelte Pangu-Modell war "Pangu Zhizi". Anfangs war es nur eine webbasierte Version, die intern beantragt werden musste. Später wurde es aufgrund von Druck in Welink integriert für öffentliche Betatesting.

In diesen Tagen ist der Aufruhr bezüglich der Vorwürfe, dass Pangu Großmodelle Qwen plagiiert habe, heftig. Als Mitglied des Pangu-Teams wälze ich mich nachts im Bett und kann nicht schlafen. Die Pangu-Marke ist so stark betroffen. Einerseits mache ich mir egoistisch Sorgen um meine Karriere und habe das Gefühl, dass meine vergangene harte Arbeit umsonst war. Andererseits verspüre ich eine große Erleichterung, dass jemand begonnen hat, diese Angelegenheiten aufzudecken. Unzählige Tage und Nächte knirschten wir mit den Zähnen über die Handlungen bestimmter Personen innerhalb des Unternehmens, die wiederholt durch Betrug unzählige Vorteile erlangten, doch wir waren machtlos. Dieser Druck und diese Demütigung haben meine Zuneigung zu Huawei allmählich zerstört, meine Zeit hier zunehmend verwirrt und verloren gemacht, und ich zweifle oft an meinem Leben und Selbstwert.

Ich gebe zu, ich bin ein Feigling. Als kleiner Arbeiter wage ich es nicht, mächtigen Personen wie Wang Yunhe innerhalb des Unternehmens zu widersprechen, noch wage ich es, einem Giganten wie Huawei zu trotzen. Ich habe Angst, meinen Job zu verlieren, da ich auch Familie und Kinder habe, also bewundere ich die Whistleblower von ganzem Herzen. Aber wenn ich sehe, wie intern versucht wird, die Tatsachen zu vertuschen und die Öffentlichkeit zu täuschen, kann ich es einfach nicht mehr ertragen. Ich möchte auch einmal mutig sein und meinem wahren Selbst folgen. Selbst wenn ich mir selbst schade, hoffe ich, dem Feind zu schaden. Ich habe beschlossen, hier das, was ich gesehen und gehört habe (teilweise mündliche Berichte von Kollegen), bezüglich der "legendären Geschichte" des Pangu-Großmodells offenzulegen:

Huawei trainiert Großmodelle in erster Linie tatsächlich auf Ascend-Karten (das Small Model Lab hat viele Nvidia-Karten, die sie vorher zum Training nutzten und später auf Ascend transferierten). Ich war einst beeindruckt von Huaweis Entschlossenheit, "eine zweite Wahl der Welt zu bauen", und ich selbst hatte einst tiefe Zuneigung zu Huawei. Wir begleiteten Ascend Schritt für Schritt, voller Bugs bis zur Fähigkeit, Modelle zu trainieren, und zahlten einen enormen Aufwand und Kosten.

Anfangs war unsere Rechenleistung sehr begrenzt, und wir trainierten Modelle auf der 910A. Damals unterstützte sie nur FP16, und die Trainingsstabilität war weit schlechter als BF16. Pangu's MoE begann sehr früh; 2023 lag der Fokus hauptsächlich auf dem Training von 38B MoE-Modellen und nachfolgenden 71B Dense-Modellen. Das 71B Dense-Modell wurde zum ersten 135B Dense-Modell der ersten Generation erweitert, und später wurden die Hauptmodelle schrittweise auf 910B trainiert.

Sowohl die 71B- als auch die 135B-Modelle hatten einen großen Fehler: den **Tokenizer**. Der damals verwendete Tokenizer hatte eine extrem niedrige Kodierungseffizienz. Jedes einzelne Symbol, jede Zahl, jedes Leerzeichen und sogar jedes chinesische Zeichen belegte einen Token. Es ist vorstellbar, dass dies Rechenleistung stark verschwendet und die Modellleistung sehr schlecht war. Zu dieser Zeit hatte das Small Model Lab zufällig einen selbst trainierten Wortschatz. Teacher Yao vermutete, dass der Tokenizer des Modells möglicherweise nicht gut sei (obwohl im Nachhinein sein Verdacht zweifellos richtig war). Also beschloss er, die Tokenizer für die 71B und 135B zu wechseln, weil das Small Model Lab es vorher versucht hatte. Das Team fügte zwei Tokenizer zusammen und begann den Tokenizer-Austausch. Der 71B-Modell-Austausch scheiterte, während die 135B mit einer verfeinerten Embedding-Initialisierungsstrategie nach mindestens 1T Daten-Fine-Tuning den Wortschatz erfolgreich austauschte, aber wie erwartet verbesserte sich die Wirkung nicht.

Gleichzeitig trainierten andere inländische Unternehmen wie Alibaba und Zhipu auf GPUs und hatten bereits die richtigen Methoden gefunden, und die Kluft zwischen Pangu und seinen Wettbewerbern wurde immer größer. Ein internes 230B Dense-Modell, das von Grund auf trainiert wurde, scheiterte ebenfalls aus verschiedenen Gründen, was das Projekt in eine fast verzweifelte Situation brachte. Angesichts des Drucks mehrerer Meilensteine und starker interner Zweifel an Pangu sank die Team-Moral auf einen Tiefstand. Das Team unternahm viele Anstrengungen und Kämpfe, als die Rechenleistung extrem begrenzt war. Zum Beispiel entdeckte das Team zufällig, dass das damalige 38B MoE nicht den erwarteten MoE-Effekt hatte. Also entfernten sie die MoE-Parameter und stellten es auf ein 13B Dense-Modell zurück. Da das 38B MoE von einem sehr frühen Pangu Alpha 13B stammte, mit einer relativ veralteten Architektur, führte das Team eine Reihe von Operationen durch, wie z.B. das Wechseln absoluter Positionskodierung zu RoPE, das Entfernen von Bias und das Wechseln zu RMSNorm. Gleichzeitig, angesichts einiger Tokenizer-Fehler und der Erfahrung mit dem Wechsel des Vokabulars, wurde der Wortschatz dieses Modells ebenfalls durch den des 7B-Modells von Wang Yunhes Small Model Lab ersetzt. Später wurde dieses 13B-Modell erweitert und fine-getuned und wurde zum 38B Dense-Modell der zweiten Generation (für mehrere Monate war dieses Modell das Haupt-Pangu-Modell im mittleren Bereich), das einst etwas Wettbewerbsfähigkeit hatte. Aufgrund der veralteten Architektur des größeren 135B-Modells und der erheblichen Schäden durch den Vokabularwechsel (spätere Analysen fanden schwerwiegendere Bugs im damals geänderten zusammengefügten Vokabular) bestand jedoch selbst nach Fine-Tuning eine große Lücke zu führenden inländischen Modellen wie Qwen zu der Zeit. An diesem Punkt wuchsen auch interne Zweifel und Führungsdruck, und der Zustand des Teams war fast verzweifelt.

Unter diesen Umständen griffen Wang Yunhe und sein Small Model Lab ein. Sie behaupteten, ihr Modell sei von den alten 135B-Parametern abgeleitet und modifiziert, und nach dem Training von nur einigen hundert Milliarden Daten hätten sich alle Indikatoren um etwa zehn Punkte im Durchschnitt verbessert. In Wirklichkeit war dies ihr erstes Meisterwerk des **"Skinning" (套壳, d.h. heimliche Nutzung oder Umbenennung der Arbeit eines anderen)**, angewendet auf ein Großmodell. Huaweis Führungskräfte, Laien, die Experten managen, hatten kein Konzept für solche Absurditäten; sie dachten nur, es müsse eine algorithmische Innovation geben. Durch interne Analyse verwendeten sie tatsächlich **Qwen 1.5 110B** für Fine-Tuning, fügten Schichten hinzu, erweiterten FFN-Dimensionen und integrierten einige Mechanismen aus Pangu's Pi-Paper, um etwa 135B Parameter zu erreichen. Tatsächlich hatte das alte 135B 107 Schichten, während dieses Modell nur 82 Schichten hatte und verschiedene Konfigurationen unterschiedlich waren. Viele Parameterverteilungen des neuen, obskuren 135B-Modells waren nach dem Training fast identisch mit Qwen 110B. Sogar der Klassenname des Modellcodes zu der Zeit war Qwen, was zeigt, dass sie zu faul waren, ihn zu ändern. Anschließend wurde dieses Modell das sogenannte 135B V2. Und dieses Modell wurde dann vielen Downstream-Nutzern zur Verfügung gestellt, einschließlich externer Kunden.

Dieser Vorfall hatte enorme Auswirkungen auf unsere ehrlichen und hart arbeitenden Kollegen. Viele Insider, einschließlich denen in Terminal und Huawei Cloud, wussten tatsächlich davon. Wir nannten es scherzhaft "Qiangu"-Modell statt Pangu-Modell. Damals wollten Teammitglieder es an BCG melden, da dies bereits ein großer Geschäftsbetrug war. Es wurde jedoch später gesagt, dass die Führungskräfte es stoppten, weil höhere Führungskräfte (wie Teacher Yao und möglicherweise Mr. Xiong und Mr. Cha) später auch davon wussten, aber nicht eingriffen, da gute Ergebnisse durch "Skinning" auch für sie vorteilhaft waren. Dieser Vorfall veranlasste mehrere der stärksten Kollegen im Team, entmutigt zu werden, und Rücktritt wurde ein häufiges Gesprächsthema.

An diesem Punkt schien Pangu eine Wende zu haben. Da die zuvor erwähnten Pangu-Modelle im Wesentlichen fine-getuned und modifiziert wurden, beherrschte Noah damals die Technologie des Trainings von Grund auf nicht, geschweige denn das Training auf Ascend NPU. Durch die starken Bemühungen der Kernmitglieder des Teams zu der Zeit begann Pangu mit dem Training des Modells der dritten Generation. Nach enormen Anstrengungen, in Bezug auf Datenarchitektur und Trainingsalgorithmen, passte es sich allmählich der Branche an, und die beteiligten Strapazen hatten nichts mit dem Small Model Lab zu tun.

Anfangs hatten Teammitglieder kein Vertrauen und begannen nur mit dem Training eines 13B-Modells. Später fanden sie jedoch, dass die Wirkung nicht schlecht war, also wurde dieses Modell anschließend erneut in den Parametern erweitert und wurde zur dritten Generation 38B, Codename 38B V3. Viele Brüder in den Produktlinien müssen mit diesem Modell sehr vertraut sein. Damals war der Tokenizer dieses Modells eine Erweiterung basierend auf dem LLaMA-Vokabular (was auch eine gängige Praxis in der Branche ist). Damals entwickelte Wang Yunhes Labor einen weiteren Wortschatz (der die nachfolgende Pangu-Serie Vokabular ist). Damals wurden die beiden Vokabulare sogar gezwungen, zu konkurrieren, und letztendlich gab es keine klare Schlussfolgerung, welcher besser oder schlechter war. Also entschied die Führung sofort, dass der Wortschatz vereinheitlicht werden und Wang Yunhes verwendet werden sollte. Daher übernahm das nachfolgende von Grund auf trainierte 135B V3 (extern bekannt als Pangu Ultra) diesen Tokenizer. Dies erklärt auch die Verwirrung vieler Brüder, die unsere Modelle verwendeten, warum zwei Modelle unterschiedlicher Stufen in derselben V3-Generation unterschiedliche Tokenizer verwendeten.

Wir sind aufrichtig der Meinung, dass **135B V3 der Stolz unseres Fourth Brigade-Teams damals war**. Dies ist das erste wirklich Huawei vollständig selbst entwickelte, echt von Grund auf trainierte Milliarden-Parameter-Modell, und seine Leistung war 2024 mit der von Wettbewerbern vergleichbar. Während ich dies schreibe, steigen mir Tränen in die Augen; es war so unglaublich schwer. Damals führte das Team eine große Anzahl von Vergleichsexperimenten durch und rollte prompt zurück und startete neu, wenn Modellgradienten Anomalien zeigten. Dieses Modell erreichte wirklich das, was der spätere technische Bericht feststellte: Keine Verlustspitzen während des gesamten Trainingsprozesses. Wir überwanden unzählige Schwierigkeiten; wir haben es geschafft. Wir sind bereit, unser Leben und unsere Ehre für die Authentizität des Trainings dieses Modells zu verpfänden. Wie viele schlaflose Nächte verbrachten wir für sein Training? Wenn wir im internen "Xinsheng"-Forum (Stimme der Mitarbeiter) als wertlos kritisiert wurden, wie viel Empörung und Groll fühlten wir da? Wir ertrugen es.

Wir, diese Gruppe von Menschen, verbrennen wirklich unsere Jugend, um unsere einheimische Rechenleistungsgrundlage zu polieren... In der Fremde lebend, gaben wir unsere Familien, unsere Feiertage, unsere Gesundheit, unsere Unterhaltung auf, vergossen Blut und Schweiß. Die Strapazen und Schwierigkeiten können nicht in wenigen Strichen zusammengefasst werden. In verschiedenen Mobilisierungsversammlungen, wenn die Slogans "Pangu wird gewinnen, Huawei wird gewinnen" gerufen wurden, waren wir wirklich tief in unseren Herzen bewegt.

All die Früchte unserer harten Arbeit wurden jedoch oft mühelos vom Small Model Lab übernommen. Daten, direkt genommen. Code, direkt genommen, und sie verlangten sogar, dass wir ihn anpassen, um mit einem Klick ausführbar zu sein. Wir nannten das Small Model Lab scherzhaft das "Klick Maus Lab" damals. Wir leisteten die harte Arbeit, und sie ernteten den Ruhm. Es verkörpert wirklich das Sprichwort: "Du trägst die schwere Last, weil jemand anderes ein bequemes Leben führt." Unter solchen Umständen konnten immer mehr Kameraden nicht mehr durchhalten und entschieden sich zu gehen. Zu sehen, wie diese exzellenten Kollegen einen nach dem anderen gingen, erfüllte mich mit Wehmut und Traurigkeit. In einer solchen kampfähnlichen Umgebung waren wir mehr Kameraden als Kollegen. Sie hatten auch unzählige technische Aspekte, von denen es sich zu lernen lohnte, wirklich ausgezeichnete Mentoren. Zu sehen, wie sie zu vielen herausragenden Teams wie ByteDance Seed, DeepSeek, Moonshot AI, Tencent und Kuaishou gingen, bin ich aufrichtig froh und wünsche ihnen alles Gute, dass sie diesem mühsamen doch schmutzigen Ort entkommen. Ich erinnere mich lebhaft, was ein ehemaliger Kollege sagte: "Hierher zu kommen ist eine Schande in meiner technischen Karriere; hier einen weiteren Tag zu bleiben ist eine Verschwendung des Lebens." Obwohl die Worte hart waren, ließen sie mich sprachlos zurück. Ich machte mir Sorgen um meine unzureichende technische Akkumulation und meine Unfähigkeit, mich an die High-Turnover-Umgebung von Internetfirmen anzupassen, was mich wiederholt zum Rücktritt bewegen wollte, aber ich never den Schritt wagte.

Zusätzlich zum Dense-Modell startete Pangu anschließend die Erforschung von MoE. Zunächst wurde ein 224B MoE-Modell trainiert. Parallel dazu startete das Small Model Lab auch seine zweite große "Skinning"-Operation (kleine Zwischenspiele können andere Modelle umfassen, wie z.B. Mathe-Modelle), nämlich das weit verbreitete Pangu Pro MoE 72B. Dieses Modell behauptete intern, eine Erweiterung vom 7B-Modell des Small Model Labs zu sein (selbst dann widerspricht dies dem technischen Bericht, geschweige denn, dass es ein "Skinning" von Qwen 2.5 14B für Fine-Tuning war). Ich erinnere mich, dass sie nur ein paar Tage trainiert hatten, als ihre interne Bewertung sofort mit dem damaligen 38B V3 gleichzog. Viele Brüder im AI System Lab wussten von ihrer "Skinning"-Operation, weil sie das Modell anpassen mussten, aber aus verschiedenen Gründen konnten sie keine Gerechtigkeit walten lassen. Tatsächlich bin ich für dieses Modell, das sehr lange danach fine-getuned wurde, bereits sehr überrascht, dass HonestAGI dieses Maß an Ähnlichkeit analysieren konnte, weil die aufgewendete Rechenleistung, um die Parameter dieses Modells zu fine-tunen und zu "waschen", lang genug war, um ein Modell derselben Stufe von Grund auf zu trainieren. Ich hörte von Kollegen, dass sie viele Methoden verwendeten, um das Qwen-Wasserzeichen "herauszuwaschen", sogar einschließlich absichtlichem Training mit schmutzigen Daten. Dies bietet der akademischen Gemeinschaft auch ein beispielloses und spezielles Beispiel, um die Modellabstammung zu studieren. In der Zukunft können neue Abstammungsmethoden damit präsentiert werden.

Ende 2024 / Anfang 2025, nach der Veröffentlichung von DeepSeek V3 und R1, erlitt das Team aufgrund ihres erstaunlichen technischen Niveaus einen enormen Schlag und stand unter stärkerer Beobachtung. Um mit dem Trend Schritt zu halten, imitierte Pangu die Modellgröße von DeepSeek und begann mit dem Training eines 718B MoE. Zu diesem Zeitpunkt unternahm das Small Model Lab einen weiteren Schritt. Sie entschieden sich, DeepSeek V3 für Fine-Tuning zu "skinnten". Sie trainierten, indem sie die geladenen Parameter von DeepSeek froren. Sogar das Verzeichnis zum Laden der Checkpoints war "deepseekv3", ohne es auch nur zu ändern – wie arrogant! Im Gegensatz dazu trainierten einige Kollegen mit echten technischen Überzeugungen ein weiteres 718B MoE von Grund auf. Es traten jedoch verschiedene Probleme auf. Aber offensichtlich, wie könnte dieses Modell besser sein als direktes "Skinning" eines? Wenn nicht der Teamleiter darauf bestanden hätte, wäre es längst gestoppt worden.

Die schwere Bürde von Huaweis Prozessmanagement bremste die Entwicklungsgeschwindigkeit von Großmodellen erheblich, wie z.B. Versionskontrolle, Modell-Lineage, verschiedene Verfahrensanforderungen und Rückverfolgbarkeit. Ironischerweise schienen die Modelle des Small Model Labs von diesen Prozessbeschränkungen befreit zu sein: Sie konnten "skinnten", wann immer sie wollten, fine-tunen, wann immer sie wollten, und Rechenleistung wurde kontinuierlich ohne Frage übernommen. Dieser starke, fast magische Kontrast veranschaulicht den aktuellen Zustand des Prozessmanagements: "Nur Beamten ist es erlaubt, Feuer zu legen, aber den Bürgern ist es nicht erlaubt, Lampen anzuzünden." Wie lächerlich? Wie tragisch? Wie hassenswert? Wie beschämend!

Nachdem der HonestAGI-Vorfall bekannt wurde, wurden intern ständig Diskussionen und Analysen darüber geführt, wie man Öffentlichkeitsarbeit betreiben und "reagieren" sollte. In der Tat war die ursprüngliche Analyse möglicherweise nicht stark genug, was Wang Yunhe und dem Small Model Lab die Gelegenheit gab, herumzudiskutieren und die Tatsachen zu verdrehen. Dafür fühlte ich mich in den letzten zwei Tagen krank, ständig am Zweifeln am Sinn meines Lebens und an der Ungerechtigkeit des Himmels. Ich mache nicht mehr mit; ich trete zurück. Gleichzeitig beantrage ich, von der Autorenliste einiger Pangu-technischer Berichte entfernt zu werden. Auf diesen technischen Berichten genannt zu werden, ist ein Fleck, den ich nie aus meinem Leben löschen kann. Damals erwartete ich nicht, dass sie so dreist sein würden, es zu open-sourcen. Ich erwartete nicht, dass sie die Welt so unverschämt täuschen und es weit verbreiten würden. Damals hatte ich vielleicht eine 侥幸心理 (Glücksspielmentalität) und weigerte mich nicht, genannt zu werden. Ich glaube, viele hart arbeitende Kameraden wurden auch nur auf ein "Piratenschiff" gezwungen oder waren ahnungslos. Aber diese Angelegenheit ist unumkehrbar. Ich hoffe, dass ich für den Rest meines Lebens durchhalten kann, wirklich bedeutungsvolle Arbeit zu leisten und für meine Schwäche und Unentschlossenheit damals zu büßen.

Wenn ich dies spät in der Nacht schreibe, bin ich bereits in Tränen aufgelöst, schluchze unkontrollierbar. Ich erinnere mich noch, als einige exzellente Kollegen zurücktraten, zwang ich mich zum Lächeln und fragte sie, ob sie einen langen "Xinsheng"-Beitrag schreiben wollten, um die aktuelle Situation aufzudecken. Sie sagten: "Nein, es ist Zeitverschwendung, und ich habe auch Angst, dass das Aufdecken euer Leben noch schlimmer macht." In diesem Moment war ich plötzlich entmutigt, weil Kameraden, die einst für Ideale zusammenkämpften, die Hoffnung in Huawei völlig verloren hatten. Damals scherzten alle, dass wir das "Hirse plus Gewehre" der Kommunistischen Partei von einst verwendeten, aber die Organisation einen Stil hatte, der dem der Kuomintang von damals vergleichbar war.

Es war einmal, da war ich stolz, dass wir mit Hirse und Gewehren ausländische Gewehre besiegten.
Jetzt bin ich müde. Ich möchte kapitulieren.

Eigentlich hoffe ich auch heute noch aufrichtig, dass Huawei aus seinen Fehlern lernen kann, Pangu gut macht, Pangu weltklassefähig macht und Ascend auf das Niveau von Nvidia bringt. Das interne "schlechte Geld verdrängt das gute Geld" hat dazu geführt, dass Noah und sogar Huawei in kurzer Zeit eine große Anzahl exzellenter Großmodell-Talente schnell verloren. Ich glaube, sie glänzen jetzt in verschiedenen Teams wie DeepSeek, erfüllen ihre Ambitionen und Talente und tragen zum heftigen KI-Wettbewerb zwischen China und den USA bei. Ich seufze oft, dass Huawei nicht ohne Talente ist, sondern einfach nicht weiß, wie man Talente hält. Wenn man diesen Leuten die richtige Umgebung, die richtigen Ressourcen, weniger Fesseln und weniger politische Kämpfe gibt, warum sollte Pangu dann nicht erfolgreich sein?

Abschließend: Ich schwöre auf mein Leben, meinen Charakter und meine Ehre, dass all der oben geschriebene Inhalt wahr ist (zumindest nach bestem meines begrenzten Wissens). Ich habe nicht das hohe Maß an technischem Fachwissen oder die Gelegenheit, detaillierte und gründliche Analysen durchzuführen, noch wage ich es, interne Aufzeichnungen direkt als Beweise zu verwenden, aus Angst, aufgrund von Informationssicherheit erwischt zu werden. Ich glaube jedoch, dass viele meiner ehemaligen Kameraden für mich zeugen werden. Brüder innerhalb von Huawei, einschließlich der Produktlinien-Brüder, denen wir einst dienten, ich glaube, dass die unzähligen Details in diesem Artikel mit euren Eindrücken übereinstimmen und meine Aussagen bestätigen werden. Ihr wurdet vielleicht auch getäuscht, aber diese grausamen Wahrheiten werden nicht begraben werden. Die Spuren unseres Kampfes sollten nicht verzerrt oder begraben werden.

Nachdem ich so viel geschrieben habe, werden bestimmte Personen mich definitiv finden und zum Schweigen bringen wollen. Das Unternehmen möchte mich vielleicht auch zum Schweigen bringen oder zur Rechenschaft ziehen. Wenn das wirklich passiert, könnten meine persönliche Sicherheit und sogar die Sicherheit meiner Familie bedroht sein. Zum Selbstschutz werde ich in der nahen Zukunft täglich meine Sicherheit allen melden.

Wenn ich verschwinde, betrachtet es als ein Opfer für die Wahrheit und Ideale, für die bessere Entwicklung von Rechenleistung und KI in Huawei und sogar China. Ich bin bereit, an dem Ort begraben zu werden, an dem ich einst kämpfte.

Auf Wiedersehen, Noah.
Geschrieben in Shenzhen am 6. Juli 2025, in den frühen Morgenstunden.