---
audio: true
lang: de
layout: post
title: Künstliche Intelligenz Denken
translated: true
---

- Satya Nadella erwähnte das Jevons-Paradoxon. Es lohnt sich, es zu lernen.

- Yin Wang: Es gibt kein 'Intelligenz' im Bereich der künstlichen Intelligenz, kein 'neuronal' in neuronalen Netzen, kein 'Lernen' im maschinellen Lernen und kein 'Tiefe' im deep learning. Was in diesem Bereich wirklich funktioniert, nennt man 'Differentialrechnung'. Daher bevorzuge ich, diesen Bereich 'differenzierbare Berechnung' zu nennen, und den Prozess des Modellaufbaus 'differenzierbares Programmieren'.

- Yin Wang: Maschinelles Lernen ist wirklich nützlich, man könnte sogar sagen, es ist eine schöne Theorie, weil es einfach Differentialrechnung nach einer Umgestaltung ist! Es ist die alte und große Theorie von Newton, Leibniz, in einer einfacheren, eleganteren und mächtigeren Form. Maschinelles Lernen besteht im Wesentlichen darin, Differentialrechnung zu verwenden, um einige Funktionen abzuleiten und anzupassen, und deep learning besteht im Anpassen komplexerer Funktionen.

- Derzeit können große Sprachmodelle nicht nach Dateisprache wie YAML oder Python filtern. Allerdings ist ein erheblicher Teil der Informationen in der realen Welt auf diese Weise organisiert. Dies bedeutet, dass wir große Sprachmodelle mit Dateien trainieren könnten.

- Für das Training großer Sprachmodelle könnten wir ein System entwickeln, das genaue Übereinstimmungen findet. Vielleicht können wir den KMP-Algorithmus (Knuth-Morris-Pratt) mit der Transformer-Architektur kombinieren, um die Suchfunktion zu verbessern.