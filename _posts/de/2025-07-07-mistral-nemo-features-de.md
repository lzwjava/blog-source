---
audio: false
generated: true
lang: de
layout: post
title: 'Mistral NeMo: Leistungsstarkes 12-Milliarden-Parameter-Sprachmodell'
translated: true
type: note
---

**Mistral NeMo** ist ein leistungsstarkes Large Language Model (LLM) mit 12 Milliarden Parametern, das von **Mistral AI in Zusammenarbeit mit NVIDIA** entwickelt wurde. Es wurde entwickelt, um state-of-the-art Fähigkeiten in der natürlichen Sprachverarbeitung zu liefern und überzeugt insbesondere in seiner Größenkategorie in den Bereichen Reasoning, Weltwissen und Genauigkeit bei der Code-Generierung.

Hier sind einige wichtige Merkmale und Aspekte von Mistral NeMo:

*   **12 Milliarden Parameter:** Dies macht es zu einem relativ kompakten, aber dennoch äußerst leistungsfähigen Modell, das eine gute Balance zwischen Leistung und Effizienz bietet.
*   **Großes Kontextfenster:** Es verfügt über ein Kontextfenster von bis zu 128.000 Tokens, was es ihm ermöglicht, viel längere Texte, komplexe Dokumente und Mehrfachdialoge effektiver zu verarbeiten und zu verstehen.
*   **State-of-the-Art Leistung:** Mistral NeMo setzt neue Maßstäbe für Modelle seiner Größenklasse und zeigt starke Leistungen bei Aufgaben in den Bereichen Reasoning, allgemeines Weltwissen und Code-Generierung.
*   **Mehrsprachige Unterstützung:** Für globale Anwendungen konzipiert, beherrscht es viele Sprachen, darunter Englisch, Französisch, Deutsch, Spanisch, Italienisch, Portugiesisch, Chinesisch, Japanisch, Koreanisch, Arabisch und Hindi.
*   **Tekken Tokenizer:** Es verwendet einen neuen Tokenizer namens Tekken (basierend auf Tiktoken), der im Vergleich zu früheren Tokenizern effizienter natürliche Sprache und Quellcode über mehr als 100 Sprachen hinweg komprimiert.
*   **Function Calling:** Das Modell wurde auf Function Calling trainiert, was seine Fähigkeit verbessert, mit spezifischen programmatischen Funktionen zu interagieren und diese auf der Grundlage natürlicher Spracheingaben auszuführen.
*   **Quantization Awareness:** Durch das Training mit Quantization Awareness ermöglicht es eine FP8-Inferenz ohne Leistungseinbußen, was für einen effizienten Einsatz entscheidend ist.
*   **Open-Source-Lizenz:** Mistral NeMo wird unter der Apache-2.0-Lizenz veröffentlicht, was eine breite Übernahme, Anpassung und Integration durch Forscher und Unternehmen fördert.
*   **Einfache Integration:** Seine Standardarchitektur macht es einfach, es als Drop-in-Ersatz für Systeme zu verwenden, die bereits Mistral 7B nutzen.
*   **Zusammenarbeit mit NVIDIA:** Das Modell wurde auf der NVIDIA DGX Cloud AI-Plattform trainiert und nutzt NVIDIAs optimierte Hardware- und Software-Ökosystem, einschließlich TensorRT-LLM für beschleunigte Inferenzleistung und der NeMo-Entwicklungsplattform zum Erstellen benutzerdefinierter generativer KI-Modelle. Es ist auch als NVIDIA NIM Inference Microservice verfügbar.

Im Wesentlichen zielt Mistral NeMo darauf ab, ein vielseitiges, leistungsstarkes und effizientes LLM bereitzustellen, das auf einer einzelnen GPU laufen kann, um fortschrittliche KI-Fähigkeiten für eine breite Palette von Unternehmensanwendungen wie Chatbots, Zusammenfassungen, Sprachübersetzung und Code-Generierung zugänglicher zu machen.