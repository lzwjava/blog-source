---
audio: false
generated: true
image: false
lang: de
layout: post
model: x-ai/grok-4-fast
title: FineWeb NanoGPT GPU-Speicherleitfaden
translated: true
type: note
---

Sie trainieren ein kleines GPT (nanoGPT-Stil) auf dem **FineWeb**-Datensatz (der bereinigte 100B-Token-Version von Common Crawl, der zum Trainieren von LLaMA, Mistral, etc. verwendet wurde), aber Sie mÃ¶chten dies mit begrenztem GPU-Speicher tun (wahrscheinlich 16â€“24 GB, wie eine RTX 3090/4090 oder A6000).

Ihre aktuelle Konfiguration ist bereits recht konservativ und gut fÃ¼r eine einzelne Consumer-GPU, aber hier ist eine detaillierte Anleitung, **wie Sie die besten Parameter** fÃ¼r nanoGPT auf FineWeb auswÃ¤hlen, nach Wichtigkeit und Speicherbedarf geordnet.

### Empfohlene Konfigurationen nach GPU-Speicher

| GPU VRAM       | ModellgrÃ¶ÃŸe (Parameter) | n_layer | n_head | n_embd | batch_size | grad_acc | Effektive BS | block_size | Anmerkungen |
|----------------|---------------------|---------|--------|--------|------------|----------|--------------|------------|-------|
| 80 GB (A100/H100) | ~350M              | 12      | 12     | 768    | 64         | 8        | 512          | 1024       | Nahe am originalen GPT-2 Medium, trainiert gut |
| 48 GB (A6000/3090) | ~250â€“300M         | 12      | 12     | 672    | 32         | 16       | 512          | 1024       | Sehr guter Kompromiss |
| 24 GB (4090/3090) | ~160â€“200M          | 10      | 10     | 640    | 32         | 12â€“16    | 384â€“512      | 1024       | Ihr Sweet Spot |
| 16â€“20 GB (4080, 3090 Ti) | ~125M        | 8       | 8      | 512    | 32         | 12       | 384          | 1024       | Stabil |
| <16 GB (4070 Ti, etc.) | ~84M           | 6       | 6      | 384    | 16â€“32      | 16â€“24    | 256â€“512      | 512â€“1024   | Ihre aktuelle Konfiguration |

Ihre aktuelle Konfiguration (`6L 6H 384embd`) ist sicher, aber etwas klein. Sie kÃ¶nnen grÃ¶ÃŸer gehen.

### Beste Konfiguration fÃ¼r 24 GB GPU (RTX 4090 / 3090) auf FineWeb
Dies ist derzeit das beliebtes Setup in der nanoGPT-Community:

```python
out_dir = 'out-fineweb-160M'
eval_interval = 1000
eval_iters = 200
log_interval = 100
always_save_checkpoint = True

wandb_log = True
wandb_project = 'fineweb'
wandb_run_name = '160M-fineweb'

dataset = 'fineweb'
gradient_accumulation_steps = 16   # 32 * 16 = 512 effektive Batch-GrÃ¶ÃŸe
batch_size = 32
block_size = 1024                  # Wichtig: FineWeb wurde mit 1024+ trainiert

n_layer = 10
n_head = 10
n_embd = 640
dropout = 0.0                      # Kann spÃ¤ter 0.1 versucht werden
learning_rate = 6e-4               # Etwas hÃ¶her fÃ¼r kleinere Modelle
max_iters = 50000                  # ~50â€“100B Token insgesamt ist ideal
warmup_iters = 2000
lr_decay_iters = 50000
min_lr = 6e-5
beta2 = 0.99
```

â†’ Das sind ~160M Parameter, lÃ¤uft bequem auf einer 4090 mit ~20â€“22 GB VRAM-Nutzung.

### Noch besser: 200M+ Modell (wenn Sie 24 GB+ haben)
```python
n_layer = 12
n_head = 12
n_embd = 768    # â†’ ~350M Parameter (Original GPT-2 Medium GrÃ¶ÃŸe)
batch_size = 32
gradient_accumulation_steps = 16   # Effektive BS 512
block_size = 1024
learning_rate = 5e-4
max_iters = 60000
```
Viele trainieren dies erfolgreich auf einer einzelnen 4090.

### Wichtige Faustregeln fÃ¼r FineWeb + nanoGPT

1. **block_size = 1024** wird stark empfohlen  
   FineWeb wurde mit 1024 Kontext gefiltert und trainiert. Die Verwendung von 512 schadet der Perplexity mehr, als Sie denken.

2. **Effektive Batch-GrÃ¶ÃŸe â‰ˆ 512** ist der Sweet Spot  
   Original LLaMA verwendete ~4M Token pro Batch. FÃ¼r nanoGPT, 512 Sequenzen Ã— 1024 Token = ~0.5M Token/Batch â†’ nah genug am Scaling Law.

3. **Lernrate**  
   - ~100M Parameter: 6eâ€“8e-4  
   - ~350M Parameter: 5eâ€“6e-4  
   - ~770M Parameter: 3e-4  

4. **Trainieren Sie fÃ¼r mindestens 50B Token** (besser 100B+)  
   Mit 160M Modell:  
   - 50k Iterationen Ã— 32 Ã— 16 Ã— 1024 Token = ~83B Token  
   Gute Konvergenz.

5. **Verwenden Sie den offiziellen FineWeb-Datensatz (nicht openwebtext)**  
   Ihr Skript ist bereits korrekt:  
   FineWeb >> OpenWebText in QualitÃ¤t.  
   Verwenden Sie die 100B Teilmenge: `fineweb100B` oder volles `fineweb` (begrenzt durch Ihre Festplatte).

6. **Optional, aber hilfreich**
```python
compile = True          # PyTorch 2.0 Compile, ~20â€“30% schneller
bias = False            # Kein Bias in Linear-Layern (wie LLaMA)
weight_decay = 0.1
```

### Schneller Befehl zum Herunterladen von FineWeb (100B Version)
```bash
pip install datasets
python -c "
from datasets import load_dataset
ds = load_dataset('HuggingFaceFW/fineweb', name='sample-100BT', split='train')
ds.save_to_disk('data/fineweb100B')
"
```
Dann zeigen Sie nanoGPT auf diesen Ordner.

### Zusammenfassung: Aktualisieren Sie Ihre aktuelle Konfiguration
Ersetzen Sie Ihren aktuellen Modellteil durch dies (fÃ¼r 24 GB GPU):

```python
n_layer = 10
n_head = 10
n_embd = 640
batch_size = 32
gradient_accumulation_steps = 16
block_size = 1024
learning_rate = 6e-4
max_iters = 50000
```

Sie erhalten ein viel stÃ¤rkeres Modell mit fast keinen zusÃ¤tzlichen Kosten.

Die Leute erhalten **sehr gute 160Mâ€“350M Modelle** auf FineWeb mit diesen Einstellungen â€” oft Ã¼bertreffen sie die ursprÃ¼ngliche GPT-2 Leistung.

Lassen Sie mich Ihre genaue GPU wissen und ich kann Ihnen die optimale Konfiguration geben! ðŸš€