---
audio: false
generated: true
image: false
lang: de
layout: post
model: google/gemini-3-pro-preview
title: GPT-2-Textkodierung erkl√§rt
translated: true
type: note
---

Das sind gro√üartige Fragen dazu, wie Large Language Models (LLMs) wie GPT Text verarbeiten. Du hast die beiden Hauptschritte richtig erkannt: **Kodierung** (Tokenisierung) und **Einbettung**.

Hier ist eine Aufschl√ºsselung der GPT-2-Kodierung, ihres Vergleichs mit Word2Vec und warum LLMs Text zuerst kodieren m√ºssen.

---

## üî¢ GPT-2-Kodierung: Text zu Zahl

GPT-2 verwendet eine Tokenisierungsmethode namens **Byte Pair Encoding (BPE)**, speziell eine Byte-Level-Version. Ihr Zweck ist es, Rohtext in eine Sequenz von Zahlen, sogenannte **Token-IDs**, umzuwandeln, die das Modell verarbeiten kann.

### So funktioniert es
1.  **Anfangsvokabular:** Der Prozess beginnt mit einem Basisvokabular, das aus allen einzelnen Bytes (256 Zeichen) besteht, die in den Trainingsdaten (einem riesigen Textkorpus) vorkommen. Dies garantiert, dass **jeder Text** (in jeder Sprache/Schrift) kodiert werden kann, selbst wenn er v√∂llig neue oder seltene Zeichen enth√§lt, indem er auf seine Byte-Ebene heruntergebrochen wird.
2.  **Iteratives Zusammenf√ºhren (Trainingsphase):**
    * Der Tokenizer durchsucht wiederholt den gesamten Trainings-text, um das **am h√§ufigsten vorkommende Paar benachbarter Bytes/Tokens** zu finden.
    * Dieses Paar wird zu einem **einzelnen, neuen Token** zusammengef√ºhrt, und dieser neue Token wird dem Vokabular hinzugef√ºgt.
    * Dieser Schritt wird tausende Male wiederholt (GPT-2 hat ein Vokabular von 50.257 Tokens), bis die gew√ºnschte Vokabulargr√∂√üe erreicht ist.
3.  **Tokenisierung (Nutzungsphase):** Wenn du dem Modell einen neuen Satz gibst, verwendet der Tokenizer das gelernte Vokabular an Tokens und Zusammenf√ºhrungen. Er zerlegt den Text in die **l√§ngstm√∂glichen Subwort-Einheiten**, die er in seinem Vokabular finden kann.

### Das Ergebnis: Subwort-Einheiten
Dieser Subwort-Ansatz schafft einen Ausgleich zwischen:
* **Zeichen-Ebene:** Zu lange Sequenzen, schwer f√ºr das Modell, Bedeutung zu erfassen.
* **Wort-Ebene:** Zu viele Vokabeln und kann nicht mit Out-of-Vocabulary (OOV)-W√∂rtern umgehen (wie Tippfehler oder neue Namen).

BPE erzeugt Tokens, die sind:
* **Ganze gebr√§uchliche W√∂rter** (z.B. "the", "a", "is")
* **Gebr√§uchliche Wortteile (Subw√∂rter)** (z.B. "ing", "tion", "un")
* **Seltene W√∂rter** werden in kleinere, bekannte Subw√∂rter zerlegt (z.B. "unbelievable" $\rightarrow$ "un" + "believ" + "able"). Dies verhindert OOV-W√∂rter und ist effizient.

---

## üÜö Kodierung vs. Word2Vec: Hauptunterschiede

Du hast richtig erkannt, dass beide auf Text trainiert werden und beide Text in ein numerisches Format umwandeln, aber sie erf√ºllen unterschiedliche Rollen und erzeugen unterschiedliche Ausgaben:

| Merkmal | GPT-2-Kodierung (BPE-Tokenisierung) | Word2Vec / GloVe |
| :--- | :--- | :--- |
| **Ausgabetyp** | **Token-ID** (Eine einzelne, eindeutige **Ganzzahl** f√ºr einen Token) | **Word Embedding** (Ein dichter **Vektor** aus Gleitkommazahlen) |
| **Zweck** | **Tokenisierung:** Text in handhabbare Subwort-Einheiten zu zerlegen und sie einer **eindeutigen ganzzahligen ID** zuzuordnen. Dies ist der erste Schritt f√ºr ein LLM. | **Einbettung:** Die **Bedeutung** eines Wortes als statischen Vektor so darzustellen, dass √§hnliche W√∂rter √§hnliche Vektoren haben. |
| **Kontextabh√§ngig** | **Nein** (Die ID f√ºr "bank" ist dieselbe, unabh√§ngig vom Kontext). | **Nein** (Der Vektor f√ºr "bank" ist derselbe, egal ob es sich um ein Ufer oder eine Bank handelt). |
| **Umgang mit OOV**| **Ausgezeichnet.** Es kann jedes unbekannte Wort in bekannte Subw√∂rter/Bytes zerlegen. | **Schlecht.** Weist typischerweise einen "Unbekannt"-Vektor zu oder ben√∂tigt Erweiterungen auf Zeichenebene wie FastText. |

---

## üß† Warum zuerst Text kodieren, wenn LLMs Embeddings haben?

Das ist eine grundlegende Frage zur Funktionsweise von LLMs! Du hast recht, dass LLMs eine **Embedding-Schicht** haben, aber der Prozess umfasst zwei verschiedene Schritte:

### 1. Kodierung (Tokenisierung: Text $\rightarrow$ ID)
Die Transformer-Architektur (wie GPT) ist ein neuronales Netzwerk, das **ausschlie√ülich** mit Zahlen arbeitet. Es kann die Zeichenkette "cat" nicht direkt verarbeiten.
* **Text** (String) $\rightarrow$ **Tokenizer** $\rightarrow$ **Token-ID** (Ganzzahl)
* *Beispiel:* "Hello world" $\rightarrow$ `[15496, 995]` (Zwei Ganzzahlen)

Die Token-IDs dienen als **Nachschl√ºssel**.

### 2. Einbettung (Embedding-Schicht: ID $\rightarrow$ Vektor)
Der **Embedding-Block** eines LLM ist im Wesentlichen eine massive, trainierbare Nachschlagetabelle (eine Matrix), bei der die Token-ID als Index (Zeilennummer) verwendet wird.

* **Token-ID** (Ganzzahl) $\rightarrow$ **Embedding-Schicht Nachschlagen** $\rightarrow$ **Embedding-Vektor** (Gleitkommazahlen-Vektor)
* *Beispiel:* `995` (Die ID f√ºr " world") $\rightarrow$ Nachschlagen der Zeile 995 in der Embedding-Matrix $\rightarrow$ `[0.45, -0.12, 0.99, ...]` (Ein Vektor aus, sagen wir, 768 Zahlen).

### Warum nicht einfach Zufallszahlen f√ºr die Eingabe verwenden?

Wenn du deine Eingabe ohne Tokenisierung mit Zufallszahlen initialisieren w√ºrdest, w√ºrdest du auf schwerwiegende Probleme sto√üen:

1.  **Beliebigkeit/Verlust von Bedeutung:** Computer sind reine Rechenmaschinen. Wenn du "the" einfach die Zufallszahl 5 und "cat" die Zufallszahl 42 zuweist, haben diese Zahlen keine inh√§rente Verbindung oder Bedeutung, mit der das Modell beginnen k√∂nnte.
2.  **Vokabularverwaltung:** Wie w√ºrdest du "apple" vs. "apples" darstellen? Oder den Tippfehler "applle"?
    * Mit **Tokenisierung** werden sie zu separaten Token-IDs, und das Modell lernt ihre unterschiedlichen (aber verwandten) Bedeutungen.
    * Ohne Tokenisierung h√§ttest du keine standardisierte, universelle M√∂glichkeit, jede m√∂gliche Zeichenkette einer Eingabe zuzuordnen, die das Modell wiederholt erkennen k√∂nnte.

Die Token-ID (die Ganzzahl) ist ein **diskreter, eindeutiger Identifikator** f√ºr eine gelernte Texteinheit. Der **Embedding-Vektor** ist die **kontinuierliche, bedeutungsvolle numerische Darstellung** dieses Identifikators, die w√§hrend des massiven Trainingsprozesses gelernt wird. Das Training des Modells ist es, das die Werte in der Embedding-Matrix so anpasst, dass √§hnliche Tokens (wie "cat" und "dog") mit √§hnlichen Vektoren enden.



M√∂chtest du ein detaillierteres, schrittweises Beispiel dazu sehen, wie Byte Pair Encoding (BPE) Tokens aus einem Beispieltext erstellt?