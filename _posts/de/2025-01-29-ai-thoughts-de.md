---
audio: false
lang: de
layout: post
title: KI-Gedanken
translated: true
---

- Satya Nadella erwähnte das Jevons-Paradoxon. Es lohnt sich, es zu lernen.

- Yin Wang: Es gibt keine „Intelligenz“ in künstlicher Intelligenz, kein „neural“ in neuronalen Netzen, kein „Lernen“ im maschinellen Lernen und keine „Tiefe“ im Deep Learning. Was wirklich in diesem Bereich funktioniert, heißt „Analysis“. Daher bevorzuge ich es, dieses Feld „differenzierbares Rechnen“ zu nennen, und der Prozess des Modellbaus heißt „differenzierbares Programmieren“.

- Yin Wang: Maschinelles Lernen ist eine wirklich nützliche, man könnte sogar sagen schöne Theorie, denn es ist einfach Analysis in neuem Gewand! Es ist die alte und großartige Theorie von Newton, Leibniz, in einer einfacheren, eleganteren und leistungsfähigeren Form. Maschinelles Lernen ist im Grunde die Verwendung von Analysis, um einige Funktionen abzuleiten und anzupassen, und Deep Learning ist die Anpassung komplexerer Funktionen.

- Derzeit können große Sprachmodelle nicht nach Dateisprachen wie YAML oder Python filtern. Ein erheblicher Teil der Informationen in der realen Welt ist jedoch auf diese Weise organisiert. Das bedeutet, dass wir große Sprachmodelle mit Dateien trainieren könnten.

- Zum Trainieren großer Sprachmodelle könnten wir ein System entwickeln, das exakte Übereinstimmungen findet. Vielleicht können wir den KMP-Suchalgorithmus (Knuth-Morris-Pratt) mit der Transformer-Architektur kombinieren, um die Suchfunktionen zu verbessern.

- Es gibt keine technologischen Geheimnisse. Open Source wird alle Geheimnisse offenbaren, die streng bewahrt werden.

- KI wird viele Tools beeinflussen, auch indirekte. Die Leute sagen, sie bräuchten Figma nicht mehr, um Prototypen zu zeichnen, sie würden direkt zum Code gehen. Ich denke, Postman wird ähnlich sein; die Leute werden direkt Python oder andere Skripte verwenden, um APIs aufzurufen oder zu testen.

- Ein Grund, warum wir Postman oder Figma im KI-Zeitalter nicht verwenden, ist, dass ihre Funktionalitäten nicht über Text generiert werden können. Ihnen fehlt auch eine Befehl + K-Verknüpfung, um den Komponentenersatz auszulösen.

- Benutzeroberflächen werden im KI-Zeitalter zu einer Barriere. Warum Postman für das Testen von Anwendungen mit KI verbessern, wenn wir direkt die Requests-Bibliothek von Python oder andere Programmiersprachen zum Testen von Code verwenden können, da letztere von KI unterstützt werden?

- Warum Figma für die KI-gestützte UI-Erstellung verbessern, wenn die codebasierte UI-Generierung, verbessert durch KI, einen direkteren und potenziell leistungsstärkeren Ansatz bietet?

- LLMs werden zuerst textbezogene Anwendungen verändern, wie Google, Suchmaschinen, Texteditoren und Schreibwerkzeuge, Quizlet, Zendesk, DeepL, Medium, WordPress, Trello, Asana, Gmail, GitHub, Goodreads, Duolingo und Feedly.

- Umgekehrt ist es unwahrscheinlich, dass LLMs Technologien wie Git, Linux, ffmpeg, Mobiltelefone, Hardware, Browser, Betriebssysteme oder Sprach- und Videoanrufe revolutionieren werden. Diese Technologien sind codezentriert, und ihr Code wird nicht so leicht von KI generiert wie API-Testtools wie Postman.

- Technologien mit mehr Code sind schwer von KI zu revolutionieren, wie OpenOffice, MySQL, Mozilla Firefox, Chromium, VLC Media Player, Qt Framework, LLVM/Clang und GNOME. Wenn KI helfen könnte, diese Technologien zu entwickeln, würden sie nicht ersetzt werden. KI sollte helfen, bessere Technologien zu entwickeln, und dazu braucht KI mehr Rechenleistung, um Code derselben Größenordnung zu generieren.

- [Neue Plattformen, die von KI-Workflows angetrieben werden](./ai-workflow-en)

- [Die nächste Richtung von KI-Code-Editoren](./ai-code-en)

- [Wie ich gut im KI- und Blockchain-Zeitalter lebe](./ai-blockchain-en)
