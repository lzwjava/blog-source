---
audio: true
lang: de
layout: post
title: KI-Gedanken
translated: true
---

- Satya Nadella erwähnte das Jevons-Paradoxon. Es lohnt sich, es zu lernen.

- Yin Wang: Es gibt keine "Intelligenz" in künstlicher Intelligenz, kein "neuronal" in neuronalen Netzen, kein "Lernen" in maschinellem Lernen und kein "Tiefe" in tiefem Lernen. Was in diesem Bereich wirklich funktioniert, wird "Differentialrechnung" genannt. Daher bevorzuge ich, diesen Bereich "differenzierbare Berechnung" zu nennen und den Prozess des Modellbaus "differenzierbare Programmierung".

- Yin Wang: Maschinelles Lernen ist wirklich nützlich, man könnte sogar sagen, es ist eine schöne Theorie, weil es einfach Differentialrechnung in einem neuen Gewand ist! Es ist die alte und großartige Theorie von Newton und Leibniz in einer einfacheren, eleganteren und leistungsfähigeren Form. Maschinelles Lernen besteht im Wesentlichen darin, Differentialrechnung zur Ableitung und Anpassung einiger Funktionen zu verwenden, und tiefes Lernen ist die Anpassung komplexerer Funktionen.

- Derzeit können große Sprachmodelle nicht nach Dateisprache wie YAML oder Python filtern. Ein erheblicher Teil der Informationen in der realen Welt ist jedoch auf diese Weise organisiert. Das bedeutet, dass wir große Sprachmodelle mit Dateien trainieren könnten.

- Für das Training großer Sprachmodelle könnten wir ein System entwickeln, das exakte Übereinstimmungen findet. Vielleicht können wir den KMP-Algorithmus (Knuth-Morris-Pratt) mit der Transformer-Architektur kombinieren, um die Suchfähigkeiten zu verbessern.

- Es gibt keine technologischen Geheimnisse. Open Source wird alle gut gehüteten Geheimnisse offenlegen.