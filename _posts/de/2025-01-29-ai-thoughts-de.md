---
audio: false
lang: de
layout: post
title: Künstliche Intelligenz Gedanken
translated: true
---

- Satya Nadella erwähnte das Jevons-Paradoxon. Es lohnt sich, es zu lernen.

- Yin Wang: In "künstlicher Intelligenz" gibt es keine "Intelligenz", in "neuronalen Netzen" kein "Neuron", in "maschinellem Lernen" kein "Lernen" und in "tiefem Lernen" keine "Tiefe". Was in diesem Bereich wirklich funktioniert, nennt man "Differentialrechnung". Daher bevorzuge ich, diesen Bereich "differenzierbares Rechnen" zu nennen und den Prozess des Modellbaus "differenzierbares Programmieren".

- Yin Wang: Maschinelles Lernen ist wirklich nützlich, man könnte sogar sagen, eine schöne Theorie, weil es einfach Differentialrechnung ist, die einen Makeover bekommen hat! Es ist die alte und großartige Theorie von Newton und Leibniz in einer einfachereren, eleganteren und mächtigeren Form. Maschinelles Lernen besteht im Wesentlichen darin, Differentialrechnung zu verwenden, um Funktionen abzuleiten und anzupassen, und tiefes Lernen ist das Anpassen komplexerer Funktionen.

- Derzeit können große Sprachmodelle nicht nach Dateisprache wie YAML oder Python filtern. Ein erheblicher Teil der Informationen in der realen Welt ist jedoch auf diese Weise organisiert. Das bedeutet, dass wir große Sprachmodelle mit Dateien trainieren könnten.

- Zum Trainieren großer Sprachmodelle könnten wir ein System entwickeln, das exakte Übereinstimmungen findet. Vielleicht können wir den KMP-Algorithmus (Knuth-Morris-Pratt) mit der Transformer-Architektur kombinieren, um die Suchfähigkeiten zu verbessern.

- Es gibt keine technologischen Geheimnisse. Open Source wird alle gut gehüteten Geheimnisse offenlegen.

- [Neue Plattformen, die von AI-Workflows angetrieben werden](./ai-workflow-en)

- [Die nächste Richtung von AI-Code-Editoren](./ai-code-en)

- [Wie ich im Zeitalter von AI und Blockchain gut lebe](./ai-blockchain-en)