---
audio: false
generated: true
image: false
lang: ja
layout: post
model: x-ai/grok-4-fast
title: ResNetが深層学習に革命をもたらした
translated: true
type: note
---

ResNet（Residual Network）は、深層学習の歴史上最も重要なブレークスルーの一つです。2015年にMicrosoft ResearchのKaiming Heらによって導入され、驚異的なトップ5エラー率3.57%（当時の人間の性能を上回る）で同年のImageNetコンペティションを優勝しました。

ResNetがこれほど重要になった理由は以下の通りです：

1. **「劣化問題」（非常に深いネットワークにおける勾配消失/爆発）を解決した**  
   ResNet以前は、「20層がうまくいくなら、56層はもっとうまくいくはず」と考えられていました。実際には、層を積み重ねると精度が悪化しました。これは過学習ではなく、最適化自体が難しくなるためです。ResNetは100層以上のネットワーク（論文では152層、その後1000層以上）を訓練しても性能が向上することを実証しました。

2. **残差ブロック + スキップ接続（核心となるアイデア）**  
   層にH(x)を直接学習させる代わりに、ResNetは残差F(x) = H(x) − xを学習するため、ブロックはx + F(x)を計算します。  
   数学的には：  
   y = F(x, {W_i}) + x  
   この「恒等ショートカット」により、勾配がスキップ接続を介してネットワークを直接流れるため、極めて深いネットワークの訓練が可能になります。

3. **即時的かつ大規模な実用的インパクト**  
   - 2016年から2020年頃まで（そして現在も広く使用され）、ほぼすべての最先端視覚モデルの基盤となりました。  
   - 物体検出（Faster R-CNN + ResNet）、セグメンテーション（DeepLab, Mask R-CNN）、生成モデル（StyleGANなど）、動画、医療画像などに使用。  
   - ResNet-50、ResNet-101、ResNeXt、Wide ResNet、ResNet-RSなどの変種は、2025年現在でも標準的なベースラインです。

4. **「深層学習が本当に深くなる」時代全体を可能にした**  
   ResNet以前：ImageNetの最高モデルは約20–30層（VGG、GoogLeNet）。  
   ResNet以降：100–1000層以上が標準的に。この深さはより豊富な特徴を学習するために決定的でした。

5. **極めてシンプルなアイデアでありながら、革新的**  
   論文は非常に明快で、巧妙なトリック一つ（スキップ接続 + 要素ごとの加算）のみを追加しています。新しい活性化関数や損失関数はなく、純粋にアーキテクチャの洞察です。

要約すると：ResNetは、アーキテクチャを適切に設計すれば「より深い方がより良い」が実際に機能することを証明し、その後10年間にわたるほとんどすべてのコンピュータビジョンモデルのデフォルトの出発点となりました。

2025年現在、TransformerやConvNeXt/ViTベースのモデルがリーダーボードを支配している状況でも、ResNet-50は依然として最も一般的なベンチマークの基盤です。その理由は、高速で、よく理解されており、非常に信頼性が高いからです。

論文（今でも読む価値あり）：  
[Deep Residual Learning for Image Recognition](https://arxiv.org/abs/1512.03385)