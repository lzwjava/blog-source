---
audio: false
generated: true
lang: ja
layout: post
title: 長期短期記憶ネットワーク
translated: true
type: note
---

Long Short-Term Memory (LSTM)ネットワークは、リカレントニューラルネットワークの一種で、勾配消失問題に対処し、シーケンシャルデータにおける長期的な依存関係を学習できるように特別に設計されたアーキテクチャです。これは、**LSTMセル**と呼ばれるより洗練された内部構造を通じて実現されています。

以下に、LSTMセルの仕組みを詳しく説明します：

**核となるアイデア：セル状態**

LSTMの中心的な概念は、**セル状態**（多くの場合 'C<sub>t</sub>' と表記されます）です。セル状態は、シーケンス全体を通って流れるコンベアベルトのように考えてください。これは、シーケンスの長期的な履歴に関連する情報を運びます。情報は、**ゲート**と呼ばれる構造を介して、ネットワークを流れる際にセル状態に追加されたり、削除されたりします。

**ゲート**

LSTMセルには、情報の流れを調節する3つの主要なゲートがあります：

1.  **忘却ゲート:** このゲートは、前のセル状態からのどの情報を破棄すべきかを決定します。
    * 前の隠れ状態 (h<sub>t-1</sub>) と現在の入力 (x<sub>t</sub>) を受け取ります。
    * これらはニューラルネットワーク層を通り、その後**シグモイド活性化関数**に渡されます。
    * シグモイド関数は0から1の間の値を出力します。0に近い値は「この情報を完全に忘れる」ことを意味し、1に近い値は「この情報を完全に保持する」ことを意味します。
    * 数学的には、忘却ゲートの出力 (f<sub>t</sub>) は以下のように計算されます：
        ```
        f_t = σ(W_f * [h_{t-1}, x_t] + b_f)
        ```
        ここで：
        * σ はシグモイド関数です。
        * W<sub>f</sub> は忘却ゲートの重み行列です。
        * [h<sub>t-1</sub>, x_t] は前の隠れ状態と現在の入力の連結です。
        * b<sub>f</sub> は忘却ゲートのバイアスベクトルです。

2.  **入力ゲート:** このゲートは、現在の入力からのどの新しい情報をセル状態に追加すべきかを決定します。このプロセスには2つのステップが含まれます：
    * **入力ゲート層:** シグモイド層は、どの値を更新するかを決定します。
        ```
        i_t = σ(W_i * [h_{t-1}, x_t] + b_i)
        ```
        ここで：
        * σ はシグモイド関数です。
        * W<sub>i</sub> は入力ゲートの重み行列です。
        * [h<sub>t-1</sub>, x_t] は前の隠れ状態と現在の入力の連結です。
        * b<sub>i</sub> は入力ゲートのバイアスベクトルです。
    * **候補値層:** tanh層は、セル状態に追加される可能性のある新しい候補値（候補セル状態、'C̃<sub>t</sub>' と表記）のベクトルを作成します。tanh関数は-1から1の間の値を出力し、ネットワークの調節に役立ちます。
        ```
        C̃_t = tanh(W_C * [h_{t-1}, x_t] + b_C)
        ```
        ここで：
        * tanh は双曲線正接関数です。
        * W<sub>C</sub> は候補セル状態の重み行列です。
        * [h<sub>t-1</sub>, x_t] は前の隠れ状態と現在の入力の連結です。
        * b<sub>C</sub> は候補セル状態のバイアスベクトルです。

3.  **出力ゲート:** このゲートは、現在のセル状態からのどの情報を、現在のタイムステップの隠れ状態として出力すべきかを決定します。
    * 前の隠れ状態 (h<sub>t-1</sub>) と現在の入力 (x<sub>t</sub>) を受け取ります。
    * これらはニューラルネットワーク層を通り、その後**シグモイド活性化関数**に渡され、セル状態のどの部分を出力するかを決定します。
        ```
        o_t = σ(W_o * [h_{t-1}, x_t] + b_o)
        ```
        ここで：
        * σ はシグモイド関数です。
        * W<sub>o</sub> は出力ゲートの重み行列です。
        * [h<sub>t-1</sub>, x_t] は前の隠れ状態と現在の入力の連結です。
        * b<sub>o</sub> は出力ゲートのバイアスベクトルです。
    * 次に、セル状態は**tanh関数**を通り、値を-1から1の間に圧縮します。
    * 最後に、シグモイドゲートの出力と、セル状態に適用されたtanh関数の出力が要素ごとに乗算されます。これが新しい隠れ状態 (h<sub>t</sub>) となり、次のタイムステップに渡され、予測を行うためにも使用されます。
        ```
        h_t = o_t * tanh(C_t)
        ```

**セル状態の更新**

セル状態は、忘却ゲートと入力ゲートによってなされた決定に基づいて更新されます：

```
C_t = f_t * C_{t-1} + i_t * C̃_t
```

* 忘却ゲート (f<sub>t</sub>) は、前のセル状態 (C<sub>t-1</sub>) のどの程度を保持するかを決定します。f<sub>t</sub> が0に近い場合、情報はほとんど忘れられます。1に近い場合、情報はほとんど保持されます。
* 入力ゲート (i<sub>t</sub>) は、新しい候補値 (C̃<sub>t</sub>) のどの程度をセル状態に追加すべきかを決定します。i<sub>t</sub> が0に近い場合、新しい情報はほとんど無視されます。1に近い場合、新しい情報はほとんど追加されます。

**LSTMが勾配消失問題を解決する方法：**

LSTMが長期的な依存関係を扱う能力の鍵は、セル状態とゲート機構にあります。セル状態は、情報が多くのタイムステップにわたって、重みによる繰り返しの乗算（これは従来のRNNで勾配消失が起こる主な原因です）を受けることなく流れるための直接的な経路として機能します。

ゲートにより、LSTMはセル状態内の情報をいつ保存し、アクセスし、忘れるかを学習できます。この選択的記憶メカニズムにより、ネットワークは長いシーケンスにわたって関連する情報を保持し、必要時にそれを使用することが可能になり、勾配消失問題を効果的に軽減し、長期的な依存関係の学習を可能にします。

本質的に、LSTMはネットワークが長期間にわたって「記憶」を維持する方法を提供し、長距離の依存関係を含むシーケンシャルデータに関するタスクに非常に効果的です。