---
audio: false
generated: true
image: false
lang: ja
layout: post
model: x-ai/grok-4-fast
title: ランダムワードサラダのトレーニング
translated: true
type: note
---

もしnanoGPT（あるいはGPT-2 smallに似たトランスフォーマー）のような小さなモデルを、完全に**ランダムな英語風の単語**（つまり、実際の英語の単語を一様ランダムに選んだシーケンス、文法も意味もない単語の寄せ集め）で2〜3GB訓練すると、実際には以下のようなことが起こります：

### モデルは学習すべき**構造が存在しない**ことを学習する
- トークンの分布（単語のユニグラム頻度）を非常に素早く把握する
- 次の単語は基本的に前の単語から独立している（あるいはほぼ独立に近い）ことを理解する
- 訓練データに対するパープレキシティは極めて速く最低値に達する（数エポック後にはほぼ一様単語分布のエントロピーに近づく）
- モデルは本質的に**ユニグラムサンプラー**と化す — 訓練データ中の周辺頻度に従って単語を出力するだけで、コンテキスト依存性はほぼゼロになる

### 生成されるテキストは純粋なナンセンスよりはわずかにマシだが、それでも完全なゴミである
訓練後に得られるテキストの例：

> the cat quickly under yesterday apple banana elephant running purple however tomorrow cheese lightning quickly zebra sandwich forever mountain bicycle orange elephant yesterday banana

以下の特性を持つ：
- 実際の英語の単語を使用する ✓
- 単語の頻度を尊重する（「the」「and」「to」などの一般的な単語が頻繁に出現する） ✓
- 完璧なスペリング ✓
- 100%文法的・意味的に支離滅裂 ✗

以下は学習**しない**：
- 文法や構文
- 句構造
- 機能語の配置
- あらゆる種類の意味やトピックの一貫性
- 弱い統計的ノイズを超えたバイグラム/トライグラムさえ

### 実際のテキストでの訓練との比較
nanoGPTを実際のテキスト（シェイクスピアやOpenWebTextなど）で2〜3GB訓練すると、豊富で予測可能な構造が存在するため、驚くほど早く実際の文章が現れ始める。

純粋にランダムな単語では、どれだけ長く訓練しても一貫性の閾値を越えることはない。モデルは「次の単語は頻度に従ってランダム」という状態にほぼ即座に収束し、改善が止まる。

### 実際に試せる面白い実験
以下のようにして簡単にそのようなデータセットを生成できる：

```python
import random
words = open('big-wordlist.txt').read().splitlines()  # 例：30万の英語単語
with open('random_3gb.txt', 'w') as f:
    for _ in range(500_000_000):  # 約3GB
        f.write(random.choice(words) + ' ')
```

これを数時間nanoGPTで訓練すると、損失が約4〜5ナッツ（基本的に英語の単語分布のエントロピー）まで低下し、その後完全に平坦化するのがわかる。その後サンプリングすると、まさに上記のような単語の寄せ集めが得られる。

### 結論
はい、生成されるテキストはほぼ純粋なナンセンスになる — 単語を辞書から一様にサンプリングする場合よりわずかに「自然に見える」ナンセンスにはなる。なぜなら実際の単語頻度に一致するからだ。しかし、意味や文法に沿ったものを一切表現することはない。

これは、実際の言語がどれだけ長距離依存性と構造に依存しているかを証明する優れた方法である — なぜならそれらをすべて除去すると、GPTのような強力なアーキテクチャでさえ、本質的に役立つものを何も学習しないからだ。