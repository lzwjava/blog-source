---
audio: false
generated: true
lang: ja
layout: post
title: NVIDIAのAIとGPU革命
translated: true
type: note
---

NVIDIAは過去10年間でコンピューティングに革命をもたらし、主にゲーム向けのグラフィックスカード会社から、特に人工知能（AI）とハイパフォーマンスコンピューティング（HPC）における加速コンピューティングの強豪へと変貌を遂げました。

### NVIDIA GPUの簡単な歴史（過去10年）

過去10年で、NVIDIAのGPUは従来のグラフィックスレンダリングを超えて進化し、データセンターとAIに不可欠な存在となりました。主なマイルストーンは以下の通りです：

* **Pascalアーキテクチャ（2016年頃）：** GeForce GTX 10シリーズなどのカードで導入され、ゲームにおける大幅な性能向上をもたらし、Tesla P100によるディープラーニングへの注力の拡大も見られました。
* **Voltaアーキテクチャ（2017年）：** これはAIにとってゲームチェンジャーでした。VoltaベースのTesla V100は、ディープラーニングのトレーニングと推論に不可欠な行列乗算を加速するために設計された専用処理ユニットであるTensor Coreを導入しました。これにより、NVIDIAのAIハードウェア分野での支配力が確固たるものになりました。
* **Turingアーキテクチャ（2018年）：** GeForce RTX 20シリーズにより、TuringはコンシューマーGPUにリアルタイムレイトレーシングとDLSS（Deep Learning Super Sampling）をもたらし、Tensor Coreと新しいRT Coreを活用してより現実的なグラフィックスを実現しました。
* **Ampereアーキテクチャ（2020年）：** GeForce RTX 30シリーズとデータセンター向けのA100 GPU（Ampereベース）は、さらに限界を押し広げました。A100はV100のAI性能を大幅に改善し、より高いスループットとメモリ帯域幅を提供し、多くのAI研究と導入イニシアチブの主力となりました。
* **Ada Lovelaceアーキテクチャ（2022年）：** このアーキテクチャは、フラグシップのRTX 4090を含むGeForce RTX 40シリーズを駆動します。大幅に改善された性能、効率性、そして第4世代Tensor Coreと第3世代RT Coreによる強化されたAI機能を誇り、レイトレーシングとDLSS 3をさらに洗練させました。
* **Hopperアーキテクチャ（2022年）：** H100 GPUはHopper世代のフラグシップであり、大規模AIとHPCのために特別に設計されています。Ampereを基盤とし、さらに強力なTensor Core、LLM向けの専用Transformer Engine、そして大規模なスケーラビリティのためのNVLink Switch Systemを備えています。
* **Blackwellアーキテクチャ（2024年発表）：** NVIDIAの最新アーキテクチャであるBlackwellは、AIにおける次の大きな飛躍となることが期待されており、B200とGB200（Grace CPUとBlackwell GPUを組み合わせたもの）は、将来の大規模言語モデルのトレーニングと推論において前例のない性能を目指しています。

### 主要なNVIDIA GPU：H100とRTX 4090

* **NVIDIA H100 Tensor Core GPU：** これはNVIDIAの現在のトップティアデータセンターGPUであり、Hopperアーキテクチャに基づいています。特に大規模言語モデル（LLM）向けのAIとHPCワークロードのために特化して構築されています。H100は、前世代（A100）と比較して桁違いの性能向上をもたらし、高度なTensor Core、Transformer Engine、高帯域幅メモリ（HBM3/HBM3e）を特徴とします。大規模なスケーラビリティのために、NVIDIAのNVLink Switch Systemを介して接続された大規模クラスタに導入されるように設計されています。
* **NVIDIA GeForce RTX 4090：** これはAda LovelaceアーキテクチャからのフラグシップコンシューマーゲーミングGPUです。ゲームにおいて非常に強力（レイトレーシングとDLSS 3による超高性能と現実的なグラフィックスを提供）である一方、その基礎となるアーキテクチャと純粋な処理能力は、データセンター規模の導入は必要としないが、相当量のローカルGPUアクセラレーションを必要とする個人のクリエイター、AI開発者、研究者の間でも人気の選択肢となっています。24GBのGDDR6Xメモリと、膨大な数のCUDA Core、RT Core、Tensor Coreを誇ります。

### ビッグテックが近年使用しているもの

ビッグテック企業は、NVIDIAのハイエンドデータセンターGPU、特にA100および現在はH100に対する需要の主要な推進力です。彼らはより大きく、より洗練されたAIモデルを構築する競争をしており、NVIDIAのGPUはこれに必要な比類のないコンピューティングパワーを提供します：

* **Microsoft：** Azureクラウドサービスと自社のAI開発（大規模言語モデルを含む）のために、NVIDIA GPUの主要な消費者です。
* **Google (Alphabet)：** Google Cloud Platformや自社のAI研究（例えば、Geminiのようなモデルのトレーニング）において、特にNVIDIA GPUを利用しています。Googleは自社のカスタムAIチップ（TPU）も開発していますが、より広範なAIインフラストラクチャでは依然としてNVIDIAに大きく依存しています。
* **Amazon (AWS)：** 大規模な顧客であり、AWSクラウドサービスにおいてNVIDIA GPUを活用し、幅広いクライアントにAIとHPCサービスを提供しています。
* **Meta Platforms：** 自社のさまざまなプラットフォーム向けの大規模言語モデルのトレーニングを含む、AIへの野望を支えるためにNVIDIA GPUに多額の投資をしています。
* **Oracle：** 同樣に重要な購入者であり、NVIDIAの強力なGPUを備えたクラウドサービスを拡大しています。

これらの企業は、自社のAIスーパーコンピュータとインフラストラクチャを構築するためにしばしば数万個のGPUを購入し、またこれらのGPUへのアクセスをクラウド顧客にサービスとして提供しています。

### クラウドプラットフォームでの選択肢

主要なクラウドプロバイダーは、NVIDIA GPUの多様なアレイをサービスとして提供しており、企業や研究者が多額の初期ハードウェア投資を必要とせずに強力なコンピューティングリソースにアクセスできるようにしています。これらのプラットフォームは通常、様々なGPUタイプを提供し、使用量に基づいた価格設定を行っています：

* **Amazon Web Services (AWS)：** NVIDIA A100およびH100 GPU（例：p5インスタンス）や、A10のような旧世代を含む、多様な選択肢を提供します。
* **Google Cloud Platform (GCP)：** 生成AIに理想的な最新のL4 GPU、H100、A100、T4、V100など、幅広いNVIDIA GPUへのアクセスを提供します。彼らはしばしば新しいNVIDIAハードウェアを迅速に利用可能にします。
* **Microsoft Azure：** 様々な機械学習およびHPCワークロード向けに、NVIDIA A100、H100、その他A10などのGPUを特徴とします。
* **Oracle Cloud Infrastructure (OCI)：** H100を含む強力なNVIDIA GPUの提供を増やしています。
* **その他の専門的なクラウドGPUプロバイダー：** CoreWeave、Lambda、Paperspace、DigitalOcean、E2E Cloudなどの企業は、GPU加速クラウドコンピューティングを専門としており、しばしば競争力のある価格を提供し、最新のH100、A100、RTX A6000/RTX 6000 Adaを含む幅広いNVIDIA GPUを提供しています。

これらのクラウドプラットフォームは、必要に応じてコンピュートリソースをスケールアップまたはスケールダウンする柔軟性を提供し、高性能なNVIDIA GPUをより広い範囲のユーザーが利用できるようにしています。