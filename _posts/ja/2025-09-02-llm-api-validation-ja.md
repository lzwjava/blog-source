---
audio: false
generated: true
image: false
lang: ja
layout: post
title: 安全なLLM API検証レイヤー
translated: true
type: note
---

**LLM API**を構築または使用する際には、一般的に、レスポンスが安全で有効かつ有用であることを保証するための保護策が必要です。チェックを追加する主な方法は以下の通りです：

---

### 1. **入力検証**

*   **スキーマチェック**: リクエストJSONが期待されるスキーマに従っていることをLLMに送信する前に確認します。
*   **プロンプトのサニタイズ**: 望ましくない文字、悪意のある指示、インジェクション攻撃の試みを取り除く、またはエスケープします。
*   **長さチェック**: 不必要なコストや切り捨てを避けるため、入力に対する最大トークン数/文字数の制限を実施します。

---

### 2. **出力検証**

*   **JSONスキーマ検証**: LLMがJSONを返すことになっている場合、`json.loads()`で処理し、スキーマに対して検証します（例: `pydantic`, `jsonschema`を使用）。
*   **正規表現/フォーマットチェック**: メールアドレス、URL、数値に対して、パターンを強制します。
*   **型チェック**: フィールドが正しい型（文字列、整数、リストなど）であることを確認します。
*   **範囲チェック**: 数値や日付の値が期待される範囲内にあることを確認します。

---

### 3. **安全性とコンテンツのチェック**

*   **有害性・不適切表現フィルター**: 出力を分類器で処理します（例: Perspective API, OpenAI moderation API）。
*   **ポリシーフィルター**: 特定のキーワードやカテゴリを含むレスポンスをブロックするルールを定義します。
*   ** hallucination 検出**: 事実確認のステップを追加します（検索拡張チェック、複数モデルによる相互検証、ルールベースのサニティチェックを経由）。

---

### 4. **ビジネスロジックの制約**

*   **一貫性ルール**: レスポンスがシステムデータ（例: 利用可能な残高、設定値）と一致しなければならない場合、ユーザーに返す前に照合検証します。
*   **ガードレール**: **Guardrails AI**, **LMQL**, **Instructor** などのライブラリを使用して、構造化された出力と制約を宣言的に強制します。
*   **後処理フック**: LLMのレスポンスをカスタムロジックでラップします（大文字小文字の正規化、デフォルト値の強制、予期しない回答の拒否）。

---

### 5. **冗長性と相互検証**

*   **自己チェックプロンプト**: LLM自身に回答を再検証または説明するように依頼します。
*   **デュアルモデルチェック**: より小型/高速なLLMを使用して、メインモデルの出力を検証します（例: 「このレスポンスはスキーマXに準拠していますか？」）。
*   **投票/合意**: 複数の呼び出しを実行し、回答を比較して安定性を確認します。

---

### 6. **運用上の保護策**

*   **レート制限**: APIの悪用を防ぎます。
*   **タイムアウト**: ハングしたリクエストを中止します。
*   **ロギングと監査証跡**: リクエストとフィルタリングされたレスポンスの記録を保持します。
*   **フォールバック**: 検証が失敗した場合、調整されたプロンプトで再試行するか、安全なデフォルト/エラーを返します。

---

👉 実際には、優れたLLM APIスタックはこれらを層状に構成します：

1.  入力を検証。
2.  LLMを呼び出し。
3.  スキーマとビジネスルールに対して出力を検証。
4.  安全性/コンテンツフィルターを実行。
5.  チェックが通過した場合のみ返信（そうでない場合は再試行/フォールバック）。

---

**Pythonの例**で、LLM API呼び出しをJSONスキーマ検証とモデレーションチェックでラップする方法をお見せしましょうか？