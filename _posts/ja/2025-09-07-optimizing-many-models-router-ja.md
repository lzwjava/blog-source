---
audio: false
date: 2025-09-07
generated: true
image: false
lang: ja
layout: post
title: 最適化されたマルチモデルコードルーター
translated: true
type: note
---

短い答え: はい。あなたの「多数モデルルーター」は、Codex（OpenAIのコードアシスタント）のような、単一のモデルファミリーでエンドツーエンドに調整され、深いIDEコンテキストを持つ緊密に統合されたスタックと比較して、確かにパフォーマンスが低くなる可能性があります。その理由と迅速な修正方法を以下に示します。

### Codexが優れていると感じられる理由

*   **タスクごとの単一頭脳。** 単一モデルに留まることで、その潜在的な計画とコーディングスタイルが維持されます。タスク途中でモデルを切り替えると、その計画がリセットされ、しばしば矛盾する規約が導入されます。
*   **垂直統合。** Codex（OpenAI）は、モデル → ツール呼び出し → リポジトリインデックス → テスト → インライン編集を制御しています。このクローズドループにより、より豊富なシグナル（シンボルグラフ、ファイルヒートマップ、失敗したテスト）を、グルーコードによる遅延を少なくして得られます。
*   **プロンプトとポリシーの整合性。** 彼らのプロンプト、コードフォーマッター、および「最小限でコンパイル可能な差分を作成する」ヒューリスティックは、GPT-5ファミリー向けに共同設計されています。一般的なルーターは、一部のモデルに対して温度、停止シーケンス、またはパッチ形式を誤って指定しやすいです。
*   **レイテンシ/スループット。** 追加されるホップ（プロキシ、OpenRouterミドルウェア、モデルネゴシエーション）ごとにジッターが増加します。コーディングワークフローはフィードバックが密です。ターンごとに300〜800 msのレイテンシが追加されると、「流れ」が顕著に損なわれます。
*   **コンテキストの質。** リポジトリマップ（ファイル、シンボル、最近の変更のトポロジ）を計算するIDE統合は、「単に長いコンテキストをダンプする」よりも優れています。構造のない長いコンテキストはトークンを浪費し、注意力を希薄にします。

### あなたの設定で問題を引き起こしている可能性が高いもの

*   **モデルの乱立。** あなたはジェネラリスト、コーダー、思考モデルを混在させています。「思考」バリアント（例: `claude-3.7-sonnet:thinking`, `deepseek-r1`）は証明には優れていますが、コード編集には遅く、冗長です。
*   **デフォルトルートの不一致。** `default: "openrouter,x-ai/grok-code-fast-1"` は Grok Code Fast を使用したいように見えますが、あなたの `models` 配列にリストされていません。これはサイレントなフォールバックと不整合を引き起こす可能性があります。
*   **スコープが定義されていないインテント。** すべてに対して単一の「デフォルト」を使用することは、小さな編集、大規模なリファクタリング、長いコンテキストの読み取りがすべて同じ経路を争うことを意味します。
*   **温度/フォーマットのずれ。** モデルごとに低温＋厳格なパッチ形式を強制しない場合、出力はプロバイダー間で大きくばらつきます。

### あなたのルーターを「Codex的」に感じさせる方法

1.  **主要モデルを選択し、タスクごとに固定する。** 強力なコーダーモデルを1つデフォルトとして選択し（例: `openai/gpt-5` または `x-ai/grok-code-fast-1` または `qwen/qwen3-coder`）、明確な理由（非常に長いコンテキストまたは高度な数学）がある場合にのみ切り替えます。
2.  **ブランドではなくインテントによって振り分ける。**

    *   *小さな編集/クイックフィックス:* 高速モデル (GPT-5-mini または Gemini-Flash)。
    *   *リファクタリング/マルチファイル変更:* GPT-5 (または Claude Sonnet 3.7 非思考モード)。
    *   *超長コンテキスト読み取り:* Kimi-K2。
    *   *コーディング前の難しい推論:* DeepSeek-R1 でアウトラインを作成 → コーダーモデルに引き継いでパッチを生成。
3.  **厳格なパッチ契約を強制する。** 常に unified diff または明示的な「ApplyPatch」JSON（ファイルパス＋ハンクを含む）を要求します。それ以外は拒否し、自動的に再プロンプトします。
4.  **コードのランダム性を低くする。** `temperature: 0–0.2`、frequency/presence penalties なし、`top_p` は ~0.9 に制限。
5.  **単なるトークンではなく、リポジトリマップを提供する。** コンパクトなシンボルインデックス（エントリーポイント、主要なクラス/関数、テストターゲット、最近の差分）を投入します。これは200kトークンの生ファイルをダンプするよりも優れています。
6.  **ツールループを密にする。** 1つのモデルがパッチを提案 → あなたがビルド/テスト/リンターを実行 → 失敗を*同じ*モデルにフィードバックして反復します。ループ途中でのモデル切り替えは避けてください。
7.  **モデルリストを削減する。** 最大3〜5個に保ちます。追加するごとに分散とルーティングエラーが増加します。
8.  **ホットパスから「思考」モデルを削除する。** それらは短い設計計画を作成するためだけに使用し、実際の編集ではコーダーモデルに切り替えてください。

### よりリーンな設定 (導入のアイデア; 名前はあなたのフレームワークに合わせて調整)

```json
{
  "PROXY_URL": "http://127.0.0.1:7890",
  "LOG": true,
  "Providers": [
    {
      "name": "openrouter",
      "api_base_url": "https://openrouter.ai/api/v1/chat/completions",
      "api_key": "",
      "models": [
        "openai/gpt-5",
        "openai/gpt-5-mini",
        "x-ai/grok-code-fast-1",
        "moonshotai/kimi-k2",
        "anthropic/claude-3.7-sonnet",
        "qwen/qwen3-coder"
      ],
      "transformer": {"use": ["openrouter"]}
    }
  ],
  "Router": {
    "default": "openrouter,openai/gpt-5",
    "quickEdit": "openrouter,openai/gpt-5-mini",
    "refactor": "openrouter,openai/gpt-5",
    "reasonPlan": "openrouter,anthropic/claude-3.7-sonnet",
    "longContext": "openrouter,moonshotai/kimi-k2",
    "longContextThreshold": 220000,
    "webSearch": "openrouter,qwen/qwen3-coder"
  }
}
```

### 出力を安定させるプロンプトスキャフォールド (テーブルなし)

*   **システム:** 「あなたはシニアソフトウェアエンジニアです。最小限でコンパイル可能なパッチを生成してください。スタイルはリポジトリと一貫性を保ってください。不明な点がある場合は、単一の明確化する質問をし、それ以外は進めてください。」
*   **ルール:** 「unified diff (git format) **のみ**を出力してください。解説を含めないでください。ファイルが存在しない場合は作成し、差分に含めてください。ライセンスヘッダーは保持してください。」
*   **コンテキストブロック:** リポジトリマップ（ファイル、主要シンボル）、現在のエラー、テストターゲット。
*   **ツールループ:** 差分後に自動的に `build + tests + lints` を実行し、失敗のみを同じモデルに返します。

### 実践的な習慣

*   モデルリストを絞り込み、デフォルトを `models` に実際に存在するモデルに修正する。
*   ルートごとの温度を設定する。コードは0–0.2に保つ。
*   リポジトリマップをターン間でキャッシュする。ファイルが変更されない限り、巨大なコンテキストを再送信しない。
*   タスク途中でのモデル切り替えを避ける。
*   「このファイル全体を置き換え」よりも「差分」を優先し、幻覚的なインポートを減らす。

もしよろしければ、あなたのルーターのプロンプトと温度設定を貼り付けてください。各ルートに対してそれらを調整し、Claude Codeとあなたのスタックが、Codex的な信頼性と一貫性を持つようにします。