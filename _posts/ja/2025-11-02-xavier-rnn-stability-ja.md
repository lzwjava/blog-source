---
audio: false
generated: true
image: false
lang: ja
layout: post
model: x-ai/grok-4-fast
title: リカレントニューラルネットワークの安定性のためのザビエル初期化
translated: true
type: note
---

### RNNにおけるXavier初期化の重要性

RNN（リカレントニューラルネットワーク）は、安定した訓練ダイナミクスを維持するために、Xavier（Glorotとも呼ばれる）初期化を重み行列に依存しています。具体的には、`Wxh`（入力から隠れ層へ）、`Whh`（隠れ層から隠れ層へ、再帰重み）、`Why`（隠れ層から出力へ）といった重み行列に対して、ガウス（正規）分布を`sqrt(1 / fan_in)`でスケーリングしたものを使用します。ここで、`fan_in`はその層への入力ニューロンの数です。これにより、重みの分散がおよそ`1 / fan_in`に保たれ、入力信号が過度に増幅または圧縮されるのを防ぎます。

以下に、これがRNNにとって重要であり、単純な一様乱数[0, 1]では問題が生じる理由を説明します：

#### 1. **層と時間ステップ間での信号分散の維持**
   - フィードフォワードネットワークでは、Xavier初期化は、信号が前方に伝播（そして勾配が後方に伝播）する際に*活性化の分散*をほぼ一定に保つのに役立ちます。これがないと、深い層では活性化が爆発（非常に大きくなる）または消失（ほぼゼロになる）し、訓練が不可能になります。
   - RNNは、時間的に*展開された*「深い」ネットワークのようなものです：再帰重み`Whh`は各時間ステップで隠れ状態を乗算し、乗算の連鎖（例えば、系列長*T*に対して、*T*層の深さに相当）を生成します。`Whh`の重みの分散が>1の場合、勾配は時間を遡って指数関数的に爆発します（長い系列では問題）。<1の場合、勾配は消失します。
   - Xavierのスケーリング（例えば、`Whh`に対して`* sqrt(1. / hidden_size)`）は、隠れ状態の期待分散を~1に保ち、これを防ぎます。[0,1]の一様初期化の場合：
     - 平均~0.5（正に偏り、ドリフトを引き起こす）。
     - 分散~1/12 ≈ 0.083——大きな`hidden_size`（例：512）に対しては小さすぎ、信号が急速に消失します。

#### 2. **層の次元への適応**
   - Xavierは、*fan_in*（層への入力数）と時には*fan_out*（出力数）を考慮します。`Wxh`では、`sqrt(1 / input_size)`によるスケーリングは、語彙サイズや埋め込み次元に基づいて正規化します。`Whh`では、再帰ループに合わせて`hidden_size`に基づきます。
   - [0,1]は次元を無視します：大きな隠れ層（例：1000ユニット）では、スケーリングなしに多数の[0,1]入力を合計すると、次の層が圧倒されます。小さな層では、アンダーフローを起こします。

#### 3. **時間方向の誤差逆伝播法における勾配の流れ**
   - RNNの訓練はBPTTを使用し、勾配は*T*ステップにわたって後方に流れます。不適切な初期化は、「勾配消失問題」（素のRNNで一般的であり、LSTM/GRUが存在する理由）を引き起こします。Xavierは、バランスの取れた大きさで開始することでこれを緩和します。
   - 実証研究（例：Glorot & Bengioの原論文）では、Xavierが深い/再帰的なネットワークにおける収束に必要なエポック数を2-10倍減少させることが示されています。

#### 代替案と[0,1]が「機能する」可能性がある場合
- [0,1]は、非常に小さなトイモデル（例：単層、短い系列）では「機能する」可能性がありますが、スケールでは失敗します——損失の爆発やNaNを考えてください。
- 現代的な調整：He初期化（ReLU用、`sqrt(2 / fan_in)`でスケーリング）または直交初期化（RNN用、ノルムを正確に保持）。しかし、XavierはRNNで一般的なsigmoid/tanh活性化関数に対する安全なデフォルトです。

要するに、Xavierは*安定性を最優先*します：それは、勾配が自由に流れるように、重みを小さすぎず大きすぎない、次元を考慮した値に設定します。ランダムな[0,1]はあまりにも粗雑です——バランスを取っていないタイヤで車をフロアリングするようなものです。

より深く知るために：  
[Understanding the Difficulty of Training Deep Feedforward Neural Networks](http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf) (Glorot & Bengio, 2010)  
[Recurrent Neural Network Regularization](https://arxiv.org/abs/1409.2329) (Zaremba et al., RNNの初期化問題について)