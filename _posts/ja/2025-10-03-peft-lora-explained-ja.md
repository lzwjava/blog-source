---
audio: false
generated: true
image: false
lang: ja
layout: post
model: x-ai/grok-4-fast
title: PEFTとLoRAの構造的解説
translated: true
type: note
---

以下に、提供されたテキストの詳細で構造化された説明を提供します。基礎概念から始め、段階的に構築して、Parameter Efficient Fine-Tuning (PEFT) と Low-Rank Adaptation (LoRA) の詳細に至るまで解説します。類推、数学的直感、実世界の文脈を用いて、アクセスしやすくしながらも徹底的に説明します。この説明は、大規模言語モデル (LLM) における機械学習 (ML) の根底にあるアイデア、動機、含意を拡張しながら、提供されたテキストに直接基づいています。

### 1. 現代の言語モデルの規模：事前学習とその重要性
テキストは、今日の主要な LLM の莫大な規模を強調することから始まっています：「今日の主要な言語モデルは、1兆を超えるパラメータを含み、数十兆のトークンで事前学習されています。ベースモデルのパフォーマンスは規模とともに向上し続けており、これらの兆単位のパラメータは、書き留められた人類の知識のすべてのパターンを学習し表現するために必要です。」

#### パラメータとトークンとは何か？
- **パラメータ** は、ニューラルネットワーク内の「重み」です。これは、モデルが学習中に学習する数値です。モデルの「記憶」または「知識のつまみ」と考えてください。1兆パラメータのモデル (例: GPT-4 や PaLM) には約1,000億のそのような値があり、これは数百万枚の高解像度画像のデータストレージにほぼ相当します。
- **トークン** は、モデルが処理するテキストの基本単位です (例: 単語またはサブワード)。事前学習では、モデルに**数十兆**のこれらのトークン (例: 書籍、ウェブサイト、コードリポジトリから) を供給して、文法、事実、推論などの一般的なパターンを学習させます。

#### なぜ規模がパフォーマンスを向上させるのか？
- LLM はトランスフォーマーベースのアーキテクチャ (2017年の論文「Attention is All You Need」で紹介) であり、注意メカニズムとフィードフォワードネットワークの層を通じて複雑なパターンを捕捉することに優れています。
- 経験的なスケーリング則 (例: OpenAI の Kaplan et al., 2020) は、パフォーマンス (例: 質問応答などのタスクにおける精度) が、より多くのパラメータ、データ、計算資源とともに予測可能に向上することを示しています。パラメータを倍増させることは、「創発的能力」 (例: モデルが突然数学や翻訳が得意になる) における対数的な向上をもたらすことがよくあります。
- **直感**: 人類の知識は広大で相互接続されています。それをすべて表現するためには (例: あらゆる言語の構文、歴史的事実、科学原理)、モデルはこれらを低レベルの相関としてエンコードするための巨大な「パラメータ空間」を必要とします。より小さなモデル (例: 10億パラメータ) は表面的なパターンに過学習し、微妙なタスクで失敗しますが、兆規模のモデルはよりよく一般化します。
- **トレードオフ**: この規模には大規模な計算資源 (例: 数千の GPU を数週間) とエネルギーが必要ですが、Llama や GPT シリーズのような「ベースモデル」の基礎となります。

要するに、事前学習は、人類の書き言葉のコーパスからパターンを力ずくで学習することにより、汎用目的の「脳」を構築します。テキストは、これを専門化前のベースラインとして強調しています。

### 2. 事後学習 (ファインチューニング)：狭い焦点と効率性の課題
テキストは、事前学習と「事後学習」を対比させています。事後学習は「より小さなデータセットを含み、一般に知識の狭い領域と行動の範囲に焦点を当てます。1テラビットの重みを使用して、1ギガビットまたはメガビットのトレーニングデータからの更新を表現するのは浪費のように思えます。」

#### 事後学習/ファインチューニングとは何か？
- 事前学習後、ベースモデルはより小さなタスク固有のデータセット (例: 1-1000万例 vs. 数兆トークン) で「ファインチューニング」されます。これにより、チャットボット (例: 指示追従)、感情分析、医療Q&Aなどのアプリケーションに適合させます。
- 例: カスタマーサポートのログで GPT-3 をファインチューニングして親切なアシスタントを作成する、または法律文書で契約レビューのためにファインチューニングする。
- **なぜより小さなデータセットなのか？** ファインチューニングは、一般的な言語理解を再発明することなく、基本知識への「更新」または「上書き」—例えば、丁寧さやドメイン固有の専門用語を教える—を対象とします。

#### 浪費の直感
- **データとモデルサイズの不一致**: ベースモデルが約1兆パラメータ (1パラメータあたりほぼ1ビットなのでテラビット規模) を持つが、ファインチューニングデータが小さい (ギガビットまたはメガビット規模) 場合、*すべての*パラメータを更新することは、1つの脚注のために百科事典全体を書き直すようなものです。モデルの重みの大部分は新しいタスクに関係ないままです。
- **完全ファインチューニング (FullFT) の問題点**:
  - **計算オーバーヘッド**: すべてのパラメータを更新するには、各トレーニングステップでモデル全体の勾配 (誤差信号) を再計算する必要があります。これによりメモリと時間コストが10-100倍増加します。
  - **破滅的忘れ**: FullFT はモデルの一般的な能力を低下させる可能性があります (例: 数学にチューニングされたモデルが詩を忘れる)。
  - **ストレージの肥大化**: ファインチューニングされたモデルはベースと同じ大きさ (兆単位のパラメータ) であるため、デプロイが高価になります (例: クラウドコストはサイズに比例してスケール)。
- **類推**: 単独のパフォーマンスのためにすべての音楽家を再訓練して巨大なオーケストラを調整することを想像してください。ソリストをコーチすれば済むところで、やりすぎです。

この非効率性が **Parameter Efficient Fine-Tuning (PEFT)** の動機となりました：パラメータのごく一部 (例: 0.1-1%) のみを更新しながら、FullFT のパフォーマンス向上の90-100%を達成する方法です。

### 3. Parameter Efficient Fine-Tuning (PEFT)：核心となるアイデア
「PEFT...は、はるかに小さなパラメータのセットを更新することによって、大規模なネットワークを調整します。」

- **核心的な動機**: ベースモデルの強みを保持しながら、最小限の変更でタスク固有の更新を注入します。これにより、計算、メモリ、ストレージが削減され、AIの民主化 (例: 小規模なチームがスーパーコンピュータなしで Llama 2 のようなモデルをファインチューニングできるようにする) に不可欠です。
- **一般的な PEFT 技術** (後述の LoRA 以外):
  - **アダプター**: トランスフォーマー層間に小さな「プラグイン」モジュール (例: ボトルネック層) を挿入し、それらのみをトレーニングします。
  - **プロンプトチューニング**: 入力に付加されるソフトプロンプト (例: 仮想トークン) を学習し、約0.01%のパラメータのみを更新します。
  - **プレフィックステューニング**: 同様ですが、注意層のプレフィックスを調整します。
- **なぜ機能するのか**: ファインチューニングの更新はしばしば「低次元」です—それらは完全なパラメータ空間の部分空間に存在します。すべてを微調整する必要はなく、いくつかの標的を絞った変更がネットワークを通じて伝播します。
- **経験的成功**: PEFT 手法は、GLUE (自然言語理解) などのベンチマークで、計算量を10-100倍少なくして FullFT に匹敵するかそれを上回ります。Hugging Face の PEFT ライブラリのようなライブラリにより、これがプラグアンドプレイになります。

PEFT は、「すべてをトレーニングする」から「外科的に編集する」へとパラダイムを転換し、テキストの効率性のテーマに沿っています。

### 4. Low-Rank Adaptation (LoRA)：主要な PEFT 手法
「主要な PEFT 手法は、Low-Rank Adaptation、または LoRA です。LoRA は、元のモデルの各重み行列 W を、修正版 W′ = W + γ B A で置き換えます。ここで、B と A は一緒になって W よりもはるかに少ないパラメータを持つ行列であり、γ は定数のスケーリング係数です。事実上、LoRA はファインチューニングによってもたらされる更新の低次元表現を作成します。」

#### 数学的詳細
LoRA は、トランスフォーマー内の重み行列 **W** (例: 注意やフィードフォワード層におけるクエリ/キー/値射影) を標的とします。これらは通常 d × k 行列です (例: 4096 × 4096、各々数百万パラメータ)。

- **公式**: ファインチューニング中、W を直接更新する代わりに、LoRA は出力を次のように計算します:
  ```
  h = W x + γ (B A) x  (ここで x は入力)
  ```
  - **W**: 凍結された元の重み (変更なし)。
  - **A**: 低ランク行列、ランダムに初期化 (例: r × k、ここで r << d、r=8-64 など)。
  - **B**: 別の低ランク行列 (d × r)、ゼロに初期化 (これにより初期更新はゼロとなり、破壊を回避)。
  - **γ (ガンマ)**: スケーリング係数 (例: γ = α / r、ここで α は 16 のようなハイパーパラメータ) で、更新の大きさを制御しトレーニングを安定化。
  - 完全な更新された重み: **W' = W + γ B A**。

- **なぜ「低ランク」なのか？**
  - 行列は特異値分解 (SVD) によって分解できます: 任意の行列 ≈ U Σ V^T、ここで「ランク」は重要な特異値の数です。
  - ファインチューニングの更新 ΔW = W' - W はしばしば**低ランク** (r << min(d,k)) であり、それらは圧縮された部分空間における変化を捕捉することを意味します (例: 「安全性を強調」や「コードに焦点」のようないくつかの方向)。
  - **B A** は、ランク r で ΔW を近似します (パラメータ数: d*r + r*k vs. W の d*k)。4096×4096 の W で r=8 の場合、LoRA は ~65k パラメータを使用 vs. 16M — 99.6% の削減!
  - **直感**: 更新は高次元空間内のベクトルのようなものです；LoRA はそれらを低次元の「高速道路」 (ランク r) に投影し、広大なパラメータ空間内のノイズを無視します。

- **トレーニングの仕組み**:
  1. 順伝播: W + γ B A を使用して h を計算するが、A と B のみをトレーニング (W は凍結)。
  2. 誤差逆伝播: 勾配は A/B のみに流れ、メモリを低く保つ。
  3. 推論: 単一モデルのためにマージ (W' = W + B A) するか、モジュール性のために分離したまま保持。
- **論文から (Hu et al., 2021)**: LoRA は視覚/言語モデル向けに導入されましたが、NLP で爆発的に普及しました。要約などのタスクでアダプターを上回り、より少ないメモリを使用します。QLoRA のような変種は、さらに小さなフットプリントのためにベースモデルを量子化します。

本質的に、LoRA は、ファインチューニングをコンパクトな線形変換として表現する軽量の「デルタ」 (B A) を追加することでモデルを「ハック」します。

### 5. 完全ファインチューニング (FullFT) に対する LoRA の利点
テキストは、生の効率性を超えた実用性を強調して、運用的な利点を列挙しています。それぞれについて詳しく説明します。

#### a. 事後学習のコストと速度
- LoRA は ~0.1% のパラメータのみを更新するため、100-1000倍高速/安価にトレーニングします。例: 単一の A100 GPU で Llama-7B をファインチューニング (FullFT は 8+ GPU が必要) するのに、数日ではなく数時間。
- 低い精度 (例: bfloat16) で十分であり、エネルギー使用量を削減。

#### b. マルチテナントサービング
「LoRA は元の重みを変更せずにアダプター (すなわち、A および B 行列) をトレーニングするため、単一の推論サーバーはメモリ内に多くのアダプター (異なるモデルバージョン) を保持し、バッチ処理された方法でそれらから同時にサンプリングできます。Punica: Multi-Tenant LoRA Serving (Chen, Ye, et al, 2023) 現代の推論エンジン、例えば vLLM や SGLang はこの機能を実装しています。」

- **意味すること**: ベース W は共有され、アダプターは小さい (完全モデルの GB に対して MB)。サーバーは 1 つの W + N 個のアダプター (例: コーディング、執筆、翻訳用) をロードします。
- **マルチテナンシー**: ベースを再ロードすることなく、複数のユーザー/モデルを並行してサービスします。アダプター間でリクエストをバッチ処理して効率化。
- **実世界への影響**: 本番環境 (例: Hugging Face Spaces や Azure ML) では、これにより「モデルスープ」またはオンデマンドでのペルソナ切り替えが可能になります。Punica (2023) はページングを通じてメモリを最適化します；vLLM/SGLang はページドアテンションを使用して 10 倍のスループットを実現。
- **類推**: チューンナップごとに新車を購入する vs. 単一のエンジン (W) と交換可能なターボキット (アダプター)。

#### c. トレーニングのためのレイアウトサイズ
「モデル全体をファインチューニングする場合、オプティマイザの状態は元の重みとともに、しばしばより高い精度で保存する必要があります。その結果、FullFT は通常、同じモデルからのサンプリングよりも一桁多いアクセラレーターを必要とします... トレーニングでは、重みを保存する以外に、通常、すべての重みに対する勾配とオプティマイザのモーメントを保存する必要があります；さらに、これらの変数は、推論に使用される重みの保存精度 (bfloat16 またはそれ以下) よりも高い精度 (float32) で保存されることがよくあります。LoRA ははるかに少ない重みをトレーニングし、はるかに少ないメモリを使用するため、サンプリングに使用されるレイアウトよりわずかに大きいレイアウトでトレーニングできます。」

- **トレーニングメモリの内訳**:
  - FullFT: 重み (1T パラメータ @ bfloat16 = ~2TB) + 勾配 (同様) + オプティマイザ状態 (例: Adam: パラメータあたり 2 モーメント @ float32 = 合計 ~8TB)。分散「レイアウト」 (例: データ/モデル並列) で 100 台以上の GPU が必要。
  - LoRA: A/B のみ (~0.1% パラメータ) が勾配/状態を取得 (追加 ~2-10GB)。推論レイアウトと同じ 1-2 GPU でトレーニング。
- **精度の詳細**: 推論は速度のために低精度 (bfloat16/float16) を使用；トレーニングは勾配安定性のために float32 を必要とします。LoRA はこのオーバーヘッドを最小限に抑えます。
- **アクセシビリティ**: 愛好家/スタートアップは、コンシューマー向けハードウェア (例: RTX 4090) でファインチューニング可能、FullFT はエンタープライズクラスターを必要とします。効率性: LoRA は変数が少ないため、より速く収束することが多い。

#### d. ロードと転送の容易さ
「保存する重みが少ないため、LoRA アダプターはセットアップやマシン間での転送が迅速かつ容易です。」

- アダプターは <100MB のファイル (完全モデルの 100 GB に対して) であり、圧縮可能で共有可能 (例: GitHub または torrent 経由)。
- デプロイメント: ベースを一度ロードし、アダプターを数秒で交換。永続性のためにマージはオプション。
- **エッジケース**: バージョン管理 (例: アダプター用の Git)、フェデレーテッドラーニング (オンデバイスでアダプターをトレーニング、デルタのみを送信)、または反復的チューニング (複数の LoRA をスタック)。

### より広範な含意と制限
- **なぜ LoRA が支配的なのか**: シンプルで効果的であり、(例: PEFT ライブラリで) シームレスに統合されます。Stable Diffusion のファインチューニングや Alpaca (指示チューニングされた Llama) のようなモデルで使用されます。
- **制限**: 非常にニッチなタスクでは FullFT ほど性能が出ないことがある；ランク r の選択が重要 (低すぎると未学習、高すぎると効率が悪い)。すべての更新タイプ (例: アーキテクチャの変更) を扱うわけではありません。
- **将来のトレンド**: DoRA (重みをさらに分解) や VeRA (ベクトルベース、さらにスパース) のような拡張。モデルが 10T+ パラメータにスケールするにつれ、PEFT/LoRA は持続可能性のために不可欠になります。

このテキストは、AI における画期的な転換を要約しています：リソースを貪る完全な再トレーニングから、エレガントでモジュール的な更新へ。数学的な導出、コード例 (例: PyTorch)、または関連論文への深堀りをご希望の場合は、お知らせください！