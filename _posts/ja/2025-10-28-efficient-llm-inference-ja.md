---
audio: false
generated: true
image: false
lang: ja
layout: post
model: x-ai/grok-4-fast
title: vLLMによる効率的なLLM推論
translated: true
type: note
---

### vLLM: 効率的なLLM推論とサービス提供

vLLMは、大規模言語モデル(LLM)の高速かつメモリ効率の良い推論とサービス提供のために設計されたオープンソースライブラリです。カリフォルニア大学バークレー校Sky Computing Labで最初に開発され、現在はコミュニティ主導のプロジェクトとして、LlamaやGPTバリアントなどのLLMをデプロイするために広くプロダクション環境で使用されています。その中核的な革新は**PagedAttention**であり、これはキーバリュー(KV)キャッシュメモリを仮想メモリページのように扱い、無駄を削減し、非連続ブロックを動的に割り当てることでスループット向上を可能にする技術です。

#### 仕組み
- **連続バッチ処理**: 完全なバッチを待つ従来のシステムとは異なり、vLLMは実行中にリクエストを動的に追加/削除し、デコード時のGPUアイドル時間を最小限に抑えます。
- **メモリ管理**: PagedAttentionはKVキャッシュ(シーケンス長に応じて増加する)のフラグメンテーションを回避し、メモリ不足エラーなしでより長いコンテキストをサポートします。
- **最適化された実行**: 高速なカーネル起動のためにCUDA/HIPグラフを使用し、注意機構の計算のためにFlashAttention/FlashInferと統合され、量子化(例: AWQ, GPTQ, FP8)をサポートしてメモリ使用量を最大4分の1に削減します。
- **高度な機能**: 投機的デコード(トークンを推測して検証)、チャンク化プリフィル(長い入力用)、マルチLoRA(モデルのオンザフライ適応)、分散並列処理(テンソル、パイプライン、エキスパート)を含みます。

vLLMはOpenAI互換のAPIサーバーを公開し、Hugging Faceモデルとシームレスに統合し、多様なハードウェア(NVIDIA/AMD/Intel GPU、TPU、CPU)上で動作します。サービス提供ベンチマークにおいてHugging Face Transformersなどのベースラインと比較して2〜10倍の高速化を達成し、高スループットのシナリオに理想的です。

#### 主なユースケース
- ストリーミング出力を伴うチャットボットやAPIのためのオンラインサービス提供。
- 要約などのタスクのためのオフラインバッチ推論。
- カスタムの配管作業なしでのマルチGPUクラスターへのスケーリング。

### Ray: AIとPythonアプリをスケールするための統一フレームワーク

Rayは、Pythonコード―特にAI/MLワークロード―を1台のマシンから大規模クラスターへと簡単にスケールできるようにするオープンソースの分散コンピューティングフレームワークです。Anyscaleによって(カリフォルニア大学バークレー校にルーツを持ち)作成され、スケジューリング、フォールトトレランス、オーケストレーションなどの分散システムの複雑さを抽象化し、開発者がロジックに集中できるようにします。

#### 主要コンポーネント
- **Ray Core**: 基盤となる部分―タスク(並列関数)、アクター(ステートフルサービス)、オブジェクト(分散データ共有)のためのPythonicなプリミティブを提供します。自動スケーリング、リトライ、リソース割り当てを自動的に処理します。
- **Ray AIライブラリ**: Core上に構築されたドメイン固有のツール:
  - **Ray Data**: データセットの前処理のためのスケーラブルなETL。
  - **Ray Train**: (PyTorch、TensorFlow、Hugging Faceとの)統合による分散トレーニング。
  - **Ray Tune**: 大規模なハイパーパラメータ最適化。
  - **Ray Serve**: ルーティング、バッチ処理、A/Bテストを備えた推論のためのモデルデプロイメント。
  - **RLlib**: 強化学習ツールキット。
- **Rayクラスター**: クラウド(AWS、GCP)、Kubernetes、またはオンプレミスにデプロイするための管理されたインフラストラクチャ層で、需要に基づいた自動スケーリングを備えています。

#### 仕組み
Rayはノード上でデーモンとして実行され、調整用のヘッドノードを持つクラスターを形成します。並列処理のために関数を`@ray.remote`でデコレートすると、CPU/GPU間で実行を分散します。MLの場合、ServeのようなライブラリがHTTPエンドポイントと負荷分散を処理し、Coreはフォールトトレラントなスケーリング(例: 失敗したタスクの再起動)を保証します。

#### ML/AIにおける主なユースケース
- エンドツーエンドのパイプライン: データロード → トレーニング → チューニング → サービス提供。
- 数百のGPUでのハイパーパラメータ探索。
- レコメンデーションシステムのようなリアルタイム推論フリート。
- シミュレーションなどのカスタム分散アプリ。

Rayの強みはその統一されたAPIにあります: SparkやDaskと比較して定型コードが少なく、一度書けばどこでもスケールできます。

### vLLMとRayの関係
SLOs-Serve(以前の議論から)のようなシステムでは、vLLMが低レベルのLLMバッチ処理/推論を担当し、Rayがマルチレプリカのデプロイメントをオーケストレーションし、ノード間でリクエストをルーティングしてバースト処理とスケーリングを行います。

[vLLMドキュメント](https://docs.vllm.ai/en/latest/)
[Rayドキュメント](https://docs.ray.io/en/latest/ray-overview/index.html)