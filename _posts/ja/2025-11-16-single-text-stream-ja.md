---
audio: false
generated: true
image: false
lang: ja
layout: post
model: x-ai/grok-4-fast
title: 単一連結テキストストリームでの学習
translated: true
type: note
---

nanoGPT（そして基本的にはあらゆるGPTスタイルのトレーニング、オリジナルのGPT-2/GPT-3トレーニングを含む）では、**個々の記事や文書を個別のシーケンスとして一つずつ与えるのではなく、単一の巨大な連結されたテキストストリームでトレーニングしなければなりません**。

その理由と、FineWeb（そしてAndrejのnanoGPTコード）で実際にどのように行われるかを以下に説明します：

### すべきこと（正しい方法）
1. すべての個々の記事を、特別なテキスト終了トークン `<|endoftext|>`（または任意の区切り文字）*のみ*で区切って、**一つの巨大な.txtファイル**（または複数の大きなシャードファイル）に連結します。
2. トレーニング中、nanoGPTは以下のことを行います：
   - 連結されたテキスト全体を一つの巨大なトークン配列として読み込みます。
   - それを固定長のブロック（例：block_size = 1024 トークン）に分割します。
   - これらのブロックは非常に頻繁に文書の境界を跨ぎます — それは問題ないだけでなく、望ましいことです。

これはまさにOpenAIがGPT-2とGPT-3をトレーニングした方法であり、Andrejがシェイクスピア、OpenWebText、FineWeb-Eduなどで行っている方法です。

### extract_fineweb.pyスクリプトが行っていること
あなたが実行したあのスクリプトは、すでに正しいことを行っています：

```bash
$ python ./scripts/train/extract_fineweb.py
```

このスクリプトはすべてのparquetファイルを読み込み、すべての行（すなわち、すべてのウェブページ/記事）の `text` カラムのみを抽出し、生のテキストを次々に `fineweb_extracted.txt` に書き込み、（最新バージョンでは）自動的に文書間に `<|endoftext|>` 区切り文字を挿入します。

したがって、結果のファイルは以下のようになります：

```
<記事1のテキスト> <|endoftext|> <記事2のテキスト> <|endoftext|> <記事3のテキスト> <|endoftext|> ...
```

これは完璧です。あなたはその単一の巨大なファイル（または、`train.py` がそれから作成するシャード化されたbinファイル）でトレーニングすべきです。

### すべきでないこと
- 各記事をモデルへの個別の例/シーケンスとして与え、すべての記事に対してアテンションをリセットするようなことは**しないでください**。
- すべての記事を正確にblock_sizeに切り詰めたりパディングしたりし、文書内だけでトレーニングするようなことは**しないでください**。
- バッチ内の各シーケンスが一つの完全な文書である、文書レベルのバッチ処理を**使用しないでください**。

それは、モデルが複数の文書にまたがる長距離依存関係を学習する能力を破壊し、パフォーマンスを大幅に損なわせます。

### まとめ
はい → **単一の巨大な連結テキストファイル**（`extract_fineweb.py` が生成するものそのもの）でトレーニングする。
いいえ → 記事ごとに個別のシーケンスとしてトレーニングするようなことは**しない**。

これがGPTが常にトレーニングされてきた方法であり、nanoGPTの `train.py` がデータを単なる長いトークンのストリームとして扱う理由です。

ですから、通常のnanoGPTワークフローを続けてください：

```bash
python scripts/train/extract_fineweb.py   # これはすでに実行済み
python train.py --dataset=your_extracted_file_or_fineweb10B etc.
```

これで準備は完了です！