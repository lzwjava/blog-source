---
audio: false
generated: true
image: false
lang: ja
layout: post
model: x-ai/grok-4-fast
title: トランスフォーマー：起源、設計、未来
translated: true
type: note
---

### Ashish Vaswani著「『How I Learned to Stop Worrying and Love the Transformer』要約」

この45分間の講演は、2023年11月7日にスタンフォード大学のCS25 Transformers Unitedシリーズの一環として行われたもので、Transformerアーキテクチャの起源、設計、進化、未来について振り返りながら深く掘り下げています。画期的な2017年の論文「Attention Is All You Need」の共著者であるVaswaniは、Google Brain時代の個人的な逸話を共有し、重要な決断の背景を明らかにし、AIの次の段階に対する楽観的でありながら現実的なビジョンを提示します。歴史的文脈、中核となる革新、Transformer以後の進展、そして将来を見据えたアイデアを中心に構成されており、なぜTransformerが現代AIの基盤となったのかを理解するのに最適です。

#### 歴史的背景とTransformer誕生の火花
Vaswaniは、1956年のダートマス会議に言及して講演を始めます。そこではAIの先駆者たちが、ルールベースのシステムを用い、短期間での成功を想定し、視覚、言語などを横断する人間の知能を模倣した統一機械の夢を見ていました。70年が経過し、AIの冬期を経て、我々はTransformerがマルチモーダルモデルを支える形でこの夢に回帰しつつあります。彼はこれを、機械翻訳（例えば、単語アライメント、句抽出、ニューラルリスコアリング）などのタスクのためにパイプラインが雑然と組み合わされた2000年代の自然言語処理と対比させます。2013年までに、この分野は感情分析や対話などのサイロに分断され、進展は統一理論ではなく資金によって推進されていました。

転換点は何だったのでしょうか？分散表現（例えば、word2vecの「king - man + woman ≈ queen」）とseq2seqモデル（2014–2015年）でした。これらは多様なタスクをエンコーダ-デコーダフレームワークに統合しました。しかし、LSTMのようなリカレントネットは悩みの種でした：逐次処理が並列性を損ない、隠れ状態が情報のボトルネックとなり、長距離依存性が弱かったのです。畳み込み（例えば、ByteNet, ConvS2S）は速度面で貢献しましたが、遠くの関係性を捉えるのに苦労しました。

**内部関係者の逸話：** 2016年にGoogle Neural Machine Translation (GNMT)に取り組んでいたVaswaniのチームは、パイプラインを廃止して純粋なLSTMを使用し、大量のデータで最先端の結果を達成しました。しかし、LSTMは「苛立たしい」ものでした—GPU上では遅く、スケーリングが困難でした。「なるほど」と思ったのは、完全な並列化への欲求、つまり、入力をエンコードし、出力をデコードすることをステップバイステップの退屈な作業なしに行うことでした。初期の非自己回帰の夢（すべてを一度に生成し、その後洗練する）は失敗しました。なぜなら、モデルが左から右へのガイダンスなしでは順序付けを学習できず、そのガイダンスが自然に確率の低い経路を刈り込むからです。

#### 中核となる設計選択：オリジナルTransformerの構築
Transformerは、リカレンスと畳み込みを廃止し、純粋なアテンションを採用しました。これにより、コンテンツの類似性に基づく直接的なトークン間の対話（例えば、視覚タスクにおける非ローカル平均法を用いたノイズ除去での類似画像パッチの参照に似ている）が可能になりました。自己アテンションは順序不変ですが並列処理に適しており、O(n² d)の計算量は、シーケンスが無限でなければGPUにとって有利です。

主要な構成要素：
- **スケーledドット積アテンション：** 入力からのQ, K, V射影；スコアはsoftmax(QK^T / √d_k)でVに重み付け。スケーリングは勾配消失を回避（単位分散を仮定）。デコーダ用の因果マスキングは先読みを防止。加算的アテンションより行列積の速さから選択。
- **マルチヘッドアテンション：** シングルヘッドでは平均化しすぎ（例：「猫が手を舐めた」の役割がぼやける）。ヘッドは次元を部分空間に分割—マルチテープチューリングマシンのように—焦点を絞った部分空間のため（例：あるヘッドが特定の詳細に確率1で集中）。追加計算なしで、畳み込みのような選択性。
- **位置エンコーディング：** 正弦波が順序を付与、相対位置を目標（距離で分解可能）。当初は完全に相対位置を学習しなかったが、機能した。
- **スタックと安定化：** エンコーダ-デコーダスタックに残差接続とレイヤ正規化（後により深いネットワークには事前正規化を採用）。フィードフォワードネットワークはResNetのように拡大/収縮。エンコーダ：自己アテンション；デコーダ：マスク付き自己アテンション＋クロスアテンション。

これは、LSTMアンサンブルより8倍少ないFLOPsでWMTベンチマークを圧倒し、構文解析へ一般化し、マルチモーダルの可能性を示唆しました。解釈可能性？ヘッドは専門化（長距離、他の局所畳み込み的など）されましたが、Vaswaniはそれを「茶葉占い」—有望だが不明確—と冗談を交えて述べています。

#### 進化：修正とスケーリングによる成功
Transformerはシンプルだったために「定着」しましたが、微調整がその効果を増幅しました：
- **位置2.0：** 正弦波は相対位置に不十分；相対的埋め込み（ペアごとのバイアス）が翻訳/音楽を改善。ALiBi（学習された距離バイアス）は長さを外挿；RoPE（絶対/相対をブレンドする回転）が現在主流—メモリ節約、相対位置を正確に処理。
- **長文コンテキスト：** 二次関数的な呪い？ローカルウィンドウ、スパースパターン（ストライド/グローバル）、ハッシュ（Reformer）、検索（Memorizing Transformer）、低ランクの工夫。Flash Attentionはメモリ書き込みをスキップして高速化；Multi-Queryは推論時にKVヘッドを削減。大型モデルではアテンションのコストは相対的に小さくなる。
- **その他の改良：** 事前正規化で安定化；投機的デコーディング（高速下書き、低速検証）は非自己回帰的速度をプロダクションで模倣。

**内部関係者の小話：** 効率的な相対アテンションのハッキングは「行列の体操」のようでしたが、ハードウェアの物理的特性（例：アクセラレータ向けドット積）が選択を導きました。

#### 将来の方向性：スケーリングを超えて
Vaswaniは楽観的です：自己教師ありの巨人モデルは、文脈内エージェントを可能にし、ダートマス会議の統一機械の考えと共鳴します。スケーリング則は支配的ですが、RNNの復活やより優れたアーキテクチャに注目すべきです。優先事項：
- **マルチモーダルエージェント：** 数千をプロンプトプログラミング；ツールを橋渡しとして（単純なものは内部化、複雑なものは協調）。
- **データとインフラ：** 優れたデータから2倍の利益；帯域幅のためのFP8/INT8、InfiniBand規模のトレーニング。
- **適応的知能：** 小型モデル＋プランナー/データ表現が大型モデルに匹敵；推論時の少数ショット学習；不確実性のシグナリング；スキル構築（例：Minecraftボット）。
- **フルスタックの魔法：** ワークフローのためのフィードバックループ（例：データ分析を「暗黙知」マイニングとして）。
- **創発現象：** おもちゃのモデルでのグロッキングからGPT-4の謎まで—小規模を研究して大規模を理解。
- **システム：** メモリ帯域幅がボトルネック；すべてに検索を。

彼は共著者のNoam Shazeerとのスタートアップをほのめかしています：データ処理からのワークフロー自動化、フィードバックの魔法のためのフルスタック。ソフトウェアの知能を活用し、「Transformerずくめ」になるまでツールを。

#### Q&Aハイライト
講演後の質疑応答で：
- 非自己回帰の障害：独立性のために「オラクル順序付け」が必要；潜在空間は役立つがマルチモーダリティが問題となる。
- テキストからの世界：LLM + RLHFがロボティクスプランナーを抽出；トレーニングデータのブレンドが「新しい」創造性を引き起こす。
- モジュール性：MoEはエキスパートを誘導；マルチエージェント調整（目標設定、更新、検証）が鍵。
- アドバイス：人間-AIのクローズドループを追い求める；この分野は驚きに広く開かれている。

時代を超える内容—歴史の雰囲気、技術的深さ、誇大宣伝なしのインスピレーション。AIに携わるなら、なぜTransformerが勝ったかの文脈を得るために必見です。

[How I Learned to Stop Worrying and Love the Transformer (YouTube)](https://www.youtube.com/watch?v=1GbDTTK3aR4)