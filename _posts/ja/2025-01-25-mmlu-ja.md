---
audio: true
lang: ja
layout: post
title: MMLUベンチマーク
translated: true
---

この記事では、言語モデルをMMLU（Massive Multitask Language Understanding）ベンチマークで評価します。

MMLUベンチマークは、モデルがさまざまな科目にわたる多様なタスクを実行する能力を包括的にテストするものです。数学、歴史、法律、医学など多岐にわたる分野の多肢選択問題で構成されています。

**データセットリンク:**

*   [Papers with Code](https://paperswithcode.com/dataset/mmlu)
*   [Hugging Face Datasets](https://huggingface.co/datasets/cais/mmlu)

```python
import torch
from datasets import load_dataset
import requests
import json
from tqdm import tqdm

# MMLUデータセットをロード
subject = "college_computer_science"  # 科目を選択
dataset = load_dataset("cais/mmlu", subject, split="test")

# 少数ショットの例なしでプロンプトをフォーマット
def format_mmlu_prompt(example):
    prompt = "以下は{}に関する多肢選択問題です。".format(subject.replace("_", " "))
    prompt += "正しい選択肢の文字（A、B、C、またはD）のみを答えてください。"
    prompt += "説明は不要です。文字のみを答えてください。"
    
    # 現在の質問を追加
    prompt += f"質問: {example['question']}\n"
    prompt += "選択肢:\nA. {}\nB. {}\nC. {}\nD. {}\n".format(*example['choices'])
    return prompt

# 評価ループ
correct = 0
total = 0

for i, example in tqdm(enumerate(dataset), total=len(dataset), desc="評価中"):
    prompt = format_mmlu_prompt(example)
    
    # llama-serverにリクエストを送信
    url = "http://localhost:8080/v1/chat/completions"
    headers = {"Content-Type": "application/json"}
    data = {
        "messages": [{"role": "user", "content": prompt}]
    }
    
    print(f"APIへの入力: {data}")
    response = requests.post(url, headers=headers, data=json.dumps(data))
    
    if response.status_code == 200:
        output_text = response.json()["choices"][0]["message"]["content"]
        predicted_answer = output_text.strip()[0] if len(output_text.strip()) > 0 else ""
        print(f"APIからの出力: {output_text}")
    else:
        predicted_answer = ""
        print(f"エラー: {response.status_code} - {response.text}")
    
    # 正解と比較
    
    answer_map = {0: "A", 1: "B", 2: "C", 3: "D"}
    ground_truth_answer = answer_map.get(example["answer"], "")
    is_correct = predicted_answer.upper() == ground_truth_answer
    if is_correct:
        correct += 1
    total += 1
    
    print(f"質問: {example['question']}")
    print(f"選択肢: A. {example['choices'][0]}, B. {example['choices'][1]}, C. {example['choices'][2]}, D. {example['choices'][3]}")
    print(f"予測された答え: {predicted_answer}, 正解: {ground_truth_answer}, 正解か: {is_correct}")
    print("-" * 30)

    if (i+1) % 10 == 0:
        accuracy = correct / total
        print(f"処理済み {i+1}/{len(dataset)}. 現在の正解率: {accuracy:.2%} ({correct}/{total})")


# 正解率を計算
accuracy = correct / total
print(f"科目: {subject}")
print(f"正解率: {accuracy:.2%} ({correct}/{total})")
```

ログ:

```bash
% python scripts/mmlu.py

評価中:   9%| 9/100 [01:31<15:19, 10.10s/it]処理済み 10/100. 現在の正解率: 0.00% (0/10)
評価中:  19%| 19/100 [03:14<12:47,  9.47s/it]処理済み 20/100. 現在の正解率: 0.00% (0/20)
評価中:  26%| 26/100 [04:30<13:44, 11.14s/it]

...

処理済み 100/100. 現在の正解率: 40.00% (40/100)
評価中: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [15:15<00:00,  9.16s/it]
科目: college_computer_science
正解率: 40.00% (40/100)
```