---
audio: false
generated: true
image: false
lang: ja
layout: post
title: クロードソネットランタイムメトリクス解説
translated: true
type: note
---

以下は、Google Vertex AI を通じた `anthropic/claude-sonnet-4` の推論から得られた**ランタイムメトリクス**です。詳しく説明します：

*   **First token latency (2.85秒):**
    これは、プロンプトを送信してから最初の出力トークンを受け取るまでの遅延時間です。ネットワークレイテンシ、リクエストのキューイング、モデルの初期計算が含まれます。Claude Sonnetのような大規模モデルでは、数秒程度は正常な範囲です。

*   **Throughput (32.9 トークン/秒):**
    最初のトークンが生成された後、モデルは後続のトークンをストリーミング出力します。1秒あたり32.9トークンは、健全なレートです — 可能な限り最速ではありませんが、大規模モデルとしては良好です。スループットは、モデルサイズ、プロバイダー (Vertex) の負荷、およびプロンプトの長さによって変動します。

*   **Finish reason (stop):**
    これは、モデルが生成テキストの終端に達したために自然に停止したことを示しています（長さ制限、打ち切り、またはエラーによる中断とは対照的です）。

つまり、**「なぜ？」** → これらの数値は、大規模なLLMをVertexで実行した際の、正常な推論の特性を説明しているからです：

*   ある程度の起動時のレイテンシは想定内です（コンパイル、最適化、ルーティングなど）。
*   生成が開始されると、スループットは安定します。
*   Stopは意図された完了条件です。

これらの数値が、他のモデル（GPT-5、Gemini Pro、DeepSeekなど）の典型的な性能範囲と比較して「速い」のか「遅い」のか、文脈の中で確認したいですか？