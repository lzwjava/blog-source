---
audio: false
generated: true
image: false
lang: ja
layout: post
title: 苦い教訓が変革をもたらすトランスフォーマー進化
translated: true
type: note
---

スタンフォードCS25イベントでの講演において、Hyung Won Chungは**Transformerアーキテクチャ**の詳細な歴史的分析と将来予測を提示し、その進化を駆動してきた力を理解することがAIの未来を予測する鍵であると論じました。彼の主張の核心は、AI開発における「苦い教訓」にあり、Transformerモデルの進化を通してこれを説明しています。

***

### AI進歩の駆動力

Chungは、AI研究を推進する最も重要な要因は**計算コストの指数関数的な減少**であるとし、これがモデルとデータの規模拡大を可能にしてきたと主張します。この分野の急速な変化を理解するためには、個々のアーキテクチャ革新の詳細に惑わされるのではなく、この支配的な駆動力に焦点を当てる必要があると述べています。

彼は**「苦い教訓」**という概念を紹介します。これは、長期的なAIの進歩は、組み込まれた仮定（帰納的バイアス）が少ない、よりシンプルで汎用的な手法を支持するという考えです。高度に構造化されたドメイン特化型モデルは短期的な利益をもたらすかもしれませんが、計算資源とデータがスケールするにつれて、最終的にはボトルネックになるとしています。研究者には、自身のモデルの基礎構造を常に問い直し、単純化することを推奨しています。

***

### Transformerアーキテクチャの進化

Chungは自身の主張を例示するために、3つの主要なTransformerアーキテクチャを挙げています：

1.  **エンコーダー-デコーダー (オリジナルTransformer):** 機械翻訳などのタスクに元来使用されていたこのアーキテクチャは、より多くの固有の構造を持ちます。エンコーダーとデコーダーでパラメータを分け、特定のクロスアテンションパターンを使用します。入出力が明確に分かれたタスクには効果的ですが、この構造は大規模で汎用的なモデルが主流となる時代において重要性が薄れつつあります。

2.  **エンコーダーのみ (例: BERT):** 生成を捨て、分類などのタスクに焦点を当てた、よりシンプルなアーキテクチャです。特定のベンチマークでは強力ですが、その焦点の狭さゆえに、多様な現代のアプリケーションへの汎化性は低くなります。

3.  **デコーダーのみ (例: GPT-3):** これは3つの中で最も汎用的で、構造が最も少ないアーキテクチャです。単一の層スタックと共有パラメータを使用し、多様なタスクを処理し、一般的な知識を学習し、長文生成を実行する現代の大規模言語モデルに非常に適しています。

***

### 主な要点と予測

Chungによれば、趨勢は明らかです：AIは**よりシンプルで、より汎用的で、課された構造が少ないアーキテクチャ**へと向かっています。これにより、モデルは固定された仮定に縛られることなく、膨大なデータから複雑なパターンと関係性を学習する自由を得ます。

彼は、AIの次のボトルネックはアーキテクチャそのものではなく、**学習目標**にあるかもしれないと示唆しています。Human Feedbackからの強化学習(RLHF)のような手法を、構造化されていない、学習された目的関数への一歩であり、オープンエンドで複雑な生成タスクにより適していると見なしています。彼は聴衆に、これらの原則を批判的に考察し、AIの未来を形作る一助となることを奨励しています。

講演の全編は [https://www.youtube.com/watch?v=3gb-ZkVRemQ&pp=0gcJCfwAo7VqN5tD](https://www.youtube.com/watch?v=3gb-ZkVRemQ&pp=0gcJCfwAo7VqN5tD) で視聴できます。