---
audio: false
generated: true
lang: ja
layout: post
title: トランスフォーマーのKQVメカニズム
translated: true
type: note
---

**Key (K)、Query (Q)、Value (V)** メカニズムは、Transformerベースの大規模言語モデル（LLM）における**self-attention**メカニズムの基本的な構成要素です。このメカニズムは、画期的な論文 *"Attention is All You Need"* (Vaswani et al., 2017) で導入され、モデルがテキストを処理または生成する際に、シーケンス内の異なる単語の重要度を重み付けできるようにします。以下では、Transformer LLMにおける **K, Q, V** メカニズムの働きについて、その直感的な理解、数学的定式化、self-attentionにおける実装、およびより広範なアーキテクチャ内での役割を網羅的に紹介します。

---

### 1. **Self-AttentionにおけるK, Q, Vの直感的な理解**
Self-attentionメカニズムにより、Transformerモデルは、各単語（またはトークン）に対してシーケンスの関連する部分に焦点を当てながら入力シーケンスを処理できます。**K, Q, V** コンポーネントはこのプロセスの構成要素であり、モデルが入力のどの部分が互いに関連性が高いかを動的に決定できるようにします。

-   **Query (Q):** トークンがシーケンス内の他のトークンに対して尋ねる「質問」を表します。各トークンについて、クエリベクトルは、そのトークンがシーケンスの残りの部分から探している情報をエンコードします。
-   **Key (K):** シーケンス内の各トークンの「説明」を表します。キーベクトルは、トークンが他のトークンに提供できる情報をエンコードします。
-   **Value (V):** トークンが持つ実際の内容または情報を表します。モデルが（QとKの相互作用を通じて）どのトークンが関連するかを決定すると、対応するバリューベクトルを取得して出力を構成します。

**Q**と**K**の間の相互作用は、各トークンが他のすべてのトークンにどれだけ注意を向けるべきかを決定し、**V**ベクトルはこの注意に基づいて重み付けされ、結合され、各トークンの出力を生成します。

図書館の検索に例えてみましょう：
-   **Query**: あなたの検索クエリ（例：「機械学習」）。
-   **Key**: 図書館内の本のタイトルやメタデータ。これらをクエリと比較して関連する本を見つけます。
-   **Value**: 関連する本を特定した後で取得する実際の本の内容。

---

### 2. **Self-AttentionにおけるK, Q, Vの働き**
Self-attentionメカニズムは、**Value**ベクトルの重み付き和を計算します。この重みは、**Query**ベクトルと**Key**ベクトル間の類似度によって決定されます。このプロセスのステップバイステップの内訳は以下の通りです：

#### ステップ 1: 入力表現
-   Transformerレイヤーへの入力は、トークンのシーケンス（例：単語またはサブワード）であり、それぞれが高次元の埋め込みベクトル（例：次元 \\( d_{\text{model}} = 512 \\)）として表現されます。
-   \\( n \\) 個のトークンのシーケンスに対して、入力は行列 \\( X \in \mathbb{R}^{n \times d_{\text{model}}} \\) であり、各行はトークンの埋め込みです。

#### ステップ 2: K, Q, Vを生成するための線形変換
-   各トークンについて、3つのベクトルが計算されます：**Query (Q)**、**Key (K)**、**Value (V)**。これらは、入力埋め込みに学習された線形変換を適用することで得られます：
  \\[
  Q = X W_Q, \quad K = X W_K, \quad V = X W_V
  \\]
  - \\( W_Q, W_K, W_V \in \mathbb{R}^{d_{\text{model}} \times d_k} \\) は学習された重み行列です。
  - 通常、\\( d_k = d_v \\) であり、しばしば \\( d_{\text{model}} / h \\) に設定されます（\\( h \\) は後述するアテンションヘッドの数）。
  - 結果は以下の通り：
    - \\( Q \in \mathbb{R}^{n \times d_k} \\)：すべてのトークンのクエリ行列。
    - \\( K \in \mathbb{R}^{n \times d_k} \\)：すべてのトークンのキー行列。
    - \\( V \in \mathbb{R}^{n \times d_v} \\)：すべてのトークンのバリュー行列。

#### ステップ 3: アテンションスコアの計算
-   アテンションメカニズムは、あるトークンのクエリベクトルとすべてのトークンのキーベクトルとの**内積**を計算することで、各トークンが他のすべてのトークンにどれだけ注意を向けるべきかを計算します：
  \\[
  \text{Attention Scores} = Q K^T
  \\]
  - これは \\( \in \mathbb{R}^{n \times n} \\) の行列を生成し、各要素 \\( (i, j) \\) はトークン \\( i \\) のクエリとトークン \\( j \\) のキーとの間の正規化されていない類似度を表します。
-   勾配を安定させ、大きな値を防ぐために、スコアはキー次元の平方根でスケーリングされます：
  \\[
  \text{Scaled Scores} = \frac{Q K^T}{\sqrt{d_k}}
  \\]
  - これは**スケーledドット積アテンション**と呼ばれます。

#### ステップ 4: Softmaxを適用してアテンション重みを取得
-   スケーリングされたスコアは、**softmax**関数に通され、確率（アテンション重み）に変換されます。この確率は各トークンについて合計が1になります：
  \\[
  \text{Attention Weights} = \text{softmax}\left( \frac{Q K^T}{\sqrt{d_k}} \right)
  \\]
  - 結果は \\( \in \mathbb{R}^{n \times n} \\) の行列であり、各行は、シーケンス内のすべてのトークンに対するトークンのアテンション分布を表します。
  - 高いアテンション重みは、対応するトークンが互いに非常に高い関連性を持つことを示します。

#### ステップ 5: 出力の計算
-   アテンション重みを使用して、**Value**ベクトルの重み付き和を計算します：
  \\[
  \text{Attention Output} = \text{softmax}\left( \frac{Q K^T}{\sqrt{d_k}} \right) V
  \\]
  - 出力は \\( \in \mathbb{R}^{n \times d_v} \\) の行列であり、各行は、それらの関連性に基づいて他のすべてのトークンからの情報を組み込んだ、トークンの新しい表現です。

#### ステップ 6: マルチヘッドアテンション
-   実際には、Transformerは**マルチヘッドアテンション**を使用します。ここでは、上記のプロセスが（異なる \\( W_Q, W_K, W_V \\) で）並列に複数回実行され、異なるタイプの関係を捕捉します：
  - 入力は \\( h \\) 個のヘッドに分割され、各ヘッドはより小さな次元 \\( d_k = d_{\text{model}} / h \\) の \\( Q, K, V \\) ベクトルを持ちます。
  - 各ヘッドは独自のアテンション出力を計算します。
  - すべてのヘッドからの出力は連結され、最終的な線形変換を通されます：
    \\[
    \text{MultiHead}(Q, K, V) = \text{Concat}(\text{head}_1, \text{head}_2, \dots, \text{head}_h) W_O
    \\]
    ここで、\\( W_O \in \mathbb{R}^{h \cdot d_v \times d_{\text{model}}} \\) は学習された出力射影行列です。

---

### 3. **Transformer LLMにおけるK, Q, Vの役割**
**K, Q, V** メカニズムは、アテンションのタイプに応じて、Transformerアーキテクチャの異なる部分で使用されます：

-   **エンコーダーにおけるSelf-Attention (例: BERT):**
    -   すべてのトークンが入力シーケンス内の他のすべてのトークンに注意を向けます（双方向アテンション）。
    -   \\( Q, K, V \\) はすべて同じ入力シーケンス \\( X \\) から導出されます。
    -   これにより、モデルは前後のトークンからの文脈を捕捉でき、テキスト分類や質問応答などのタスクに有用です。

-   **デコーダーにおけるSelf-Attention (例: GPT):**
    -   GPTのような自己回帰モデルでは、デコーダーは将来のトークンに注意を向けることを防ぐために**マスク付きself-attention**を使用します（モデルはテキストを順次生成するため）。
    -   マスクは、各トークン \\( i \\) について、トークン \\( j > i \\) に対するアテンションスコアがsoftmaxの前に \\(-\infty\\) に設定され、実質的に重みがゼロになることを保証します。
    -   \\( Q, K, V \\) は依然として入力シーケンスから導出されますが、アテンションは因果的です（前のトークンのみに注意を向けます）。

-   **エンコーダー-デコーダーモデルにおけるCross-Attention (例: T5):**
    -   エンコーダー-デコーダーアーキテクチャでは、デコーダーはエンコーダーの出力に注意を向けるためにcross-attentionを使用します。
    -   ここでは、\\( Q \\) はデコーダーの入力から導出され、\\( K \\) と \\( V \\) はエンコーダーの出力から来るため、デコーダーは出力を生成する際に入力シーケンスの関連部分に焦点を当てることができます。

---

### 4. **K, Q, Vが非常に効果的な理由**
**K, Q, V** メカニズムが強力な理由はいくつかあります：
-   **動的文脈化**: 固定されたパターン（例：RNNやCNNのように）に依存するのではなく、各トークンがその内容に基づいて他のトークンから情報を収集できるようにします。
-   **並列化**: リカレントニューラルネットワークとは異なり、self-attentionはすべてのトークンを同時に処理するため、GPUのような現代のハードウェア上で非常に効率的です。
-   **柔軟性**: マルチヘッドアテンションにより、モデルは \\( Q, K, V \\) に対して異なる射影を学習することで、多様な関係（例：統語的、意味的）を捕捉できます。
-   **スケーラビリティ**: このメカニズムは長いシーケンスにうまくスケールします（ただし、計算コストはシーケンス長に対して二次的に増加します。これはスパースアテンションや効率的なTransformerなどの技術によって軽減されます）。

---

### 5. **数学的まとめ**
スケーledドット積アテンションの公式は：
\\[
\text{Attention}(Q, K, V) = \text{softmax}\left( \frac{Q K^T}{\sqrt{d_k}} \right) V
\\]
マルチヘッドアテンションの場合：
\\[
\text{MultiHead}(Q, K, V) = \text{Concat}(\text{head}_1, \dots, \text{head}_h) W_O
\\]
ここで：
\\[
\text{head}_i = \text{Attention}(Q W_{Q_i}, K W_{K_i}, V W_{V_i})
\\]

---

### 6. **実用的な例**
次の文を考えます：*"The cat sat on the mat."*
-   **入力**: 各単語は埋め込みベクトル（例：単語埋め込み層を介して）に変換されます。
-   **Q, K, Vの計算**: 各トークンについて、モデルは学習された重みを使用してクエリ、キー、バリューベクトルを計算します。
-   **アテンションスコア**: 単語「cat」について、クエリベクトルは、文脈的に関連性が高いため「sat」と「mat」に高いアテンションスコアを割り当て、一方で「the」には低いスコアを与えるかもしれません。
-   **出力**: アテンションメカニズムは、「cat」の新しい表現を生成します。この表現は、「sat」と「mat」からの情報を、それらのアテンションスコアで重み付けして組み込んだものです。

---

### 7. **拡張と最適化**
-   **効率的なアテンション**: 長いシーケンスを扱うために、Performer、Linformer、Longformerなどの変種は、アテンションの計算複雑性を軽減します。
-   **位置エンコーディング**: self-attentionは順序不変であるため、Transformerは入力埋め込みに位置エンコーディングを追加して単語の順序を捕捉します。これは間接的に \\( Q, K, V \\) に影響を与えます。
-   **レイヤーの積層**: Transformerはself-attentionの複数のレイヤーを積層し、\\( Q, K, V \\) がますます抽象的な表現で動作できるようにします。

---

### 8. **結論**
**K, Q, V** メカニズムは、Transformerのself-attentionの核心であり、LLMがテキスト内の複雑な関係を効率的かつ効果的にモデル化できるようにします。クエリとキーに基づいてアテンションスコアを計算し、バリューベクトルから情報を集約することにより、Transformerは言語モデリング、翻訳などのタスクで最先端の性能を達成します。このメカニズムを理解することは、現代のLLMがどのように人間らしいテキストを処理し生成するかを把握するための鍵です。

特定の側面（例：マルチヘッドアテンション、コード実装、特定のモデルへの応用）についてさらに深く掘り下げたい場合は、お知らせください！