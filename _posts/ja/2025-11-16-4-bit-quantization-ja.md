---
audio: false
generated: true
image: false
lang: ja
layout: post
model: x-ai/grok-4-fast
title: 効率的なLLMのための4ビット量子化
translated: true
type: note
---

### 4ビット量子化とは（QLoRAやGPTQの文脈において）？

4ビット量子化は、大規模言語モデル（LLM）のメモリ使用量を劇的に削減する技術で、各重みを通常の16ビット（FP16/BF16）や32ビット（FP32）精度ではなく、わずか4ビットで保存します。

通常（フル精度）のモデル：
- FP32: パラメータあたり4バイト → 7Bモデル ≈ 28 GB
- FP16/BF16: パラメータあたり2バイト → 7Bモデル ≈ 14 GB

4ビット量子化：
- パラメータあたり約0.5バイト → 7Bモデル ≈ 3.5–4 GB（正確な手法によって元のサイズの約1/4から1/8）

現在一般的に目にする主要な4ビットアプローチは2つあります：

| 手法    | 正式名称                      | 主な論文 / 年       | 典型的な使用例                      | 主な特徴                                                                                      |
|---------|-------------------------------|---------------------|-------------------------------------|------------------------------------------------------------------------------------------------|
| GPTQ    | GPTQ                          | 2023                | 学習後量子化（推論専用）            | ワンショット、非常に高精度、再学習不要。学習後に重みを4ビットに丸める。                         |
| QLoRA   | Quantized Low-Rank Adaptation | 2023 (6月)          | 効率的なファインチューニング / 指示チューニング | 4ビット保存 + LoRAアダプタ + ページングされたオプティマイザを組み合わせる。単一の24–48 GB GPUで65B以上のモデルのファインチューニングを可能にする。 |

#### QLoRAの詳細（通常「4ビットQLoRA」と言う場合に意味するもの）
QLoRAは4つの賢いことを同時に行います：

1. 4ビットNormalFloat（NF4）量子化
   - 正規分布に従う重み（ほとんどのLLMの重みは学習後≈ガウス分布）に最適化された特別な4ビットデータ型。
   - プレーンなINT4より優れており、正規分布データに対して理論的に最適。

2. 二重量子化
   - 量子化定数（スケーリング係数）自体をFP16→8ビットに量子化し、さらに数MBを節約。

3. ページングされたオプティマイザ
   - オプティマイザの状態（AdamWのモーメント）をCPU RAMに保存し、NVIDIA unified memoryを使用してGPUにページング。学習中のOOMを防止。

4. LoRAアダプタ
   - ベースの4ビットモデルは凍結したまま、小さな低ランク行列（r=64以下）のみを学習。

結果：QLoRAを使用すれば、65BのLlama/Mistralモデルを単一の48 GB RTX A6000で、さらには70Bモデルを単一の80 GB A100で完全にファインチューニングできます。一方、通常の完全なファインチューニングでは8台以上のA100が必要になります。

#### GPTQ（推論に焦点を当てたもの）
- 学習が完了した後に行われる。
- 重みを4ビットに圧縮する際の丸め誤差を最小化するために、二階（ヘッシアン）情報を使用。
- 非常に高精度 — FP16と比べて通常<0.1のパープレキシティ劣化。
- AutoGPTQ、ExLlama、vLLM、llama.cpp（GGUFもGPTQスタイルのモードを持つ）などのツールで人気。

### グラディエントチェックポイントング（「アクティベーションチェックポイントング」とも呼ばれる）

学習中に使用される、まったく別のメモリ節約技術。

仕組み：
- 通常、誤差逆伝播法は順伝播の中間活性化をすべて保存する必要がある → メモリ使用量はモデルの深さ/バッチサイズ/系列長に比例して増加。
- グラディエントチェックポイントングは、計算を犠牲にしてメモリを節約する：
  - 順伝播中、いくつかの「チェックポイント」活性化のみを保存。
  - 逆伝播中、失われた活性化をその場で再計算。

典型的なメモリ節約効果： 活性化メモリの50–80%、ただし学習速度が約20–30%低下する代償。

4ビット重みを使用しても、活性化がGPUメモリを逼迫するため、QLoRAやあらゆる大規模モデルのファインチューニングとほぼ常に併用されます。

### TL;DR まとめ

「4ビット量子化（QLoRAスタイルまたはGPTQ）+ グラディエントチェックポイントング」は、以下のための現代的なレシピです：

- 巨大なモデル（33B、70B、405B）をコンシューマーまたは単一のエンタープライズGPUで実行またはファインチューニングする。
- QLoRAスタイルの4ビット → 非常に少ないGPUメモリでのファインチューニング。
- GPTQスタイルの4ビット → 非常に少ないGPUメモリでの高速推論。
- グラディエントチェックポイントング → 長い系列での実際の逆伝播をOOMなしで可能にする。

これらが組み合わさることで、今日、人々が単一の3090/4090でLlama-3-70Bをファインチューニングできる理由です。