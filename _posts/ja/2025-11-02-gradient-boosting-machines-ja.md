---
audio: false
generated: true
image: false
lang: ja
layout: post
model: x-ai/grok-4-fast
title: 勾配ブースティングマシンの紹介
translated: true
type: note
---

### 概要
2001年のJerome H. Friedmanによる論文 *「Greedy Function Approximation: A Gradient Boosting Machine」* は、回帰や分類などの教師あり学習タスクのための強力なアンサンブル学習手法であるGradient Boosting Machines (GBM)を紹介している。この論文は、ブースティングを関数空間における勾配降下法の一形態として位置づけ、指定された損失関数を最小化するために、単純な「弱」学習器（多くの場合は決定木）が加法モデルに逐次的に追加されていく枠組みを提供する。このアプローチは、以前のブースティングアルゴリズム（例: AdaBoost）を一般化し、関数空間における貪欲な最適化を強調することで、非常に精度が高く、ロバストで解釈性のあるモデルを実現する。

### 抄録（言い換え）
GBMは、微分可能な損失関数の最小化関数を近似するために、弱学習器を逐次的、加法的に組み合わせることで、柔軟な予測モデルを構築する。回帰木を基底学習器として使用すると、回帰と分類において競争力がありロバストな手法が得られる。この手法は、経験的テストにおいて、多変量適応回帰スプライン (MARS) などの代替手法を凌駕し、多様なデータセットにおいて低い誤り率を示す。

### 主要な手法
中核となる考え方は、現在のモデルの予測値に関する損失関数の*負の勾配*（疑似残差）に新しい学習器を繰り返しフィッティングすることで、関数空間における勾配降下法を模倣することである。

- **モデル構造**: 最終モデルは \\( F_M(x) = \sum_{m=1}^M \beta_m h_m(x) \\) であり、各 \\( h_m(x) \\) は弱学習器（例: 小さな回帰木）である。
- **更新ルール**: 反復 \\( m \\) において、残差 \\( r_{im} = -\left[ \frac{\partial L(y_i, F(x_i))}{\partial F(x_i)} \right]_{F=F_{m-1}} \\) を計算し、次に最小二乗法を用いてこれらの残差に \\( h_m \\) をフィットさせる。ステップサイズ \\( \gamma_m \\) は直線探索によって最適化される: \\( \gamma_m = \arg\min_\gamma \sum_i L(y_i, F_{m-1}(x_i) + \gamma h_m(x_i)) \\)。
- **縮小**: 加算を \\( \nu \in (0,1] \\)（例: \\( \nu = 0.1 \\)）でスケーリングすることで、過学習を抑制し、より多くの反復を可能にする。
- **確率的変種**: 各ステップでデータの一部（例: 50%）をサブサンプリングすることで、学習を高速化し、汎化性能を向上させる。
- **TreeBoostアルゴリズム**（疑似コードの概要）:
  1. 損失を最小化する定数で \\( F_0(x) \\) を初期化する。
  2. \\( m = 1 \\) から \\( M \\) まで繰り返す:
     - 疑似残差 \\( r_{im} \\) を計算する。
     - 木 \\( h_m \\) を \\( \{ (x_i, r_{im}) \} \\) にフィットさせる。
     - 直線探索によって最適な \\( \gamma_m \\) を見つける。
     - \\( F_m(x) = F_{m-1}(x) + \nu \gamma_m h_m(x) \\) で更新する。
  3. 反復回数または損失の改善度合いに基づいて停止する。

サポートされる損失関数には以下が含まれる:
- 最小二乗法（回帰）: \\( L(y, F) = \frac{1}{2}(y - F)^2 \\)、残差 = \\( y - F \\)。
- 最小絶対偏差（ロバスト回帰）: \\( L(y, F) = |y - F| \\)。
- 対数尤度（二値分類）: \\( L = -\sum [y \log p + (1-y) \log(1-p)] \\)、ただし \\( p = \frac{1}{1 + e^{-F}} \\); 残差 = \\( y - p \\)。
- Huber損失（外れ値にロバスト）。

LogitBoostなどの変種は、特定の損失関数（例: 二項逸脱度）に対してこれを適応させる。

### 貢献
- **統一フレームワーク**: 勾配を通じてブースティングをあらゆる微分可能な損失関数に拡張し、AdaBoost（指数損失）とLogitBoostを統一した。
- **実用的な拡張**: 過学習と計算量をより良く制御するために、縮小と確率的サブサンプリングを導入した。
- **柔軟性**: 様々な基底学習器とタスク（回帰、分類、ランキング）で動作する。木を用いることで高次元データを効率的に扱える。
- **解釈性のツール**: 変数重要度（不純度減少の総量による）と部分依存プロットについて議論した。

### 主な知見
- **性能**: GBMは経験的に優れており、実世界のデータセット（例: 金融、物理シミュレーション）において、単一の木、バギング、ニューラルネットワーク、MARSなどのベースラインと比較して、誤り率を10-50%削減した。
- **ロバスト性**: 確率的GBMはより速く収束する。深い木（6-8レベル）は精度を向上させるが、正則化が必要である。LAD/Huber損失は最小二乗法よりも外れ値に対してより効果的である。
- **効率性**: 大規模データに対して高速である。最適な設定は \\( \nu = 0.1 \\)、1000回以上の反復、サブサンプリングを使用する。
- プロットは、代替手法と比較して、損失の急速な収束と低い分散/バイアスを示している。

この論文は、XGBoostやLightGBMのような現代的な実装の基礎を築いた。

### 参考文献
- [Greedy Function Approximation: A Gradient Boosting Machine (PDF)](https://jerryfriedman.su.domains/ftp/trebst.pdf)