---
audio: false
generated: true
image: false
lang: ja
layout: post
model: x-ai/grok-4-fast
title: GPT-1のアーキテクチャとトレーニング概要
translated: true
type: note
---

### GPT-1アーキテクチャの概要

GPT-1（Generative Pre-trained Transformer 1）は、OpenAIが2018年に導入した基礎モデルです。このモデルは、言語モデリング目標を用いてラベルなしテキストで大規模言語モデルを事前学習し、その後下流タスク用にファインチューニングするというアプローチを開拓しました。アーキテクチャは**デコーダのみのTransformer**であり、オリジナルのTransformer論文（Vaswani et al., 2017）から adaptation され、自己回帰生成のためにデコーダスタックのみに絞り込まれています。この設計により、モデルはシーケンス内の次のトークンを予測することができ、連続したテキストを含むタスクに適しています。

BERTのような双方向モデルとは異なり、GPT-1は**マスクされた自己注意**を使用して因果性を確保します。つまり、各位置は前の位置にのみ注意を向けることができ、将来のトークンからの情報漏洩を防ぎます。

### 主要コンポーネントとハイパーパラメータ

- **モデルタイプ**: マスクされたマルチヘッド自己注意と位置ごとのフィードフォワードネットワークを備えた多層Transformerデコーダ。
- **レイヤー数**: 12のTransformerブロック（レイヤー）。
- **注意メカニズム**: レイヤーごとに12の注意ヘッド。各ヘッドは64次元の状態を処理（モデル総次元数: 768）。
- **埋め込み次元**:
  - 隠れサイズ (d_model): 768。
  - フィードフォワード内部次元 (d_ff): 3072（隠れサイズの4倍、Transformerの標準）。
- **位置エンコーディング**: トークン埋め込みに加算される学習済み位置埋め込み（正弦波エンコーディングは使用しない）。
- **活性化関数**: フィードフォワードレイヤーにおけるGaussian Error Linear Units (GELU)。
- **語彙とトークン化**: コーパスで学習された40,000回のマージによるByte-Pair Encoding (BPE)。
- **総パラメータ数**: 約1億1,700万。
- **シーケンス長**: 512トークンのシーケンスで学習。
- **正則化**:
  - Dropout: 残差、埋め込み、注意に対して0.1。
  - 重み減衰: 非バイアス/非レイヤ正規化重みに対する修正L2正則化 (0.01)。
- **初期化**: 正規分布N(0, 0.02)からの重み初期化。

### 学習の詳細

- **事前学習**:
  - **データセット**: BooksCorpus、約7,000冊の未出版書籍（総語数約8億語）のコレクション。ファンタジー、ロマンス、アドベンチャーなどのジャンルを含む。テキストは（ftfyライブラリなどを介して）クリーンアップされ、spaCyでトークン化された。
  - **目的**: 教師なし言語モデリング（次トークン予測）。
  - **オプティマイザ**: Adam、β1=0.9、β2=0.999、ε=1e-8。
  - **学習率スケジュール**: 2,000回の更新で線形ウォームアップを行い最大2.5×10⁻⁴まで上昇、その後コサインアニーリングで0まで減少。
  - **バッチサイズ**: ミニバッチあたり64シーケンス。
  - **エポック数**: 100。
  - **パフォーマンス指標**: 学習コーパスでパープレキシティ18.4を達成。
- **ファインチューニング**:
  - 最小限の変更: タスク固有の予測のために線形出力層を追加し、入力構造化のために区切りトークン（例: <s>, <e>, $）を使用して入力を単一シーケンスとして構成。
  - 補助言語モデリング損失をファインチューニング中に重み0.5で使用。
  - ハイパーパラメータ: 低い学習率 (6.25×10⁻⁵)、バッチサイズ32、3エポック、0.2%のウォームアップと線形減衰。
  - この転移学習アプローチにより、テキスト含意関係認識や質問応答などのタスクで、タスク固有のアーキテクチャなしに強力な性能を発揮することが可能になった。

### 注目すべき革新点と限界

GPT-1は、単一の事前学習モデルが、単純な入力フォーマットを介して多様なNLPタスクに効果的に転移し、当時の一部のタスクチューニング済みモデルを凌駕することを実証しました。しかし、現代のLLM（例: GPT-3は1,750億パラメータ）と比較して相対的に小さく、512トークンのコンテキスト長に制限されていました。このアーキテクチャは、GPTシリーズ全体の基礎を築き、事前学習におけるスケーラビリティの重要性を強調しました。

[Improving Language Understanding by Generative Pre-Training](https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf)  
[GPT-1 - Wikipedia](https://en.wikipedia.org/wiki/GPT-1)