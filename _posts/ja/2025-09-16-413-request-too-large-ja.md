---
audio: false
generated: true
image: false
lang: ja
layout: post
model: openai/gpt-oss-120b
title: 413リクエストサイズ過大エラーの解決
translated: true
type: note
---

## エラーの意味

- **413 Request Too Large** は、ペイロードが許可されたサイズを超えているため、サーバーがリクエストの処理を拒否したことを示すHTTPステータスコードです。OpenAIの推論サービスにおいて、「ペイロード」とは、単一のリクエストで送信されるトークンの総数（入力＋出力）を指します。[1]

- **Tokens-per-minute (TPM) 制限**: `openai/gpt-oss-120b` モデルは、オンデマンドサービスティアにおいて、最大 **8,000 TPM** で設定されています。送信されたリクエストには **13,701 トークン** が含まれており、これは制限を **5,701 トークン** 超過しています。[1]

- **原因**:
  1. **大きなプロンプト** – 入力テキスト（システム指示、ユーザーメッセージ、付加データを含む）が非常に長い可能性があります。
  2. **高い max_tokens 設定** – リクエストでモデルに長い完了文の生成を求めている場合、予測出力トークン数が入力数に加算されます。
  3. **短時間での連続した呼び出し** – 短時間に多数のリクエストが行われると、個々のリクエストが制限以下であっても、現在の1分間における累積トークン数がTPMクォータを超える可能性があります。

## 解決方法

| オプション | 対応方法 | 効果 |
|--------|------------|--------|
| **プロンプトを短縮する** | 不要なテキストを削除する、簡潔な指示を使用する、またはデータを圧縮する（例：大きなブロックを送信前に要約する）。 | 入力トークン数を減らす。 |
| **`max_tokens`を減らす** | より短い完了文をリクエストする（例：`max_tokens`を2000ではなく500に設定する）。 | 予測出力トークン数を減らす。 |
| **リクエストを分割する** | 大きなドキュメントを複数の小さなチャンクに分割し、モデルを順次呼び出して、後で結果を結合する。 | 各呼び出しをTPM制限以下に保つ。 |
| **ティアをアップグレードする** | TPM制限がより大きい、または無制限の上位ティア（例：Dev Tier）に移行する。 | エラーなしでより大きな単一リクエストのペイロードを許可する。 |
| **呼び出しを調整する** | リクエスト間に短い遅延を追加する、またはレート制限を実装して、1分あたりの総トークン数が8,000以下になるようにする。 | 累積的なTPM超過を防ぐ。 |

## リクエスト送信前の簡単チェックリスト

1. **トークンを数える** – OpenAIのトークナイザーツール（または `tiktoken`）を使用して、総トークン数（プロンプト＋max_tokens）を見積もる。
2. **オンデマンドティアでは ≤ 8,000 TPM を守る**。
3. **制限に近い場合**、上記の軽減策のいずれかを検討する。

リクエストが8,000トークン/分のクォータ内に収まるようにすることで、413「Request Too Large」エラーは回避されます。[1]

Citations:
[1] https://www.reddit.com/r/OpenAI/comments/1nbtl2p/wow_weve_been_burning_money_for_6_months/