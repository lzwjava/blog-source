---
audio: false
generated: true
image: false
lang: ja
layout: post
title: AIの速度が実用性に出会う時
translated: true
type: note
---

以下は論文の**主なポイント**です：

---

### 核心理念

* **従来のAIベンチマーク**は主に精度/品質に焦点を当てているが、実世界の時間制約が厳しいアプリケーション（例：自動運転、カスタマーサービス）において重要な**処理速度**を軽視している。
* 既存の速度指標（TTFT、TPS）は**トークン中心**であり、マルチモーダルAIの未来には不十分。
* **提案する新指標**: **インテリジェンス・バンド幅** — AIが単位時間あたりに生成できる有用な情報の量。

---

### インテリジェンス・バンド幅の近似手法

1. **ベンチマークスコア/時間**

   * 正規化されたベンチマーク性能を要した時間で除算。
   * 実用的なタスクにおいて、トークン/秒よりも情報量が多い。

2. **情報理論的アプローチ**

   * 確率分布を介して出力情報量を測定。
   * 情報 ≠ 有用性であり、確率ベクトルへのアクセスが必要なため限界がある。

3. **生出力ビット/秒**

   * 最もシンプルで、モダリティに依存しない。
   * AIの出力（テキスト、画像、動画）のビット/秒を測定。
   * 有用性を直接測定しないが、高パフォーマンスモデルにのみ適用する場合は有効。

---

### 歴史的経緯

* 速度がこれまで無視されてきた理由：

  1. AIはそれを必要とするほど進歩していなかった。
  2. アプリケーションは限定的でタスク特化型だった。
* **LLM**と**マルチモーダルAI**の登場により、**統一された速度指標**が必要となった。

---

### 人間-AIインタラクションへの示唆

* **ムーアの法則**や**ニールセンの法則**と同様に、この指標は成長トレンドを明らかにできる。
* **閾値の概念**: AIの出力速度が人間の知覚速度（例：読む、聞く）を超えると、リアルタイムインタラクションが可能になる。
* AIは既に人間の読む速度や聞く速度を超えている；次のフロンティアは**リアルタイムの画像と動画の統合**。
* 未来: AIは**リアルタイムの視覚的推論、ホワイトボードスタイルのデザイン、没入型仮想環境**をサポートする可能性がある。

---

### 実験とデータ

* **生出力ビット/秒**を用いた、歴代LLM、画像生成モデル、動画生成モデルにわたる測定。
* **観察結果**:

  * LLM: 0–3 KB/s。
  * 画像生成モデル: 指数関数的成長。
  * 動画生成モデル: 現在は遅れているが、加速が見込まれる。
  * **Gemini 2.5 Flash**は例外（低レイテンシに最適化されている）。

---

### Jinの法則

* **定式化**: *公開されている最高性能のホステッドAIモデルのインテリジェンス・バンド幅は、毎年倍増する。*
* 現在は**画像生成モデル**が先行；サービングが改善されるにつれ、動画が主導権を握ると予想。
* **予測**:

  1. **画像統合AIチャット**は1年以内に標準化される。
  2. **リアルタイム動画インタラクション**は約3年以内に実現可能（約8倍の速度向上が必要）。

---

### 限界

* **「有用な情報」**の定義は未解決のまま。
* 現在のデータは乏しいため、1年での倍増という推定は不確か。
* 成長は技術進歩だけでなく、エネルギー/経済的要因によって制限される可能性がある。

---

### 要点

* AI評価は、人間の知能評価と同様に、**正確性と速度の両方**を考慮しなければならない。
* **インテリジェンス・バンド幅**は、特に**リアルタイムのマルチモーダルインタラクション**におけるAIの未来に対する予測的な枠組みを提供する。

---

Jinの法則の予測（テキスト → 画像 → 動画 → 没入型環境）を**ビジュアルスタイルのタイムライン**としても作成しましょうか？一目で理解しやすくなると思います。