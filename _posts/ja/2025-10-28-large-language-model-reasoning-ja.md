---
audio: false
generated: true
image: false
lang: ja
layout: post
model: x-ai/grok-4-fast
title: 大規模言語モデルの推論技術
translated: true
type: note
---

### 概要
Denny Zhou（おそらく意図された「Danny Zhou」）は、スタンフォード大学のCS25: Transformers United コース（バージョン5）の一環として、「大規模言語モデルの推論」と題した講義を行いました。この講義では、大規模言語モデル(LLM)における推論について、実用的な技術、理論的基礎、そして進行中の課題を強調しながら包括的な概要を提供しています。以下は、彼のスライドと付随するノートから直接引き出された主要なポイントを構造化してまとめたものです。

### LLMにおける推論の定義
- LLMにおける推論は、基本的に、最終的な答えに直接飛びつくのではなく、入力プロンプトと最終出力の間で**中間トークン（またはステップ）を生成する**ことです。このプロセスにより、モデルは複雑な問題を分解できます。
- 必ずしも人間の推論を正確に模倣する必要はありません。目標は効果的な問題解決です。例えば、「『artificial intelligence』の最後の2文字は何か？」という問題を、単語の終わりを段階的に連結して「le」を得ることで解決する例は、中間ステップが計算をどのように助けるかを示しています。
- 理論的裏付け：サイズ*T*のブール回路で解ける問題に対して、定数サイズのTransformerは*O(T)*個の中間トークンを生成することで対処可能であり、大規模なモデルスケーリングの必要性を回避できます。

### 動機
- 事前学習済みLLMは、特別なプロンプトやファインチューニングなしに本質的に推論能力を備えています。LLMに推論ができないという通説は覆されています。問題は、推論された出力を表面化させられないデコード方法から生じます。
- このアプローチは「The Bitter Lesson」に沿っています。人間らしい近道よりも、（トークン生成による）計算を活用し、次のトークン予測を通じて人間らしい振る舞いを創発させます。
- 高価な人手による注釈の代わりに、モデルが生成したデータを使用し、正答率のような最終目標の指標に対して最適化することに焦点を当てます。

### 核心的なアイデア
- **Chain-of-Thought (CoT) デコーディング**: 複数の候補となる応答を生成し、最終的な答えに対する確信度が最も高いものを選択します。推論されたパスは、直接的な推測（例：シナリオ内でのリンゴの数え上げ）よりも高い確信度を持つことが多いです。
- **深度ではなく長さによるスケーリング**: 系列的な問題に対して、モデルがより長いシーケンス（*O(T)* トークン）を生成するように訓練します。これにより、モデルサイズを肥大化させることなく、任意の能力を持たせることができます。
- **単一ショットよりも集約**: 複数の応答を生成して組み合わせる（例：多数決による）ことは、単一の出力よりも優れています。類似問題の検索＋推論は、推論のみよりも優れた性能を発揮します。
- 例：Gemini 2.0の「思考モード」は、演算の優先順位付け（例：45 × 45 = 2025）を行うことで、1から10の数字を使って2025を作るパズルを解きます。

### 主要な技術
- **プロンプティング**: 中間ステップを引き出すために、少数ショットの例や「段階的に考えましょう」のようなフレーズを使用します（例：数学の文章題）。ゼロショットでも動作しますが、信頼性は低いです。
- **教師ありファインチューニング (SFT)**: 人間が注釈を付けた段階的な解答データで訓練し、推論パスが生成される尤度を高めます。
- **自己改善**: モデルの出力から正しく推論された解答をフィルタリングして、自身の訓練データを生成します。
- **強化学習ファインチューニング (ReFT)**: 検証器を使用して、正しい完全な応答（推論＋答え）を報酬とし、間違った応答を罰することで、反復的に学習します。これは検証可能なタスクに対して最もよく汎化します。Jonathan Laiらのチームメンバーに謝意。
- **Self-Consistency**: 複数の推論パスをサンプリングし、その後（例：最も頻出する答えで）集約します。自由回答形式のタスクに対する普遍的な変種では、モデル自身に選択させます。
- **検索＋推論**: 関連する例を引き込んでブートストラップします（例：面積の問い合わせに対して距離の公式を思い出す）。
- **その他の強化技術**: 抽象化のための「Take a Step Back」、確率的デコーディングのバイアスを修正する周辺化など。

### 限界
- **プロンプティング**: シンプルだが脆い—タスク固有の例が必要。汎用的なプロンプトは性能が低い。
- **SFT**: 分布外の問題への汎化性が低い（例：訓練後も新しい「strawberry」の文字数カウント問題で失敗する）。
- **RL**: 信頼できる検証器に依存するが、すべてのタスクで利用可能とは限らない（例：創造的ライティング）。
- **一般的な課題**: LLMは確率的な予測器であるため、貪欲デコーディングは流暢だが間違った答えを好みます。集約は助けになりますが、検証不可能な領域の問題は解決しません。
- デコーディングの不整合: 強力なモデルであっても、介入なしでは推論された出力を最高に評価しない可能性があります。

### 今後の方向性
- ベンチマークを超えて、実世界のアプリケーション、特に検証不可能なタスク（例：完璧な指標がない翻訳最適化）へ押し進める。
- 集約と検索をスケールさせて、より大きな利益を得る。
- 周辺化などを通じてデコーディングを改善し、確率的な出力と正答性の整合性を高める。
- セッションを超えて成功した推論パスを「記憶」するシステムを構築する。

この講義は、強化学習ファインチューニングを現在の主力技術として強調し、推論をスケーラブルな生成問題として捉える視点への移行を示しています。

### 参考文献
- [LLM Reasoning Slides (PDF)](https://dennyzhou.github.io/LLM-Reasoning-Stanford-CS-25.pdf)
- [Denny Zhou's X Post on Key Points](https://x.com/denny_zhou/status/1948499173986201915)
- [Lecture Video on YouTube](https://www.youtube.com/watch?v=ebnX5Ur1hBk)
- [CS25 Course Page](https://web.stanford.edu/class/cs25/)