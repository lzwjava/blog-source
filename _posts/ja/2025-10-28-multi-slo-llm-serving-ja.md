---
audio: false
generated: true
image: false
lang: ja
layout: post
model: x-ai/grok-4-fast
title: マルチSLO LLMサービングの最適化
translated: true
type: note
---

### 概要
論文「SLOs-Serve: Optimized Serving of Multi-SLO LLMs」は、SLOs-Serveを紹介しています。これは、各ステージ（例：入力処理のためのプリフィル、トークン生成のためのデコード）とアプリケーション（例：チャットボット、コーディングアシスタント）が特定のサービスレベル目標（SLO）を持つ多段階アプリケーションにおいて、大規模言語モデル（LLM）を効率的に提供するために設計されたシステムです。これらのSLOは、プリフィルにおける最初のトークンまでの時間（TTFT）やデコードにおける出力時間あたりのトークン数（TPOT）など、ユーザーに向けた側面での低遅延を保証します。vLLMやSarathi-Serveのような従来の提供システムはスループットを優先しますが、特にバースト時や混合ワークロード下では、共有リソースにおいてこれらのきめ細かいSLOを侵害することがよくあります。

### 主な課題と貢献
著者らは、マルチSLO提供における以下の課題を特定しています：
- **多段階リクエスト**: 推論LLM（「思考」段階での厳格なSLO）やツール呼び出しエージェント（厳格なプリフィル/デコードを含むループ）などのアプリケーションは、段階固有の保証を必要とします。
- **リソース競合**: 共有GPUは、同一配置または分離構成においてSLO違反を引き起こします。
- **バースト的なトラフィック**: 急激な急増がスケジューラを圧倒します。

SLOs-Serveの貢献は以下を含みます：
- SLOを満たしながらスループットを最大化するために、トークン割り当て（プリフィル予算、バッチサイズ）を最適化する動的計画法（DP）ベースのスケジューラ。
- チャンク化プリフィル、SLO適応的投機的デコード（SLO階層ごとに投機長をカスタマイズ）、およびソフトなアドミッション制御（許可されたリクエストのSLOを保証し、他を延期）のサポート。
- vLLMをバッチ処理に、Rayをオーケストレーションに使用して構築された、マルチレプリカルーティングとバースト耐性を備えた分散アーキテクチャ。

| アプリケーション | プリフィルSLO | デコードSLO | 例 |
|-------------|-------------|------------|---------|
| 要約 | 厳格 (例: 最大3倍の低速化) | 緩い (100ms TPOT) | 文書処理 |
| コーディング | 緩い | 厳格 (50ms TPOT) | コード生成 |
| チャットボット | 緩い | 緩い | 対話型クエリ |
| ツール呼び出し | 厳格 (ループ内) | 厳格 (ループ内)、緩い (最終) | エージェントワークフロー |
| 推論 | 厳格 (思考中) | 厳格 (思考中)、緩い (応答) | 連鎖思考 |

### システム設計
- **スケジューラ (アルゴリズム1)**: DPを使用してリクエストを許可しバッチを計画し、Rooflineに着想を得た予測器（R² > 0.8 精度）を通じて実行時間をモデル化します。状態はメモリ、プリフィル予算、許可されたリクエストを追跡し、遷移は早期のデッドラインとSLO達成を優先します。
- **バッチ形成**: 最も厳しいTPOTに基づく動的サイズ設定（512+トークンまで）。SLO違反なしでより高いスループットを得るために、より大きなバッチを可能にします。
- **投機的デコード**: プリフィル予算を増強するために、SLO階層ごとに投機長（例: 1-10トークン）を適応させ、最適なプリフィル/デコードのバランスを見つけるために列挙法で解決します。
- **マルチレプリカとバースト**: 集中型コントローラがリクエストを proactive にルーティングします。達成不能なリクエストはベストエフォートキューに送られ、必要に応じてプリエンプトされます。

この設計は、より大きなバッチがスループットを向上させるが遅延のリスクを高めるといったトレードオフを探求します（SLO達成可能領域を示す図で可視化されています）。

### 評価
実トレース（Azure LLMワークロード）とデータセット（ShareGPT、HumanEvalなど）を使用し、6つのシナリオ（チャットボット、コーディング、要約、混合、ツール呼び出し、推論）でテストされました。モデル：OPT-7B/13B/30B、ToolLlama-7B。ハードウェア：4×A100 GPU（メイン）、最大8×H100。

- **容量向上**: vLLM/Sarathi-Serve/DistServeに対する2.2倍の平均改善（シナリオ全体の幾何平均）。例：推論で2.4倍、バースト的なコーディングで2.1倍。
- **スケーリング**: ルーティングによりレプリカ数に対して超線形的（ツール呼び出しでバースト下最大6.2倍）。
- **アブレーション研究**: 投機的デコードが1.66倍、ルーティングが1.19倍、バースト処理が1.34倍の効果。
- オーバーヘッド: スケジュール呼び出しあたり<10ms。

図表は、SLOs-Serveが急増時にp99レイテンシを目標値近くに維持するのに対し、ベースラインは劣化することを示しています。

### 結論
SLOs-Serveは、多様でバースト的な環境においてマルチSLOを保証することでLLM提供を進歩させ、2倍以上の効率向上を実現します。将来の課題としては、未知のデコード長やより多くの段階への拡張が考えられます。このシステムは、SLOにカスタマイズされたスケジューリングがプロダクショングレードのLLM導入にとって重要であることを実証しています。

[SLOs-Serve: Optimized Serving of Multi-SLO LLMs](https://arxiv.org/abs/2504.08784)