---
audio: false
generated: true
lang: ja
layout: post
title: 決定木
translated: true
type: note
---

**決定木**は、機械学習やデータ分析において、入力データに基づいて意思決定を行うための予測モデリングツールです。意思決定とその可能な結果（偶然の事象の結果を含む）を木構造で表現します。決定木は、分類（例：顧客が商品を購入するかどうかの予測）や回帰（例：住宅価格の予測）などのタスクに広く使用されています。直感的で解釈が容易であり、単純なデータセットから複雑なデータセットまで効果的に機能します。

この包括的なガイドでは、決定木とは何か、その仕組み、構成要素、構築プロセス、利点、限界、実践的な考慮事項について、例を交えて説明します。

---

### **決定木とは何か？**

決定木は、意思決定とその可能な結果をフローチャートのように表現したものです。ノードと枝で構成されます：
- **ノード**: 意思決定、条件、または結果を表します。
- **枝**: 意思決定や条件の可能な結果を表します。
- **葉**: 最終的な出力（例：分類におけるクラスラベル、回帰における数値）を表します。

決定木は教師あり学習で使用され、モデルはラベル付けされた訓練データから学習し、新しい未見のデータに対する結果を予測します。カテゴリデータと数値データの両方を扱える汎用性があります。

---

### **決定木の構成要素**

1. **ルートノード**:
   - 木の最上位のノード。
   - データセット全体と最初の決定点を表します。
   - 最も多くの情報を提供する、または不確実性を最も減少させる特徴に基づいて分割されます。

2. **内部ノード**:
   - ルートと葉の間にあるノード。
   - 特定の特徴と条件に基づく中間的な決定点を表します（例：「年齢 > 30？」）。

3. **枝**:
   - ノード間の接続。
   - 意思決定や条件の結果を表します（例：二値分割における「はい」または「いいえ」）。

4. **葉ノード**:
   - 最終的な出力を表す終端ノード。
   - 分類では、葉はクラスラベル（例：「購入」または「非購入」）を表します。
   - 回帰では、葉は数値（例：予測価格）を表します。

---

### **決定木の仕組み**

決定木は、入力データを特徴量の値に基づいて領域に再帰的に分割し、その領域内の多数派クラスまたは平均値に基づいて意思決定を行うことで機能します。その動作を段階的に説明します：

1. **入力データ**:
   - データセットには特徴量（独立変数）と目的変数（従属変数）が含まれます。
   - 例えば、顧客が商品を購入するかどうかを予測するデータセットでは、特徴量は年齢、収入、閲覧時間など、目的変数は「購入」または「非購入」です。

2. **データの分割**:
   - アルゴリズムは、データを部分集合に分割するための特徴量と閾値（例：「年齢 > 30」）を選択します。
   - 目標は、クラスの分離を最大化（分類の場合）または分散を最小化（回帰の場合）する分割を作成することです。
   - 分割基準には、**ジニ不純度**、**情報利得**、または**分散減少**などの指標が使用されます（後述）。

3. **再帰的分割**:
   - アルゴリズムは各部分集合に対して分割プロセスを繰り返し、新しいノードと枝を作成します。
   - これは停止条件が満たされるまで続きます（例：最大深度、ノードあたりの最小サンプル数、これ以上の改善がない場合）。

4. **出力の割り当て**:
   - 分割が停止すると、各葉ノードに最終的な出力が割り当てられます。
   - 分類では、葉はその領域内の多数派クラスを表します。
   - 回帰では、葉はその領域内の目的変数の値の平均（または中央値）を表します。

5. **予測**:
   - 新しいデータポイントの結果を予測するには、木がルートから葉まで、データポイントの特徴量の値に基づく決定ルールをたどります。
   - 葉ノードが最終的な予測を提供します。

---

### **分割基準**

分割の質は、木がデータをどれだけうまく分離するかを決定します。一般的な基準は以下の通りです：

1. **ジニ不純度 (分類)**:
   - ノードの不純度（クラスがどれだけ混ざっているか）を測定します。
   - 式: \( \text{Gini} = 1 - \sum_{i=1}^n (p_i)^2 \)、ここで \( p_i \) はノード内のクラス \( i \) の比率です。
   - ジニ不純度が低いほど、より良い分割（より均質なノード）を示します。

2. **情報利得 (分類)**:
   - **エントロピー**に基づきます。エントロピーはノード内のランダム性または不確実性を測定します。
   - エントロピー: \( \text{Entropy} = - \sum_{i=1}^n p_i \log_2(p_i) \)。
   - 情報利得 = 分割前のエントロピー - 分割後の重み付き平均エントロピー。
   - 情報利得が高いほど、より良い分割を示します。

3. **分散減少 (回帰)**:
   - 分割後の目的変数の分散の減少を測定します。
   - 分散: \( \text{Variance} = \frac{1}{n} \sum_{i=1}^n (y_i - \bar{y})^2 \)、ここで \( y_i \) は目的変数の値、\( \bar{y} \) は平均値です。
   - アルゴリズムは分散減少を最大化する分割を選択します。

4. **カイ二乗 (分類)**:
   - 分割がクラスの分布を有意に改善するかどうかを検定します。
   - CHAIDなどの一部のアルゴリズムで使用されます。

アルゴリズムは各特徴量についてすべての可能な分割を評価し、最良のスコア（例：最も低いジニ不純度、または最も高い情報利得）を持つ分割を選択します。

---

### **決定木の構築方法**

決定木の構築には以下のステップが含まれます：

1. **最良の特徴量の選択**:
   - 選択された基準（例：ジニ、情報利得）を使用して、すべての特徴量と可能な分割点を評価します。
   - データを最もよく分離する特徴量と閾値を選択します。

2. **データの分割**:
   - 選択された特徴量と閾値に基づいてデータセットを部分集合に分割します。
   - 各部分集合に対して子ノードを作成します。

3. **再帰的に繰り返す**:
   - 停止条件が満たされるまで、各子ノードに同じプロセスを適用します：
     - 木の最大深度に到達。
     - ノード内の最小サンプル数。
     - 分割基準に有意な改善がない。
     - ノード内のすべてのサンプルが同じクラスに属する（分類の場合）、または同様の値を持つ（回帰の場合）。

4. **木の剪定 (オプション)**:
   - 過学習を防ぐため、予測精度にほとんど寄与しない枝を除去して木の複雑さを軽減します。
   - 剪定は**事前剪定**（構築中に早期停止）または**事後剪定**（構築後に枝を除去）があります。

---

### **例：分類決定木**

**データセット**: 年齢、収入、閲覧時間に基づいて顧客が商品を購入するかどうかを予測。

| 年齢 | 収入 | 閲覧時間 | 購入? |
|-----|--------|---------------|------|
| 25  | Low    | Short         | No   |
| 35  | High   | Long          | Yes  |
| 45  | Medium | Medium        | Yes  |
| 20  | Low    | Short         | No   |
| 50  | High   | Long          | Yes  |

**ステップ 1: ルートノード**:
- すべての特徴量（年齢、収入、閲覧時間）を評価し、最良の分割を探します。
- 「収入 = High」が最も高い情報利得を与えると仮定します。
- データを分割：
  - 収入 = High: すべて「Yes」（純粋なノード、ここで停止）。
  - 収入 = Low または Medium: 混合（分割を継続）。

**ステップ 2: 子ノード**:
- 「Low または Medium 収入」の部分集合に対して、残りの特徴量を評価します。
- 「年齢 > 30」が最良の分割を与えると仮定します：
  - 年齢 > 30: ほとんど「Yes」。
  - 年齢 ≤ 30: すべて「No」。

**ステップ 3: 停止**:
- すべてのノードが純粋（1つのクラスのみを含む）または停止条件を満たす。
- 木は以下のようになります：
  - ルート: 「収入はHighですか？」
    - はい → 葉: 「購入」
    - いいえ → 「年齢は30より大きいですか？」
      - はい → 葉: 「購入」
      - いいえ → 葉: 「非購入」

**予測**:
- 新しい顧客: 年齢 = 40, 収入 = Medium, 閲覧時間 = Short。
- 経路: 収入 ≠ High → 年齢 = 40 > 30 → 「購入」と予測。

---

### **例：回帰決定木**

**データセット**: サイズと場所に基づく住宅価格の予測。

| サイズ (平方フィート) | 場所 | 価格 ($K) |
|--------------|----------|------------|
| 1000         | Urban    | 300        |
| 1500         | Suburban | 400        |
| 2000         | Urban    | 600        |
| 800          | Rural    | 200        |

**ステップ 1: ルートノード**:
- 分割を評価（例：サイズ > 1200, 場所 = Urban）。
- 「サイズ > 1200」が分散を最小化すると仮定します。
- 分割：
  - サイズ > 1200: 価格 = {400, 600} (平均 = 500)。
  - サイズ ≤ 1200: 価格 = {200, 300} (平均 = 250)。

**ステップ 2: 停止**:
- ノードが十分に小さい、または分散減少が最小。
- 木：
  - ルート: 「サイズは1200より大きいですか？」
    - はい → 葉: $500Kと予測。
    - いいえ → 葉: $250Kと予測。

**予測**:
- 新しい住宅: サイズ = 1800, 場所 = Urban → サイズ > 1200 → $500Kと予測。

---

### **決定木の利点**

1. **解釈性**:
   - 理解しやすく可視化も容易なため、非技術的な関係者に決定を説明するのに理想的です。
2. **混合データの扱い**:
   - 大規模な前処理なしで、カテゴリデータと数値データの両方で動作します。
3. **ノンパラメトリック**:
   - 基礎となるデータ分布についての仮定を必要としません。
4. **特徴量の重要度**:
   - どの特徴量が予測に最も寄与しているかを特定します。
5. **高速な予測**:
   - 一度訓練されると、予測は単純な比較を含むため高速です。

---

### **決定木の限界**

1. **過学習**:
   - 深い木は訓練データを記憶し、汎化性能が低下する可能性があります。
   - 解決策: 剪定の使用、最大深度の制限、ノードあたりの最小サンプル数の設定。
2. **不安定性**:
   - データの小さな変化が完全に異なる木を生み出す可能性があります。
   - 解決策: ランダムフォレストや勾配ブースティングなどのアンサンブル手法の使用。
3. **優位クラスへのバイアス**:
   - 1つのクラスが支配的な不均衡なデータセットでは苦戦します。
   - 解決策: クラスの重み付けやオーバーサンプリングなどの技術の使用。
4. **貪欲なアプローチ**:
   - 分割は局所最適化に基づいて選択されるため、大域的に最適な木にならない可能性があります。
5. **線形関係の扱いの悪さ**:
   - 特徴量と目的変数の間の関係が線形または複雑なデータセットでは効果が低いです。

---

### **実践的な考慮事項**

1. **ハイパーパラメータ**:
   - **最大深度**: 過学習を防ぐため、木の深度を制限します。
   - **分割に必要な最小サンプル数**: ノードを分割するために必要な最小サンプル数。
   - **葉の最小サンプル数**: 葉ノード内の最小サンプル数。
   - **最大特徴量数**: 各分割で考慮する特徴量の数。

2. **剪定**:
   - 事前剪定: 木の構築中に制約を設定します。
   - 事後剪定: 検証性能に基づいて木を構築した後で枝を除去します。

3. **欠損値の扱い**:
   - 一部のアルゴリズム（例：CART）は、誤差を最小化する枝に欠損値を割り当てます。
   - あるいは、訓練前に欠損値を代入します。

4. **スケーラビリティ**:
   - 決定木は、中小規模のデータセットでは計算効率が良いですが、特徴量が多い非常に大規模なデータセットでは遅くなる可能性があります。

5. **アンサンブル手法**:
   - 限界を克服するため、決定木はしばしばアンサンブルで使用されます：
     - **ランダムフォレスト**: データと特徴量のランダムな部分集合で訓練された複数の木を組み合わせます。
     - **勾配ブースティング**: 各木が前の木の誤差を補正するように、順次木を構築します。

---

### **決定木の応用例**

1. **ビジネス**:
   - 顧客離反予測、与信スコアリング、マーケティングセグメンテーション。
2. **医療**:
   - 疾患診断、リスク予測（例：心臓病）。
3. **金融**:
   - 不正検出、ローンのデフォルト予測。
4. **自然言語処理**:
   - テキスト分類（例：感情分析）。
5. **回帰タスク**:
   - 住宅価格や販売予測などの連続的な結果の予測。

---

### **可視化の例**

決定木がデータをどのように分割するかを説明するために、2つの特徴量（例：年齢と収入）と2つのクラス（購入、非購入）を持つ単純な分類データセットを考えます。以下は、決定木が特徴空間をどのように分割するかを示す概念図です。

```
chartjs
{
  "type": "scatter",
  "data": {
    "datasets": [
      {
        "label": "Buy",
        "data": [
          {"x": 35, "y": 50000},
          {"x": 45, "y": 60000},
          {"x": 50, "y": 80000}
        ],
        "backgroundColor": "#4CAF50",
        "pointRadius": 6
      },
      {
        "label": "Not Buy",
        "data": [
          {"x": 20, "y": 20000},
          {"x": 25, "y": 30000}
        ],
        "backgroundColor": "#F44336",
        "pointRadius": 6
      }
    ]
  },
  "options": {
    "scales": {
      "x": {
        "title": { "display": true, "text": "Age" },
        "min": 15,
        "max": 60
      },
      "y": {
        "title": { "display": true, "text": "Income ($)" },
        "min": 10000,
        "max": 100000
      }
    },
    "plugins": {
      "title": { "display": true, "text": "Decision Tree Feature Space" },
      "legend": { "display": true }
    }
  }
}
```

このチャートは、2次元の特徴空間におけるデータポイントを示しています。決定木はこの空間を（例：年齢 = 30 または収入 = 40000 で）分割し、「購入」と「非購入」を分離するかもしれません。

---

### **実装における実践**

決定木は以下のライブラリを使用して実装できます：
- **Python**: Scikit-learn (`DecisionTreeClassifier`, `DecisionTreeRegressor`), XGBoost, LightGBM。
- **R**: `rpart`, `party`。
- **その他のツール**: Weka, MATLAB, Spark MLlib。

Python (Scikit-learn) での例:
```python
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score

# サンプルデータ
X = [[25, 20000], [35, 50000], [45, 60000], [20, 30000], [50, 80000]]  # 特徴量: 年齢, 収入
y = [0, 1, 1, 0, 1]  # ラベル: 0 = 非購入, 1 = 購入

# データ分割
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# 決定木を訓練
clf = DecisionTreeClassifier(max_depth=3, random_state=42)
clf.fit(X_train, y_train)

# 予測
y_pred = clf.predict(X_test)

# 評価
print("Accuracy:", accuracy_score(y_test, y_pred))
```

---

### **結論**

決定木は、分類および回帰タスクにおける強力で解釈可能なモデルです。これらは、ジニ不純度や情報利得などの基準に基づいて特徴条件でデータを再帰的に分割することで機能します。使用と可視化が容易ですが、過学習や不安定性を起こしやすく、これは剪定やランダムフォレストなどのアンサンブル手法を使用することで軽減されることがよくあります。

さらに探求するには、実際のデータセットを使用して決定木を実験し、ハイパーパラメータを調整し、パフォーマンスを向上させるためにそれらをアンサンブル手法と組み合わせてみてください。特定のデータセットやユースケースがあれば、 tailored な例や可視化を提供できます！