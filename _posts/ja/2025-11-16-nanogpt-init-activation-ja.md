---
audio: false
generated: true
image: false
lang: ja
layout: post
model: x-ai/grok-4-fast
title: ナノGPT初期化とアクティベーションの詳細
translated: true
type: note
---

アンドレイ・カーパシーの **NanoGPT**（2022年末から2025年にかけて人気のミニマルGPT実装）では、モデルはReLUを使用**しません**。代わりに**GELU**（Gaussian Error Linear Unit）を活性化関数として使用しており、これは現代のGPTスタイルのモデル（GPT-2、GPT-3、Llamaなどと同じ）における標準的な選択です。

### NanoGPTの主要なパラメータ初期化範囲（最新2025年版時点）

| パラメータ / コンポーネント      | 初期化方法                                     | 典型的なスケール / 範囲                              | 備考 |
|----------------------------------|------------------------------------------------|-----------------------------------------------------|-------|
| 埋め込み層（トークン＋位置）     | `nn.Embedding` → 正規分布                    | std = 0.02                                          | 初期のロジットを小さく保つため非常に小さい |
| FFN内の線形層（proj）            | `nn.Linear` → デフォルトのPyTorch初期化 → その後スケーリング | 重み: スケーリング後 std ≈ 0.02–0.03               | カーパシーは一部の設定で `0.02 / sqrt(n_embd)` などのスケーリング係数を適用 |
| 最終LMヘッド（出力投影）         | 埋め込み層と同じ（重み共有）                   | std = 0.02                                          | トークン埋め込みと重みを共有 |
| LayerNormのバイアス              | ゼロ初期化                                     | 0                                                   | 標準的 |
| LayerNormの重み                  | 1で初期化                                      | 1.0                                                 | 標準的 |
| 残差接続のスケーリング（初期化後）| 重みに小さな係数を乗算                         | 多くの場合 `weight *= 0.02` または `weight *= sqrt(2/n_layers)` | 初期化時の訓練安定化のための重要なトリック |
| AttentionのQKV投影               | 上記と同じスケーリング                         | 実効的な std ≈ 0.02                                | 他の線形層と同じスケーリング |
| Attentionの出力投影              | 追加スケーリング: `cproj.weight *= 0.02` など | さらに小さい（残差経路を補償するため）              | 深いモデルでは非常に重要 |

### NanoGPTの特別な初期化トリック
`model.py` では、各 `nn.Linear` 層を作成した後、カーパシーは以下のような処理を行っています（正確な値は設定に依存）：

```python
if isinstance(m, nn.Linear):
    torch.nn.init.normal_(m.weight, mean=0.0, std=0.02)
    if m.bias is not None:
        torch.nn.init.zeros_(m.bias)
    # 深いネットワークにおける重要な残差スケーリング
    if "cproj" in name:          # AttentionまたはFFNの出力投影
        m.weight.data *= 0.02    # または場合によっては *= 1/sqrt(n_layer)
```

または、より最近のバージョン（2024–2025）では、設定駆動のスケーリングを使用しています：

```python
std = 0.02 / math.sqrt(2 * n_layer)   # 深いモデルに有効
```

これにより、深度が増加しても残差ストリームの分散が爆発したり消失したりしないことが保証されます。

### まとめ: NanoGPTはReLUを使用しない
- 活性化関数: **GELU**（ReLUではない）
- 主な初期化の標準偏差: **~0.02**
- 出力投影への追加スケーリング: **0.02 またはそれより小さい**（特に重要）
- ほとんどの線形層ではバイアスを使用しない（LayerNormを除く）

この非常に小さな初期化と残差スケーリングが、NanoGPTが最小構成において学習率のウォームアップなしでも安定して訓練できる理由です。

独自のNanoGPTをスクラッチから訓練する場合は、これらの正確な初期化トリックをコピーしてください — これらは多くの人が考える以上に重要です！