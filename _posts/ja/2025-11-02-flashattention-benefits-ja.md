---
audio: false
generated: true
image: false
lang: ja
layout: post
model: openai/gpt-5
title: フラッシュアテンションアルゴリズムとその利点
translated: true
type: note
---

# FlashAttentionの詳細解説（Tri Dao et al., 2022）

短い要約：FlashAttentionは、オンチップSRAM（レジスタ/共有メモリ）内でアテンションを**ブロック単位**で計算し、アテンションパイプライン全体を単一カーネルに融合し、数値的に正確なブロック単位のソフトマックス累積を使用することで、GPU DRAM（HBM）に完全な \\(N\times N\\) アテンションマトリックスを割り当てる必要性を排除します。これにより、HBMトラフィックとメモリ使用量を \\(O(N^2)\\) から実質的に \\(O(N)\\) に大幅に削減し、長いシーケンスに対するGPUでの実効速度を大幅に向上させます。citeturn0search0turn0search9

---

## 問題点：標準的なアテンションがIOバウンドである理由
Transformerのセルフアテンション（スケーledドット積）は通常、以下の3つのステップで実装されます：

1. スコア \\(S = Q K^\top\\)（サイズ \\(N\times N\\)）を計算；
2. 行単位のソフトマックス \\(P = \mathrm{softmax}(S)\\) を計算；
3. 出力 \\(O = P V\\) を計算。

ナイーブな実装では、\\(S\\)（そして多くの場合 \\(P\\)）をGPU DRAMに実体化します。シーケンス長 \\(N\\) に対して、これは \\(O(N^2)\\) のメモリを使用し、2つのIO問題を引き起こします：
- 大きなDRAMフットプリント（多くの場合、GPUメモリを最初に逼迫させる要因）、そして
- DRAM（HBM）とオンチップSRAM/レジスタ間の多数の読み書き — そしてこれらのHBM↔SRAM転送が、現代のGPUにおける真のボトルネックです。

FlashAttentionは、アテンションを単なるFLOP問題ではなく、**IO問題**として再定義し、HBMアクセスの削減を目指します。citeturn0search0

---

## 核となるアイデア（概要）
1. 行列 \\(Q, K, V\\) をオンチップSRAM（共有メモリ/レジスタ）に収まるブロックに**タイル分割**します。
2. **アテンションをブロック単位で処理**：与えられた \\(Q\$-タイルとストリーミングされる一連の \\(K,V\\)-タイルに対して、出力への部分的な寄与を計算し、即座にそれらを累積します — 完全な \\(N\times N\\) スコア行列をDRAMに実体化することは決してありません。
3. **すべてを単一カーネルに融合**：カーネルはタイルをSRAMにロードし、そのタイルペアに対して \\(QK^\top\\) を計算し、ソフトマックスロジックを適用し、\\(V\\)-タイルを乗算し、部分的な出力を書き込みます — すべて、中間的な大きな行列のDRAMへの往復転送なしに行われます。カーネル融合は、命令とメモリのオーバーヘッドを削減します。
4. **ブロック単位の数値的に安定したソフトマックス累積**：行全体に対するソフトマックスにはグローバルな最大値と合計が必要なため、FlashAttentionは実行中の最大値/合計値（log-sum-expスタイル）を使用して、複数の \\(K\\)-タイルからのソフトマックスの寄与を、スコアの行全体を保存することなく、正確かつ安定して結合します。
5. **再計算による逆伝播**：逆伝播のための大きな中間値を保存する代わりに、逆伝播パス中に各ブロックの順伝播アテンションを再計算します（余分なFLOPsと引き換えに、DRAM IOを大幅に削減）。節約されたDRAM IOは、通常、DRAM IOが支配的であるため、正味の速度向上をもたらします。citeturn0search2turn0search10

これらのアイデアを組み合わせることで、メモリ削減と実効速度の両方の改善が達成されます。citeturn0search0

---

## ブロック単位のアルゴリズム — ステップバイステップ（順伝播）
シーケンス長 \\(N\\)、ヘッド次元 \\(d\\) の単一のアテンションヘッドを考えます。タイルサイズ \\(B\\) を選択し、\\(B\times B\\) のスコアブロックと対応する \\(Q\\), \\(K\\), \\(V\\) タイルがSRAMに収まるようにします。

各クエリタイル \\(Q_{i}\\)（行 \\(iB:(i+1)B\\)）に対して：

1. 出力アキュムレータ \\(O_i \leftarrow 0\\) を初期化します。
2. 実行中の正規化状態を初期化：`row_max`（クエリ行ごと）を \\(-\infty\\)、`row_sum` を 0 に設定します。これらは、複数のKタイルにわたるソフトマックスの数値的に安定した分母を追跡します。
3. 各キー/値タイル \\(K_{j}, V_{j}\\)（列 \\(jB:(j+1)B\\)）に対して：
   - \\(Q_i\\), \\(K_j\\), \\(V_j\\) をSRAMにロードします。
   - 生のスコアのタイル \\(S_{ij} = Q_i K_j^\top / \sqrt{d}\\)（形状 \\(B\times B\\)、ベクトル化された形式）を計算します。
   - \\(S_{ij}\\) の各行について、ローカルの行最大値 \\(m_{ij}\\) と指数化された値 \\(\exp(S_{ij} - m_{ij})\\) を計算します。
   - このタイルの指数値をlog-sum-expのトリックを使用して実行中の行正規化にマージします：
     - \\(M = \max(\text{row\_max}, m_{ij})\\) とします。
     - `row_sum` := `row_sum` · exp(row_max − M) + local_sum · exp(m_{ij} − M) で更新します。
     - `row_max` := \\(M\\) を設定します。
   - 適切にスケーリングされた指数値を使用して、アキュムレータへのタイルの寄与を計算します： \\(O_i \mathrel{+}= \text{(tile-softmax)} \times V_j\\) を累積します。（すべてSRAM内で行われます。）
4. すべてのKタイルをストリーミングした後、row_sum と row_max を使用して正規化を最終化し、正しいソフトマックス出力を生成します； \\(O_i\\) をDRAMに書き込みます。

重要なポイント：\\(N\times N\\) 行列は決してDRAMに書き込まれません；小さなタイルと最終的な出力のみが書き込まれます。実行中の最大値と合計値を使用した数値的に正確な累積により、タイルごとのソフトマックスの断片が、行全体に対する完全なソフトマックスと同じ結果に正確に結合されます。citeturn0search2turn0search10

---

## カーネル融合とSRAMタイル化が実際に効果を発揮する理由
- **HBMアクセスの削減：** 標準的なアテンションは、DRAMへの読み書きが \\(O(N^2)\\) 要素（スコア、ソフトマックス）です。FlashAttentionは、各 \\(Q,K,V\\) 要素を定数回読み取り、すべての一時的なスコア/ソフトマックス値はSRAM内のみに存在します。論文のIO分析は、より少ないHBMアクセスと、SRAMサイズが与えられた場合にFlashAttentionがIO最適である範囲を示しています。citeturn0search0
- **レイテンシと帯域幅の制限がFLOPsよりも重要：** GPUはFP乗算累算が非常に高速です；DRAMトラフィックが実行時間を支配する場合、DRAM転送の削減はFLOPsの削減よりも重要です。カーネル融合は中間的なDRAMトラフィックを除去し、カーネル起動オーバーヘッドを削減します。citeturn0search0
- **逆伝播パスのトレードオフ：** 逆伝播中に順伝播ブロックを再計算するとFLOPsは増加しますが、大きな中間値をDRAMに保存することを回避します。再計算はSRAMで行われ、DRAMトラフィックを制限するため、多くの場合、実効時間において正味の利益となります。citeturn0search10

論文およびその後の研究からの実証結果は、複数倍の高速化（報告されたベンチマークでは、モデルとシーケンス長に応じて2〜7倍）とピークメモリの大幅な削減を示しています。citeturn0search0turn0search10

---

## 重要な実装の詳細とトレードオフ

- **タイルサイズの選択：** タイル \\(B\\) は、ワーキングセット（Q, K, Vのタイル、スコアバッファ、部分アキュムレータ、追加のスクラッチ）がスレッドブロックごとのオンチップSRAMに収まるように選択されなければなりません。最適な \\(B\\) は、ヘッド次元、データタイプ（FP16/FP32/FP8）、およびGPUアーキテクチャ（共有メモリ/レジスタの量）に依存します。小さすぎると計算効率が低下し、大きすぎるとSRAMに収まりません。citeturn0search2

- **数値的安定性：** アルゴリズムは行ごとの実行中の最大値と合計値（log-sum-expマージ）を使用して、最終的なソフトマックスが完全行列のソフトマックスと等しくなることを保証します。これは極めて重要です：FlashAttentionは、その安定した累積のために、**正確なアテンション**（近似ではありません）です。citeturn0search0

- **マスキングと因果性：** 因果的マスキング（自己回帰）は、ストリーミングされるタイル内のマスクされた位置からの寄与を単純にスキップまたはゼロ化し、実行中の正規化をそれに応じて更新することで処理されます。ブロック単位のロジックは依然として機能しますが、マスクされた要素がアキュムレータを汚染しないようにするために、注意深いタイル順序付けが必要な場合があります。citeturn0search2

- **逆伝播パスとメモリレイアウト：** FlashAttentionは最小限のメタデータ（例えば、ブロックごとのrow_max/row_sum）のみを保存し、逆伝播中に順伝播のタイル積を再計算します。実装では、再利用を最大化し、レジスタプレッシャーを最小化するために作業の順序を注意深く変更します。citeturn0search10

- **精度とデータタイプ：** FP16/FP8の使用は、タイルバッファリングと累積の選択に影響を与えます。後の研究（FlashAttention-2 / FlashAttention-3）では、混合精度と新しいGPU機能（Hopper, H100）のための最適化が追加され、利用率とFPスループットをさらに押し上げています。citeturn0search4turn0search11

- **並列性のマッピング：** カーネルは、ワープ/CTAブロックをクエリタイルにマッピングします；CTA内では、ワープが協調してK/Vタイルをロードし、タイルの行列乗算とリダクションを計算します。効率的なワープレベルのリダクションと融合乗算加算命令の使用は、ピークスループットにとって重要です。citeturn0search2

---

## FlashAttention 対 近似ロングアテンション手法
FlashAttentionは**正確な**アテンションの意味論を保持します（浮動小数点丸めまでの範囲で完全なアテンションと同じ数値結果）。一方、多くのロングアテンション手法はアテンションを近似し（スパース性、低ランク、FAVOR+など）、品質とメモリ/時間をトレードオフします。FlashAttentionは代わりに、正確な計算を保持しながらメモリ/IOコストを削減するため、モデル品質は変化せず、スループット/メモリが改善されます。それが広く魅力的である理由です：精度のトレードオフなし、単により優れた低レベルカーネルです。citeturn0search0

---

## 実用的な利用可能性とエコシステム
- 著者らは実装（CUDA）と、FlashAttentionおよび後のFlashAttention-2を維持するリポジトリを公開しました。多くのフレームワーク（Hugging Face Transformers、XLA/PyTorchフォーク、Tritonベースの実装）が、flash-attnオペレータを呼び出すか、同様の融合カーネルを提供しています。`flash_attn` オペレータまたはそれを公開するライブラリを使用できます；PyTorchでは、最近のバージョンにメモリ効率の良いアテンションプリミティブも含まれており、サードパーティの `flash_attn` パッケージは多くのワークロードに対してドロップインの速度/メモリ改善を提供します。公式リポジトリでインストーラーとAPIの例を確認してください。citeturn0search9turn0search4

注意点：「カスタムカーネルは不要」というのは部分的にのみ真実です — FlashAttention *は* フレームワークが呼び出すカスタム融合カーネル（リポジトリ内の作業）です。現代のPyTorchバージョンは内部的に同等の融合カーネルを含むか、ベンダーライブラリに委譲するかもしれませんが、核となるアイデアは融合カーネル実装（CUDA、Triton、またはベンダーコードのいずれか）を必要とします。重要な教訓：あなた（モデルユーザーとして）はそれらのカーネルを自分で書く必要はありません — 提供されているオペレータを使用してください。citeturn0search9turn0search7

---

## 拡張とフォローアップ
- **FlashAttention-2 (2023):** 並列性、作業分割、およびマルチコアスケーリングを改善し、さらに優れたGPU利用率とスループットを実現します。citeturn0search4
- **FlashAttention-3 およびその他のエンジニアリング作業 (2024+):** 新しいハードウェア（Hopper/H100）、FP8、およびさらに高いTFLOP利用率のためのさらなる調整。これらは、ハードウェアを意識した融合アテンションカーネルの傾向を継続します。citeturn0search11

---

## FlashAttentionが最も効果を発揮する場合（経験則）
- **長いシーケンス**（数千以上）または大きなバッチ/ヘッドサイズ — メモリを最も節約し、最大の高速化をもたらします。
- **DRAM帯域幅がボトルネックである場合** — 例えば、ナイーブなアテンションがDRAMを激しく使用するような大きな \\(N\\) を持つ大規模モデル。
- **大きなコンテキストでのトレーニング** — 再計算に適した逆伝播がピークメモリを削減するため（より大きなバッチ/コンテキストを可能にします）。citeturn0search0

---

## クイック擬似コード（概念的）
```
各クエリタイル Qi に対して：
    row_max = -inf (各行ごと)
    row_sum = 0    (各行ごと)
    out_acc = 0    (B x d_v)
    各キー/値タイル Kj, Vj に対して：
        Qi, Kj, Vj を SRAM にロード
        S = Qi @ Kj.T / sqrt(d)
        m = max_row(S)
        expS = exp(S - m)      # ローカルな指数値
        local_sum = sum_row(expS)
        # 正規化子を安定してマージ：
        M = max(row_max, m)
        row_sum = row_sum * exp(row_max - M) + local_sum * exp(m - M)
        row_max = M
        # スケーリングされた expS で部分出力を累積：
        out_acc += (expS / (マージ後の row_sum)) @ Vj
    out_acc を DRAM に書き込み
```
（実装では、より多くのメタデータを保存し、正確なスケーリングを注意深く配置します；上記は概念的です。）citeturn0search2

---

## 参考文献 / 次に読むべき資料
- 元のFlashAttention論文（Tri Dao et al., 2022）— アルゴリズム、IO分析、ベンチマーク。citeturn0search0turn0search2
- CUDAカーネルとPyTorchバインディングを含む公式GitHub。citeturn0search9
- FlashAttention-2論文（Tri Dao, 2023）および後のバージョンに関するエンジニアリングノート。citeturn0search4turn0search11

---

必要に応じて、以下を行うことができます：
- ナイーブなアテンションループとブロック単位のFlashAttention擬似コードの行ごとの比較を示す（幅広いテーブルなし）、
- 具体的な例（例えば、ヘッド次元64、タイル128）に対するCUDA/Tritonカーネルの構成とレジスタ/共有メモリの予算の概略を描く、または
- `torch.nn.functional.scaled_dot_product_attention` を FlashAttention に置き換えるための正確なPyTorch API呼び出しとサンプルスニペットを指摘する。
次にどれが最も役立つでしょうか？