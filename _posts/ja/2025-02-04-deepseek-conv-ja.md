---
audio: false
generated: false
lang: ja
layout: post
title: ディープシーク - 会話
translated: true
type: note
---

A: DeepSeek-V3の技術レポートを読んでいて、このモデルの規模に本当に驚かされています。6710億パラメータもあるのに、トークンごとに活性化されるのは370億だけ？これは大規模なMoEアーキテクチャですね。どうやって機能しているのですか？

B: そうですね、本当に素晴らしい成果です！DeepSeek-V3はMixture-of-Experts（MoE）フレームワークを基盤としており、各トークンに対してパラメータの一部のみを活性化できます。具体的には256のルーティングされたエキスパートを使用していますが、トークンごとに活性化されるのは8つだけです。これにより、すべてのパラメータがすべてのトークンに対して活性化される密なモデルと比較して、非常に効率的になっています。

A: なるほど。でも、どのエキスパートを活性化するかはどうやって決めるのですか？ランダムなのか、それとも何らかのルーティングメカニズムがあるのですか？

B: 良い質問です！ルーティングはトークンとエキスパートの親和性スコアに基づいています。各トークンは各エキスパートに対してスコアが割り当てられ、最も高いスコアを持つ上位K個のエキスパートが活性化されます。DeepSeek-V3はこれらのスコアを計算するためにシグモイド関数を使用しており、これによりエキスパート間の負荷バランスを取るのに役立っています。

A: つまり、ランダムではなく、トレーニング中に学習されるのですね。でも、それではエキスパートの使用に偏りが生じませんか？MoEモデルではそれが一般的な問題だと聞いたことがあります。

B: その通りです！エキスパートの使用の不均衡は問題になる可能性がありますが、DeepSeek-V3はこれを処理するために補助損失不要の戦略を導入しています。負荷分散を促進するための別個の損失項を追加する代わりに、各エキスパートのバイアス項を動的に調整します。エキスパートが過負荷の場合、そのバイアスは減少し、負荷が少ない場合はバイアスが増加します。これにより、モデルのパフォーマンスを低下させることなく負荷バランスが保たれます。

A: 賢い方法ですね。つまり、補助損失がないということは、主要なトレーニング目標への干渉が少ないということです。しかし、これは補助損失を使用する従来のMoEモデルと比較してどうですか？

B: そうです。従来のMoEモデルはしばしば負荷分散を促進するために補助損失を使用しますが、これらの損失は時としてパフォーマンスを損なうことがあります。DeepSeek-V3の補助損失不要のアプローチは、このトレードオフを回避します。実際、 ablation study では、特にコーディングや数学のようなタスクにおいて、補助損失に依存するモデルよりも一貫して優れたパフォーマンスを示しています。

A: 興味深いです。コーディングと数学について言えば、DeepSeek-V3はHumanEvalやMATHのようなベンチマークで特に優れたパフォーマンスを発揮していることに気づきました。そこにはどのような秘密があるのですか？

B: その大きな部分は、マルチトークン予測（MTP）目標にあります。単に次のトークンを予測するのではなく、DeepSeek-V3は各位置で複数の将来のトークンを予測します。これによりトレーニング信号が濃縮され、モデルが先を見越して計画するのに役立ちます。これは特にコーディングや数学のように逐次的な推論を必要とするタスクに有効です。

A: 待ってください、つまり一度に複数のトークンを予測しているのですか？推論中はそれはどのように機能するのですか？MTPはまだ使用されるのですか、それともトレーニングのためだけですか？

B: 推論中、MTPモジュールは破棄することができ、モデルは標準的な自己回帰モデルのように振る舞います。しかし、ここでクールな部分は、MTPモジュールを speculative decoding に転用できることです。これにより、複数のトークンを並列に予測し、その後それらを検証することで生成が高速化されます。

A: それは素敵なトリックですね。つまり、トレーニング中はMTPの利点を得て、その後それを推論の加速に利用するということです。しかし、アテンションメカニズムについてはどうですか？Multi-head Latent Attention（MLA）について何か見ましたが、それはどのように組み込まれているのですか？

B: MLAはもう一つの重要な革新です。これはKey-Value（KV）キャッシュを圧縮することでメモリフットプリントを削減します。完全なアテンションキーと値を保存する代わりに、低ランクの共同圧縮を使用してそれらを表現します。これにより、標準的なMulti-Head Attentionと同等のパフォーマンスを維持しながら、推論中のKVキャッシュサイズが大幅に削減されます。

A: 効率性にとって大きな勝利ですね。しかし、圧縮は情報の損失を引き起こしませんか？パフォーマンスをどのように維持するのですか？

B: 良い指摘です。圧縮は、キーと値の本質的な特徴を捉える潜在ベクトルに焦点を当てることで、最も重要な情報を保存するように設計されています。モデルはまた、位置情報を維持するためにRotary Positional Embedding（RoPE）を使用しており、これが圧縮からの損失を軽減するのに役立ちます。

A: 分かりました。つまり、MLAはあまりパフォーマンスを犠牲にすることなくメモリ使用量を削減するWin-Winのソリューションなのですね。しかし、トレーニングについてはどうですか？このサイズのモデルをトレーニングするのは非常に高価に違いありません。DeepSeek-V3はどのようにコストを抑えているのですか？

B: トレーニング効率は主要な焦点です。DeepSeek-V3はFP8混合精度フレームワークを使用しており、メモリ使用量を削減し計算を高速化します。また、パイプラインパラレリズムのためにDualPipeアルゴリズムを採用しており、パイプラインバブルを最小限に抑え、計算と通信を重ね合わせます。これらの最適化により、モデルは14.8兆トークンをわずか278万8000 H800 GPU時間でトレーニングすることができます。

A: 印象的ですね。しかし、FP8トレーニングは扱いが難しいかもしれません。低精度トレーニングは不安定性を引き起こすと聞いたことがあります。

B: その通りです。FP8トレーニングは限られたダイナミックレンジのために困難です。DeepSeek-V3はこれを細粒度量子化で対処しています。活性化と重みはより小さなタイルまたはブロックにグループ化され、独立してスケーリングされます。これにより外れ値の影響が減少し、トレーニングが安定します。また、重要な操作には高精度累積を使用して精度を維持しています。

A: なるほど。つまり、効率性と精度のバランスなのですね。しかし、データについてはどうですか？14.8兆トークンは膨大なデータセットです。どのような種類のデータでトレーニングされているのですか？

B: データセットは多様で高品質であり、英語と中国語のテキストに焦点を当てています。また、数学とプログラミングのデータも多く含まれており、これがモデルがそれらの領域で優れている理由です。データパイプラインは冗長性を最小限に抑えながら多様性を維持するように最適化されており、文書パッキングのような技術を使用してデータの完全性を確保しています。

A: それでコーディングと数学のタスクで強力なパフォーマンスを発揮するのですね。しかし、多言語パフォーマンスについてはどうですか？他の言語も上手く扱えますか？

B: はい、DeepSeek-V3は多言語コーパスでトレーニングされており、非英語タスクを含むMMMLUのようなベンチマークでも良好なパフォーマンスを発揮します。特に中国語において強力で、C-EvalやCMMLUのような中国語ベンチマークでQwen2.5のようなモデルを凌駕しています。

A: 印象的ですね。しかし、長文コンテキストタスクについてはどうですか？128Kトークンまでサポートしているのを見ました。そんなに長い入力をどのように処理するのですか？

B: DeepSeek-V3はそのコンテキスト長を2段階で拡張します：まず32Kトークンに、その後YaRN技術を使用して128Kトークンに拡張します。これにより、文書要約や検索のような長文コンテキストタスクを効果的に処理できます。また、長文コンテキスト理解を評価する 'Needle In A Haystack' テストでも良好なパフォーマンスを発揮します。

A: 以前のモデルからは大きな改善ですね。しかし、デプロイについてはどうですか？そんなに大きなモデルの推論をどのように処理するのですか？

B: 推論はH800クラスタで処理され、GPUはNVLinkとInfiniBandを使用して相互接続されています。デプロイ戦略は、プリフィリングとデコーディングの段階を分離し、高スループットと低レイテンシの両方を確保します。また、冗長エキスパートを使用して推論中の負荷を分散し、効率を維持するのに役立てています。

A: 多くの最適化がありますね。しかし、制限は何ですか？このサイズのモデルには何らかのトレードオフがあるはずです。

B: 一つの制限はデプロイ単位のサイズです。DeepSeek-V3は効率的な推論のために比較的大きなクラスタを必要とし、これは小規模なチームには課題となる可能性があります。また、MTPを使用した speculative decoding が役立つとはいえ、生成速度には改善の余地があります。

A: もっともです。しかし全体的に見れば、大きな前進のようです。DeepSeek-V3の今後はどうなりますか？将来の方向性として探られているものはありますか？

B: いくつかの領域を検討しています。例えば、無限のコンテキスト長をサポートするためのアーキテクチャの改良、追加のトレーニング信号源の探索、モデルの推論能力の強化などです。また、モデルのパフォーマンスをより適切に評価するための包括的な評価方法にも取り組んでいます。

A: 彼らがすぐに速度を落とすことはなさそうですね。すべてを説明してくれてありがとう—DeepSeek-V3は間違いなくオープンソースLLM空間におけるゲームチェンジャーです。

B: まったく同感です！オープンソースモデルがここまで進化してきたのを見るのはエキサイティングです。DeepSeek-V3は境界を押し広げており、彼らが次に何をするのか待ちきれません。

A: DeepSeek-V3がFP8混合精度トレーニングを使用しているとおっしゃいましたが、気になります—それはBF16やFP16と比較してどうですか？FP8は本当にそんなに大きなモデルのトレーニングに十分安定しているのですか？

B: それは素晴らしい質問です。FP8は確かに限られたダイナミックレンジのために困難ですが、DeepSeek-V3はこれを軽減するために細粒度量子化戦略を使用しています。例えば、活性化は1x128タイルにグループ化され、重みは128x128ブロックにグループ化されます。各グループは独立してスケーリングされ、これが外れ値の処理とトレーニングの安定化に役立ちます。

A: 興味深いです。つまり、単なる一括したFP8量子化ではなく、よりニュアンスがあるのですね。しかし、それではすべてのグループとスケーリング係数を管理するための追加のオーバーヘッドが発生しませんか？

B: 発生しますが、オーバーヘッドは利益と比較して最小限です。重要なのは、FP8がメモリ使用量を削減し計算を高速化することであり、これはそんなに大きなモデルをトレーニングするために不可欠です。また、行列乗算のような重要な操作には高精度累積を使用して、数値的安定性を確保しています。

A: 分かりました。つまり、精度と効率性のトレードオフですが、彼らは良いバランスを取ることに成功したのですね。DualPipeアルゴリズムについてはどうですか？それはどのように機能するのですか？

B: DualPipeは、パイプラインパラレリズムにおけるパイプラインバブルを最小化するように設計されています。各作業チャンクを4つのコンポーネント—アテンション、all-to-all dispatch、MLP、all-to-all combine—に分割することで、計算と通信を重ね合わせます。後方パス中には、計算を「入力のための後方」と「重みのための後方」にさらに分割し、より効率的な重ね合わせを可能にします。

A: 複雑に聞こえますが、理にかなっています。つまり、本質的には通信オーバーヘッドを計算と重ね合わせることで隠しているのですね。これは1F1BやZero Bubbleのような他のパイプラインパラレリズム方法と比較してどうですか？

B: DualPipeは1F1BやZero Bubbleと比較してパイプラインバブルが少ないです。また、双方向スケジューリングを可能にし、マイクロバッチがパイプラインの両端から供給されます。これによりアイドル時間がさらに減少し、全体的な効率が向上します。実際、DualPipeはほぼゼロのall-to-all通信オーバーヘッドを達成しており、これはMoEモデルをスケールアップするために重要です。

A: 印象的ですね。しかし、メモリ使用量についてはどうですか？DualPipeは他の方法よりも多くのメモリを必要としますか？

B: モデルパラメータの2つのコピーを保持するため、わずかに多くのメモリを必要としますが、増加は管理可能です。メモリフットプリントは、RMSNormとMLAアッププロジェクションの再計算のような技術を通じて最適化され、中間活性化を保存する必要がなくなります。

A: ああ、つまり、彼らはより良い効率のためには少しのメモリを犠牲にしているのですね。それは公平なトレードオフのようです。メモリと言えば、そんなに大きなコンテキスト長のKVキャッシュをどのように処理するのですか？128Kトークンには巨大なキャッシュが必要に違いありません。

B: そこでMLAが本当に輝きます。KVキャッシュを圧縮することで、そのサイズを大幅に削減します。完全なアテンションキーと値を保存する代わりに、はるかに小さい圧縮された潜在ベクトルを保存します。これにより、DeepSeek-V3はメモリのボトルネックに遭遇することなく長いコンテキストを処理できます。

A: それは賢い解決策ですね。しかし、アテンションの品質についてはどうですか？圧縮はモデルが正しいトークンに注意を向ける能力に影響しますか？

B: 圧縮は最も重要な情報を保存するように設計されているため、アテンションの品質への影響は最小限です。また、RoPE（Rotary Positional Embedding）を使用して位置情報を維持しており、これが圧縮されたキーと値があってもトークンの相対位置をモデルが理解するのに役立ちます。

A: 理にかなっています。つまり、MLAはWin-Winであり、あまりパフォーマンスを犠牲にすることなくメモリ使用量を削減するのですね。しかし、トレーニングデータについてはどうですか？14.8兆トークンだとおっしゃいましたが、そんなに大規模なデータセットの品質と多様性をどのように確保するのですか？

B: データセットは高品質で多様なトークンを含むように注意深くキュレーションされています。データパイプラインを最適化して冗長性を最小限に抑えながら多様性を維持し、文書パッキングのような技術を使用してデータの完全性を確保しています。コーパスは英語と中国語のテキストの混合を含み、数学とプログラミングのサンプルを重視しています。

A: それでコーディングと数学のタスクで強力なパフォーマンスを発揮するのですね。しかし、多言語タスクについてはどうですか？他の言語も上手く扱えますか？

B: はい、DeepSeek-V3は多言語コーパスでトレーニングされており、非英語タスクを含むMMMLUのようなベンチマークでも良好なパフォーマンスを発揮します。特に中国語において強力で、C-EvalやCMMLUのような中国語ベンチマークでQwen2.5のようなモデルを凌駕しています。

A: 印象的ですね。しかし、長文コンテキストタスクについてはどうですか？128Kトークンまでサポートしているのを見ました。そんなに長い入力をどのように処理するのですか？

B: DeepSeek-V3はそのコンテキスト長を2段階で拡張します：まず32Kトークンに、その後YaRN技術を使用して128Kトークンに拡張します。これにより、文書要約や検索のような長文コンテキストタスクを効果的に処理できます。また、長文コンテキスト理解を評価する 'Needle In A Haystack' テストでも良好なパフォーマンスを発揮します。

A: 以前のモデルからは大きな改善ですね。しかし、デプロイについてはどうですか？そんなに大きなモデルの推論をどのように処理するのですか？

B: 推論はH800クラスタで処理され、GPUはNVLinkとInfiniBandを使用して相互接続されています。デプロイ戦略は、プリフィリングとデコーディングの段階を分離し、高スループットと低レイテンシの両方を確保します。また、冗長エキスパートを使用して推論中の負荷を分散し、効率を維持するのに役立てています。

A: 多くの最適化がありますね。しかし、制限は何ですか？このサイズのモデルには何らかのトレードオフがあるはずです。

B: 一つの制限はデプロイ単位のサイズです。DeepSeek-V3は効率的な推論のために比較的大きなクラスタを必要とし、これは小規模なチームには課題となる可能性があります。また、MTPを使用した speculative decoding が役立つとはいえ、生成速度には改善の余地があります。

A: もっともです。しかし全体的に見れば、大きな前進のようです。DeepSeek-V3の今後はどうなりますか？将来の方向性として探られているものはありますか？

B: いくつかの領域を検討しています。例えば、無限のコンテキスト長をサポートするためのアーキテクチャの改良、追加のトレーニング信号源の探索、モデルの推論能力の強化などです。また、モデルのパフォーマンスをより適切に評価するための包括的な評価方法にも取り組んでいます。

A: 彼らがすぐに速度を落とすことはなさそうですね。すべてを説明してくれてありがとう—DeepSeek-V3は間違いなくオープンソースLLM空間におけるゲームチェンジャーです。

B: まったく同感です！オープンソースモデルがここまで進化してきたのを見るのはエキサイティングです。DeepSeek-V3は境界を押し広げており、彼らが次に何をするのか待ちきれません。