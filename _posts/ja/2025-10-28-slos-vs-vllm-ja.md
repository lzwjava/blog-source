---
audio: false
generated: true
image: false
lang: ja
layout: post
model: x-ai/grok-4-fast
title: SLO対vLLM機能比較
translated: true
type: note
---

### 比較: SLOs-Serve vs. vLLM

SLOs-Serveは、vLLMをコアのバッチ処理エンジンとして統合しつつ、マルチSLO（サービスレベル目標）保証のためのレイヤーを追加することでvLLMを拡張しており、多様でレイテンシに敏感なワークロードを扱う本番環境により適しています。vLLMは主に、単一SLOまたはスループット最大化シナリオにおける高スループット推論に焦点を当て、PagedAttentionなどの技術をメモリ効率化に利用しています。以下は、SLOs-Serveの論文とvLLMの設計に基づく主要な側面からの構造化された比較です。

| 側面                  | SLOs-Serve                                                                 | vLLM                                                                 |
|-------------------------|----------------------------------------------------------------------------|----------------------------------------------------------------------|
| **主な焦点**      | マルチステージLLMアプリのためのマルチSLOサービス（例: 推論におけるPrefillの厳格なTTFT、コーディングにおけるDecodeの厳格なTPOT）。バースト性のある混合ワークロードを、ステージごとの保証付きで扱う。 | 連続的なDecodeのための高スループットバッチ処理。PagedAttentionによるメモリバウンドなワークロードに最適化。均一なSLOを想定するか、集約スループットを優先する。 |
| **SLOの扱い**       | 明示的なマルチSLOサポート: ステージごと（Prefill/Decode）およびアプリごとのSLO（例: チャットでは100ms TPOT、コーディングでは50ms TPOT）。違反リクエストを拒否/延期するソフトなアドミッション制御を使用。 | ネイティブなマルチSLOサポートなし。静的設定（例: 最大バッチサイズ）に依存。競合下ではSLO違反が発生しやすい（例: バースト時における2倍以上のレイテンシピーク）。 |
| **スケジューラ**          | 動的計画法（DP）ベース: SLOティアごとにPrefillバジェット、バッチサイズ、投機的実行長を最適化。Rooflineモデルによる実行時間予測（R² > 0.8の精度）。 | 連続バッチ処理スケジューラ: リクエストを動的バッチに貪欲に詰め込み、Decode負荷の高いワークロードに焦点。SLOを考慮した計画機能なし。 |
| **Prefillの最適化**| SLOに応じたチャンク化Prefillと適応的投機的実行（SLOごとに1-10トークン）。Decodeとのバランスを取るために「Prefillバジェット」を割り当て。 | リクエストごとの単一ショットPrefill。チャンク化をサポートするがSLO適応なし。混合負荷ではヘッドオブラインブロッキングが発生しやすい。 |
| **Decodeの最適化**| TPOTターゲットに合わせたSLO適応型バッチサイジング（512+トークンまで）と投機的Decode。 | 先読みバッチ処理による効率的な連続Decode。高スループット（例: Hugging Face比10-20倍）だが、リクエストごとのデッドラインは無視。 |
| **リソース管理**| Rayを利用したマルチレプリカルーティング。ベストエフォートキューとプリエンプションによるバースト耐性。分離構成を扱える。 | シングルノードまたは基本的な分散構成（Ray統合経由）。積極的なルーティングやSLO優先割り当てなし。 |
| **スループット & 容量**| vLLMに対する2.2倍の平均容量向上（チャットボット、コーディングなど6シナリオの幾何平均）。例: 推論バースト時で2.4倍。レプリカ数に対して超線形スケーリング。 | スループットのベースライン: Decode負荷の高いトレースでは代替手段より最大24倍高速だが、SLO制約下では性能低下（例: 混合ワークロードで50%容量損失）。 |
| **オーバーヘッド**           | スケジュールあたり<10ms。DPの効率性（O(n)状態）により最小限。             | 低い（<1ms バッチ処理）。ただしSLOロジックがないため、追加オーバーヘッドもなし。      |
| **ユースケース**          | 厳格なSLAを伴う本番アプリ: ツール呼び出しエージェント、推論チェーン、バースト性のあるトラフィック（例: Azureトレース）。 | 研究/開発におけるスループット: 単純なサービス、長文コンテキスト生成、均一な負荷。 |
| **制限事項**        | 既知のSLO/デッドラインを想定。未知の長さへの対応は将来の拡張に依存。vLLM上に構築されているため、そのDecodeの強みを継承。 | マルチステージ/可変SLOへの対応が困難。共有GPU環境では高いテイルレイテンシ（評価においてp99が目標の2倍以上）。 |
| **実装**     | vLLM + Ray上のオープンソースプロトタイプ。A100/H100 GPU上のOPT/ToolLlamaモデルをサポート。 | 成熟したオープンソース（GitHub）。広く採用され、100以上のモデルをサポート、容易なデプロイ。 |

要約すると、vLLMはスループット指向のサービスにおける生の効率性に優れますが、複雑な実世界のシナリオにおけるSLO準拠性では不足があり、これがSLOs-Serveにおけるターゲットを絞った最適化による2倍以上のゲインにつながっています。もしあなたのワークロードがレイテンシ保証なしで純粋にスループット最大化を目指すものであれば、vLLMはよりシンプルでデプロイが高速です。

[SLOs-Serve: Optimized Serving of Multi-SLO LLMs](https://arxiv.org/abs/2504.08784)  
[vLLM: Easy, Fast, and Cheap LLM Serving with PagedAttention](https://arxiv.org/abs/2309.06180)