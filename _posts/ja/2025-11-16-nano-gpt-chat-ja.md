---
audio: false
generated: true
image: false
lang: ja
layout: post
model: x-ai/grok-4-fast
title: カーパシーのナノGPTとナノチャット
translated: true
type: note
---

### 概要
nanoGPTとnanochatは、Andrej Karpathyによって作成された両方ともオープンソースプロジェクトであり、大規模言語モデル（LLM）のトレーニングをアクセスしやすく教育的なものにすることを目的としています。nanoGPT（2023年リリース）はスクラッチからのGPTスタイルモデルのトレーニングの基礎に焦点を当てているのに対し、nanochat（2025年10月リリース）はそれを基盤として、ChatGPTのようなチャットボットを作成するためのより包括的な「フルスタック」パイプラインとして構築されています。主な違いは、スコープ、トレーニング段階、コードベースの複雑さ、およびエンドツーエンドの使いやすさにあります。nanochatは本質的に、会話型AIのための本番環境に近い完全なシステムへとnanoGPTを進化させたものです。

### トレーニングコードの主な違い
nanochatのトレーニングコードは、nanoGPTのアプローチを拡張し洗練させたものですが、チャットアプリケーション向けに調整された追加の段階、最適化、および統合を組み込んでいます。以下に詳細を示します：

| 観点                  | nanoGPT                                                                 | nanochat                                                                 |
|-------------------------|-------------------------------------------------------------------------|--------------------------------------------------------------------------|
| **主な焦点**      | 生テキストデータ（例：OpenWebTextやシェイクスピア）でのTransformerベースのGPTモデルの事前トレーニング。トークン化、モデルアーキテクチャ、基本的なトレーニングループなどの核心概念を教える。 | フルパイプライン：事前トレーニング + 中間トレーニング（会話/多肢選択問題） + 教師ありファインチューニング（SFT） + オプションの強化学習（GRPOによるRLHF） + 評価 + 推論。デプロイ可能なチャットボットを構築。 |
| **トレーニング段階**    | - 単一段階の事前トレーニング。<br>- 基本的な評価（例：パープレキシティ）。 | - **事前トレーニング**: nanoGPTと同様だが、FineWebデータセットを使用。<br>- **中間トレーニング**: SmolTalk（ユーザーとアシスタントの対話）、多肢選択式Q&A、ツール使用データを使用。<br>- **SFT**: チャットの整合性のためにファインチューニング。MMLU、ARC-E/C、GSM8K（数学）、HumanEval（コード）などのベンチマークで評価。<br>- **RL**: GSM8Kでの選好整合性のためのオプションのRLHF。<br>- メトリクス（例：COREスコア）を含む自動化されたレポートカード生成。 |
| **コードベースのサイズと構造** | 合計約600行（例：`train.py` 約300行、`model.py` 約300行）。最小限でハック可能なPyTorch；完全性よりもシンプルさを優先。nanochatを推奨するため非推奨。 | 約8,000行のクリーンでモジュール化されたPyTorchコード。Rustベースのトークナイザー、効率的な推論エンジン（KVキャッシュ、プリフィル/デコード）、ツール統合（例：Pythonサンドボックス）、Web UIを含む。より統合的だが、まだフォーク可能。 |
| **オプティマイザとハイパーパラメータ** | 標準的なAdamW；中規模モデル（例：GPT-2 124Mパラメータ）向けに調整された学習率。 | Muon + AdamWハイブリッド（modded-nanoGPTに触発）；適応型学習率（例：過学習を避けるため小規模データセットでは低く設定）。モデルサイズに対して`--depth`フラグでスケーリング。 |
| **データ処理**      | 生テキストコーパス；基本的なBPEトークナイザーのトレーニング。 | 強化版：カスタムトークナイザーのトレーニング（語彙サイズ約65K）；Hugging Faceデータセットを使用（事前トレーニング用FineWeb、会話用SmolTalk）。個性を注入するための合成データをサポート。 |
| **トレーニング時間とコスト** | GPT-2相当のモデルで8xA100約4日（~$500+）。教育的な実行に焦点。 | 基本的な560Mパラメータモデルで8xH100約4時間（~$100）；約12時間でGPT-2を超える；より強力なモデル（例：24時間後MMLU 40%）では~$1,000までスケール。 |
| **推論とデプロイメント** | 基本的なテキスト生成；組み込みのチャットやUIなし。 | KVキャッシュを備えた最適化されたエンジン；CLIチャット；ChatGPTスタイルのWeb UI；ツール使用（例：コード実行）。モデルは「素朴/間抜け」だが小規模では面白い。 |
| **教育的目標**   | 言語モデルの「核心」（脳）を構築。アーキテクチャを実験する初心者に最適。 | 完全なChatGPTクローン（脳からインターフェースまで）を構築。LLM101nのような高度なコースの総合演習；潜在的な研究ベンチマーク。 |

### それぞれを使うべき場合
- **nanoGPTを選ぶべき場合**: LLMが初めてで、事前トレーニングへの軽量な入門（例：アーキテクチャやデータセットの微調整）を望む場合。よりシンプルだが、非会話型モデルに限定される。
- **nanochatを選ぶべき場合**: エンドツーエンドのチャットボット開発の場合。チャットタスクでnanoGPTを上回り（例：一部のRLHFテストで58%改善）、データから動作するUIまでに必要な全てを1つのリポジトリに含む。

両方ともハック可能性を重視しています—例えば、nanochatは簡単なスケーリング（モデルの深さの変更）やカスタマイズ（個性のための合成データの追加）を可能にします。nanochatはモデルに自己認識さえ与えます（例：自身のサイズと制限を知っている）。

実践的な探求には：
- [nanoGPT GitHub](https://github.com/karpathy/nanoGPT)
- [nanochat GitHub](https://github.com/karpathy/nanochat)

これらを実行する場合は、Lambda LabsのようなクラウドGPU上でnanochatの`speedrun.sh`スクリプトから始めて、完全な体験をしてください。