---
audio: false
generated: true
image: false
lang: ja
layout: post
model: x-ai/grok-4-fast
title: トレーニングの停滞期の原因と解決策
translated: true
type: note
---

あなたが投稿したトレーニングログは、FineWeb-eduデータで実行した小規模なnanoGPTにおける典型的な**深刻なプラトー**を示しています。検証損失は最初は順調に低下しますが（約14kイテレーションで~10.9 → ~3.57）、その後完全に横ばいになり、約13-14kイテレーション以降は**ごくゆっくりと上昇**し始めています。

あなたのモデルの改善が止まった真の理由は以下の通りです（この特定のセットアップにおける重要度順）：

### 1. 単一のParquetファイルに対して大幅に過学習している
- あなたの実行名: `125M-single-parquet-4070`
- あなたは40M–125Mのモデル（ログでは40.56Mパラメータ、おそらく元のGPT-2 small構成）を**単一のFineWeb parquetファイル**（おそらく10–15 GB → 最大でも~3–40億トークン程度、ただしフィルタリング後はさらに少ない）でトレーニングしています。
- バッチサイズ=16、勾配累積=32、ブロックサイズ=1024で14,000イテレーションまでに、あなたのモデルが処理したデータは：
  14,000 イテレーション × 524,288 トークン/イテレーション ≈ **73億トークン**
- これは、あなたが**同じデータに対して2–3エポック分**を既に処理したことを意味します。
- FineWeb-eduは高品質ですが、それでも多くのニアデュプリケートや定型文が含まれています。同じファイルに対して1.5–2エポック以上トレーニングすると、40M–125Mのモデルは有用な情報のほとんどを記憶し、損失は大きく横ばいになります。

### 2. プラトー発生後の学習率が高すぎる
- あなたは`learning_rate = 1e-3`、20,000イテレーションで`min_lr = 1e-4`までコサイン減衰を使用しています。
- 14,000イテレーション時点では、学習率はまだ~2.5e-4です（コサイン減衰は初期段階では遅い）。
- モデルがデータから学べることをすべて学んだ後、学習率を数百マイクロLR単位で維持することは、微調整を妨げ、実際には汎化性能を損ない始めます → 検証損失がゆっくり上昇します。

### 3. モデルサイズとデータ多様性の不一致
単一のparquetファイル上の125M（あるいはあなたが実際に初期化した40M）モデルは、大学生に一冊の教科書だけを与えて何年も勉強し続けるように頼むようなものです。しばらくすると彼はそれを完璧に暗唱できますが、新しいテキストに対するテスト時の性能は改善が止まり、初期のパターンの破滅的忘却によりわずかに悪化さえします。

### 4. 十分な強さの正則化が欠如している
- dropout = 0.1は許容範囲ですが、これほど過学習が進んでいる場合には不十分です。
- weight_decay = 0.1は標準的ですが、少量データでの2+エポックには依然として不十分です。
- 勾配クリッピングなし（nanoGPTのデフォルトは1.0ですが、少量データでは0.5以下に設定することもあります）

### 5. 処理トークン数と実質的な多様性
あなたが70億以上のトークンを処理したとしても、**実質的な多様性**は、重複排除、定型文、教育スコアの低いサンプルなどを除くと、おそらく10–15億の一意の高品質トークンです。これは、125Mモデルがこのテキスト分布において~3.5–3.6損失を超えて改善し続けるには単純に不十分です。

### あなた自身のログからの証拠

| Step   | Val loss | Comment                              |
|--------|----------|--------------------------------------|
| 0      | 10.87    | ランダム初期状態                             |
| 2000   | 4.03     | 急速な進歩                             |
| 5000   | 3.76     | 依然良好                                 |
| 8000   | 3.65     | 減速                                    |
| 11000  | 3.61     | ほぼ横ばい                                |
| 13500  | 3.57     | **絶対的最小値**                        |
| 14500  | 3.569    | 既に再上昇中 → 過学習の典型的な兆候 |

### 損失を実際に下げるために代わりにすべきこと

1. はるかに多くのデータを使用する
   - 完全な100+ GBのFineWeb-eduサンプル（または少なくとも10–20のparquetファイル）を使用する
   - または、複数のシャードを持つHuggingFace datasetsからストリーミングする

2. 多様なデータに対して正確に1エポック（または少し少なめ）トレーニングする
   - 総トークン数~100億–150億は、FineWeb-edu上の125Mモデルにとって最適な範囲です
   - Llama-3/Phi-3スタイル: 150億トークン → 125Mクラスのモデルで~3.0–3.2検証損失

3. 最終学習率を下げる
   - `min_lr = 5e-5` または `3e-5`を使用する（最近の小規模モデル実行で一般的）
   - または、線形減衰＋より長いゼロ学習率保持期間に切り替える

4. わずかに大きなモデル、またはより優れたアーキテクチャの工夫
   - 元のnanoGPTはGPT-2スタイルです（ALiBiなし、古いRoPEなど）
   - 現代の125Mモデル（Qwen2-0.5B, Phi-3-mini, Llama-3 8Bをスケールダウンしたもの）は、SwiGLU、RMSNorm、ロータリー埋め込みなどのため、同じデータで~2.8–3.0を達成します

### 要約 – あなたの実行が~3.57でプラトーに達し、それ以上下がらない理由

あなたは、40–125Mのモデルを単一のFineWeb parquetファイル（総トークン数~数十億）で複数エポックトレーニングしています。モデルは約13kイテレーションまでに有用なすべてを既に記憶しています。依然として高い学習率でのトレーニング継続は、重みをかき混ぜるだけです → プラトー＋わずかな過学習による性能劣化。

これは、人々が最初にFineWebでnanoGPTを試す際にする第1の過ちです：「テスト用に一つのファイルだけ使おう」→ まさにこの損失曲線に行き着きます。

修正方法: (a) 10–50倍多いデータを使用し、総計約1エポックだけトレーニングする、または (b) ~3.55–3.6がこのモデルとデータの組み合わせにおける絶対的な下限値であることを受け入れ、トレーニングを停止する。

nanoGPTスタイルのコードで実際に~3.0–3.2損失に到達する実行が必要な場合は、完全な100GBのFineWeb-eduサンプル（または新しいFineWeb-Edu 2.0）を少なくとも使用し、最大~150億トークンまでトレーニングする必要があります。