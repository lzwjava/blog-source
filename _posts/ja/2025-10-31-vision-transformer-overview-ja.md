---
audio: false
generated: true
image: false
lang: ja
layout: post
model: x-ai/grok-4-fast
title: ビジョントランスフォーマー概説
translated: true
type: note
---

### Vision Transformer（ViT）入門

Vision Transformer（ViT）は、自然言語処理向けに設計されたトランスフォーマーモデルをコンピュータビジョンタスク、特に画像分類に適応させたニューラルネットワークアーキテクチャです。2020年にGoogleのAlexey Dosovitskiyらによる論文『An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale』で導入され、ViTは画像を従来の畳み込みニューラルネットワーク（CNN）で処理するのではなく、パッチのシーケンスとして扱います。これにより、トランスフォーマーの長距離依存関係の処理と並列計算の強みを活用できます。

ViTは、大規模なデータセット（ImageNetなど）において、CNNと同等かそれ以上の性能を示しています。特に大量のデータ（例：JFT-300M）で事前学習された場合に顕著です。DeiT（Data-efficient Image Transformers）のような変種は、より小規模なデータセットでの効率を向上させます。今日、ViTに着想を得たモデルは、DALL-E、Stable Diffusion、現代的な分類器など、多くのビジョンモデルの中核をなしています。

### ViTの仕組み：全体アーキテクチャとワークフロー

ViTの核となるアイデアは、画像を固定サイズのパッチのシーケンスに「トークン化」することです。これは、テキストが単語やトークンに分割されるのと似ています。このシーケンスはその後、標準的なトランスフォーマーエンコーダー（生成的なテキストモデルとは異なり、デコーダーはなし）によって処理されます。その仕組みをステップバイステップで説明します：

1.  **画像前処理とパッチ抽出**:
    *   サイズ \\(H \times W \times C\\)（高さ×幅×チャネル、例：RGB画像で224×224×3）の入力画像から始めます。
    *   画像を固定サイズ \\(P \times P\\)（例：16×16ピクセル）の重ならないパッチに分割します。これにより、\\(N = \frac{HW}{P^2}\\) 個のパッチが得られます（例：224×224画像を16×16パッチに分割すると196パッチ）。
    *   各パッチは、長さ \\(P^2 \cdot C\\) の1次元ベクトルに平坦化されます（例：16×16×3の場合768次元）。
    *   なぜパッチなのか？生のピクセルでは非現実的な長さのシーケンス（例：高解像度画像で数百万）が生成されてしまうため、パッチは次元を削減する「ビジュアルワード」として機能します。

2.  **パッチ埋め込み**:
    *   学習可能な線形射影（単純な全結合層）を各平坦化されたパッチベクトルに適用し、固定された埋め込み次元 \\(D\\)（例：768、BERTのようなトランスフォーマーに合わせる）にマッピングします。
    *   これにより、サイズ \\(D\\) の埋め込みベクトルが \\(N\\) 個生成されます。
    *   オプションで、分類タスクのためのBERTと同様に、シーケンスの先頭に特別な[CLS]トークン埋め込み（サイズ \\(D\\) の学習可能なベクトル）を追加します。

3.  **位置埋め込み**:
    *   空間情報をエンコードするために、パッチ埋め込みに学習可能な1次元の位置埋め込みを加えます（これがないとトランスフォーマーは順序不変です）。
    *   完全な入力シーケンスは次のようになります： \\([ \text{[CLS]}, \text{patch}_1, \text{patch}_2, \dots, \text{patch}_N ] + \text{positions}\\)、形状は \\((N+1) \times D\\) の行列です。

4.  **トランスフォーマーエンコーダーブロック**:
    *   このシーケンスを \\(L\\) 層（例：12層）のスタックされたトランスフォーマーエンコーダーレイヤーに入力します。
    *   各レイヤーは以下で構成されます：
        *   **マルチヘッド自己注意機構（MSA）**: （[CLS]を含む）すべてのパッチのペア間の注意スコアを計算します。これにより、モデルはCNNの局所受容野とは異なり、「この猫の耳は100パッチ離れたひげに関連している」といった大域的な関係を捕捉できます。
            *   式： Attention(Q, K, V) = \\(\text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right) V\\)、ここでQ, K, Vは入力の射影です。
        *   **多層パーセプトロン（MLP）**: 位置ごとに適用されるフィードフォワードネットワーク（GELU活性化関数を持つ2つの線形層）。
        *   レイヤ正規化と残差接続： Input + MSA → Norm → MLP → Norm + Input。
    *   出力：洗練された埋め込みのシーケンス、形状は依然として \\((N+1) \times D\\)。

5.  **分類ヘッド**:
    *   画像分類では、[CLS]トークンの出力を抽出します（またはすべてのパッチ埋め込みの平均を取ります）。
    *   それを単純なMLPヘッド（例：1つまたは2つの線形層）に通して、クラスのロジットを出力します。
    *   訓練中は、ラベル付きデータに対して交差エントロピー損失を使用します。事前学習では、マスクされたパッチ予測や他の自己教師ありタスクがしばしば含まれます。

**主要なハイパーパラメータ**（オリジナルのViT-Baseモデルから）:
*   パッチサイズ \\(P\\): 16
*   埋め込み次元 \\(D\\): 768
*   レイヤー数 \\(L\\): 12
*   ヘッド数: 12
*   パラメータ数: ~86M

ViTはスケーラブルです：より大きなモデル（例：\\(D=1024\\), \\(L=24\\) のViT-Large）は性能が向上しますが、より多くのデータ/計算資源を必要とします。

**訓練と推論**:
*   **訓練**: ラベル付きデータでのエンドツーエンド学習。数十億の画像での事前学習から非常に大きな恩恵を受けます。
*   **推論**: エンコーダーを通した順伝播（注意による計算量は~O(N²)ですが、FlashAttentionなどの最適化により効率的です）。
*   CNNとは異なり、ViTには並進不変性などの帰納的バイアスはありません。すべてが学習されます。

### テキストトランスフォーマーとの比較：類似点と相違点

ViTは基本的に、テキストトランスフォーマー（例：BERT）のエンコーダー部分と*同じアーキテクチャ*ですが、2次元の視覚データ向けに適応されています。以下に並べて比較します：

| 観点              | テキストトランスフォーマー (例: BERT)                  | Vision Transformer (ViT)                       |
|---------------------|------------------------------------------------|------------------------------------------------|
| **入力表現** | ベクトルに埋め込まれたトークン（単語/サブワード）のシーケンス。 | ベクトルに埋め込まれた画像パッチのシーケンス。パッチは「ビジュアルトークン」のようなもの。 |
| **シーケンス長** | 可変 (例: 文に対して512トークン)。   | 画像サイズ/パッチサイズに基づいて固定 (例: [CLS]を含めて197)。 |
| **位置エンコーディング** | 語順のための1次元（絶対的または相対的）。     | パッチ順序のための1次元（学習可能）(例: 行優先の平坦化)。組み込みの2次元構造はなし。 |
| **核心的なメカニズム**  | 依存関係をモデル化するためのトークン間の自己注意。 | パッチ間の自己注意 — 数式は同じだが、構文的関係の代わりに空間的「関係」に注意を向ける。 |
| **出力/タスク**    | 分類/Masked LM用のエンコーダー；生成用のデコーダー。 | 分類専用のエンコーダー；検出/セグメンテーション向けに拡張可能。 |
| **強み**       | 長距離のテキスト依存関係を扱う。         | 画像内の大域的な文脈 (例: シーン全体の理解)。 |
| **弱み**      | 巨大なテキストコーパスを必要とする。                      | データ飢餓；CNNによる事前学習なしでは小規模データセットで苦戦。 |
| **予測スタイル**| デコーダーにおける次のトークン予測（自己回帰的）。 | 本質的に「次」の予測は行わない — 画像全体を全体的に分類する。 |

本質的に、ViTは「プラグアンドプレイ」な交換です：トークン埋め込みをパッチ埋め込みに置き換えるだけで、ビジョンモデルが得られます。両者ともシーケンス内の関係を重み付けするための注意機構に依存しますが、テキストは本質的に順次的/線形的であるのに対し、画像は空間的です（ViTは注意を通じてこれを学習します）。

### ViTにおける「次のトークン」と「次のピクセル」について

いいえ、ViTは、テキストトランスフォーマーが自己回帰的生成（例：GPT）で「次のトークン」を予測するように、「次のピクセル」を予測*しません*。その理由は以下の通りです：

*   **テキストトランスフォーマー（自己回帰的）**: GPTのようなモデルでは、デコーダーは順次的に — 一度に一つのトークンを、それ以前のすべてのトークンを条件として — 生成します。一部の生成的画像モデル（例：PixelRNN）ではピクセル単位ですが、非効率的です。

*   **ViTのアプローチ**: ViTは*非自己回帰的*で全体的です。エンコーダーを通して画像*全体*（すべてのパッチ）を並列に処理します。推論中に順次的な「次」の予測はありません — 注意機構は*すべて*のパッチ間の関係を同時に計算します。生成タスク（例：インペインティング）では、ViTの変種がマスクされたパッチを自己回帰的に予測するためにデコーダーを使用するかもしれませんが、基本のViTは分類のためのものであり、生成のためのものではありません。

*   **なぜピクセルではないのか？** 個々のピクセルをトークンとして扱うと、シーケンス長が爆発的に増加します（例：224×224×3 = ~150Kトークン）。これにより、二次の注意計算量（O(N²) = 兆単位の演算）が計算的に非現実的になります。パッチは約256ピクセル（16×16）を一つの「トークン」にグループ化し、Nを管理可能な数（~200）に保ちます。

もし生成的ビジョン（次のピクセルや次のパッチの予測）をお望みなら、VQ-VAE + Transformer や Parti のような、ViTのアイデアに基づいて構築されたモデルを参照してください。

詳細については、原論文を参照してください：[An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale](https://arxiv.org/abs/2010.11929)。コード例（例：PyTorch）や特定の変種との比較をご希望の場合は、お知らせください！