---
audio: false
generated: false
lang: ja
layout: post
title: DeepSeek V3
translated: true
---

### 概要と主要なハイライト
1. モデル名: DeepSeek-V3、6710億パラメータを持つMixture-of-Experts (MoE)言語モデルで、トークンごとに370億がアクティブ化されます。
2. トレーニングデータセット: 14.8兆の多様で高品質なトークンで事前トレーニングされました。
3. コアイノベーション: Multi-Head Latent Attention (MLA)とDeepSeekMoEアーキテクチャを採用し、効率を高めるためのオプション損失なしのロードバランスを実装しています。
4. トレーニング効率: 2.788万H800 GPU時間で完全なトレーニングを達成しました。
5. コスト効率: トレーニングコストは、1 GPU時間あたり2 USDを前提に5.576M USDと見積もられています。

---

### アーキテクチャのイノベーション
6. Transformerベースのフレームワーク: スケーラビリティと柔軟性を保つためにTransformerアーキテクチャを保持しています。
7. Multi-Head Latent Attention (MLA): キー値キャッシュを圧縮することで、パフォーマンスを損なうことなく推論メモリを削減します。
8. DeepSeekMoE: コスト効率の高いトレーニングと高い計算効率を実現するために、共有された専門家とルーティングされた専門家の組み合わせを利用します。
9. オプション損失なしのロードバランス: パフォーマンスを損なうことなく専門家のロードをバランスを保つためにバイアス項を導入します。
10. Multi-Token Prediction (MTP): 各位置で複数のトークンを順次予測し、データ効率と表現の事前計画を向上させます。

---

### トレーニングフレームワーク
11. FP8混合精度トレーニング: 細かい量子化と低精度ストレージを活用してメモリと計算を最適化します。
12. DualPipeアルゴリズム: 計算と通信フェーズを重ね合わせ、パイプラインのバブルを減らし、並列処理を向上させます。
13. 高効率なクロスノード通信: 全てのオール・トゥ・オール操作に最適化されたカーネルを使用し、NVLinkとInfiniBandの帯域幅を活用します。
14. 低精度の最適化ステート: BF16で最適化ステートを保存し、メモリ消費を減らすことなくパフォーマンスを維持します。
15. メモリ最適化技術: バックプロパゲーション中に特定の操作（例：RMSNorm）を再計算してメモリを節約します。

---

### 事前トレーニングの詳細
16. 安定したトレーニングプロセス: 事前トレーニング中に回復不能な損失のスパイクやロールバックは発生しませんでした。
17. コンテキスト長の拡張: 2段階でコンテキスト長を32Kから128Kに拡張しました。
18. トレーニングコスト: 事前トレーニングに2.664M GPU時間、コンテキスト拡張に119K GPU時間、事後トレーニングに5K GPU時間がかかりました。
19. トークン効率: トークンごとのGPU時間を最小限に抑えることでトレーニング効率を確保しました。
20. 高品質なデータ: 多様性と関連性を考慮した事前トレーニングデータセットをカレントしました。

---

### 事後トレーニングの向上
21. 監視付き微調整（SFT）: モデル出力を人間の好みに合わせます。
22. 強化学習（RL）: Group Relative Policy Optimizationを使用して微調整します。
23. 知識の蒸留: DeepSeek-R1モデルからの推論能力を統合します。
24. 出力スタイルの制御: 正確性、生成長、スタイルをバランスさせます。
25. パフォーマンスの精製: 事後トレーニングによりベンチマーク結果がさらに向上します。

---

### ベンチマークパフォーマンス
26. MMLU（教育ベンチマーク）: 88.5を達成し、他のオープンソースモデルを上回ります。
27. GPQA（一般知識）: 59.1のスコアで、GPT-4oとClaude-3.5-Sonnetと比較可能です。
28. 数学ベンチマーク: 数学的推論タスクで最先端のパフォーマンスを発揮します。
29. コードコンペティション: LiveCodeBenchなどのコーディングベンチマークで優れた成績を収めます。
30. 事実知識: 英語と中国語の事実性ベンチマークで優れた結果を示します。

---

### 推論とデプロイメント
31. プリフィルステージ: テンソル並列（TP4）、シーケンス並列（SP）、専門家並列（EP32）を組み合わせて効率化します。
32. デコードステージ: EP320とIBGDAを使用して低遅延通信を実現します。
33. 動的冗長性: 専門家のロードを動的に調整してリソース利用を最適化します。
34. ステージの分離: プリフィルとデコードステージを分離してスループットを向上させます。
35. ハードウェア利用: H800 GPUとNVLink、InfiniBandインターコネクトに最適化されています。

---

### ロードバランスとデコードのイノベーション
36. バイアスベースのルーティング: バイアス項を導入して専門家のロードを動的にバランスを保ちます。
37. 予測デコード: MTPモジュールを使用して生成遅延を向上させます。
38. 冗長な専門家: 高負荷の専門家を複製してGPUのワークロードをバランスを保ちます。
39. ノード制限ルーティング: トークンルーティングを最大4ノードに制限して通信オーバーヘッドを減らします。
40. トークンのドロップなし: トレーニングと推論中にすべてのトークンを保持します。

---

### 技術的詳細
41. クラスター構成: 2048のNVIDIA H800 GPUを使用したクラスターでトレーニングされました。
42. パイプライン並列処理: スケーラビリティのために16方向の並列処理スキームを採用します。
43. メモリフットプリント: メモリ使用を最適化してコストの高いテンソル並列を避けます。
44. カスタムカーネル: クロスノード操作を効率的に処理するために専用の通信カーネルを開発します。
45. 混合精度の最適化: 最適なトレーニングダイナミクスを実現するためにFP8とBF16形式を組み合わせます。

---

### 評価と結果
46. 総合ベンチマーク: 教育、コーディング、推論を含む多様なドメインで評価されました。
47. オープンソースのリーダーシップ: カテゴリ内の最強のオープンソースベースモデルとして登場します。
48. 閉鎖型モデルとの比較: GPT-4oとClaude-3.5-Sonnetとのパフォーマンスが比較可能です。
49. 中国知識の強さ: 中国語の事実性ベンチマークでリーディングモデルを上回ります。
50. 長コンテキスト処理: 拡張されたコンテキスト処理が必要なタスクで優れたパフォーマンスを発揮します。

---

### 今後の方向性
51. 動的冗長性の探索: より適応性の高い冗長性戦略を調査します。
52. 予測デコードの拡張: 推論加速のためのMTPのさらに多くの用途を探索します。
53. ハードウェア共同設計: 次世代GPUに適応してパフォーマンスを向上させます。
54. より広範なベンチマークカバレッジ: より多様なタスクに評価を拡大します。
55. 持続可能性: アルゴリズムとハードウェアの最適化によりトレーニングコストをさらに削減します。

---

このドキュメントは、DeepSeek-V3のアーキテクチャ、トレーニング方法、ベンチマークパフォーマンス、そして今後の展望を包括的にまとめたものです。特定のセクションや追加のポイントについてさらに詳しく説明が必要な場合はお知らせください！