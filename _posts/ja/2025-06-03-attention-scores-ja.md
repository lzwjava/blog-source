---
audio: false
generated: true
lang: ja
layout: post
title: トランスフォーマーの注意スコア安定化
translated: true
type: note
---

スケーリングされた内積注意機構（式 \\(\text{スケーリング済みスコア} = \frac{Q K^T}{\sqrt{d_k}}\\) のように）において、キーの次元の平方根 \\(\sqrt{d_k}\\) で除算することは、勾配を安定させ、特にトランスフォーマーの文脈において大きな値を防ぐための意図的な選択です。以下に、単に \\(d_k\\) ではなく \\(\sqrt{d_k}\\) が使用される理由を説明します：

1.  **内積の分散**：
    - 内積 \\( Q K^T \\) は、クエリ (\\( Q \\)) とキー (\\( K \\)) ベクトル間の類似度を計算します。ここで、各ベクトルの次元は \\( d_k \\) です。\\( Q \\) と \\( K \\) の要素が独立で、平均 0、分散 1 を持つと仮定すると（初期化後や正規化後に一般的）、内積 \\( Q_i \cdot K_j \\) (単一のクエリとキーのベクトルペアに対して) の分散は \\( d_k \\) になります。これは、\\( d_k \\) 個の独立した標準正規変数の積の和の分散が \\( d_k \\) に比例して線形に増加するためです。
    - スケーリングを行わない場合、\\( Q K^T \\) の大きさは \\( d_k \\) とともに増加し、大きな \\( d_k \\) (トランスフォーマーでは 64, 128 またはそれ以上が一般的) に対して非常に大きな値をもたらします。注意スコア内の大きな値は、softmax 関数に渡されると問題を引き起こす可能性があります。

2.  **Softmax の安定性**：
    - 注意スコア \\( \frac{Q K^T}{\sqrt{d_k}} \\) は、注意重みを計算するために softmax に入力されます。スコアが大きすぎる場合（スケーリングなし、または不十分なスケーリングではそうなります）、softmax 関数は非常に鋭い分布を生成する可能性があります。つまり、1つの要素が支配的（1に近づく）になり、他は0に近くなります。これにより、ほとんどの要素で勾配消失が発生し、モデルが効果的に学習することが難しくなります。
    - \\(\sqrt{d_k}\\) で除算することで、スケーリングされたスコアの分散が約1に保たれ、スコアが適切な範囲に収まります。これにより、softmax 関数が良好に動作し、よりバランスの取れた注意重みと安定した勾配を生成します。

3.  **なぜ \\( d_k \\) ではないのか？**：
    - \\(\sqrt{d_k}\\) の代わりに \\( d_k \\) で除算すると、内積が過度にスケーリングされ、スコアの分散が \\( \frac{1}{d_k} \\) に減少します。大きな \\( d_k \\) に対して、これはスコアを非常に小さくし、softmax がほぼ一様な分布を生成する原因となります（softmax への入力が小さいと、出力は \\( \frac{1}{n} \\) に近くなるため）。これにより、スコア間の差が減少し、注意機構が関連するキーに焦点を当てる能力が損なわれます。
    - \\( d_k \\) による過度のスケーリングは、場合によっては数値的不安定性を引き起こす可能性もあります。非常に小さな値は、浮動小数点演算で正確に扱うのが難しいためです。

4.  **なぜ \\(\sqrt{d_k}\\) なのか？**：
    - \\(\sqrt{d_k}\\) で除算すると、内積の分散が約1に正規化されます。なぜなら、\\( \text{Var}\left(\frac{Q K^T}{\sqrt{d_k}}\right) = \frac{\text{Var}(Q K^T)}{d_k} = \frac{d_k}{d_k} = 1 \\) となるからです。これにより、スコアが適切な範囲に保たれ、softmax が過度に鋭くもなく、過度に均一でもない、意味のある注意重みを生成します。
    - 平方根は、内積の分散という数学的特性から自然に導き出されます。これは、値の爆発（スケーリングなしで発生する）を防ぎつつ、スコアの過度の抑制（\\( d_k \\) で除算すると発生する）を避ける、バランスの取れた選択です。

5.  **経験的な有効性**：
    - \\(\sqrt{d_k}\\) の選択は、オリジナルの Transformer 論文（"Attention is All You Need" by Vaswani et al., 2017）で導入され、多くのトランスフォーマーベースのモデルで経験的に検証されています。これは広範な \\( d_k \\) の値に対して実際にうまく機能し、安定した訓練と良好な性能を保証します。

要約すると、\\(\sqrt{d_k}\\) で除算することは、注意スコアの分散を安定させ、softmax が良好に動作する領域で動作することを保証し、勾配の爆発や消失の問題を回避する、数学的に動機づけられた選択です。\\( d_k \\) で除算すると、スコアが過度に減少し、注意重みの識別性が低下し、モデルの性能を損なう可能性があります。