---
audio: false
generated: true
image: false
lang: ja
layout: post
model: x-ai/grok-4-fast
title: ImageNet分類における2025年の進展
translated: true
type: note
---

ImageNetにおける最先端技術（SOTA）は、2012年以降の劇的な改善を経て、着実に進歩を続けています。2025年11月現在、主要なモデルはトップ1精度で約91%（トップ1エラーレートは約9%）を達成しています。トップ5精度はさらに高く、通常99%を超え、トップ5エラーレートは1%未満となっています。

### 主要なSOTAモデル（Papers With Code リーダーボード上位5件）
以下は、トップ1精度に基づく現在のトップパフォーマーのスナップショットです（ImageNet-1Kでファインチューニング済み）。トップ5精度は、これらの非常に高性能なモデルでは常に明示的に再報告されるわけではありませんが（ほぼ完璧なレベルで飽和しているため）、同様の最近のアーキテクチャとの相互参照により、すべてのモデルでトップ5エラーが1%未満であることが示唆されています。

| 順位 | モデル | トップ1精度 | 推定トップ5精度 | パラメータ数 | 備考 |
|------|--------|----------------|---------------------|------------|-------|
| 1 | CoCa (ファインチューニング版) | 91.0% (エラー率 9.0%) | ~99.5% (エラー率 <0.5%) | 2.1B | マルチモーダル画像-テキストモデル。ゼロショット（トップ1 86.3%）や凍結エンコーダ設定（トップ1 90.6%）で優れた性能を発揮。 |
| 2 | Model Soups (BASIC-L) | 90.98% (エラー率 9.02%) | ~99.4% (エラー率 <0.6%) | ~1B | ファインチューニングされたモデルのアンサンブル平均化により、ロバスト性を向上。 |
| 3 | Model Soups (ViT-G/14) | 90.94% (エラー率 9.06%) | ~99.4% (エラー率 <0.6%) | 1.8B | ViTベース。分布外データへの強い一般化性能。 |
| 4 | DaViT-Giant | 90.4% (エラー率 9.6%) | ~99.3% (エラー率 <0.7%) | 1.4B | デュアルアテンションViT。15億の画像-テキストペアで学習。 |
| 5 | ConvNeXt V2-Huge | 88.9% (エラー率 11.1%) | ~99.0% (エラー率 ~1.0%) | 660M | マスク化オートエンコーダ事前学習を採用したCNNの復興。エッジデバイス向けに効率的。 |

### 「今日では<3%」に関する補足
- これは**トップ1エラーレート**ではなく、**トップ5エラーレート**を指します。2012年以前のベースライン（例：SVM）では、トップ5エラーは約25%でした。2017年（SENet）までに約2.3%に低下しました。現代のSOTAモデルはこれを1%未満に押し上げており、トップ5予測はほとんどの実世界のユースケースで本質的に信頼できるものとなっています。
- トップ1エラーは依然として高く（約9%）、これは*正確に*トップの予測が一致することを要求する、より厳格で、曖昧な画像のようなエッジケースに対してより敏感な指標であるためです。

進歩は近年、データセットの飽和により鈍化しています。現在の改善は、大規模な事前学習（例：ImageNet-21KやWeb規模のデータ）や、モデルアンサンブリングや蒸留などの技術によってもたらされています。人間の専門家の性能は、トップ1エラーで約5%（または研究により変動するがトップ5で2-12%）と推定されているため、AIはこのベンチマークにおいて人間を凌駕しています。

[ImageNet Benchmark (Papers With Code)](https://paperswithcode.com/sota/image-classification-on-imagenet)  
[Image Classification: State-of-the-Art Models in 2025 (HiringNet)](https://hiringnet.com/image-classification-state-of-the-art-models-in-2025)  
[CoCa: Contrastive Captioners are Image-Text Foundation Models (arXiv)](https://arxiv.org/abs/2205.01917)