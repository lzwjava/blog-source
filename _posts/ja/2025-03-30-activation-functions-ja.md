---
audio: false
generated: true
lang: ja
layout: post
title: ニューラルネットワークにおける活性化関数
translated: true
type: note
---

**シグモイド**、**ReLU**、**Leaky ReLU**は、いずれもニューラルネットワークで使用される活性化関数です。それぞれに固有の特性があり、解決する問題の種類に応じて異なるタスクに適しています。以下に違いを詳しく説明します。

### 1. **シグモイド**:
   - **数式**:
     \\[
     \sigma(x) = \frac{1}{1 + e^{-x}}
     \\]
   - **出力範囲**: (0, 1)
   - **利点**:
     - 滑らかな勾配であり、最適化に役立つ。
     - 出力が0と1の間に制限されるため、確率や二値分類に適している。
   - **欠点**:
     - **勾配消失問題**: 入力値が非常に大きいか小さい場合、勾配が非常に小さく（ほぼゼロに）なり、特に深いネットワークでは学習が遅くなる可能性がある。
     - 出力がゼロ中心ではないため、勾配更新が一方向に偏る問題を引き起こす可能性がある。
   - **使用例**: 二値分類タスク（例：ロジスティック回帰）の出力層でよく使用される。

### 2. **ReLU (Rectified Linear Unit)**:
   - **数式**:
     \\[
     f(x) = \max(0, x)
     \\]
   - **出力範囲**: [0, ∞)
   - **利点**:
     - **高速でシンプル**: 計算が容易で実践的に効率的。
     - 勾配が適切に伝播するため、勾配消失問題を解決する。
     - スパース性を促進する（多くのニューロンが非活性化される）。
   - **欠点**:
     - **Dying ReLU問題**: 出力が常にゼロ（すなわち、負の入力に対して）の場合、学習中にニューロンが「死ぬ」ことがある。これにより、一部のニューロンが再び活性化されなくなる可能性がある。
   - **使用例**: 畳み込みニューラルネットワークや深層ニューラルネットワークの隠れ層で非常に一般的に使用される。

### 3. **Leaky ReLU**:
   - **数式**:
     \\[
     f(x) = \max(\alpha x, x)
     \\]
     ここで、\\( \alpha \\) は小さな定数（例: 0.01）。
   - **出力範囲**: (-∞, ∞)
   - **利点**:
     - 負の \\( x \\) 値に対して小さな負の傾きを許容することで、**Dying ReLU問題**を防ぐ。
     - ReLUと同様に、勾配消失問題の解決に役立ち、スパース性を促進する。
   - **欠点**:
     - 依然として負の出力を生成する可能性があるが、標準的なReLUよりも問題は少ない。
     - \\( \alpha \\) の選択が難しく、場合によっては調整が必要になることがある。
   - **使用例**: ReLUの問題を回避するために、多くの深層学習モデル、特にDying ReLU問題が発生する可能性のあるネットワークで使用される。

---

### **比較まとめ**:

- **シグモイド**:
  二値分類タスクの出力層に最適だが、勾配消失問題に悩まされる。

- **ReLU**:
  隠れ層における汎用性の高い活性化関数。学習を高速化し、大規模なネットワークでうまく機能するが、ニューロンの死の問題が発生する可能性がある。

- **Leaky ReLU**:
  ReLUの修正版であり、負の入力に対して小さな非ゼロの勾配を許容することでDying ReLU問題に対処する。深いモデルで広く使用される。

### **実際の使用例**:
- **シグモイド**は、二値分類タスクの最終層など、出力を(0, 1)の範囲にする必要がある場合にしばしば使用される。
- **ReLU**は、そのシンプルさと勾配消失問題への対応力から、隠れ層におけるデフォルトの選択肢となることが多い。
- **Leaky ReLU**は、学習中にニューロンが「死ぬ」懸念がある場合にReLUの代わりに使用される。

これらの活性化関数は、構築するニューラルネットワークの構造と目的に応じて、それぞれに適した役割を持っています！