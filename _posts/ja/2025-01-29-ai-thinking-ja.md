---
audio: false
generated: false
lang: ja
layout: post
title: AI思考
translated: true
---

- Satya Nadellaはジェヴンスパラドックスを挙げています。学ぶ価値があります。

- 王尹：人工知能には「知」がありません、ニューラルネットワークには「神経」がありません、マシンラーニングには「学習」がありません、ディープラーニングには「深さ」がありません。この分野には「計算」が本当に働いています。だからこの分野を「微分可能な計算」と呼ぶことが好みです。モデルを構築するプロセスを「微分可能なプログラミング」と呼びます。

- 王尹：マシンラーニングは本当に有用で、甚至美しい理論です。それは単にカルクスの化粧をしたものです！それはニュートン、ライブニッツの古い大きな理論の簡潔でエレガントでかつ強力な形です。マシンラーニングは基本的にカルクスを使っていくつかの関数を導出し、最適化することです。ディープラーニングはそれに比べてより複雑な関数を最適化することです。

- 大きな言語モデルは現在、YAMLやPythonのようなファイル言語でフィルタリングできません。しかし現実世界の情報の大部分はこのように整理されています。つまり、ファイルを使って大きな言語モデルをトレーニングすることができるかもしれません。

- 大きな言語モデルをトレーニングするためには、完全一致を見つけるシステムを開発することができます。可能性として、KMP（ナット-モリス-プラット）検索アルゴリズムをトランスフォーマーアーキテクチャと組み合わせて検索能力を強化することができるかもしれません。