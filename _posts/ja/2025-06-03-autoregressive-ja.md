---
audio: false
generated: true
lang: ja
layout: post
title: 自己回帰モデル生成
translated: true
type: note
---

GPT（Generative Pre-trained Transformer）のようなモデルにおいて、「オートレグレッシブ（自己回帰）」という用語は、モデルが出力シーケンスを生成する方法を指します。具体的には、モデルがシーケンス内の各項目を1つずつ生成し、その際に以前の出力を次の項目を生成するための追加の入力コンテキストとして使用することを意味します。

以下に、このようなモデルにおけるオートレグレッシブ処理の仕組みを詳しく説明します：

1. **逐次生成**: モデルはテキストを逐次的に生成します。初期入力（通常はプロンプトと呼ばれる）から開始し、一度に1つのトークン（単語、単語の一部、または文字）を生成します。

2. **以前のトークンへの依存性**: 新しいトークンを生成するたびに、モデルは以前に生成されたすべてのトークンを考慮します。これは、各ステップでの出力が、以前のすべてのステップの出力に依存していることを意味します。

3. **フィードバックループ**: モデルはフィードバックループを使用し、タイムステップ \\( t \\) でのモデルの出力がタイムステップ \\( t+1 \\) での入力として再利用されます。これにより、モデルは長いシーケンスにわたって一貫性とコンテキストを維持できます。

4. **確率分布**: 各ステップで、モデルは可能な次のトークンに対する確率分布を生成し、この分布に基づいて（サンプリングや最尤トークンの選択などの技術を使用して）1つのトークンを選択します。

要約すると、GPTおよび類似のモデルにおける「オートレグレッシブ」とは、モデルがステップごとにシーケンスを生成し、各ステップが前のステップに依存することで、一貫性があり文脈に適したテキストを生成できることを意味します。