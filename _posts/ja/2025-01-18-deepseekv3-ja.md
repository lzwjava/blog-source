---
audio: true
lang: ja
layout: post
title: 'DeepSeek V3: マルチヘッド潜在アテンションとマルチトークン予測'
translated: true
---

DeepSeek v3について探求します。参考にしたのは「Multi-Head Latent Attention and Multi-token Prediction in Deepseek v3」という動画です [https://youtu.be/jL49fLOJYNg?si=4uE2kfe-BlKC1ngO](https://youtu.be/jL49fLOJYNg?si=4uE2kfe-BlKC1ngO)。この動画の文字起こしにはGoogle Cloud Speech-to-Textを使用し、さらにいくつかのコードを使って文字起こしを整理しました。

---

A: Deep tagへようこそ。今日は大規模言語モデルの世界に深く潜り込んでいきます。具体的には、DeepSeek V3についてです。

B: いいですね。6710億パラメータのモデルで、その効率性と性能のユニークなアプローチが話題を呼んでいますよね？

A: そして、あなたはそのアーキテクチャを詳細に説明した学術論文を共有しましたね。

B: はい。

A: そして、機械学習の専門家として、DeepSeek V3がどのようにして高い性能と経済的なトレーニングを両立させているかを理解したいと思っています。

B: その通りです。

A: おや、こんにちは、どうしたの？

C: MLAの詳細、MLAの仕組みについて教えてください。

A: ああ、もちろん。それは素晴らしいアイデアです。MLA（Multi-Head Latent Attention）について深く掘り下げることができます。MLAの細かい部分に興味があるんですね。では、これを解き明かしましょう。DeepSeek V3の効率性の鍵の一つは、Mixture of Experts（MoE）アーキテクチャですよね？各トークンに対してパラメータの一部だけが活性化される仕組みです。そして、DeepSeek V3はMLAとDeepSeek Moを使ってさらに一歩進んでいます。

B: その通りです。では、今はMLAに焦点を当てましょう。

A: 了解。リアルタイムアプリケーションでは、速度が重要です。

B: そうですね。推論中に必要なキー・バリューキャッシュが大きなボトルネックになることがあります。

A: まさにその通りです。そこでMLAが活躍します。従来のアテンションメカニズムでは、以前のトークンに関する多くの情報を保存する必要があります。

B: そうですね。長いテキストシーケンスではそれが問題になりますよね？

A: しかし、MLAはこの情報を巧妙に圧縮し、キャッシュのフローを大幅に削減し、推論をはるかに高速化します。まるで分厚い百科事典を要点だけに凝縮するようなものです。

B: いい例えですね。不必要な重さを省きつつ、重要な情報を保持します。リアルタイムアプリケーションには非常に有用です。

A: はい。では、実際にどのように動作するのかについて話しましょう。MLAはどのようにこの圧縮を実現するのでしょうか？

B: そうですね、MLAはアテンションキーとバリューの低ランク結合圧縮を使用します。

A: なるほど、キーとバリューを圧縮するわけですが、具体的にはどういう意味ですか？少し技術的な話をしましょう。MLAメカニズムは、入力された隠れ表現を受け取り、それをクエリ、キー、バリューベクトルに投影します。ここで面白いのは、MLAがクエリを2つの部分に分離することです。

B: 2つの部分？

A: はい。1つはコンテンツ用で、もう1つは位置情報用です。これはRopeと呼ばれるものを使います。

B: Rope？なんだか技術的な響きですね。

A: これはrotary position embeddingsの略で、モデルがシーケンス内のトークンの位置を理解するのに役立ちます。そして、キーとバリューは低次元の潜在空間に圧縮されます。つまり、データを縮小してメモリを節約するわけです。

B: その通りです。重要な情報は保持されますが、不必要な部分は捨てられます。この圧縮された表現により、推論中のKVキャッシュが大幅に小さくなり、処理が高速化されます。

A: また、マルチヘッド処理も使用しています。

B: はい。従来のアテンションと同様に、MLAも複数のヘッドを使用します。

A: どうぞ。

C: つまり、2つの潜在空間と1つの隠れ入力があるわけですね。

A: その通りです。コンテンツの潜在空間とキー・バリューの潜在空間の2つがあります。

B: その通りです。そして、これらの潜在空間はRope（rotary position embeddings）を通じて処理されます。

A: つまり、Ropeが位置情報を取得する方法なんですね。

B: はい。コンテンツとキー・バリューの潜在空間の両方に適用されます。この圧縮された表現を処理し、すべてを再結合します。

A: はい。そして、キャッシングの最適化により、シーケンシャル処理中のオーバーヘッドがさらに削減されます。これがMLAが処理を高速化する仕組みです。

B: その通りです。性能を犠牲にすることなく、効率的なアテンションを実現する巧妙な方法です。

A: なるほど、これはかなり賢いトリックですね。でも、ちょっと待って。

B: どうしたの？

A: DeepSeek Moについて話を進めましょう。従来のMoEモデルとどう違うのでしょうか？

B: DeepSeek Moは... あ、リスナーの方、どうしましたか？

C: 隠れ空間についてもっと話しましょう。隠れ空間から何が得られるのでしょうか？

A: もちろん... あなたが何を言いたいのかわかります。隠れ空間は非常に興味深いです。あなたは、先ほど話した潜在空間、つまり「洞窟」の中で何が起こっているのかについて興味を持っているんですね。潜在空間の数だけでなく、そこで何が起こっているのかについてです。

B: それは面白いですね。

A: その通りです。MLA内には、コンテンツ用とキー・バリュー用の2つの異なる潜在空間があります。情報を保存するための2つの別々のユニットのようなものです。そして、これらの潜在空間は、Rope操作を通じて処理されます。rotary positional embeddingは、アテンションメカニズムに位置情報を埋め込むのに非常に重要です。要約すると、クエリは分割され、キーとバリューも圧縮されます。

B: はい。これらは2つの別々の潜在空間に配置され、1つはコンテンツ用、もう1つはキー・バリューペア用です。これらの潜在空間は、MLAの効率性にとって非常に重要です。

A: その通りです。では、これらの操作についてもう少し詳しく話しましょう。MLAは実際にどのようにしてこれらの潜在空間変換を実行するのでしょうか？

B: 入力は、コンテンツとキー・バリューの表現の両方に対して並列処理されます。つまり、洞窟の中に2つのパスがあるようなものです。

A: はい。それぞれの潜在空間に対して1つずつです。そして、これらの空間内で、情報はRopeを使って処理されます。

B: その通りです。これにより、モデルは洞窟の中を通りながら位置情報を保持します。つまり、モデルはテキストのどの部分がどこにあるのかを知ることができます。

A: その通りです。そして、この処理は次の結合ステージの前に行われます。洞窟の中を通りながら、何が結合されているのでしょうか？

B: メカニズムは2つの主要な結合操作を実行します。クエリ表現が結合され、キー表現も結合されます。つまり、洞窟の中で重要なピースをすべてまとめるようなものです。

A: はい。これらの結合により、コンテンツと位置情報が組み合わされます。そして、これらの結合された表現がアテンション計算に使用されますよね？

B: その通りです。そして、最初の圧縮により、洞窟の中を通る処理がはるかに高速化されます。MLAは、洞窟の内外で計算コストを大幅に削減します。

A: その通りです。DeepSeek V3のような大規模モデルのアテンションメカニズムを最適化します。これは素晴らしい質問です。では、洞窟を通り抜けた後、DeepSeek Moについて話を進めましょう。

B: 了解。DeepSeek Moですね。あなたが何を言いたいのかわかります。MLA内には、コンテンツ用とキー・バリュー用の2つの異なる潜在空間があります。

A: その通りです。この分離がその仕組みの鍵です。情報を保存するための2つの別々のユニットのようなものです。そして、これらの潜在空間は、Rope操作を通じて処理されます。rotary positional embeddingは、アテンションメカニズムに位置情報を埋め込むのに非常に重要です。要約すると、クエリは分割され、キーとバリューも圧縮されます。

B: はい。これらは2つの別々の潜在空間に配置され、1つはコンテンツ用、もう1つはキー・バリューペア用です。これらの潜在空間は、MLAの効率性にとって非常に重要です。

A: その通りです。では、これらの操作についてもう少し詳しく話しましょう。MLAは実際にどのようにしてこれらの潜在空間変換を実行するのでしょうか？

B: 入力は、コンテンツとキー・バリューの表現の両方に対して並列処理されます。つまり、2つのパスがあるようなものです。

A: はい。それぞれの潜在空間に対して1つずつです。そして、これらの空間内で、情報はRopeを使って処理されます。

B: その通りです。これにより、モデルは位置情報を保持します。そして、効率を高めるために、共有されたエキスパートを使用します。つまり、複数のタスクで使用できるエキスパートです。

A: はい。これにより、冗長性が避けられ、システムがさらに合理化されます。

B: そうですね。専門性を持ちつつ、他のこともできるチームのようなものです。

A: そう、それは非常に賢いアプローチです。しかし、多くの専門家がいる中で、特定のエキスパートが過負荷にならないようにするにはどうしているのでしょうか？

B: そうですね。他のエキスパートがアイドル状態にならないように。

A: そこで、彼らの革新的な補助損失なしのロードバランシングが登場します。

B: ここからが本当に面白くなります。彼らはそれをどのように実現しているのでしょうか？

A: 従来のMoEモデルでは、トレーニング中に補助損失関数を使用して、エキスパートの使用を均等に促しますが、これは実際には性能を損なうことがあります。

B: そうですね。まるでスーパーのレジで全員が同じ列を使うように強制するようなものです。

A: その通りです。一部が他の列よりも速く進んでいても、不必要な遅延を生み出します。

B: はい。そこで、DeepSeek V3は、各エキスパートの負荷に基づいてバイアス項を動的に調整します。つまり、エキスパートが多くのリクエストを受け取っている場合、システムはそのエキスパートをルーティングメカニズムに対して少し魅力的でなくし、一部のトラフィックをあまり忙しくないエキスパートに振り向けます。

A: なるほど。これにより、長いシーケンスを効率的に処理し、推論に必要なKVキャッシュのサイズを削減します。つまり、オーバーヘッドを削減しつつ、高い性能を維持するわけです。

B: その通りです。これは重要なボトルネックに対処する非常に賢いアプローチです。

A: 確かに。では、DeepSeek V3がどのようにロードバランシングを処理するかについてもカバーするべきですね。

B: はい、それも非常に重要な部分です。次にそれについて触れましょう。

A: いいですね。これでMLAとその潜在空間についての概要がよくわかったと思います。

B: はい、詳細を一緒に探求してくれてありがとう。次回もまた深掘りしていきましょう。

A: はい。まるでエキスパートのための交通管理システムのようですね。常に流れを監視し、ボトルネックを避けるために調整を行います。

B: そして、補助損失による性能低下を避けます。

A: その通りです。そして、どうぞ。

C: MTPについてもっと話しましょう。MTPモジュールがどのように埋め込みを共有しているのか、そして...

A: もちろん。それは素晴らしい質問です。MTPモジュールがどのようにリソースを共有しているのかについて話しましょう。あなたはMTPの実装の細部に興味があるんですね。

B: そうですね。では、これを解き明かしましょう。DeepSeek V3はMTP（Multi-Token Prediction）を使用して、複数のトークンを一度に予測しますよね？1つのトークンだけでなく、複数のトークンを予測します。

A: そして、ここが本当に面白いところです。あなたはMTPモジュールがどのように設定され、どのようにリソースを共有しているのかに興味があるんですね。各MTPモジュールには、共有された埋め込み層と共有された出力ヘッドが含まれています。つまり、メインモデルと同じ埋め込みと出力ヘッドを使用します。

B: その通りです。つまり、彼らはすべて同じ知識のプールから引き出しているようなものです。これにより、計算コストが節約されます。

A: はい。そして、独自のトランスフォーマーブロックを使用します。つまり、メインモデルと同じトランスフォーマーブロックを共有しているわけではありません。

B: その通りです。各MTPモジュールには、処理用の独自のトランスフォーマーブロックがあります。これにより、各トークンの予測が区別されます。

A: はい。そして、情報を結合するために、線形射影と結合が使用されます。

B: つまり、複数の場所からピースを取って、完全な絵を構築するようなものです。

A: はい。そして、すべてのMTPモジュールは並列で動作しますが、埋め込み層と出力ヘッドを共有しますよね？

B: その通りです。これがこの設計の効率性の鍵です。つまり、互いに依存し合う相互接続されたパーツのシステムのようなものです。

A: そして、このリソースの効率的な共有により、トレーニングが高速化され、性能が向上します。

B: なるほど、これはかなり賢いトリックですね。でも、ちょっと待って。

A: どうしたの？

B: 全体像について話を進めましょう。このモデルはどのようにロードバランシングを処理するのでしょうか？エキスパートはどのように選ばれるのでしょうか？

A: はい、それについて話すことができます。では、DeepSeek V3のロードバランシング戦略について深く掘り下げましょう。

B: いいですね。DeepSeek V3は、先ほど話したMTPを使用します。

C: ああ、MTPの詳細についてもっと話しましょう。

A: もちろん... MTPについてさらに深く掘り下げることに興味を持ってくれて嬉しいです。MTPの詳細について話しましょう。共有された埋め込み層と出力ヘッドについて触れましたが、各MTPモジュールには独自のトランスフォーマーブロックがあります。

B: その通りです。しかし、それだけではありません。では、詳しく見ていきましょう。

A: 了解。MTPモジュールのシーケンシャルな性質について話しましょう。

B: はい。一部のモデルとは異なり、DeepSeek V3は追加のトークンをシーケンシャルに予測します。つまり、すべてのトークンを一度に予測するわけではありません。

A: その通りです。各モジュールは前のモジュールの出力に基づいて構築されます。つまり、予測の連鎖であり、それぞれが前の予測に依存します。

B: はい。そして、各予測深度に対して因果連鎖を維持します。つまり、因果性を壊しません。

A: その通りです。これは、全体のコンテキストが正しいことを保証するために重要です。つまり、MTPモジュールは独立して動作するわけではありません。

B: その通りです。それらは相互接続されており、この予測の連鎖がトレーニングの効率を高め、テキストのより微妙な理解を可能にします。あなたはまた、モジュールがどのように埋め込みを共有しているのかにも興味がありますよね？ご存知の通り、共有された埋め込み層はトークンをベクトル表現にマッピングします。つまり、各トークンはベクトルに変換されます。

A: はい。そして、このマッピングはすべてのMTPモジュールで共有されます。これにより、予測全体で一貫性が保たれます。

B: その通りです。そして、共有された出力ヘッドは、トークンの最終的な隠れ状態を受け取り、