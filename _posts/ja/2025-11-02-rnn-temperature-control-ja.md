---
audio: false
generated: true
image: false
lang: ja
layout: post
model: x-ai/grok-4-fast
title: RNNテキスト生成の温度制御
translated: true
type: note
---

### RNNサンプリングにおける温度の理解

テキスト生成を行うリカレントニューラルネットワーク（RNN）において、温度は生成されるテキストの「ランダム性」や「予測可能性」を調整するハイパーパラメータです。これは、RNNが前の文字（またはトークン）に基づいて次の文字を予測するサンプリングステップで適用されます。温度制御がない場合、生成は硬直的になりすぎる（常に最も確率の高い次の文字を選び、退屈なループが生じる）か、あるいは純粋なランダム性になりすぎる可能性があります。温度は、可能な次の文字に対するモデルの確率分布を和らげることで、このバランスを取ります。

#### 背後にある簡単な数学
RNNは、可能な次の文字それぞれに対して*ロジット*（未加工で正規化されていないスコア）を出力します。これらはソフトマックス関数を使用して確率に変換されます：

\\[
p_i = \frac{\exp(\text{logit}_i / T)}{\sum_j \exp(\text{logit}_j / T)}
\\]

- \\(T\\) は温度です（通常は0.1から2.0の間）。
- \\(T = 1\\) の場合、これは標準的なソフトマックスです：確率はモデルの「自然な」信頼度を反映します。
- その後、常に最も確率の高いものを選ぶ（貪欲なデコード）代わりに、この分布から次の文字を*サンプリング*します（例えば、多項サンプリングを介して）。

このサンプリングは反復的に行われます：選択された文字を入力としてフィードバックし、次を予測し、繰り返すことでシーケンスを生成します。

#### 低温：反復的だが安全
- **効果**：\\(T < 1\\)（例：0.5や0に近い値）は分布を*鋭く*します。信頼度の高い予測の確率はさらに高くなり、低い確率はゼロに近づけられます。
- **出力**：テキストは「安全」で一貫性を保ちますが、すぐに反復的になります。モデルは最も確率の高いパスに固執し、ループに囚われたようになります。
- **Karpathyのブログ投稿からの例**（Paul Grahamスタイルのエッセイを生成）：非常に低い温度では、次のようなものが出力されます：
  > 「それは彼らが皆同じものであったということですスタートアップであったということですそれは彼らが皆同じものであったということですスタートアップであったということですそれは彼らが皆同じものであったということですスタートアップであったということです」

  これは確信があり文法的に正しいですが、創造性に欠けます。トレーニングデータの無限のエコーのようなものです。

#### 高温：創造的だが不安定
- **効果**：\\(T > 1\\)（例：1.5や2.0）は分布を*平坦に*します。確率がより均一になり、勝算の低い（可能性の低い文字）にもチャンスが与えられます。
- **出力**：より多様で独創的なテキストになりますが、無意味な表現、スペルミス、または大きく脱線することもあります。モデルは稀なパスを探索するため、独創性を刺激する可能性がありますが、一貫性を犠牲にします。
- **Karpathyからの例**：高い温度では、新鮮な表現を用いた多様なエッセイが生成されますが、「より多くの間違い（例：スペルミスなど）を代償に」なります。これは、RNNが自由にブレインストーミングを行っているようなものです。詩やSFには楽しいですが、事実に基づく文章にはリスクがあります。

#### RNNにおいてこれが重要な理由
RNN（またはKarpathyのセットアップにおけるLSTM）は自己回帰的です：各予測は前の予測に基づいて構築されるため、サンプリングにおける小さなバイアスが長いシーケンスにわたって累積します。温度は探索と活用のバランスを調整することを可能にします：
- 信頼性が高く、ブランドに合ったテキスト（例：商品説明）には低い \\(T\\) を使用します。
- ブレインストーミングや芸術的な生成（例：超現実的な物語）には温度を上げます。
- コードで実験：KarpathyのバニラRNNリポジトリには温度スライダーが含まれており、シェイクスピアのサンプルで、平凡なソネットから無秩序な詩への変化を確認できます。

本質的に、温度は何か新しいことを「学習」するものではありません。推論中のRNNの創造性のノブを調整しているだけです。低い温度は冷静で予測可能に保ち、高い温度は混沌を解き放ちます。

[The Unreasonable Effectiveness of Recurrent Neural Networks](http://karpathy.github.io/2015/05/21/rnn-effectiveness/)