---
audio: false
generated: true
lang: ja
layout: post
title: トランスフォーマー
translated: true
type: note
---

**Transformer**は、ほとんどの現代的な大規模言語モデル(LLM)の基盤となった画期的なニューラルネットワークアーキテクチャです。Vaswaniらによる2017年の画期的な論文"Attention Is All You Need"で導入され、以来、自然言語処理(NLP)の分野を根本的に変えました。

逐次的にデータを処理するリカレントニューラルネットワーク(RNN)のような以前の主流アーキテクチャとは異なり、Transformerは入力シーケンス全体を**並列に**処理します。この重要な違いにより、訓練速度の大幅な向上と、テキスト内の長距離依存関係を捕捉する能力が可能になります。

以下に、LLMの文脈におけるTransformerアーキテクチャの主要なコンポーネントと概念について詳しく説明します。

**1. 核心となるアイデア: アテンション機構**

Transformerの中核的な革新は、**アテンション機構**、特に**セルフアテンション**です。この機構により、モデルは特定の単語を処理する際に、入力シーケンス内の異なる単語(またはトークン)の重要度を重み付けすることができます。RNNのように直前の単語だけに依存するのではなく、セルフアテンションによりモデルは単語の意味と関係性を理解するために文脈全体を考慮できるようになります。

このように考えてみてください：文を読むとき、あなたは各単語を孤立して処理するわけではありません。脳は全体の意味と各単語がそれにどう貢献するかを理解するために、すべての単語を同時に考慮します。セルフアテンション機構はこの振る舞いを模倣します。

**セルフアテンションの仕組み(簡略化):**

入力シーケンスの各単語について、Transformerは3つのベクトルを計算します：

*   **Query (Q):** 現在の単語が他の単語に「何を探しているか」を表す。
*   **Key (K):** 他の各単語が「何の情報を含んでいるか」を表す。
*   **Value (V):** 関連する可能性のある、他の各単語が持つ実際の情報を表す。

セルフアテンション機構はその後、以下のステップを実行します：

1.  **アテンションスコアの計算:** ある単語のQueryベクトルと、シーケンス内の他のすべての単語のKeyベクトルとの内積が計算されます。これらのスコアは、他の各単語の情報が現在の単語にどれだけ関連しているかを示します。
2.  **スコアのスケーリング:** スコアはKeyベクトルの次元の平方根(`sqrt(d_k)`)で除算されます。このスケーリングは、訓練中の勾配を安定させるのに役立ちます。
3.  **Softmaxの適用:** スケーリングされたスコアはsoftmax関数に通され、0から1の間の確率に正規化されます。これらの確率は**アテンション重み**を表します – 現在の単語が他の各単語にどれだけの「注意」を向けるべきかを示します。
4.  **重み付きValueの計算:** 各単語のValueベクトルに対応するアテンション重みが乗算されます。
5.  **重み付きValueの合計:** 重み付きValueベクトルが合計され、現在の単語の**出力ベクトル**が生成されます。この出力ベクトルには、入力シーケンス内の他のすべての関連する単語からの情報が、その重要度に応じて重み付けされて含まれるようになります。

**2. マルチヘッドアテンション**

モデルが異なる種類の関係性を捕捉する能力をさらに強化するために、Transformerは**マルチヘッドアテンション**を採用しています。セルフアテンション機構を1回だけ実行するのではなく、異なるQuery、Key、Valueの重み行列のセットを使って並列に複数回実行します。各「ヘッド」は単語間の関係の異なる側面(例: 文法的依存関係、意味的関連性)に注目することを学習します。すべてのアテンションヘッドの出力は、連結され、線形変換されて、マルチヘッドアテンションレイヤーの最終的な出力が生成されます。

**3. 位置エンコーディング**

Transformerはすべての単語を並列に処理するため、シーケンス内の単語の**順序**に関する情報が失われます。この問題に対処するために、**位置エンコーディング**が入力埋め込みに加えられます。これらのエンコーディングは、シーケンス内の各単語の位置を表すベクトルです。通常、固定されたパターン(例: 正弦関数)または学習された埋め込みです。位置エンコーディングを追加することで、Transformerは言語の連続的な性質を理解できるようになります。

**4. エンコーダとデコーダのスタック**

Transformerアーキテクチャは通常、**エンコーダ**と**デコーダ**という2つの主要部分で構成され、それぞれが同じ層を複数積み重ねたものです。

*   **エンコーダ:** エンコーダの役割は、入力シーケンスを処理し、その豊富な表現を作成することです。各エンコーダ層は通常以下を含みます：
    *   **マルチヘッド・セルフアテンション**サブレイヤー。
    *   **フィードフォワードニューラルネットワーク**サブレイヤー。
    *   各サブレイヤーを囲む**残差接続**と、それに続く**レイヤー正規化**。残差接続は訓練中の勾配の流れを助け、レイヤー正規化は活性化を安定させます。

*   **デコーダ:** デコーダの役割は、出力シーケンス(例: 機械翻訳やテキスト生成において)を生成することです。各デコーダ層は通常以下を含みます：
    *   **マスク付きマルチヘッド・セルフアテンション**サブレイヤー。「マスキング」により、デコーダは訓練中にターゲットシーケンスの将来のトークンを先読みすることを防ぎ、次のトークンを予測するために以前に生成されたトークンのみを使用することを保証します。
    *   エンコーダの出力に注意を向ける**マルチヘッドアテンション**サブレイヤー。これにより、デコーダは出力を生成しながら入力シーケンスの関連部分に焦点を当てることができます。
    *   **フィードフォワードニューラルネットワーク**サブレイヤー。
    *   エンコーダと同様の**残差接続**と**レイヤー正規化**。

**5. フィードフォワードネットワーク**

各エンコーダ層とデコーダ層には、フィードフォワードニューラルネットワーク(FFN)が含まれています。このネットワークは各トークンに独立して適用され、アテンション機構によって学習された表現をさらに処理するのに役立ちます。通常、非線形活性化関数(例: ReLU)を挟んだ2つの線形変換で構成されます。

**TransformerがLLMでどのように使用されるか:**

LLMは、主に**デコーダのみ**のTransformerアーキテクチャ(GPTモデルのような)または**エンコーダ-デコーダ**アーキテクチャ(T5のような)に基づいています。

*   **デコーダのみのモデル:** これらのモデルは、先行するトークンが与えられた場合に、シーケンス内の次のトークンを予測するように訓練されます。これらは複数のデコーダ層を積み重ねます。入力プロンプトは層を通して渡され、最終層は次のトークンに対する語彙全体の確率分布を予測します。この分布から自己回帰的にサンプリングすることにより、モデルは首尾一貫した文脈的に関連性のあるテキストを生成できます。

*   **エンコーダ-デコーダモデル:** これらのモデルは入力シーケンスを受け取り、出力シーケンスを生成します。これらは完全なエンコーダ-デコーダアーキテクチャを使用します。エンコーダが入力を処理し、デコーダはエンコーダの出力を使用してターゲットシーケンスを生成します。これらは翻訳、要約、質問応答などのタスクに適しています。

**意義の深い理解:**

TransformerアーキテクチャのLLMへの影響は甚大です：

*   **長距離依存関係の処理:** セルフアテンション機構により、モデルはシーケンス内で離れた単語を直接結びつけることができ、長距離文脈を捕捉する際のRNNの限界を克服します。
*   **並列処理:** シーケンス全体を並列に処理することで、訓練時間を大幅に短縮し、はるかに大規模なデータセットとモデルの使用を可能にします。
*   **スケーラビリティ:** Transformerアーキテクチャは、データとモデルサイズの増加にうまく対応し、数十億甚至は数兆のパラメータを持つ非常に強力なLLMの開発につながりました。
*   **文脈理解:** アテンション機構により、モデルは単語の文脈をより微妙な方法で理解できるようになり、より優れた言語理解と生成を実現します。
*   **革新の基盤:** Transformerアーキテクチャは、数多くの進歩と変種の基盤として機能し、現代のLLMの多様な景観をもたらしました。

まとめると、その核心となるセルフアテンション機構、並列処理能力、エンコーダ-デコーダ構造(またはデコーダのみの変種)を備えたTransformerアーキテクチャは、NLPの分野に革命をもたらし、現代の大規模言語モデルの驚くべき能力の原動力となっています。