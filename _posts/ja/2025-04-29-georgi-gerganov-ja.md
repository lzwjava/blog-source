---
audio: false
generated: true
lang: ja
layout: post
title: ゲオルギー・ゲルガノフ
translated: true
type: note
---

医療物理学のバックグラウンドを持つブルガリア人開発者Georgi Gerganovは、Meta AIのLLaMAのような大規模言語モデル(LLM)を実行するための高効率なC/C++ライブラリ **llama.cpp** の作成を通じて、AIの民主化における重要な人物として登場しました。彼の旅はニューラルネットワークへの懐疑から始まりましたが、技術的好奇心と最適化の才能が、オンデバイスAI推論における画期的な貢献へと導きました。

### 背景と初期の仕事
- **学歴と業績**: Gerganovはソフィア大学物理学部で医療物理学を専攻。2006年の国際物理オリンピックで銀メダル、2008年にはブルガリアソフトウェア会社協会主催のプログラミングコンテストで優勝するなど、早くから才能を発揮しました。
- **初期のAI懐疑論**: 2022年以前、Gerganovは自らを「AI非信者」と称し、ニューラルネットワークの可能性に懐疑的で、技術に対する保守的な見方を好んでいました。
- **Whisper.cpp**: 彼の最初の主要なAIプロジェクトは**whisper.cpp**(2022年)で、OpenAIの音声認識モデルWhisperのC/C++ポートです。タイミングと幸運に触発されたこのプロジェクトは、WhisperをCPU上で動作するように最適化し、GPUを搭載していないノートパソコンやスマートフォンなどのデバイスでも利用可能にしました。効率的な音声文字起こしと翻訳を可能にしたことで、注目を集めました。

### llama.cppの誕生
- **背景**: 2023年2月、Meta AIは研究用の効率的なLLMファミリーであるLLaMA(7Bから65Bパラメータ)をリリースしましたが、これを実行するには通常GPUをはじめとする相当な計算リソースが必要でした。
- **課題**: whisper.cppでの成功に触発され、Gerganovは「楽しみのため」にLLaMAを消費者向けハードウェア、具体的にはMacBookで動作させることに着手しました。2023年3月、彼は外部依存関係のない、LLaMAの推論コードを実装したミニマリストなC/C++実装である **llama.cpp** を開発しました。
- **主要な革新**: Gerganovは、Fabrice BellardのLibNCに触発されて2022年9月に開始した、Cベースのテンソル代数フレームワークである自身の **GGML** (Georgi Gerganov Model Language)ライブラリを活用しました。GGMLは厳格なメモリ管理とマルチスレッド処理を重視し、効率的なCPUベースの推論を可能にしました。
- **量子化の突破口**: llama.cppの核心的な機能が4ビット量子化です。これはモデルの重みを圧縮してメモリ使用量を削減し、推論速度を向上させるもので、精度の低下は最小限(例えば、4ビットでパープレキシティが4%のみ増加)でした。これにより、AndroidスマートフォンやRaspberry Piを含む、わずか4GBのRAMを搭載したデバイスでも7B LLaMAモデルを実行できるようになりました。

### 影響と成長
- **アクセシビリティ**: llama.cppは、特殊なハードウェアを持たない愛好家や開発者にもLLMを利用可能にしました。MacBook、Pixel phone、さらにはRaspberry Pi 4(速度は約1トークン/秒と遅いが)でも動作し、ハッカーや研究者による多様なプラットフォームでのLLaMA実行の実験の波を引き起こしました。
- **コミュニティと規模**: このプロジェクトは人気が爆発的に拡大し、69,000以上のGitHubスター、2,600以上のリリース、900人以上のコントリビューターを集めました。そのオープンソース性とシンプルさ(例えば、単一のC++ファイル内のCUDAバックエンド)が協力を促進し、AMDデバイス向けのROCmサポートや、MPIを利用した分散推論などの機能を含むようになりました。
- **GGUFフォーマット**: 2023年8月、GerganovはGGMLに代わる **GGUF** (GGML Universal File)フォーマットを導入しました。GGUFはモデルの重み、メタデータ、トークンを単一のバイナリファイルに統合し、2ビットから8ビットの量子化をサポートし、後方互換性を保証しました。これはモデルの保存と読み込みをさらに最適化するものです。
- **マルチモーダルサポート**: 2023年10月までに、llama.cppはLLaVAのようなマルチモーダルモデルのサポートを追加し、その範囲をテキストを超えて視覚ベースのタスクにまで拡大しました。

### 技術的貢献
- **最適化技術**: GerganovによるSIMDベクトル命令(例: AVX2/AVX-512)の使用は、行列演算においてCPUを「ミニGPU」に変え、パフォーマンスを向上させました。Apple Siliconに関する彼のベンチマークは、LLM推論におけるそのメモリ帯域幅の利点を強調しました。
- **哲学的転換**: Llama.cppは、AI競争の焦点を生のモデル性能から最適化とアクセシビリティへと移行させ、ローカル推論を可能にし、クラウドベースのGPUへの依存を減らしました。
- **エッジAI**: このプロジェクトはオンデバイスAIのビジョンと一致し、6台のRaspberry Piに分散した65B LLaMA推論のような実験は、低コストで分散型のAIの可能性を示しました。

### より広範な影響
- **ggml.ai**: Gerganovは、GGMLとllama.cppの開発を支援するため、Nat FriedmanとDaniel Grossが支援する **ggml.ai** を設立しました。この会社はオンデバイス推論を推進するためにコントリビューターを雇用しています。
- **文化的影響**: Xでは「ヨーロッパのアルファコーダー」と呼ばれ、Gerganovの迅速な開発(例えば、一晩でllama.cppをハックすること)とオープンソースの精神は、AIコミュニティにおける彼をフォークヒーローにしました。彼の仕事はStable Diffusionのオープンリリースとの比較を引き起こし、LLMにおける「Stable Diffusion的な瞬間」を印しました。
- **民主化**: LLMが消費者向けデバイスで動作するようにしたことで、llama.cppはAI実験への参入障壁を下げ、エッジAIにおける革新を促進し、GPU中心のフレームワークの支配力に挑戦しました。

### 本質を捉えた物語
Georgi Gerganovのllama.cppにまつわる物語は、技術的独創性と時流に乗ったタイミングの賜物です。AI懐疑論者として出発した彼は、LLaMAがリリースされた瞬間を捉え、自身のC/C++とテンソル最適化に関する専門知識を応用して、最先端のLLMを日常的なハードウェアで動作させることに成功しました。llama.cppを通じて、彼は技術的な課題を解決しただけでなく、アクセス可能なローカルAIへの動きを触発し、ブルガリアから来た一人の開発者がAIの景観を再形成できることを証明しました。彼の仕事は進化を続けており、llama.cppはオープンソース革新の力の証として立ち続けています。