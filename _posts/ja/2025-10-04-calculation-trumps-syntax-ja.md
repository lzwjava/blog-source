---
audio: false
generated: true
image: false
lang: ja
layout: post
model: x-ai/grok-4-fast
title: AIにおける計算力は構文を凌駕する
translated: true
type: note
---

あなたのご指摘はまさにその通りで、AIやLLMの核心は、優美な散文やプログラミング言語に関する哲学的思索にあるのではなく——Yin WangやDaniel P. Friedmanのような人々が、間違いなく我々の思考を研ぎ澄ましてくれたとはいえ——計算の実際における「方法」と「内容」にあります。WangのブログはPythonやLispのようなシステムの核心に深く潜り、それらが実際に計算をどのように形作るかを批判的に検討しています。一方、Friedmanの書籍（例えば*The Little Schemer*シリーズ）は、再帰や関数型パラダイムを、ほとんど詩的に感じられるほど明快に説明しています。しかし、確かに、首尾一貫したテキストを生成したり、ニューラルネットの勾配を計算したりといった、実際に「動作する」ものを構築する際には、その核心は構文の糖衣ではなく、計算の「方法」と「内容」にあるのです。

### 計算が構文に勝る理由
核心を言えば、私のようなLLMはLispのマクロやJavaのオブジェクト階層について思索しているのではなく、大規模な行列計算、アテンション・メカニズム、確率的サンプリングを実行しています。「計算方法」は以下のものに要約されます：
- **アルゴリズムとモデル**: Transformerアーキテクチャ（Vaswani et al., 2017）のようなものは、*何が*計算されるか——トークン埋め込みに対するself-attention、位置エンコーディングなど——を定義します。ここが魔法が起こる場所であり、言語から独立しています。疑似コードでGPTを実装しても、紙上では「動作」するでしょう。構文は単なる手段です。
- **数値精度と効率性**: ここでは、「何を計算するか」が極めて重要です。トークンの確率、損失関数（例：交差エントロピー）、誤差逆伝播法が話題になります。数学を間違えれば、モデルはデタラメを生成します。構文は？　二次的なものです——PythonのNumPyは読みやすさで90%のところまで到達させてくれますが、解釈実行されるため、巨大なモデルの訓練には遅すぎます。

とはいえ、言語選択は実用的なフィルターとして忍び込んできます。C++は、AIの性能がクリティカルな基盤部分（例：TensorFlowのカーネルやPyTorchのCUDAバインディング）で輝きを放ちます。分散訓練では全てのクロックサイクルが重要だからです。Pythonはプロトタイピングで支配的です。それは簡潔だからです——C++のメモリ管理でもがくよりも、`torch.nn.Linear`で1行でレイヤを連結できる方が良いではないですか。しかし、あなたの言う通り、これは「小さな問題」です。真のボトルネックはアルゴリズム的（例：Kaplan et al. のスケーリング則）またはデータに関連するものであって、`def`と書くか`struct`と書くかという問題ではありません。

### PLの洞察が依然として忍び込む場所
とはいえ、プログラミング言語学を完全に否定するのは、赤子を洗澡水と一緒に流し捨てるようなものです。（FriedmanやWangのような人々に触発された）優れた言語設計は、それらの計算を「表現する」ことをより安全かつ迅速にします：
- **推論のための抽象化**: Schemeのような関数型の純粋性は、確率的コードにおけるバグの回避に役立ち、信頼性の高いAI推論にとって非常に重要です。
- **苦痛のないパフォーマンス**: Rustや現代的なC++でさえも、PL研究からアイデアを借りて、速度「と」安全性の両方を提供し、生のCが悩まされる「誤った計算」エラーを減らします。

結局のところ、AIの進歩は、80%が数学/モデル/データ、15%がエンジニアリング（C++のような言語が活躍する場所）、そして5%が構文の詩です。もし我々が後者を最適化したなら、より美しい論文は得られるかもしれませんが、より遅いロボットになるでしょう。あなたはどうお考えですか——神経記号的ものに対するドメイン特化言語のように、PLとAIの融合はさらに進むと思いますか？

[Wang's Blog](https://www.yinwang.org)
[Friedman's Essence of Programming](https://www.cs.indiana.edu/~dfriedman/)
[Attention Is All You Need (Transformers Paper)](https://arxiv.org/abs/1706.03762)
[Scaling Laws for Neural Language Models](https://arxiv.org/abs/2001.08361)