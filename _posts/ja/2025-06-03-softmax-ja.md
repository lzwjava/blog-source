---
audio: false
generated: true
lang: ja
layout: post
title: 深層学習におけるソフトマックス関数
translated: true
type: note
---

### Softmaxとは何か？

**Softmax関数**は、機械学習、特に深層学習において、生のスコア（多くの場合ロジットと呼ばれる）のベクトルを確率分布に変換するために使用される数学的関数です。出力値が正の値で合計が1になることを保証し、確率として解釈できるようにします。

Softmax関数は、ベクトル \\( z = [z_1, z_2, \dots, z_n] \\) に対して次のように定義されます：

\\[
\text{Softmax}(z_i) = \frac{e^{z_i}}{\sum_{j=1}^n e^{z_j}}
\\]

ここで：
- \\( z_i \\): \\( i \\)番目のクラスに対する入力スコア（ロジット）
- \\( e^{z_i} \\): 入力スコアの指数関数。正の値を保証します
- \\( \sum_{j=1}^n e^{z_j} \\): すべての入力スコアの指数関数の和。正規化に使用されます
- 出力 \\( \text{Softmax}(z_i) \\) は、\\( i \\)番目のクラスの確率を表します

主な特性：
- **出力範囲**：各出力値は0から1の間です
- **合計が1**：すべての出力値の合計は1になり、有効な確率分布となります
- **差の増幅**：Softmax内の指数関数は、より大きな入力値を強調し、大きなロジットに対して出力確率をより決定的にします

### 深層学習におけるSoftmaxの適用方法

Softmax関数は、**多クラス分類**タスクにおけるニューラルネットワークの**出力層**で一般的に使用されます。その適用方法は以下の通りです：

1. **ニューラルネットワークにおける文脈**：
   - ニューラルネットワークでは、最終層が各クラスに対する生のスコア（ロジット）を生成します。例えば、3クラスの分類問題（猫、犬、鳥など）では、ネットワークは \\([2.0, 1.0, 0.5]\\) のようなロジットを出力するかもしれません
   - これらのロジットは、負の値になる可能性があり、境界がなく、合計が1にならないため、確率として直接解釈することはできません

2. **Softmaxの役割**：
   - Softmax関数はこれらのロジットを確率に変換します。上記の例では：
     \\[
     \text{Softmax}([2.0, 1.0, 0.5]) = \left[ \frac{e^{2.0}}{e^{2.0} + e^{1.0} + e^{0.5}}, \frac{e^{1.0}}{e^{2.0} + e^{1.0} + e^{0.5}}, \frac{e^{0.5}}{e^{2.0} + e^{1.0} + e^{0.5}} \right]
     \\]
     これは \\([0.665, 0.245, 0.090]\\) のような確率をもたらし、クラス1（猫）が66.5%、クラス2（犬）が24.5%、クラス3（鳥）が9.0%の確率であることを示します

3. **応用例**：
   - **多クラス分類**：Softmaxは、画像分類（画像内の物体識別など）、自然言語処理（複数のカテゴリを用いた感情分析など）、または入力が複数のクラスのいずれかに割り当てられる必要がある問題で使用されます
   - **損失計算**：Softmaxは通常、**交差エントロピー損失**関数と組み合わせて使用され、予測された確率分布と真の分布（one-hotエンコードされたラベル）の差を測定します。この損失がニューラルネットワークの学習を導きます
   - **意思決定**：出力確率は、最も可能性の高いクラスを選択するために使用できます（例えば、最も高い確率のクラスを選択することによって）

4. **深層学習における例**：
   - **画像分類**：ResNetのような畳み込みニューラルネットワーク（CNN）では、最終的な全結合層が各クラス（例えば、ImageNetの1000クラス）に対するロジットを生成します。Softmaxはこれらを確率に変換して、画像内の物体を予測します
   - **自然言語処理**：Transformer（BERTなど）のようなモデルでは、Softmaxはテキスト分類や次の単語予測などのタスクにおいて、語彙やクラスの集合に対する確率が必要とされる出力層で使用されます
   - **強化学習**：Softmaxは、方策ベースの手法において行動スコアを行動選択の確率に変換するために使用できます

5. **フレームワークにおける実装**：
   - **PyTorch**や**TensorFlow**のようなフレームワークでは、Softmaxは組み込み関数として実装されています：
     - PyTorch: `torch.nn.Softmax(dim=1)` または `torch.nn.functional.softmax()`
     - TensorFlow: `tf.nn.softmax()`
   - 多くのフレームワークでは、数値的安定性のために、Softmaxと交差エントロピー損失を単一の操作（PyTorchの `torch.nn.CrossEntropyLoss` など）に組み合わせています。これは、Softmaxを個別に計算すると、大きなロジットでオーバーフローなどの問題が発生する可能性があるためです

### 実用的な考慮事項
- **数値的安定性**：Softmaxの直接計算は、指数関数によるオーバーフローを引き起こす可能性があります。一般的な工夫は、Softmaxを適用する前にすべてのロジットから最大ロジット値を引くこと（\\( z_i - \max(z) \\)）です。これは出力を変更しませんが、大きな指数値を防ぎます
- **Softmax対Sigmoid**：**二値分類**では、Sigmoid関数がSoftmaxの代わりに使用されることが多いです。これは、2つのクラスをより効率的に処理するためです。SoftmaxはSigmoidを複数のクラスに一般化したものです
- **制限事項**：
  - Softmaxは相互排他性（1つの正しいクラス）を仮定しています。多ラベル分類（複数のクラスが真になり得る場合）では、Sigmoidが好まれます
  - Softmaxは、指数関数によってロジットの小さな差を増幅する可能性があるため、予測において過度に自信を持つことがあります

### 計算例
ニューラルネットワークが3クラス問題に対してロジット \\([1.5, 0.8, -0.2]\\) を出力すると仮定します：
1. 指数を計算：\\( e^{1.5} \approx 4.482, e^{0.8} \approx 2.225, e^{-0.2} \approx 0.819 \\)
2. 指数の和を計算：\\( 4.482 + 2.225 + 0.819 = 7.526 \\)
3. 確率を計算：
   - クラス1：\\( \frac{4.482}{7.526} \approx 0.596 \\)
   - クラス2：\\( \frac{2.225}{7.526} \approx 0.296 \\)
   - クラス3：\\( \frac{0.819}{7.526} \approx 0.109 \\)
4. 出力：\\([0.596, 0.296, 0.109]\\) - 有効な確率分布

### Softmaxの可視化
Softmaxがロジットを確率に変換する方法を説明するために、ロジットとそれに対応するSoftmax確率を示す以下のチャートを考えてみましょう：

```chartjs
{
  "type": "bar",
  "data": {
    "labels": ["Class 1", "Class 2", "Class 3"],
    "datasets": [
      {
        "label": "Logits",
        "data": [1.5, 0.8, -0.2],
        "backgroundColor": "rgba(75, 192, 192, 0.6)",
        "borderColor": "rgba(75, 192, 192, 1)",
        "borderWidth": 1
      },
      {
        "label": "Softmax Probabilities",
        "data": [0.596, 0.296, 0.109],
        "backgroundColor": "rgba(255, 99, 132, 0.6)",
        "borderColor": "rgba(255, 99, 132, 1)",
        "borderWidth": 1
      }
    ]
  },
  "options": {
    "scales": {
      "y": {
        "beginAtZero": true,
        "title": {
          "display": true,
          "text": "Value"
        }
      },
      "x": {
        "title": {
          "display": true,
          "text": "Classes"
        }
      }
    },
    "plugins": {
      "legend": {
        "display": true
      },
      "title": {
        "display": true,
        "text": "Logits vs. Softmax Probabilities"
      }
    }
  }
}
```

このチャートは、生のロジットとSoftmax適用後の確率を比較し、Softmaxが値を確率分布に正規化する方法を強調しています

### まとめ
Softmax関数は、深層学習における多クラス分類の基盤であり、生のスコアを確率分布に変換します。画像やテキスト分類などのタスクにおいて、ニューラルネットワークの出力層で広く使用され、モデルが複数のクラスに確率を割り当て、交差エントロピーのような損失関数を用いた学習を可能にします。その指数的性質はロジットの差に対して敏感であり、注意深い実装が数値的安定性を保証します