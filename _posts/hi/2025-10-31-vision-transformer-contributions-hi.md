---
audio: false
generated: true
image: false
lang: hi
layout: post
model: x-ai/grok-4-fast
title: विज़न ट्रांसफॉर्मर के लेखक और योगदान
translated: true
type: note
---

### "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale" के लेखकों का परिचय

2020 में Google के एलेक्सी डोसोवित्सकी और उनके सहयोगियों द्वारा प्रस्तुत अभूतपूर्व शोध पत्र "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale" ने कंप्यूटर विजन में एक निर्णायक मोड़ ला दिया। इसमें विजन ट्रांसफॉर्मर (ViT) मॉडल प्रस्तावित किया गया, जो ट्रांसफॉर्मर आर्किटेक्चर—जो मूल रूप से नेचुरल लैंग्वेज प्रोसेसिंग के लिए डिजाइन किए गए थे—को सीधे इमेज पैच पर लागू करता है। बड़े पैमाने के डेटा (जैसे JFT-300M) पर प्री-ट्रेनिंग के बाद बड़े डेटासेट जैसे ImageNet पर इसने अत्याधुनिक प्रदर्शन हासिल किया। इस कार्य ने प्रदर्शित किया कि पर्याप्त कम्प्यूटेशनल शक्ति और डेटा की स्थिति में, शुद्ध ट्रांसफॉर्मर दक्षता और सटीकता में कन्व्होल्यूशनल न्यूरल नेटवर्क्स (CNNs) को पछाड़ सकते हैं, जिसने मल्टीमॉडल AI और स्केलेबल विजन मॉडल में बाद की प्रगति को प्रभावित किया।

यह पेपर 12 शोधकर्ताओं का एक सहयोगात्मक प्रयास था, मुख्यतः Google Brain की ज्यूरिक टीम से, जिसमें डीप लर्निंग, सीक्वेंस मॉडलिंग और बड़े पैमाने पर ट्रेनिंग में विशेषज्ञता का मेल था। नीचे प्रमुख लेखकों का एक सिंहावलोकन दिया गया है, जिसमें उनकी पृष्ठभूमि और इस क्षेत्र में उनके योगदान को उजागर किया गया है। (संक्षिप्तता के लिए, मैंने प्रमुख योगदानकर्ताओं पर ध्यान केंद्रित किया है; पूरी सूची में डिर्क वीसेनबोर्न, थॉमस उंटरथिनर, मोस्तफा देहघानी, मथियास मिंडरर, जॉर्ज हीगोल्ड, सिल्वेन गेली, और जैकब उस्ज़कोरेइट शामिल हैं—ये सभी Google के पूर्व सदस्य हैं जिनकी ट्रांसफॉर्मर्स, ऑप्टिमाइज़ेशन और विजन-लैंग्वेज इंटीग्रेशन में गहरी पैठ है।)

#### प्रमुख लेखक और पृष्ठभूमि

- **एलेक्सी डोसोवित्सकी** (मुख्य लेखक): ViT के पीछे की प्रेरक शक्ति के रूप में, डोसोवित्सकी ने इमेज को पैच के अनुक्रम के रूप में देखने का मूल विचार विकसित किया। उनके पास लोमोनोसोव मॉस्को स्टेट यूनिवर्सिटी से गणित में एमएससी और पीएचडी है, जिसके बाद उन्होंने यूनिवर्सिटी ऑफ फ्रीबर्ग में अनसुपरवाइज्ड फीचर लर्निंग पर पोस्टडॉक्टरल शोध किया। 2019 में Google Brain में शामिल होकर, 2021 में इन्सेप्टिव (बर्लिन स्थित एक AI फर्म) में जाने से पहले उन्होंने ViT के विकास का नेतृत्व किया। उनका कार्य कंप्यूटर विजन, जेनरेटिव मॉडल और बायोलॉजी-इंस्पायर्ड ML तक फैला है, जिसके 136,000 से अधिक साइटेशन हैं।

- **लुकास बेयर**: बेयर ने ViT के व्यावहारिक कार्यान्वयन, बेंचमार्क पर मूल्यांकन और दक्षता अनुकूलन में महत्वपूर्ण भूमिका निभाई। बेल्जियम मूल के, उन्होंने RWTH आखेन यूनिवर्सिटी से मैकेनिकल इंजीनियरिंग की पढ़ाई की, और 2018 में गेम AI और रीइन्फोर्समेंट लर्निंग पर ध्यान केंद्रित करते हुए रोबोटिक्स और AI में पीएचडी प्राप्त की। पीएचडी के बाद वे Google Brain ज्यूरिक में शामिल हुए, और Google DeepMind में स्टाफ रिसर्च साइंटिस्ट बने। 2025 में, वे Meta के शीर्ष AI भर्तियों में से एक बने, और विजन ट्रांसफॉर्मर्स और डेटा-केंद्रित ML पर काम जारी रखा।

- **अलेक्जेंडर कोलेसनिकोव**: कोलेसनिकोव ने ViT के स्केलिंग प्रयोगों और ट्रांसफर लर्निंग की अंतर्दृष्टि में योगदान दिया, मध्यम आकार के डेटासेट पर इसके प्रदर्शन पर जोर दिया। उन्होंने मॉस्को स्टेट यूनिवर्सिटी से गणित में मास्टर्स और इंस्टीट्यूट ऑफ साइंस एंड टेक्नोलॉजी ऑस्ट्रिया (ISTA) से 2018 में मशीन लर्निंग/कंप्यूटर विजन में पीएचडी की उपाधि प्राप्त की। 2018 में Google Brain में शुरुआत करके, वे DeepMind में स्टाफ भूमिकाओं में आगे बढ़े, इससे पहले कि वे OpenAI और फिर 2025 में Meta में शामिल हों—जहाँ उन्हें कुशल विजन मॉडल में उनकी विशेषज्ञता के लिए लिया गया था।

- **झाई ज़ियाओहुआ**: झाई ने ViT की प्री-ट्रेनिंग रणनीतियों और मल्टीमॉडल एक्सटेंशन पर ध्यान केंद्रित किया, जो रिप्रेजेंटेशन लर्निंग में उनके काम से प्रेरित था। उनके पास पेकिंग यूनिवर्सिटी से इलेक्ट्रॉनिक्स इंजीनियरिंग में पीएचडी है और वे 2015 में Google में सॉफ्टवेयर इंजीनियर के रूप में शामिल हुए, 2017 में Google Brain और 2023 में DeepMind में रिसर्च में बदलाव किया। अब Meta में एक शोधकर्ता (2025 में OpenAI ज्यूरिक के माध्यम से), उनके योगदान विजन, भाषा और सेल्फ-सुपरवाइज्ड लर्निंग के बीच सेतु बनाते हैं, जिनके 100,000 से अधिक साइटेशन हैं।

- **नील होल्स्बी** (वरिष्ठ लेखक): एक टीम लीड के रूप में, होल्स्बी ने ViT की आर्किटेक्चरल डिजाइन और विजन में स्केलिंग नियमों के व्यापक निहितार्थों की देखरेख की। उन्होंने लगभग 2010 में एक Google यूरोपीय डॉक्टरल फेलोशिप प्राप्त की और मशीन लर्निंग में अपनी पीएचडी पूरी की। अपने इंटर्नशिप के दिनों से ही एक लंबे समय से Google शोधकर्ता, उन्होंने Google Brain और DeepMind में न्यूरल आर्किटेक्चर और विजन-लैंग्वेज मॉडल पर टीमों का प्रबंधन किया। 2025 में, वे Anthropic में उनके नए ज्यूरिक कार्यालय का नेतृत्व करने के लिए शामिल हुए, सुरक्षित AI स्केलिंग पर ध्यान केंद्रित करते हुए।

इस Google Brain सहयोग (ज्यादातर ज्यूरिक-आधारित) ने बड़े पैमाने के प्रयोगों—25,000 से अधिक TPU-दिन—के लिए टीम की TPU तक पहुंच का लाभ उठाया, जिसने टेक्स्ट से परे ट्रांसफॉर्मर्स की व्यवहार्यता साबित की। कई लेखक तब से Meta, OpenAI, और Anthropic जैसी प्रमुख AI लैब्स में चले गए हैं, जो इस क्षेत्र पर ViT के स्थायी प्रभाव को दर्शाता है।

#### संदर्भ
- [An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale (arXiv)](https://arxiv.org/abs/2010.11929)
- [Alexey Dosovitskiy Google Scholar Profile](https://scholar.google.com/citations?user=FXNJRDoAAAAJ)
- [Lucas Beyer Google Scholar Profile](https://scholar.google.com/citations?user=p2gwhK4AAAAJ)
- [Alexander Kolesnikov Google Scholar Profile](https://scholar.google.com/citations?user=H9I0CVwAAAAJ)
- [Xiaohua Zhai Google Scholar Profile](https://scholar.google.com/citations?user=8gruapYAAAAJ)
- [Neil Houlsby Google Scholar Profile](https://scholar.google.com/citations?user=sm1-TZMAAAAJ)
- [From Ph.D. Detour to Meta's Top AI Hire: Lucas Beyer (Times of India)](https://timesofindia.indiatimes.com/education/news/lucas-beyer-educational-qualification-from-ph-d-detour-and-google-rejection-to-becoming-metas-top-ai-hire/articleshow/122158414.cms)
- [Zuckerberg Poaches Alexander Kolesnikov from OpenAI (Times of India)](https://timesofindia.indiatimes.com/education/news/zuckerberg-poached-alexander-kolesnikov-from-sam-altmans-openai-where-did-this-ai-genius-study/articleshow/122158654.cms)
- [Anthropic Poaches Neil Houlsby from Google DeepMind (Euronews)](https://www.euronews.com/next/2025/02/13/anthropic-poaches-google-deepmind-talent-to-lead-new-european-office)