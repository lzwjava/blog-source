---
audio: false
generated: true
image: false
lang: hi
layout: post
model: x-ai/grok-4-fast
title: बीईआरटी ने प्राकृतिक भाषा प्रसंस्करण में क्रांति ला दी
translated: true
type: note
---

### BERT क्या है?
BERT (बायडायरेक्शनल एनकोडर रिप्रेजेंटेशन्स फ्रॉम ट्रांसफॉर्मर्स) एक ट्रांसफॉर्मर-आधारित मॉडल है जिसे Google ने 2018 में पेश किया था। इसने नेचुरल लैंग्वेज प्रोसेसिंग (NLP) में क्रांति ला दी, टेक्स्ट की गहरी द्वि-दिशात्मक समझ को सक्षम करके, जिससे प्रश्नोत्तर, सेंटीमेंट विश्लेषण और नामित इकाई पहचान जैसे विभिन्न कार्यों में अत्याधुनिक प्रदर्शन प्राप्त हुआ।

### मुख्य नवाचार
BERT की सफलता इसकी प्री-ट्रेनिंग रणनीति, आर्किटेक्चर और फाइन-ट्यूनिंग दृष्टिकोण में निहित है। यहां एक विवरण दिया गया है:

- **द्वि-दिशात्मक प्री-ट्रेनिंग**:
  GPT (बाएं-से-दाएं) या ELMo (उथला द्वि-दिशात्मक) जैसे पहले के मॉडलों के विपरीत, BERT टेक्स्ट को दोनों दिशाओं में एक साथ पढ़ता है। यह प्रत्येक शब्द के आसपास के पूर्ण संदर्भ पर विचार करके, न कि केवल एक दिशा पर, समृद्ध संदर्भात्मक प्रस्तुतिकरण को कैप्चर करने की अनुमति देता है।

- **मास्क्ड लैंग्वेज मॉडलिंग (MLM)**:
  प्री-ट्रेनिंग के दौरान, BERT इनपुट में 15% शब्दों को यादृच्छिक रूप से मास्क कर देता है और मॉडल को आसपास के संदर्भ के आधार पर उनकी भविष्यवाणी करने के लिए प्रशिक्षित करता है। यह "रिक्त स्थान भरें" कार्य मॉडल को अनुक्रमिक जनन पर निर्भर हुए बिना सूक्ष्म शब्द संबंधों और व्याकरण को सीखने के लिए प्रोत्साहित करता है।

- **अगला वाक्य पूर्वानुमान (NSP)**:
  वाक्य-स्तरीय समझ को संभालने के लिए, BERT वाक्यों के जोड़े पर प्री-ट्रेन करता है: 50% लगातार (सकारात्मक) और 50% यादृच्छिक (नकारात्मक) होते हैं। मॉडल यह भविष्यवाणी करना सीखता है कि क्या दूसरा वाक्य तार्किक रूप से पहले वाक्य का अनुसरण करता है, जिससे सारांशीकरण या वार्तालाप जैसे कार्यों के लिए सुसंगतता पहचान में सुधार होता है।

- **ट्रांसफॉर्मर एनकोडर-ओनली आर्किटेक्चर**:
  BERT मूल ट्रांसफॉर्मर (कोई डिकोडर नहीं) से केवल एनकोडर स्टैक का उपयोग करता है, जो वेरिएंट (BERT-base या BERT-large) के आधार पर 12 या 24 परतों में स्टैक किया गया है। यह इसे एनकोडिंग कार्यों के लिए कुशल बनाता है और लंबी दूरी की निर्भरताओं के समानांतर प्रसंस्करण के लिए सेल्फ-अटेंशन मैकेनिज्म का लाभ उठाता है।

- **फाइन-ट्यूनिंग के माध्यम से ट्रांसफर लर्निंग**:
  बड़े पैमाने पर अनलेबल कॉर्पोरा (जैसे BooksCorpus और English Wikipedia) पर प्री-ट्रेनिंग के बाद, BERT को एक सरल टास्क-स्पेसिफिक लेयर जोड़कर लेबल डाउनस्ट्रीम टास्क पर फाइन-ट्यून किया जाता है। यह "वन-साइज-फिट्स-मोस्ट" दृष्टिकोण टास्क-स्पेसिफिक ट्रेनिंग की शुरुआत से जरूरत को काफी कम कर देता है, जिससे यह बहुमुखी और डेटा-कुशल बन जाता है।

इन नवाचारों ने NLP को टास्क-स्पेसिफिक मॉडल से जनरल-पर्पस प्री-ट्रेंड एम्बेडिंग्स की ओर स्थानांतरित कर दिया, जिससे RoBERTa और T5 जैसे मॉडलों का रास्ता साफ हुआ। BERT का प्रभाव सर्च इंजन से लेकर चैटबॉट्स तक हर चीज में इसके अपनाने में स्पष्ट है।

[BERT पेपर](https://arxiv.org/abs/1810.04805)
[BERT GitHub रिपॉजिटरी](https://github.com/google-research/bert)