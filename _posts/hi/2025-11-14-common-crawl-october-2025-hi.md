---
audio: false
generated: true
image: false
lang: hi
layout: post
model: x-ai/grok-4-fast
title: कॉमन क्रॉल अक्टूबर 2025 डेटासेट सारांश
translated: true
type: note
---

यह तालिका **Common Crawl October 2025 क्रॉल** का आधिकारिक सारांश है, विशेष रूप से **CC-MAIN-2025-43** नामक डेटासेट। Common Crawl एक गैर-लाभकारी प्रोजेक्ट है जो हर महीने या दो महीने में पूरे पब्लिक वेब को क्रॉल करता है और कच्चा डेटा मुफ्त में रिलीज़ करता है ताकि शोधकर्ता, कंपनियाँ और कोई भी इसे उपयोग में ला सकें (उदाहरण के लिए, GPT, LLaMA, Grok आदि जैसे बड़े भाषा मॉडल को ट्रेन करने के लिए)।

यहाँ तालिका के प्रत्येक भाग का अर्थ है:

### मूल तथ्य
- **नाम**: CC-MAIN-2025-43 → 2025 में जारी 43वां साप्ताहिक क्रॉल, जो मोटे तौर पर अक्टूबर 2025 में किया गया।
- **आकार**: 2.61 बिलियन व्यक्तिगत वेब पेज कैप्चर किए गए।

### विभिन्न डेटा प्रकार और उनमें क्या शामिल है

| डेटा प्रकार           | यह क्या है                                                                                  | कुल अनकम्प्रेस्ड आकार | फ़ाइलों की संख्या | कम्प्रेस्ड आकार |
|-----------------------|----------------------------------------------------------------------------------------------|------------------------|-------------------|-----------------|
| **WARC**              | कच्चा, पूर्ण क्रॉल डेटा (पूर्ण HTTP प्रतिक्रियाएँ: हेडर + HTML + एम्बेडेड संसाधन)          | ~ सैकड़ों TiB          | 100,000           | 97.73 TiB       |
| **WAT**               | WARC फ़ाइलों से निकाला गया मेटाडेटा (जैसे, आउटगोइंग लिंक, भाषा, कंटेंट-लेंथ, आदि) JSON फॉर्मेट में |                        | 100,000           | 18.39 TiB       |
| **WET**               | केवल निकाला गया सादा टेक्स्ट (कोई HTML टैग नहीं, कोई बॉयलरप्लेट नहीं, केवल साफ़ किया गया टेक्स्ट) |                        | 100,000           | 7.38 TiB        |
| **Robots.txt फ़ाइलें** | सभी robots.txt फ़ाइलें जो क्रॉल के दौरान फ़ेच की गई थीं                                      |                        | 100,000           | 0.15 TiB        |
| **Non-200 प्रतिक्रियाएँ** | वे प्रतिक्रियाएँ जो सफल नहीं थीं (404s, 500s, रीडायरेक्ट्स, आदि)                            |                        | 100,000           | 3.07 TiB        |
| **URL इंडेक्स फ़ाइलें** | इंडेक्स जो आपको देखने देता है कि किस WARC फ़ाइल में एक विशिष्ट URL है (पुराना फॉर्मेट)      |                        | 302               | 0.20 TiB        |
| **Columnar URL इंडेक्स** | नया, तेज़ columnar इंडेक्स (Parquet फॉर्मेट) बिग-डेटा टूल्स जैसे Spark, DuckDB, आदि के लिए |                        | 900               | 0.23 TiB        |

### विभिन्न फॉर्मेट क्यों मौजूद हैं
- यदि आप केवल एक भाषा मॉडल को ट्रेन करना चाहते हैं → आप आमतौर पर **WET** फ़ाइलें डाउनलोड करते हैं (पहले से साफ़ किया गया टेक्स्ट, सबसे छोटा आकार)।
- यदि आपको पूरा HTML, इमेज, CSS, आदि चाहिए → आप **WARC** फ़ाइलें डाउनलोड करते हैं (सबसे बड़ा)।
- यदि आपको मेटाडेटा (लिंक, पहचानी गई भाषा, आदि) चाहिए बिना पूरे HTML के → **WAT**।
- इंडेक्स आपको सभी 100,000 फ़ाइलों को स्कैन किए बिना पेज ढूंढने देते हैं।

### फ़ाइलें कैसे व्यवस्थित हैं
सब कुछ 100,000 **सेगमेंट** में विभाजित है (WARC के लिए मोटे तौर पर 1 TiB कम्प्रेस्ड प्रत्येक)। प्रत्येक सेगमेंट में कई छोटी .warc.gz (या .wat.gz, .wet.gz) फ़ाइलें होती हैं। जो *.paths.gz फ़ाइलें आप सूचीबद्ध देखते हैं, वे केवल टेक्स्ट फ़ाइलें हैं जिनमें प्रत्येक प्रकार के लिए S3 URLs की पूरी सूची होती है।

आप सब कुछ AWS S3 पर मुफ्त में एक्सेस कर सकते हैं:
`s3://commoncrawl/crawl-data/CC-MAIN-2025-43/`
(या HTTPS के माध्यम से: https://data.commoncrawl.org/)

संक्षेप में: यह दुनिया का सबसे बड़ा ओपन वेब डेटासेट में से एक है (अक्टूबर 2025 के 2.61 बिलियन पेज), जो विभिन्न प्रोसेस्ड रूपों में स्वतंत्र रूप से उपलब्ध है, और यह तालिका आपको बस यह बता रही है कि प्रत्येक प्रोसेस्ड वर्जन कितना बड़ा है और फ़ाइल सूचियाँ कहाँ मिलेंगी।