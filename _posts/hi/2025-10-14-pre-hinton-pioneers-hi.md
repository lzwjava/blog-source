---
audio: false
generated: true
image: false
lang: hi
layout: post
model: x-ai/grok-4-fast
title: हिंटन से पहले डीप लर्निंग के अग्रदूत
translated: true
type: note
---

### एलेक्सी इवाखनेन्को का कार्य

यूक्रेनी-सोवियत साइबरनेटिसिस्ट एलेक्सी ग्रिगोरिएविच इवाखनेन्को को 1960 और 1970 के दशक में उनके अग्रणी प्रयासों के लिए अक्सर "डीप लर्निंग का जनक" कहा जाता है। शीत युद्ध के दौर में संसाधनों की कमी के बीच काम करते हुए—जब कंप्यूटिंग शक्ति आज की तुलना में लाखों गुना सीमित थी—उन्होंने मल्टीलेयर न्यूरल नेटवर्क्स पर ध्यान केंद्रित किया जो डेटा के पदानुक्रमित प्रतिनिधित्व (hierarchical representations) को स्वचालित रूप से सीख सकते थे।

- **1965: डेटा हैंडलिंग की समूह विधि (GMDH)**: वेलेंटिन लापा के साथ, इवाखनेन्को ने सुपरवाइज्ड डीप फीडफॉरवर्ड मल्टीलेयर परसेप्ट्रॉन (MLPs) के लिए पहला सामान्य, कार्यशील लर्निंग एल्गोरिदम प्रकाशित किया। इस विधि ने इनपुट-आउटपुट डेटा जोड़े पर रिग्रेशन विश्लेषण का उपयोग करके नेटवर्क को लेयर दर लेयर प्रशिक्षित किया। इसने परतों को क्रमिक रूप से बढ़ाया, उन्हें क्रमिक रूप से प्रशिक्षित किया, और वैलिडेशन सेट के माध्यम से अनावश्यक हिडन यूनिट्स की प्रूनिंग (छँटाई) शामिल की। महत्वपूर्ण रूप से, इसने नेटवर्क को इनपुट डेटा के वितरित, आंतरिक प्रतिनिधित्व (distributed, internal representations) सीखने में सक्षम बनाया—जो आधुनिक डीप लर्निंग में एक मूल विचार है—बिना मैन्युअल फीचर इंजीनियरिंग के। यह पश्चिमी AI में समान अवधारणाओं से दशकों पहले की थी और इसे पैटर्न मान्यता और पूर्वानुमान जैसी वास्तविक दुनिया की समस्याओं पर लागू किया गया था।

- **1971: डीप नेटवर्क कार्यान्वयन**: इवाखनेन्को ने GMDH सिद्धांतों का उपयोग करके एक 8-लेयर डीप न्यूरल नेटवर्क प्रदर्शित किया, जिसमें जटिल कार्यों के लिए स्केलेबल गहराई दिखाई गई। उनके दृष्टिकोण ने डीप नेटवर्क को बहुपदीय सन्निकटन (polynomial approximation) के एक रूप के रूप में माना, जिससे स्वचालित मॉडल चयन की अनुमति मिली और उच्च-परत वास्तुकला में "अभिशप्तता विमिता" (curse of dimensionality) से बचा जा सका।

इवाखनेन्को का GMDH एक व्यापक आगमनात्मक मॉडलिंग (inductive modeling) ढांचे में विकसित हुआ, जिसने नियंत्रण प्रणालियों और अर्थशास्त्र जैसे क्षेत्रों को प्रभावित किया। इसके प्रभाव के बावजूद, उनका अधिकांश कार्य रूसी में प्रकाशित हुआ था और अंग्रेजी-भाषी AI हलकों में अनदेखा किया गया।

### शुन-इची अमारी का कार्य

जापानी गणितज्ञ और न्यूरोसाइंटिस्ट शुन-इची अमारी ने 1960 और 1970 के दशक में न्यूरल नेटवर्क सिद्धांत में मौलिक योगदान दिए, जिसमें सूचना प्रसंस्करण पर अनुकूली शिक्षण (adaptive learning) और ज्यामितीय दृष्टिकोण पर जोर दिया गया। उनके शोध ने न्यूरोसाइंस और कम्प्यूटेशन के बीच सेतु बनाया, जिसने स्व-संगठित प्रणालियों (self-organizing systems) की नींव रखी।

- **1967-1968: अनुकूली पैटर्न वर्गीकरण और स्टोकैस्टिक ग्रेडिएंट डिसेंट (SGD)**: अमारी ने एसजीडी का उपयोग करके डीप एमएलपी के एंड-टू-एंड प्रशिक्षण के लिए पहली विधि प्रस्तावित की, जो 1951 की एक ऑप्टिमाइजेशन तकनीक थी लेकिन अब इसे मल्टीलेयर नेटवर्क पर नए सिरे से लागू किया गया। पांच-परत नेटवर्क (दो परिवर्तनशील परतों) के सिमुलेशन में, उनकी प्रणाली ने गैर-रैखिक रूप से अलग करने योग्य पैटर्न को वर्गीकृत करना सीखा, सीधे परतों में वजन को समायोजित करके। इसने ग्रेडिएंट-आधारित अपडेट के माध्यम से आंतरिक प्रतिनिधित्व (internal representations) को उभरने में सक्षम बनाया, जो बैकप्रोपेगेशन जैसी विधियों का एक सीधा अग्रदूत था, और यह सब आधुनिक मानकों की तुलना में अरबों गुना कठोर कम्प्यूटेशनल बाधाओं के तहत हुआ।

- **1972: अनुकूली सहयोगी स्मृति नेटवर्क (Adaptive Associative Memory Networks)**: 1925 के लेंज-आइसिंग मॉडल (एक भौतिकी-आधारित रिकरंट आर्किटेक्चर) पर निर्माण करते हुए, अमारी ने एक अनुकूली संस्करण पेश किया जो सहसंबंधों के आधार पर कनेक्शन वजन को ट्यून करके पैटर्न को संग्रहीत और याद करना सीखता था। इसने अनुक्रम प्रसंस्करण (sequence processing) को संभाला और तंत्रिका गतिकी (neural dynamics) के माध्यम से शोर या आंशिक इनपुट से संग्रहीत पैटर्न को पुनः प्राप्त किया। पहली बार 1969 में जापानी में प्रकाशित, इस कार्य को सहयोगी स्मृति के लिए "हॉपफील्ड नेटवर्क" की उत्पत्ति माना जाता है।

अमारी ने सूचना ज्यामिति (information geometry) की भी स्थापना की, जो सांख्यिकीय मॉडल और तंत्रिका गतिकी का विश्लेषण करने के लिए अवकल ज्यामिति (differential geometry) का उपयोग करने वाला एक क्षेत्र है, और यह आधुनिक संभाव्य न्यूरल नेटवर्क (probabilistic neural networks) का आधार है।

### 2024 नोबेल विवाद का संदर्भ

अपनी 2024 की रिपोर्ट "ए नोबेल प्राइज फॉर प्लेजरिज्म" में, जुरगेन श्मिडहुबर का तर्क है कि हिंटन और हॉपफील्ड के नोबेल पुरस्कार विजेता विचारों—जैसे कि प्रतिनिधित्व सीखने के लिए बोल्ट्जमैन मशीन (1985) और सहयोगी स्मृति के लिए हॉपफील्ड नेटवर्क (1982)—ने इवाखनेन्को के लेयर-वाइज डीप लर्निंग और अमारी के एसजीडी/अनुकूली रिकरंट मॉडल्स को बिना श्रेय दोहराया (repackaged)। उदाहरण के लिए, बोल्ट्जमैन मशीन ने इवाखनेन्को के 1965 के आंतरिक प्रतिनिधित्व सीखने और अमारी के 1967 के एसजीडी के उद्धरण छोड़ दिए, जबकि हॉपफील्ड के नेटवर्क ने अमारी के 1972 के अनुकूली आइसिंग मॉडल को नजरअंदाज कर दिया। श्मिडहुबर का दावा है कि यह चूक बाद के सर्वेक्षणों और नोबेल के वैज्ञानिक पृष्ठभूमि दस्तावेज में बनी रही, जो AI के इतिहास में एक पश्चिम-केंद्रित पूर्वाग्रह को उजागर करती है।

[Alexey Ivakhnenko - Wikipedia](https://en.wikipedia.org/wiki/Alexey_Ivakhnenko)  
[Deep Learning in a Nutshell: History and Training - NVIDIA Developer](https://developer.nvidia.com/blog/deep-learning-nutshell-history-training/)  
[Shun-ichi Amari | Kyoto Prize](https://www.kyotoprize.org/en/laureates/shun-ichi_amari/)  
[Japanese scientists were pioneers of AI, yet they're being written out of history](https://modernsciences.org/japanese-ai-pioneers-history-shuninchi-amari-kunihiko-fukushima-december-2024/)  
[A Nobel Prize for Plagiarism - Technical Report IDSIA-24-24](https://people.idsia.ch/~juergen/physics-nobel-2024-plagiarism.pdf)  
[Machine Learning Stirs Controversy in Nobel Prize in Physics - Harvard Technology Review](https://harvardtechnologyreview.com/2024/11/18/machine-learning-stirs-controversy-in-nobel-prize-in-physics/)