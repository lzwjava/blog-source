---
audio: false
generated: true
image: false
lang: hi
layout: post
model: x-ai/grok-4-fast
title: बड़े भाषा मॉडल में सिग्मॉइड से क्यों बचा जाता है
translated: true
type: note
---

### सिग्मॉइड का उपयोग जीपीटी/एलएलएम मॉडल्स में क्यों नहीं होता

बड़े भाषा मॉडल (एलएलएम) जैसे जीपीटी ट्रांसफॉर्मर आर्किटेक्चर पर निर्भर करते हैं, जहां एक्टिवेशन फंक्शन फीड-फॉरवर्ड (एफएफएन) लेयर्स में नॉन-लीनियरिटी लाने और अटेंशन आउटपुट को प्रोसेस करने में अहम भूमिका निभाते हैं। हालांकि सिग्मॉइड फंक्शन—जिसे \\(\sigma(x) = \frac{1}{1 + e^{-x}}\\) के रूप में परिभाषित किया जाता है और यह इनपुट को (0, 1) की रेंज में समेटता है—शुरुआती न्यूरल नेटवर्क्स (जैसे बाइनरी क्लासिफिकेशन के लिए) में लोकप्रिय था, लेकिन आधुनिक एलएलएम में इसे आंतरिक एक्टिवेशन्स के लिए क largely हटा दिया गया है। यहां प्रमुख समस्याओं के आधार पर कारण दिए गए हैं:

#### 1. **वैनिशिंग ग्रेडिएंट्स समस्या**
   - सिग्मॉइड चरम सीमाओं पर संतृप्त हो जाता है: बड़े सकारात्मक \\(x\\) के लिए, \\(\sigma(x) \approx 1\\); बड़े नकारात्मक \\(x\\) के लिए, \\(\sigma(x) \approx 0\\)।
   - इसका डेरिवेटिव \\(\sigma'(x) = \sigma(x)(1 - \sigma(x))\\) होता है, जो इन क्षेत्रों में 0 के करीब पहुंच जाता है। बैकप्रोपगेशन के दौरान, इसके कारण ग्रेडिएंट "वैनिश" (बहुत छोटे) हो जाते हैं, जिससे डीप लेयर्स में सीखने की प्रक्रिया रुक जाती है।
   - एलएलएम में ट्रांसफॉर्मर बेहद गहरे होते हैं (उदाहरण के लिए, जीपीटी-4 में 100+ लेयर्स हैं), इसलिए यह ट्रेनिंग दक्षता को बाधित करता है। विकल्प जैसे रीलू (\\(f(x) = \max(0, x)\\)) या जीलू (जिस पर हमने पहले चर्चा की थी) नकारात्मक इनपुट के लिए पूर्ण संतृप्ति से बचते हैं, जिससे बेहतर ग्रेडिएंट फ्लो होता है।

#### 2. **नॉन-जीरो-सेंटर्ड आउटपुट्स**
   - सिग्मॉइड हमेशा सकारात्मक मान (0 से 1) आउटपुट करता है, जो ऑप्टिमाइजेशन के दौरान वेट अपडेट में पूर्वाग्रह पैदा करता है। इसके कारण ग्रेडिएंट डिसेंट के पथ "ज़िग-ज़ैग" होते हैं, जिससे शून्य-केंद्रित फंक्शन जैसे टैनह या जीलू की तुलना में अभिसरण धीमा हो जाता है।
   - ट्रांसफॉर्मर्स में, एफएफएन लेयर्स उच्च-आयामी एम्बेडिंग्स को प्रोसेस करती हैं, और शून्य-केंद्रित एक्टिवेशन रेजिडुअल कनेक्शन्स में स्थिर सिग्नल प्रसारण बनाए रखने में मदद करते हैं।

#### 3. **अनुभवजन्य अंडरपरफॉर्मेंस**
   - व्यापक प्रयोग दिखाते हैं कि एनएलपी टास्क्स में सिग्मॉइड आधुनिक एक्टिवेशन से पीछे रह जाता है। शुरुआती ट्रांसफॉर्मर (जैसे मूल जीपीटी) ने गति और सरलता के लिए रीलू का उपयोग किया। बाद के मॉडल जैसे जीपीटी-2/3 ने इसके स्मूद, प्रोबेबिलिस्टिक व्यवहार के कारण जीलू की ओर रुख किया, जो अनुभवजन्य रूप से सिग्मॉइड की कमियों के बिना सटीकता और अभिसरण को बढ़ाता है।
   - वेरिएंट जैसे सीलू (सिग्मॉइड लीनियर यूनिट, \\(f(x) = x \cdot \sigma(x)\\)) कुछ एलएलएम (जैसे इंस्ट्रक्टजीपीटी) में दिखते हैं, लेकिन यह सादा सिग्मॉइड नहीं है—इसे शून्य-केंद्रण को ठीक करने और संतृप्ति को कम करने के लिए \\(x\\) से गुणा किया जाता है।

#### 4. **आउटपुट लेयर विचार**
   - एलएलएम में अगले टोकन की भविष्यवाणी के लिए, अंतिम लेयर लॉजिट्स पर **सॉफ्टमैक्स** (एक सामान्यीकृत मल्टी-सिग्मॉइड) का उपयोग करती है, सिग्मॉइड का नहीं। सिग्मॉइड बाइनरी निर्णयों के लिए काम कर सकता है, लेकिन मल्टी-क्लास शब्दावली (जैसे 50k+ टोकन्स) के लिए विफल रहता है, क्योंकि आउटपुट 1 तक नहीं जुड़ते, जिससे अस्थिर प्रायिकताएं उत्पन्न होती हैं।
   - सॉफ्टमैक्स एक वैलिड डिस्ट्रीब्यूशन सुनिश्चित करता है, जो क्रॉस-एन्ट्रॉपी लॉस के लिए महत्वपूर्ण है।

#### सिग्मॉइड का अब भी उपयोग कब होता है?
- एलएलएम की हिडन लेयर्स में शायद ही कभी, लेकिन कभी-कभी विशेष स्थानों जैसे बाइनरी गेट्स या पुराने आरएनएन/एलएसटीएम (ट्रांसफॉर्मर-पूर्व युग) में।
- विजन मॉडल्स (जैसे कुछ सीएनएन) में, इसका उपयोग कभी-कभी पिक्सेल नॉर्मलाइजेशन के लिए होता है, लेकिन टेक्स्ट-केंद्रित एलएलएम में नहीं।

संक्षेप में, 1980 के दशक की सिग्मॉइड की सुंदरता एलएलएम की गहराई और पैमाने के अनुकूल नहीं है। रीलू/जीलू/स्विश तेज, अधिक विश्वसनीय ट्रेनिंग और कम खामियों के साथ प्रदान करते हैं। यदि आप किसी मॉडल को ट्वीक कर रहे हैं, तो ट्रांसफॉर्मर संगतता के लिए जीलू से शुरुआत करें।

#### संदर्भ
- [Linear Layers and Activation Functions in Transformer Models](https://machinelearningmastery.com/linear-layers-and-activation-functions-in-transformer-models/)
- [Deep Learning 101: Transformer Activation Functions Explainer](https://www.saltdatalabs.com/blog/deep-learning-101-transformer-activation-functions-explainer-relu-leaky-relu-gelu-elu-selu-softmax-and-more)
- [Common Activation Functions](https://www.aussieai.com/book/ch21-common-activation-functions)
- [Understanding Logits, Sigmoid, Softmax, and Cross-Entropy Loss](https://wandb.ai/amanarora/Written-Reports/reports/Understanding-Logits-Sigmoid-Softmax-and-Cross-Entropy-Loss-in-Deep-Learning--Vmlldzo0NDMzNTU3)