---
audio: false
generated: true
image: false
lang: hi
layout: post
model: x-ai/grok-4-fast
title: ट्रांसफॉर्मर का परिचय व्याख्यान सारांश
translated: true
type: note
---

### आंद्रेज कार्पाथी के "ट्रांसफॉर्मर्स का परिचय" लेक्चर का सारांश

आंद्रेज कार्पाथी का यह लेक्चर, स्टैनफोर्ड के CS25 ट्रांसफॉर्मर्स यूनाइटेड कोर्स का हिस्सा है, जो ट्रांसफॉर्मर आर्किटेक्चर - आधुनिक AI मॉडल्स जैसे GPT और BERT की रीढ़ - में एक शुरुआती-अनुकूल पर गहरा गोता है। लगभग एक घंटे में दिया गया, यह ट्रांसफॉर्मर्स के काम करने के तरीके को स्पष्ट करने के लिए सहज विज़ुअल्स, सादृश्यों और कोड स्निपेट्स (उनके "नैनोजीपीटी" इम्प्लीमेंटेशन के लाइव डेमो सहित) का उपयोग करता है। कार्पाथी उनके इतिहास का पता लगाते हैं, मैकेनिक्स को तोड़ते हैं और भाषा से परे क्षेत्रों में उनकी बहुमुखी प्रतिभा का पता लगाते हैं। यहां प्रमुख बिंदुओं का एक संरचित अवलोकन दिया गया है:

#### कोर्स का संदर्भ और बड़ी तस्वीर
- **ट्रांसफॉर्मर्स क्यों महत्वपूर्ण हैं**: 2017 के पेपर "अटेंशन इज़ ऑल यू नीड" में पेश किए गए, ट्रांसफॉर्मर्स ने तब से AI में क्रांति ला दी है, जो नेचुरल लैंग्वेज प्रोसेसिंग (NLP), कंप्यूटर विजन, बायोलॉजी (जैसे, अल्फाफोल्ड), रोबोटिक्स और अन्य में हावी हैं। ये सिर्फ टेक्स्ट के लिए नहीं हैं - ये किसी भी सीक्वेंस डेटा के लिए एक लचीला फ्रेमवर्क हैं।
- **कोर्स के लक्ष्य**: यह ट्रांसफॉर्मर्स की बुनियादी बातों, सेल्फ-अटेंशन और एप्लिकेशन्स पर एक श्रृंखला के लिए किकऑफ लेक्चर है। भविष्य के सत्र BERT/GPT जैसे मॉडल और रीयल-वर्ल्ड उपयोगों पर अतिथि वार्ता को कवर करते हैं। कार्पाथी ट्रांसफॉर्मर्स पर "एकीकृत" लर्निंग एल्गोरिदम के रूप में जोर देते हैं, जो AI उप-क्षेत्रों को स्केलेबल, डेटा-संचालित मॉडल की ओर ले जा रहा है।

#### ऐतिहासिक विकास
- **शुरुआती मॉडल से बॉटलनेक तक**: लैंग्वेज AI की शुरुआत सरल न्यूरल नेट्स (2003) से हुई, जो मल्टी-लेयर पर्सेप्ट्रॉन के माध्यम से अगले शब्दों की भविष्यवाणी करते थे। आरएनएन/एलएसटीएम (2014) ने अनुवाद जैसे कार्यों के लिए सीक्वेंस हैंडलिंग जोड़ी, लेकिन सीमाओं से टकराए: फिक्स्ड "एनकोडर बॉटलनेक" ने पूरे इनपुट को एक सिंगल वेक्टर में संपीड़ित कर दिया, लंबे सीक्वेंस पर विवरण खो दिए।
- **अटेंशन का उदय**: अटेंशन मैकेनिज्म (यान लेकुन द्वारा गढ़ा गया) ने इसे डिकोडर को वेटेड सम के माध्यम से प्रासंगिक इनपुट भागों को "सॉफ्ट-सर्च" करने देकर ठीक किया। 2017 की सफलता ने आरएनएन को पूरी तरह से छोड़ दिया, यह दांव लगाते हुए कि "अटेंशन इज़ ऑल यू नीड" समानांतर प्रोसेसिंग के लिए - तेज़ और अधिक शक्तिशाली।

#### कोर मैकेनिक्स: सेल्फ-अटेंशन और मैसेज पासिंग
- **नोड्स के रूप में टोकन्स**: इनपुट डेटा (जैसे, शब्दों) को ग्राफ में "टोकन्स" के रूप में सोचें। सेल्फ-अटेंशन नोड्स के मैसेज एक्सचेंज करने जैसा है: प्रत्येक टोकन **क्वेरीज़** (मैं क्या ढूंढ रहा हूं), **कीज़** (मैं क्या पेशकश करता हूं), और **वैल्यूज़** (मेरा डेटा पेलोड) बनाता है। क्वेरी/कीज़ के बीच डॉट-प्रोडक्ट समानता अटेंशन वेट (सॉफ्टमैक्स के माध्यम से) निर्धारित करती है, फिर वेट वैल्यू को कॉन्टेक्स्ट-अवेयर अपडेट के लिए गुणा करते हैं।
- **मल्टी-हेड अटेंशन**: समृद्ध परिप्रेक्ष्य के लिए अलग-अलग वेट के साथ समानांतर "हेड्स" में इसे चलाएं, फिर कॉन्कैटेनेट करें।
- **कॉजल मास्किंग**: जनरेशन के लिए डिकोडर्स में, भविष्यवाणी के दौरान "चीटिंग" को रोकने के लिए भविष्य के टोकन को मास्क करें।
- **पोजिशनल एन्कोडिंग**: ट्रांसफॉर्मर्स सेट को प्रोसेस करते हैं, सीक्वेंस नहीं, इसलिए ऑर्डर जानकारी इंजेक्ट करने के लिए एम्बेडिंग में साइन-आधारित एन्कोडिंग जोड़ें।
- **सहज ज्ञान**: यह डेटा-निर्भर संचार है - टोकन स्वतंत्र रूप से "चैट" करते हैं (एनकोडर) या कारण-संबंधी (डिकोडर), अनुक्रमिक बॉटलनेक के बिना लंबी दूरी की निर्भरताओं को पकड़ते हैं।

#### पूर्ण आर्किटेक्चर: संचार + गणना
- **एनकोडर-डिकोडर सेटअप**: एनकोडर बायडायरेक्शनल फ्लो के लिए टोकन को पूरी तरह से जोड़ता है; डिकोडर ऑटोरेग्रेसिव जनरेशन के लिए एनकोडर आउटपुट पर क्रॉस-अटेंशन और कॉजल सेल्फ-अटेंशन जोड़ता है।
- **ब्लॉक संरचना**: लेयर्स को स्टैक करें बारी-बारी से:
  - **संचार चरण**: मल्टी-हेड सेल्फ/क्रॉस-अटेंशन (मैसेज पासिंग)।
  - **गणना चरण**: फीड-फॉरवर्ड MLP (ReLU नॉनलीनियरिटी के साथ व्यक्तिगत टोकन प्रोसेसिंग)।
- **स्थिरता के लिए अतिरिक्त**: अवशिष्ट कनेक्शन (आउटपुट में इनपुट जोड़ें), लेयर नॉर्मलाइजेशन।
- **यह क्यों काम करता है**: जीपीयू पर समानांतर, जटिल पैटर्न के लिए अभिव्यंजक, और डेटा/कंप्यूट के साथ स्केल करता है।

#### हैंड्स-ऑन: नैनोजीपीटी के साथ निर्माण और प्रशिक्षण
- **न्यूनतम कार्यान्वयन**: कार्पाथी नैनोजीपीटी का डेमो देते हैं - PyTorch में एक छोटा डिकोडर-ओनली ट्रांसफॉर्मर। यह टेक्स्ट (जैसे, शेक्सपियर) पर अगले अक्षरों/शब्दों की भविष्यवाणी करने के लिए प्रशिक्षित होता है।
  - **डेटा तैयारी**: इंटीजर में टोकनाइज़ करें, फिक्स्ड-साइज कॉन्टेक्स्ट (जैसे, 1024 टोकन) में बैच करें।
  - **फॉरवर्ड पास**: एम्बेड टोकन + पोजिशनल एन्कोडिंग → ट्रांसफॉर्मर ब्लॉक → लॉगिट्स → क्रॉस-एन्ट्रॉपी लॉस (टारगेट्स = शिफ्टेड इनपुट)।
  - **जनरेशन**: एक प्रॉम्प्ट से शुरू करें, ऑटोरेग्रेसिवली अगले टोकन सैंपल करें, कॉन्टेक्स्ट सीमाओं का सम्मान करते हुए।
- **प्रशिक्षण युक्तियाँ**: दक्षता के लिए बैच साइज × सीक्वेंस लेंथ; जीपीटी-2 जैसे विशाल मॉडल तक स्केल करता है।
- **वेरिएंट**: एनकोडर-ओनली (मास्किंग के माध्यम से क्लासिफिकेशन के लिए BERT); अनुवाद के लिए पूर्ण एनकोडर-डिकोडर।

#### अनुप्रयोग और सुपरपावर्स
- **टेक्स्ट से परे**: इमेज/ऑडियो को टोकन में पैच करें - सेल्फ-अटेंशन पैच के पार गैर-यूक्लिडियन "संचार" को संभालता है, जो विजन ट्रांसफॉर्मर्स (ViT) को सक्षम बनाता है।
- **इन-कॉन्टेक्स्ट लर्निंग**: प्रॉम्प्ट्स में उदाहरण फीड करें; मॉडल ऑन-द-फ्लाई कार्यों को "सीखते हैं" (मेटा-लर्निंग), फाइन-ट्यूनिंग की आवश्यकता नहीं है। बड़े पैमाने पर डेटा के साथ, न्यूनतम पूर्वाग्रह चमकते हैं।
- **लचीलापन**: RL स्टेट्स/एक्शन को "भाषा" के रूप में मानें, या लंबे कॉन्टेक्स्ट के लिए बाहरी मेमोरी जोड़ें। इमेज जनरेशन जैसे पुनरावृत्त कार्यों के लिए डिफ्यूजन मॉडल के साथ हाइब्रिड।

#### चुनौतियाँ और भविष्य का दृष्टिकोण
- **बाधाएँ**: क्वाड्रेटिक कंप्यूट (O(n²) अटेंशन), हलुसिनेशन कंट्रोल, लॉन्ग-कॉन्टेक्स्ट हैंडलिंग, ब्रेन अलाइनमेंट।
- **प्रभाव**: टेक्स्ट के लिए "जनरल-पर्पस कंप्यूटर" के रूप में ट्रांसफॉर्मर्स - कठिन डेटा पर प्रशिक्षित करके "प्रोग्राम" चलाने के लिए जैसे तर्क या कोड जनरेशन।
- **आगे क्या**: डोमेन-विशिष्ट मॉडल (जैसे, फाइनेंस GPTs), मल्टीमॉडल इंटीग्रेशन, बेहतर तर्क/ऑडियो/कला उपकरण। कार्पाथी दक्षता ट्विक्स और "जीपीटी++" महत्वाकांक्षाओं की ओर इशारा करते हैं।

यह लेक्चर अपने विज़ुअल सादृश्यों (जैसे, अनुवाद में अटेंशन "कर्सर" के रूप में) और कोड वॉकथ्रू के लिए चमकता है, जो अमूर्त विचारों को ठोस बनाता है। इसे एक शीर्ष परिचय के रूप में सराहा जाता है क्योंकि यह अभिभूत किए बिना अंतर्ज्ञान का निर्माण करता है - नए लोगों के लिए आदर्श।

[स्टैनफोर्ड CS25: ट्रांसफॉर्मर्स यूनाइटेड लेक्चर सारांश](https://www.deciphr.ai/podcast/stanford-cs25-v2-i-introduction-to-transformers-w-andrej-karpathy-2)  
[ट्रांसफॉर्मर्स का परिचय नोट्स](https://ersantana.com/llm/stanford_cs25_transformers/02_intro_to_transformers)  
[YouTube वीडियो](https://www.youtube.com/watch?v=XfpMkf4rD6E)