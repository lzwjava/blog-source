---
audio: false
generated: true
image: false
lang: hi
layout: post
model: x-ai/grok-4-fast
title: ग्रेडिएंट बूस्टिंग का परिचय
translated: true
type: note
---

### ग्रेडिएंट बूस्टिंग क्या है?

ग्रेडिएंट बूस्टिंग एक शक्तिशाली एन्सेम्बल मशीन लर्निंग तकनीक है जिसका उपयोग सुपरवाइज्ड लर्निंग टास्क्स जैसे रिग्रेशन, क्लासिफिकेशन और रैंकिंग के लिए किया जाता है। यह बूस्टिंग एल्गोरिदम का एक प्रकार है जो कई कमजोर लर्नर्स (आमतौर पर सरल डिसीजन ट्री) को एक क्रमिक तरीके से जोड़कर एक मजबूत प्रेडिक्टिव मॉडल बनाता है। मुख्य विचार यह है कि पिछले मॉडल्स द्वारा की गई त्रुटियों (अवशिष्टों) पर ध्यान केंद्रित करके मॉडल को धीरे-धीरे सुधारा जाए, जिससे समग्र प्रदर्शन "बूस्ट" हो।

#### मूल अवधारणा
इसके मूल में, ग्रेडिएंट बूस्टिंग सीखने की प्रक्रिया को एक ऑप्टिमाइजेशन समस्या के रूप में मानती है। यह **ग्रेडिएंट डिसेंट** का उपयोग करके एक लॉस फंक्शन (उदाहरण के लिए, रिग्रेशन के लिए मीन स्क्वेर्ड एरर या क्लासिफिकेशन के लिए लॉग लॉस) को कम करती है। अनुक्रम में प्रत्येक नया मॉडल वर्तमान एन्सेम्बल की भविष्यवाणियों के संबंध में लॉस फंक्शन के **नकारात्मक ग्रेडिएंट** की भविष्यवाणी करने के लिए प्रशिक्षित किया जाता है। इस तरह, एल्गोरिदम पिछले मॉडल्स की गलतियों को कदम दर कदम "सही" करता है।

#### यह कैसे काम करता है: चरण-दर-चरण
1. **मॉडल को इनिशियलाइज़ करें**: एक सरल बेस मॉडल से शुरुआत करें, जो अक्सर टार्गेट वेरिएबल का माध्य (रिग्रेशन के लिए) या लॉग-ऑड्स (क्लासिफिकेशन के लिए) होता है।

2. **अवशिष्ट (स्यूडो-रेजिडुअल) की गणना करें**: प्रत्येक पुनरावृत्ति के लिए, अवशिष्टों की गणना करें—ये वास्तविक और पूर्वानुमानित मूल्यों के बीच का अंतर होते हैं। ये उन "त्रुटियों" का प्रतिनिधित्व करते हैं जिन्हें अगले मॉडल को संबोधित करने की आवश्यकता है।

3. **एक कमजोर लर्नर फिट करें**: इन अवशिष्टों पर एक नया कमजोर लर्नर (जैसे, एक उथला डिसीजन ट्री) प्रशिक्षित करें। लक्ष्य आवश्यक सुधारों की दिशा और परिमाण की भविष्यवाणी करना है।

4. **एन्सेम्बल को अपडेट करें**: नए लर्नर को एन्सेम्बल में जोड़ें, जिसे ओवरफिटिंग को रोकने के लिए एक छोटी लर्निंग रेट (सिकुड़न पैरामीटर, आमतौर पर <1) द्वारा स्केल किया गया हो। अपडेट की गई भविष्यवाणी है:
   \\[
   F_m(x) = F_{m-1}(x) + \eta \cdot h_m(x)
   \\]
   जहां \\( F_m(x) \\) एन्सेम्बल है \\( m \\) पुनरावृत्तियों के बाद, \\( \eta \\) लर्निंग रेट है, और \\( h_m(x) \\) नया कमजोर लर्नर है।

5. **दोहराएँ**: एक निश्चित संख्या में राउंड (या अभिसरण तक) के लिए पुनरावृति करें, हर बाद पूर्ण एन्सेम्बल से अपडेट किए गए अवशिष्टों का उपयोग करें।

यह प्रक्रिया "ग्रेडिएंट" है क्योंकि अवशिष्ट लॉस फंक्शन के ग्रेडिएंट का अनुमान लगाते हैं, जिससे एल्गोरिदम मॉडल्स के स्पेस में फंक्शनल ग्रेडिएंट डिसेंट का एक रूप कर पाता है।

#### मुख्य लाभ
- **उच्च सटीकता**: फीचर इंजीनियरिंग के बिना जटिल पैटर्न को पकड़कर अक्सर टैब्युलर डेटा पर अन्य एल्गोरिदम से बेहतर प्रदर्शन करता है।
- **लचीलापन**: ओवरफिटिंग को हैंडल करने के लिए कस्टम लॉस फंक्शन और रेगुलराइजेशन (जैसे, L1/L2 पेनल्टी) का समर्थन करता है।
- **दक्षता**: XGBoost जैसे इम्प्लीमेंटेशन में समानांतर योग्य, जो गति और स्केलेबिलिटी के लिए बूस्टिंग प्रक्रिया को ऑप्टिमाइज़ करता है।

#### XGBoost से संबंध
जैसा कि आपके प्रश्न में उल्लेख किया गया है, XGBoost (eXtreme Gradient Boosting) इस फ्रेमवर्क का एक लोकप्रिय, अनुकूलित कार्यान्वयन है। यह तेज प्रशिक्षण के लिए हिस्टोग्राम-आधारित स्प्लिटिंग और बिल्ट-इन क्रॉस-वैलिडेशन जैसे एन्हांसमेंट जोड़ता है, जिससे यह कागल जैसी प्रतियोगिताओं के लिए एक गो-टो बन गया है।

गहराई में जाने के लिए, ग्रेडिएंट बूस्टिंग LightGBM और CatBoost जैसे वेरिएंट्स की नींव है, जिनमें से प्रत्येक ट्री ग्रोथ या कैटेगोरिकल डेटा को हैंडल करने जैसे पहलुओं को ट्वीक करता है।

#### संदर्भ
- [Gradient Boosting Machines: A Tutorial](https://www.stat.cmu.edu/~cshalizi/402/lectures/18-gbm.pdf) (कार्नेगी मेलन विश्वविद्यालय के व्याख्यान नोट्स)
- [XGBoost Documentation: Introduction to Boosted Trees](https://xgboost.readthedocs.io/en/stable/tutorials/model.html)
- [Understanding Gradient Boosting Machines](https://towardsdatascience.com/understanding-gradient-boosting-machines-9be756fe76ab) (टुवर्ड्स डेटा साइंस लेख)