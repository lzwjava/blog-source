---
audio: false
generated: true
image: false
lang: hi
layout: post
model: x-ai/grok-4-fast
title: नैनोजीपीटी शब्दावली मेटाडेटा फ़ाइल
translated: true
type: note
---

**nanoGPT** (https://github.com/karpathy/nanoGPT) में, फ़ाइल **meta.pkl** एक वैकल्पिक पिकल (pickle) फ़ाइल है जो डेटासेट के लिए शब्दावली (vocabulary) और एन्कोडिंग मेटाडेटा को संग्रहीत करती है। इसका उपयोग तब होता है जब आप अपने स्वयं के कस्टम टेक्स्ट डेटासेट पर प्रशिक्षण लेते हैं (डिफ़ॉल्ट OpenWebText या Shakespeare चार-लेवल डेटासेट नहीं)।

### meta.pkl में आमतौर पर क्या होता है
यह एक डिक्शनरी होती है जिसमें निम्नलिखित कुंजियाँ (keys) होती हैं (उदाहरण):

```python
{
    'vocab_size': 50257,       # शब्दावली का आकार
    'itos': {0: 'A', 1: 'B', ...},  # int → स्ट्रिंग (टोकन)
    'stoi': {'A': 0, 'B': 1, ...},  # स्ट्रिंग → int (टोकन)
    'encoder': ...,            # एनकोड फ़ंक्शन
    'decoder': ...,            # डिकोड फ़ंक्शन
}
```

### जब nanoGPT, meta.pkl को लोड या बनाता है

1. प्रशिक्षण (`train.py`) के दौरान:
   - यदि `data/your_dataset/meta.pkl` पहले से मौजूद है → यह वहाँ से शब्दावली लोड करता है।
   - यदि यह मौजूद **नहीं** है → nanoGPT आपके डेटासेट के टेक्स्ट से एक नई शब्दावली बनाता है (आमतौर पर SentencePiece, tiktoken, या एक साधारण चार-लेवल vocab के साथ) और भविष्य के उपयोग के लिए इसे `meta.pkl` के रूप में सहेजता है।

2. सैंपलिंग/इन्फ़रेंस (`sample.py`) के दौरान:
   - जब आपको यह संदेश दिखाई देता है
     `No meta.pkl found, assuming GPT-2 encodings...`
     इसका मतलब है कि nanoGPT आपके डेटा डायरेक्टरी में meta.pkl फ़ाइल नहीं ढूंढ पाया।
   - इसलिए यह डिफ़ॉल्ट GPT-2 BPE टोकनाइज़र (वही जो OpenAI के GPT-2 द्वारा उपयोग किया जाता है) पर वापस आ जाता है, जिसका vocab_size = 50257 होता है। इसीलिए आपका 29.94M पैरामीटर मॉडल (जो सबसे छोटा GPT-2 124M कॉन्फ़िगरेशन है लेकिन कभी-कभी गलत रिपोर्ट किया जाता है) बिना meta.pkl के भी ठीक काम करता है जब आप OpenWebText या इसी तरह के डेटासेट पर प्रशिक्षण लेते हैं।

### संक्षेप में:

| स्थिति                                 | meta.pkl क्या करता है                                      |
|----------------------------------------|----------------------------------------------------------|
| आप एक **कस्टम डेटासेट** पर प्रशिक्षण लेते हैं | आपकी कस्टम शब्दावली को संग्रहीत करता है ताकि एन्कोडिंग/डिकोडिंग सुसंगत रहे |
| आप OpenWebText/Shakespeare पर प्रशिक्षण लेते हैं | आवश्यक नहीं → nanoGPT डिफ़ॉल्ट रूप से GPT-2 टोकनाइज़र का उपयोग करता है |
| `sample.py` को meta.pkl नहीं मिलता है  | GPT-2 टोकनाइज़र पर वापस आ जाता है (जो आपने देखा वही है)    |

यदि आप कभी अपना स्वयं का डेटासेट प्रशिक्षित करते हैं और पुनरुत्पादन योग्य टोकननाइजेशन (या एक अलग vocab आकार) चाहते हैं, तो आप देखेंगे कि nanoGPT पहले प्रशिक्षण रन पर स्वचालित रूप से meta.pkl बना देता है। उसके बाद यह इसका पुन: उपयोग करेगा और आपको अब "assuming GPT-2 encodings" चेतावनी नहीं मिलेगी।