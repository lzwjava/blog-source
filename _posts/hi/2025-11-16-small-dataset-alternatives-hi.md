---
audio: false
generated: true
image: false
lang: hi
layout: post
model: x-ai/grok-4-fast
title: छोटे उच्च-गुणवत्ता वाले डेटासेट विकल्प
translated: true
type: note
---

हाँ, OpenWebText बहुत बड़ा है (~9B टोकन, 17GB uint16 फ़ाइल) और इसे प्रीप्रोसेस करने में लंबा समय लगता है और बहुत सारी डिस्क स्पेस की आवश्यकता होती है। यहाँ बहुत छोटे उच्च-गुणवत्ता वाले विकल्प हैं जो आमतौर पर टेस्टिंग, डिबगिंग, या छोटे मॉडल्स को जल्दी से ट्रेन करने के लिए उपयोग किए जाते हैं:

| डेटासेट                  | अनुमानित टोकन | अंतिम .bin आकार (uint16) | लोग इसे क्यों पसंद करते हैं                                                                 | इसे जल्दी से कैसे प्राप्त करें                                      |
|--------------------------|----------------|--------------------------|------------------------------------------------------------------------------------|------------------------------------------------------------|
| **OpenWebText-10k**      | ~40M           | ~80MB                    | पूर्ण OpenWebText जैसा ही वितरण, बस पहले 10k दस्तावेज़             | `load_dataset("openwebtext", split="train[:10k]")`         |
| **OpenWebText-100k**     | ~400M          | ~800MB                   | फिर भी बहुत प्रतिनिधि, टोकनाइजेशन कुछ मिनटों में पूरा हो जाता है                   | `split="train[:100k]"`                                     |
| **FineWeb-Edu sample**   | 50M–1B         | 100MB–2GB                | OWT से उच्च गुणवत्ता (LLama-स्टाइल फ़िल्टरिंग), हाल ही में बहुत लोकप्रिय              | `load_dataset("HuggingFaceFW/fineweb-edu", name="sample-10BQ", split="train")` → ~50M टोकन |
| **Shakespeare**          | ~1M            | ~2MB                     | क्लासिक छोटा डेटासेट, त्वरित सैनिटी चेक के लिए परफेक्ट                             | `load_dataset("tiny_shakespeare")` या बस सिंगल .txt फ़ाइल डाउनलोड करें |
| **PG-19 (books)**        | पूर्ण 2.8B      | ~5.5GB                   | बहुत साफ पब्लिक-डोमेन किताबें, लेकिन आप बस एक हिस्सा ले सकते हैं                      | `load_dataset("pg19", split="train[:5%]")` → ~140M टोकन  |
| **C4 (subset)**          | परिवर्तनशील       | परिवर्तनशील                 | Common Crawl को T5 टीम द्वारा साफ किया गया, केवल अंग्रेजी                                      | `load_dataset("allenai/c4", "en", split="train[:1%]")` streaming=True |
| **RedPajama sample**     | 5M–200M        | 10MB–400MB               | स्रोतों का मिश्रण (CommonCrawl, Wikipedia, किताबें, आदि), LLaMA जैसी ही रेसिपी         | `load_dataset("togethercomputer/RedPajama-Data-1T", name="sample", split="train")` |

### आपके लक्ष्य के आधार पर मेरी व्यक्तिगत सिफारिशें

1. बस कुछ ऐसा चाहते हैं जो 5 मिनट से कम में चले और कहीं भी फिट हो → **tiny_shakespeare**  
   (nanoGPT उपयोगकर्ता इसे पसंद करते हैं)

2. कुछ ऐसा चाहते हैं जो अभी भी "असली इंटरनेट टेक्स्ट" हो लेकिन छोटा → OpenWebText के पहले 10k दस्तावेज़  
   ```python
   dataset = load_dataset("openwebtext", split="train[:10000]")
   ```

3. अभी (2025) उच्चतम-गुणवत्ता वाला छोटा डेटासेट चाहते हैं → FineWeb-Edu 10B-टोकन sample  
   (~50M टोकन, लेकिन OWT की तुलना में बहुत साफ, शीर्ष लैब्स द्वारा उपयोग की जाने वाली गुणवत्ता के लगभग समान)

   ```python
   dataset = load_dataset(
       "HuggingFaceFW/fineweb-edu",
       name="sample-10BQ",  # 10 बिलियन टोकन सैंपल किए गए, यदि आप स्लाइस करते हैं तो ~50M प्रति शार्ड
       split="train"
   )
   ```

4. कुछ ऐसा चाहते हैं < 500MB लेकिन फिर भी विविध → RedPajama sample (200M टोकन वर्जन)  
   ```python
   dataset = load_dataset("togethercomputer/RedPajama-Data-1T", name="sample-200M", split="train")
   ```

इन सभी को आपके ठीक उसी प्रीप्रोसेसिंग स्क्रिप्ट में बिना किसी बदलाव के डाला जा सकता है (बस load_dataset लाइन को बदलें)।

त्वरित पुनरावृत्ति के लिए मैं आमतौर पर तीन बाइनरीज़ डिस्क पर रखता हूँ:
- `shakespeare.bin`   (~2MB)   → ओवरफिट टेस्ट
- `owt_10k.bin`       (~80MB)  → नए कोड को डीबग करना
- `fineweb_edu_1B.bin` (~2GB) → वास्तविक छोटे-मॉडल ट्रेनिंग

मुझे बताएं कि आप इसे कितना बड़ा चाहते हैं और मैं आपको सटीक वन-लाइनर दे सकता हूँ!