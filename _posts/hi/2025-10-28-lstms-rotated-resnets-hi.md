---
audio: false
generated: true
image: false
lang: hi
layout: post
model: x-ai/grok-4-fast
title: एलएसटीएम सिर्फ़ घुमाए गए रेसनेट थे
translated: true
type: note
---

अपने न्यूरआईपीएस 2024 टेस्ट ऑफ टाइम अवार्ड भाषण में, जो 2014 के "सीक्वेंस टू सीक्वेंस लर्निंग विथ न्यूरल नेटवर्क्स" पेपर पर था, इलिया सुत्स्केवर ने उस दौर के महत्वपूर्ण अंतर्दृष्टियों और गलतियों पर चिंतन किया। "हमने क्या गलत किया" के तहत उन्होंने एक प्रमुख मुद्दा उठाया: एलएसटीएम (लॉन्ग शॉर्ट-टर्म मेमोरी नेटवर्क्स) की अत्यधिक जटिलता और अंतिम सीमाएँ, जिन्होंने मशीन अनुवाद जैसी शुरुआती अनुक्रम मॉडलिंग की क्रांतियों को शक्ति प्रदान की थी।

### एलएसटीएम के बारे में मूल भ्रांति
हमने एलएसटीएम को अनुक्रमिक डेटा के लिए विशेष रूप से तैयार की गई एक मौलिक रूप से नई, जटिल संरचना के रूप में देखा—कुछ "विशेष" जिसे डीप लर्निंग शोधकर्ताओं को समय निर्भरताओं, विलुप्त हो रहे ग्रेडिएंट्स और पुनरावृत्ति को संभालने के लिए सावधानीपूर्वक इंजीनियर करना पड़ा। वास्तव में, सुत्स्केवर ने समझाया, एलएसटीएम उससे कहीं अधिक सरल थे: **वे अनिवार्य रूप से 90 डिग्री घुमाया गया एक रेसनेट (रेज़िडुअल नेटवर्क) हैं**।

- **रेसनेट्स** (2015 में पेश किए गए) ने स्किप कनेक्शन (अवशेष) जोड़कर छवि प्रसंस्करण में क्रांति ला दी, जिससे सूचना सीधे परतों के पार प्रवाहित हो सकती थी और बिना प्रशिक्षण अस्थिरता के बहुत गहरे नेटवर्क सक्षम हुए।
- एलएसटीएम (1997 से) ने *समय आयाम* में कुछ ऐसा ही किया: उनके गेट्स और सेल स्टेट अवशेषों की तरह काम करते हैं, जो ग्रेडिएंट और सूचना को लंबे अनुक्रमों पर फीले बिना प्रसारित होने देते हैं। यह एक ही सिद्धांत है—बस स्थानिक स्टैकिंग (जैसे, छवि में पिक्सेल) से घुमाकर लौकिक स्टैकिंग (जैसे, वाक्य में शब्द) में कर दिया गया।

सुत्स्केवर ने मजाक में कहा: "जो लोग इससे अपरिचित हैं, उनके लिए, एलएसटीएम कुछ ऐसा है जो ट्रांसफॉर्मर से पहले गरीब डीप लर्निंग शोधकर्ताओं ने किया था। यह अनिवार्य रूप से एक रेसनेट है लेकिन 90 डिग्री घुमाया हुआ... और यह पहले आया; यह एक slightly more complex रेसनेट जैसा है, जिसमें एक इंटीग्रेटर और कुछ गुणा होता है।" यह सादृश्य इस बात को रेखांकित करता है कि एलएसटीएम एक कट्टरपंथी परिवर्तन नहीं थे; वे पुनरावृत्ति के लिए अवशिष्ट विचारों का एक प्रारंभिक, सुरुचिपूर्ण अनुप्रयोग थे।

### यह क्यों महत्वपूर्ण था (और क्या गलत हुआ)
- **क्या शानदार ढंग से काम किया**: एलएसटीएम ने अपने समय में आश्चर्यजनक रूप से अच्छा स्केल किया, जिससे seq2seq मॉडल अनुवाद कार्यों पर पारंपरिक सांख्यिकीय विधियों को हरा सका। अवशेषों ने गहरे पुनरावर्ती नेट्स को प्रशिक्षित करने योग्य बनाया, ठीक वैसे ही जैसे उन्होंने बाद में फीडफॉरवर्ड नेट्स के लिए किया।
- **हमने क्या गलत किया (और एलएसटीएम क्यों फीके पड़ गए)**: हमने एलएसटीएम की अनुक्रमिक प्रकृति के स्केलिंग को कैसे बाधित करेगी, इसका कम आकलन किया। समानांतर करने योग्य रेसनेट्स या ट्रांसफॉर्मर के विपरीत, एलएसटीएम डेटा को चरण-दर-चरण संसाधित करते हैं, जिससे वे बड़े पैमाने के डेटासेट या लंबे संदर्भों के लिए अक्षम हो जाते हैं। उन्होंने अनावश्यक जटिलता (जैसे, कई गेट्स) भी जोड़ दी, जिसे ट्रांसफॉर्मर में अटेंशन मैकेनिज्म ने सरल बना दिया। 2017 तक, ट्रांसफॉर्मर ने इन सीमाओं को उजागर कर दिया, और ध्यान बेहतर समानांतरता और लंबी दूरी की निर्भरताओं के लिए सेल्फ-अटेंशन की ओर स्थानांतरित हो गया।

स hindsight में, "गलत" हिस्सा एलएसटीएम का आविष्कार करना नहीं था—वे एक वीरतापूर्ण अंतरिम समाधान थे—बल्कि अनुक्रमों के लिए अंतिम लक्ष्य के रूप में उनसे चिपके रहना था, बजाय इसके कि अवशेषों को एक सामान्य उपकरण के रूप में पहचाना जाता और अधिक समानांतर आर्किटेक्चर की ओर जल्दी धकेला जाता।

यह परिप्रेक्ष्य एलएसटीएम को एक अवशेष के रूप में नहीं, बल्कि यह समझने में एक मौलिक कदम के रूप में पुनः परिभाषित करता है कि स्किप कनेक्शन आयामों में गहराई को कैसे अनलॉक करते हैं।

[सीक्वेंस टू सीक्वेंस लर्निंग विथ न्यूरल नेटवर्क्स: व्हाट ए डिकेड – ट्रांसक्रिप्ट और वीडियो](https://kingy.ai/news/sequence-to-sequence-learning-with-neural-networks-what-a-decade-transcript-video/)  
[इलिया सुत्स्केवर न्यूरआईपीएस 2024 फुल टॉक](https://www.youtube.com/watch?v=1yvBqasHLZs)  
[न्यूरआईपीएस 2024 में इलिया की फुल टॉक से प्रतिबिंब](https://dlyog.com/papers/one_internet_v1)