---
audio: false
generated: true
image: false
lang: hi
layout: post
model: x-ai/grok-4-fast
title: नैनोजीपीटी मॉडल आर्किटेक्चर सारांश
translated: true
type: note
---

### NanoGPT मॉडल आर्किटेक्चर अवलोकन

NanoGPT, PyTorch में GPT-2 जैसा एक डिकोडर-ओनली ट्रांसफॉर्मर मॉडल लागू करता है। आर्किटेक्चर `model.py` फ़ाइल में परिभाषित है, जिसमें एक कॉन्फ़िगरेबल `GPTConfig` है जो हाइपरपैरामीटर्स सेट करता है। डिफ़ॉल्ट कॉन्फ़िगरेशन (उदाहरण के लिए, "gpt2" वेरिएंट के लिए) निम्नलिखित का उपयोग करता है:

- **ट्रांसफॉर्मर ब्लॉक्स की संख्या**: 12 (`n_layer = 12`)
- **एम्बेडिंग डायमेंशन (लेयर साइज़)**: 768 (`n_embd = 768`)
- **अटेंशन हेड्स की संख्या**: 12 (`n_head = 12`)
- **MLP इंटरमीडिएट साइज़**: 3072 (`n_embd * 4`, क्योंकि एक्सपेंशन फैक्टर 4 है)

प्रत्येक ट्रांसफॉर्मर ब्लॉक (क्लास `Block`) रेजिडुअल कनेक्शन और लेयर नॉर्मलाइजेशन के साथ एक स्टैंडर्ड डिकोडर ब्लॉक है। इसमें शामिल हैं:
- **LayerNorm 1** (`ln1`): सेल्फ-अटेंशन से पहले लागू होता है।
- **मल्टी-हेड सेल्फ-अटेंशन** (`attn`): कॉजल (मास्क्ड) अटेंशन ताकि आगे के टोकन्स को न देखा जा सके।
- अटेंशन के बाद रेजिडुअल एडिशन।
- **LayerNorm 2** (`ln2`): MLP से पहले लागू होता है।
- **MLP** (`mlp`): एक साधारण फीड-फॉरवर्ड नेटवर्क।
- MLP के बाद रेजिडुअल एडिशन।

सम्पूर्ण मॉडल (क्लास `GPT`) टोकन और पोजीशन एम्बेडिंग के बाद इन 12 ब्लॉक्स को स्टैक करता है, इसके बाद एक फाइनल LayerNorm (`ln_f`) और वोकैबुलरी साइज़ पर एक लीनियर प्रोजेक्शन होता है।

#### MLP संरचना
MLP (`Block` के अंदर क्लास `MLP`) एक दो-लेयर फीड-फॉरवर्ड नेटवर्क है:
- पहली लीनियर लेयर (`c_fc`): `n_embd` (768) से इंटरमीडिएट साइज़ (3072) तक प्रोजेक्ट करती है।
- GELU एक्टिवेशन: पहली प्रोजेक्शन के बाद एलिमेंट-वाइज लागू होती है।
- दूसरी लीनियर लेयर (`c_proj`): 3072 से वापस `n_embd` (768) तक प्रोजेक्ट करती है।

यह आपके द्वारा बताए गए "fc -> gelu -> projection" पैटर्न का अनुसरण करता है।

#### फॉरवर्ड पास फ्लो
फॉरवर्ड पास रेजिडुअल-स्टाइल में हैं, जिसमें प्री-नॉर्म (सब-लेयर्स से पहले LayerNorm) है। यहाँ एक उच्च-स्तरीय विवरण है:

1. **मुख्य फॉरवर्ड (GPT.forward)**:
   - टोकन एम्बेडिंग: इनपुट टोकन्स (आकार `[B, T]`) → एम्बेडिंग्स (आकार `[B, T, n_embd]`)।
   - पोजिशनल एम्बेडिंग: टोकन एम्बेडिंग में जोड़ी जाती है।
   - `n_layer` (12) ट्रांसफॉर्मर ब्लॉक्स के स्टैक से पास करें: प्रत्येक के लिए `x = block(x)`।
   - फाइनल LayerNorm: `x = self.ln_f(x)`।
   - लीनियर प्रोजेक्शन: `logits = self.lm_head(x)` → आउटपुट आकार `[B, T, vocab_size]`।

   स्निपेट (सरलीकृत):
   ```python
   def forward(self, idx, targets=None):
       # ... embedding + positional
       for block in self.blocks:
           x = block(x)
       x = self.ln_head(x)
       logits = self.head(x)
       # ... loss if targets
       return logits
   ```

2. **ट्रांसफॉर्मर ब्लॉक में फॉरवर्ड (Block.forward)**:
   - इनपुट `x` पर `ln1` लागू करें।
   - सेल्फ-अटेंशन: `x = x + attn(ln1(x))` (रेजिडुअल)।
   - परिणाम पर `ln2` लागू करें।
   - MLP: `x = x + mlp(ln2(x))` (रेजिडुअल)।

   स्निपेट (सरलीकृत):
   ```python
   def forward(self, x):
       x = x + self.attn(self.ln1(x))
       x = x + self.mlp(self.ln2(x))
       return x
   ```

3. **सेल्फ-अटेंशन में फॉरवर्ड (MultiheadAttention.forward)**:
   - LayerNorm इनपुट → Q, K, V प्रोजेक्शन (लीनियर लेयर्स `c_attn`) की गणना करें।
   - हेड्स में विभाजित करें, कॉजल मास्क लागू करें, स्केल्ड डॉट-प्रोडक्ट अटेंशन लागू करें।
   - हेड्स को कॉन्कैट करें, `c_proj` के साथ वापस प्रोजेक्ट करें।
   - रेजिडुअल एड (लेकिन ब्लॉक में, यह बाहर जोड़ा जाता है)।

   मुख्य बिंदु: कॉजल मास्क सुनिश्चित करता है कि भविष्य के टोकन्स को नजरअंदाज किया जाए। आउटपुट आकार `[B, T, n_embd]` के रूप में संरक्षित रहता है।

   स्निपेट (कोर अटेंशन कम्प्यूटेशन, सरलीकृत):
   ```python
   def forward(self, x):
       B, T, C = x.shape
       qkv = self.c_attn(x).split(self.n_embd, dim=2)  # Q,K,V
       # ... reshape to heads, mask, attn = q @ k.transpose / sqrt
       # attn = attn.masked_fill(mask == 0, -1e9)
       # y = attn @ v
       # ... concat and project
       return self.c_proj(y.reshape(B, T, C))
   ```

4. **MLP में फॉरवर्ड (MLP.forward)**:
   - पहली प्रोजेक्शन: `x = self.c_fc(x)` → इंटरमीडिएट साइज़ पर।
   - GELU: `x = F.gelu(x)`।
   - दूसरी प्रोजेक्शन: `x = self.c_proj(x)` → वापस `n_embd` पर।

   स्निपेट:
   ```python
   def forward(self, x):
       x = self.c_fc(x)
       x = F.gelu(x)
       x = self.c_proj(x)
       return x
   ```

यह सेटअप GPT-2 के आर्किटेक्चर के बहुत करीब है, जिसमें डिफ़ॉल्ट कॉन्फ़िग में ~124M पैरामीटर्स हैं। सटीक कोड के लिए, रेपो चेक करें।

[model.py स्रोत](https://github.com/karpathy/nanoGPT/blob/master/model.py)