---
audio: false
generated: true
image: false
lang: hi
layout: post
model: x-ai/grok-4-fast
title: मूल्यांकन के दौरान जीपीयू मेमोरी ओवरफ्लो को ठीक करना
translated: true
type: note
---

आप RTX 4070 (12 GB VRAM) पर एक क्लासिक OOM (आउट-ऑफ-मेमोरी) त्रुटि का सामना कर रहे हैं, जबकि आप लगभग 77M पैरामीटर्स वाले GPT (जो मूल रूप से 125M के लिए था, लेकिन आपने `n_embd` को 576 कर दिया → ~77M) को `batch_size=16`, `block_size=1024`, और `gradient_accumulation_steps=32` के साथ FP16 में ट्रेनिंग दे रहे हैं।

हालांकि 77M छोटा है, लेकिन प्रभावी माइक्रो-बैच बहुत बड़ा हो जाता है क्योंकि nanoGPT `estimate_loss()` में लॉस की गणना कैसे करता है — यह मूल्यांकन बैचों पर पूर्ण फॉरवर्ड/बैकवर्ड पास ग्रेडिएंट एक्यूमुलेशन ट्रिक्स के बिना चलाता है, इसलिए eval के दौरान यह अचानक मेमोरी में बहुत बड़ा प्रभावी बैच डालने की कोशिश करता है।

### मूल कारण
`estimate_loss()` में, nanoGPT डिफ़ॉल्ट रूप से ऐसा करता है:

```python
for _ in range(eval_iters):  # 200 बार!
    x, y = get_batch('train')
    ...
    logits, loss = model(x, y)
```

प्रत्येक `get_batch` `batch_size × block_size` टोकन लौटाता है → 16 × 1024
`eval_iters = 200` के साथ → 200 अलग-अलग फॉरवर्ड पास, लेकिन अगर आप सावधान नहीं हैं तो PyTorch eval लूप के लिए संपूर्ण कम्प्यूटेशन ग्राफ और इंटरमीडिएट एक्टिवेशन्स को मेमोरी में रखता है। विशाल KV कैश (12 layers × 12 heads × 1024 × 576 × 2 bytes FP16 ≈ 1.6 GB सिर्फ कैश के लिए) के साथ मिलकर, प्लस लॉजिट्स (16×1024×50304 × 2 bytes ≈ 1.6 GB प्रति फॉरवर्ड), आप आसानी से 12 GB पार कर जाते हैं भले ही ट्रेनिंग स्वयं ठीक चल रही हो।

### त्वरित समाधान (एक को चुनें या संयोजित करें)

**विकल्प 1: Eval बैच साइज को बहुत कम करें (सबसे तेज़ समाधान)**
अपने कॉन्फ़िग में ये लाइनें जोड़ें (या CLI पर ओवरराइड करें):

```python
eval_batch_size = 4        # इसे जोड़ें
# या 2 भी अगर फिर भी OOM आता है
eval_iters = 100           # थोड़ा और कम कर सकते हैं
```

nanoGPT `eval_batch_size` का सम्मान करता है अगर आप इसे जोड़ते हैं (नए वर्जन करते हैं)। यह मूल्यांकन के दौरान सिर्फ 4 सीक्वेंस का उपयोग करता है 16 के बजाय → eval के दौरान ~4× कम मेमोरी।

**विकल्प 2: Eval के लिए केवल block_size कम करें (सटीकता बनाए रखता है)**
```python
eval_block_size = 512      # या 256
```
इसे कॉन्फ़िग में जोड़ें — कुछ फोर्क्स इसे सपोर्ट करते हैं, या `get_batch` को पैच करें ताकि जब split=='val' हो तो अलग ब्लॉक साइज का उपयोग करे।

**विकल्प 3: Eval_iters को कम करें और थोड़ा अधिक अनिश्चित eval लॉस स्वीकार करें**
```python
eval_iters = 50   # फिर भी उचित अनुमान
```

**विकल्प 4: मॉडल साइज को थोड़ा और कम करें (अभी भी अच्छी क्वालिटी)**
आप पहले से ही 77M पर हैं, लेकिन आप सुरक्षित रूप से और नीचे जा सकते हैं:

```python
n_layer = 12
n_head  = 12
n_embd  = 512     # → ~62M पैरामीटर्स, बहुत सुरक्षित
# या n_embd = 448 → ~53M भी
```

**विकल्प 5: ग्रेडिएंट चेकपॉइंटिंग सक्षम करें (सबसे बड़ा मेमोरी लाभ)**
इसे मॉडल क्रिएशन में जोड़ें (model.py एडिट करें या फ्लैग पास करें):

```python
model = GPT(GPTConfig(..., grad_checkpoint=True))
```

या ट्रेनिंग से पहले train.py में मंकी-पैच करें:

```python
if config.grad_checkpoint:
    model.gradient_checkpointing_enable()
```

यह ट्रेनिंग की गति में ~20-25% की कमी की कीमत पर एक्टिवेशन मेमोरी को ~50-60% कम कर देता है।

### आपके 4070 12 GB के लिए अनुशंसित संयोजन

```python
# config/train_fineweb.py में
batch_size = 16
gradient_accumulation_steps = 32   # प्रभावी 512 सीक्वेंस बनाए रखता है
block_size = 1024

eval_batch_size = 4        # महत्वपूर्ण
eval_iters = 100
eval_interval = 500

n_embd = 512               # ~62M मॉडल, बहुत सुरक्षित
# वैकल्पिक: ग्रेडिएंट चेकपॉइंटिंग जोड़ें
grad_checkpoint = True     # अगर आपका nanoGPT फोर्क इसे सपोर्ट करता है
```

या अगर आप `n_embd` को छूना नहीं चाहते, तो बस यह करें:

```python
eval_batch_size = 2
eval_iters = 100
```

यह अकेले लगभग निश्चित रूप से OOM को ठीक कर देगा।

फ़्रैगमेंटेशन में मदद के लिए इस एनवायरनमेंट वेरिएबल को भी सेट करें:

```bash
export PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True
```

पहले `eval_batch_size=4` (या `=2`) आज़माएं — 12 GB 4070 वाले 95% लोग जो FineWeb पर nanoGPT ट्रेनिंग के दौरान इस सटीक त्रुटि का सामना करते हैं, इसे इस तरह ठीक करते हैं।