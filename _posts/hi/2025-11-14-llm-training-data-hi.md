---
audio: false
generated: true
image: false
lang: hi
layout: post
model: x-ai/grok-4-fast
title: एलएलएम प्रशिक्षण पाठ डेटा सोर्सिंग
translated: true
type: note
---

### बड़े भाषा मॉडलों को प्रशिक्षित करने के लिए बड़े टेक्स्ट डेटा कैसे प्राप्त करें

बड़े भाषा मॉडल (LLM) को प्रशिक्षित करने के लिए विविधतापूर्ण, उच्च-गुणवत्ता वाले टेक्स्ट डेटा की भारी मात्रा की आवश्यकता होती है—अक्सर किताबों, वेबसाइटों, कोड आदि से खरबों टोकन्स। मुख्य चुनौतियाँ हैं पैमाना (टेराबाइट्स से पेटाबाइट्स), गुणवत्ता (शोर, डुप्लिकेट्स और कम-मूल्य वाली सामग्री को फ़िल्टर करना) और वैधता (कॉपीराइट का सम्मान करना, सार्वजनिक डोमेन या लाइसेंस प्राप्त डेटा का उपयोग करना)। यहाँ इसे सोर्स करने के लिए एक चरण-दर-चरण मार्गदर्शिका दी गई है:

1.  **सार्वजनिक वेब क्रॉल से शुरुआत करें**: ये अधिकांश LLM प्रशिक्षण की रीढ़ हैं। ये इंटरनेट की स्नैपशॉट कैप्चर करते हैं।
    - CC-Net या Dedup (हगिंग फेस के माध्यम से Python लाइब्रेरीज़) जैसे टूल का उपयोग करके साफ़ टेक्स्ट के लिए फ़िल्टर करें।
    - आकार को हैंडल करने के लिए चंक्स में प्रोसेस करें—डाउनलोड के लिए क्लाउड स्टोरेज (जैसे, AWS S3) का उपयोग करें।

2.  **क्यूरेटेड डेटासेट का उपयोग करें**: रिसर्च ग्रुप्स से पहले से फ़िल्टर किए गए संग्रह। API या डायरेक्ट लिंक के माध्यम से डाउनलोड करें।
    - अपनी आवश्यकताओं के अनुरूप बहुभाषी, डोमेन-विशिष्ट (जैसे, कोड, विज्ञान) सबसेट पर ध्यान केंद्रित करें।
    - हगिंग फेस डेटासेट लाइब्रेरी जैसे टूल लोडिंग को आसान बनाते हैं: `from datasets import load_dataset`.

3.  **डोमेन-विशिष्ट स्रोतों के साथ पूरक करें**:
    - किताबें: प्रोजेक्ट गुटेनबर्ग (सार्वजनिक डोमेन)।
    - विकिपीडिया: भाषा डंप।
    - कोड: GitHub आर्काइव (BigCode के माध्यम से)।
    - सिंथेटिक डेटा जनरेट करें: रीजनिंग चेन बनाने के लिए मौजूदा मॉडल (जैसे, OpenAI API के माध्यम से) का उपयोग करें, लेकिन दूषित होने से बचने के लिए इसे साफ़ करें।

4.  **कानूनी और नैतिक सुझाव**:
    - ओपन लाइसेंस (जैसे, CC-BY, MIT) पर टिके रहें।
    - डीडुप्लिकेट (MinHash जैसे टूल) करें और PII (व्यक्तिगत जानकारी) हटाएं।
    - कस्टम ट्रेनिंग के लिए, स्केलिंग से पहले छोटे (जैसे, 1-10GB पर फाइन-ट्यून) से शुरुआत करें।
    - कंप्यूटेशनल लागत: मामूली प्रशिक्षण के लिए भी 100s की GPU-घंटों की उम्मीद करें; टेस्टिंग के लिए Colab या RunPod का उपयोग करें।

5.  **प्रोसेसिंग पाइपलाइन**:
    - डाउनलोड → सफाई (HTML, गैर-टेक्स्ट हटाएं) → टोकनाइज़ (जैसे, TikToken के साथ) → प्रशिक्षण।
    - लाइब्रेरीज़: सैंपलिंग के लिए Pandas, प्रीप्रोसेसिंग के लिए spaCy/NLTK.

सार्वजनिक डेटासेट मुफ्त और विशाल हैं—शौक़ीनों या शोधकर्ताओं के लिए आदर्श। प्रोडक्शन के लिए, कंपनियाँ अक्सर मालिकाना डेटा को लाइसेंस देती हैं।

### विशिष्ट मॉडलों के लिए प्रशिक्षण डेटा स्रोत

OpenAI, Anthropic, और DeepSeek जैसे मालिकाना मॉडल प्रतिस्पर्धात्मक कारणों से सटीक रेसिपी गुप्त रखते हैं, लेकिन उन्होंने पेपर्स, ब्लॉग्स और लीक्स के माध्यम से उच्च-स्तरीय विवरण साझा किए हैं। ओपन-सोर्स मॉडल (जैसे, Llama, Mistral) अधिक पारदर्शी हैं, जो अक्सर डेटासेट ब्लूप्रिंट जारी करते हैं।

-   **OpenAI के GPT मॉडल (जैसे, GPT-4o)**:
    वे सार्वजनिक रूप से उपलब्ध इंटरनेट डेटा (फ़िल्टर किए गए वेब क्रॉल), किताबों, लेखों और कोड के मिश्रण पर प्रशिक्षण देते हैं। शुरुआती GPT ने Common Crawl का भारी उपयोग किया; बाद वाले उच्च-गुणवत्ता वाले STEM/कोडिंग स्रोतों पर जोर देते हैं। कुल: खरबों टोकन, भारी डीडुप्लिकेशन के साथ। वे लाइसेंस प्राप्त डेटा और उपयोगकर्ता इंटरैक्शन (ऑप्ट-आउट के साथ) भी शामिल करते हैं। कोई पूर्ण सार्वजनिक रिलीज़ नहीं, लेकिन यह संक्षेप में "पूरा इंटरनेट" है—स्क्रैप किया गया, फ़िल्टर किया गया और संवर्धित।

-   **Anthropic के मॉडल (जैसे, Claude 3.5)**:
    सुरक्षित, मददगार डेटा पर ध्यान दें: सार्वजनिक वेब टेक्स्ट, किताबें, और संरेखन के लिए जनरेट किए गए सिंथेटिक उदाहरण (जैसे, Constitutional AI)। वे Claude से उपयोगकर्ता चैट (ऑप्ट-आउट उपलब्ध) और HH-RLHF जैसे RLHF डेटासेट का उपयोग करते हैं। विविध, गैर-विषैले स्रोतों पर जोर; स्क्रैप किए गए YouTube ट्रांसक्रिप्ट्स पर कुछ विवाद। कुल पैमाना: समान खरबों, लेकिन नैतिकता के लिए अधिक क्यूरेट किया गया।

-   **DeepSeek मॉडल (जैसे, DeepSeek-V3, R1)**:
    चीनी ओपन-इश मॉडल जो सादे वेब पेजों, ई-बुक्स और कोड रेपो का उपयोग करते हैं। V3 को 14.8T टोकन पर जानबूझकर सिंथेटिक डेटा के बिना प्री-ट्रेन किया गया था, लेकिन R1 रिजेक्शन सैंपलिंग के माध्यम से 600K सिंथेटिक रीजनिंग सैंपल जोड़ता है (पिछले मॉडलों द्वारा जनरेट किया गया)। स्रोत: वेब क्रॉल + तकनीकी दस्तावेज; मालिकाना मिश्रण, लेकिन पेपर्स में पारदर्शी।

-   **ओपन-सोर्स मॉडल (जैसे, Llama 3, BLOOM, GPT-J)**:
    ये स्पष्ट रूप से The Pile (800GB बहुभाषी मिश्रण), C4 (Colossal Clean Crawled Corpus, 750GB अंग्रेजी वेब), या OSCAR (बहुभाषी Common Crawl) जैसे सार्वजनिक डेटासेट का उपयोग करते हैं। BLOOM ने ROOTS (1.6TB, 46 भाषाएँ) का उपयोग किया। वे मालिकाना डेटा से बचते हैं, पुनरुत्पादन पर ध्यान केंद्रित करते हैं—सटीक ब्रेकडाउन के लिए हगिंग फेस पर मॉडल कार्ड्स देखें।

संक्षेप में: सभी वेब-स्केल डेटा पर निर्भर करते हैं, लेकिन मालिकाना मॉडल गुणवत्ता के लिए फ़िल्टरिंग/लाइसेंसिंग/सिंथेटिक्स जोड़ते हैं। ओपन-सोर्स समुदाय-क्यूरेटेड सार्वजनिक डेटा पर निर्भर करते हैं।

### बड़े सार्वजनिक टेक्स्ट डेटासेट के लिए डाउनलोड लिंक

यहाँ शीर्ष मुफ्त, डाउनलोड करने योग्य स्रोत दिए गए हैं (आकार अनुमानित; अपडेट के लिए जाँचें)। यदि स्टोरेज सीमित है तो सबसेट के साथ शुरुआत करें।

-   **Common Crawl**: मासिक वेब स्नैपशॉट (कुल पेटाबाइट्स)। CC-MAIN इंडेक्स के साथ फ़िल्टर करें। [Common Crawl Archives](https://commoncrawl.org/get-started)
-   **The Pile**: 800GB विविध अंग्रेजी टेक्स्ट (किताबें, कोड, arXiv, आदि)। [EleutherAI The Pile on Hugging Face](https://huggingface.co/datasets/EleutherAI/pile)
-   **C4 (Colossal Clean Crawled Corpus)**: 750GB साफ़ की गई अंग्रेजी वेब (T5/GPT के लिए उपयोग किया गया)। [TensorFlow Datasets C4](https://www.tensorflow.org/datasets/catalog/c4)
-   **OSCAR (Open Super-large Crawled Aggregated coRpus)**: बहुभाषी वेब (22 भाषाएँ, ~10TB)। [OSCAR on Hugging Face](https://huggingface.co/datasets/oscar-corpus/OSCAR-2201)
-   **Wikipedia Dumps**: फुल टेक्स्ट एक्स्ट्रैक्ट्स (अंग्रेजी: ~20GB)। [Wikimedia Downloads](https://dumps.wikimedia.org/enwiki/latest/)
-   **BooksCorpus/OpenWebText**: 11GB किताबें + 40GB Reddit/वेब (GPT-2 युग)। [OpenWebText on GitHub](https://github.com/jcpeterson/openwebtext)
-   **RedPajama**: 1T+ टोकन, Llama पेपर्स से रेप्लिकेट किया गया। [TogetherAI RedPajama on HF](https://huggingface.co/datasets/togethercomputer/RedPajama-Data-1T)
-   **LLMDataHub**: 100+ डेटासेट (चैट, कोड, आदि) की क्यूरेटेड सूची। [GitHub LLMDataHub](https://github.com/Zjh-819/LLMDataHub)

अधिक के लिए, हगिंग फेस डेटासेट हब ब्राउज़ करें: [Hugging Face Datasets](https://huggingface.co/datasets)। लाइसेंस हमेशा सत्यापित करें!

**संदर्भ**  
[ChatGPT और हमारे फाउंडेशन मॉडल कैसे विकसित किए जाते हैं](https://help.openai.com/en/articles/7842364-how-chatgpt-and-our-language-models-are-developed)  
[GPT-oss OpenAI के ट्रेनिंग डेटा के बारे में क्या लीक करता है](https://www.lesswrong.com/posts/iY9584TRhqrzawhZg/what-gpt-oss-leaks-about-openai-s-training-data)  
[Anthropic प्रशिक्षण डेटा के लिए Claude चैट्स का उपयोग करेगा](https://www.wired.com/story/anthropic-using-claude-chats-for-training-how-to-opt-out/)  
[Anthropic का HH-RLHF डेटासेट](https://huggingface.co/datasets/Anthropic/hh-rlhf)  
[DeepSeek-R1 के प्रशिक्षण में क्या गया?](https://epoch.ai/gradient-updates/what-went-into-training-deepseek-r1)  
[DeepSeek मॉडलों की संपूर्ण मार्गदर्शिका](https://www.bentoml.com/blog/the-complete-guide-to-deepseek-models-from-v3-to-r1-and-beyond)  
[LLMDataHub: LLM प्रशिक्षण के लिए Awesome डेटासेट](https://github.com/Zjh-819/LLMDataHub)  
[LLM प्रशिक्षण के लिए ओपन-सोर्स डेटासेट](https://kili-technology.com/large-language-models-llms/9-open-sourced-datasets-for-training-large-language-models)  
[LLM प्रशिक्षण डेटा: 8 मुख्य सार्वजनिक डेटा स्रोत](https://oxylabs.io/blog/llm-training-data)