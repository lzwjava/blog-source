---
audio: false
generated: true
lang: hi
layout: post
title: डीपसीक वी3
translated: true
type: note
---

### अवलोकन और प्रमुख विशेषताएं
1. मॉडल नाम: DeepSeek-V3, एक Mixture-of-Experts (MoE) भाषा मॉडल जिसमें 671 बिलियन पैरामीटर हैं, जिनमें से 37 बिलियन प्रति टोकन सक्रिय होते हैं।
2. प्रशिक्षण डेटासेट: 14.8 ट्रिलियन विविध, उच्च-गुणवत्ता वाले टोकन पर प्री-ट्रेन किया गया।
3. मुख्य नवाचार: दक्षता के लिए Multi-Head Latent Attention (MLA) और DeepSeekMoE आर्किटेक्चर को शामिल करता है, जिसमें सहायक-हानि-मुक्त लोड संतुलन शामिल है।
4. प्रशिक्षण दक्षता: केवल 2.788 मिलियन H800 GPU घंटों में पूर्ण प्रशिक्षण प्राप्त किया।
5. लागत दक्षता: प्रशिक्षण लागत का अनुमान 5.576M अमरीकी डालर है, जो प्रति GPU घंटा 2 अमरीकी डालर मानकर लगाया गया है।

---

### आर्किटेक्चरल नवाचार
6. ट्रांसफॉर्मर-आधारित फ्रेमवर्क: स्केलेबिलिटी और लचीलेपन के लिए ट्रांसफॉर्मर आर्किटेक्चर को बरकरार रखता है।
7. Multi-Head Latent Attention (MLA): प्रदर्शन हानि के बिना की-वैल्यू कैश को संपीड़ित करके इनफेरेंस मेमोरी को कम करता है।
8. DeepSeekMoE: लागत-प्रभावी प्रशिक्षण और उच्च कम्प्यूटेशनल दक्षता के लिए साझा और रूटेड विशेषज्ञों के संयोजन का उपयोग करता है।
9. सहायक-हानि-मुक्त लोड संतुलन: प्रदर्शन से समझौता किए बिना संतुलित विशेषज्ञ लोड बनाए रखने के लिए बायस टर्म्स पेश करता है।
10. Multi-Token Prediction (MTP): डेटा दक्षता और प्रतिनिधित्व पूर्व-योजना में सुधार के लिए प्रति पोजीशन कई टोकन का क्रमिक रूप से पूर्वानुमान लगाता है।

---

### प्रशिक्षण फ्रेमवर्क
11. FP8 मिश्रित परिशुद्धता प्रशिक्षण: मेमोरी और कम्प्यूटेशन को अनुकूलित करने के लिए सूक्ष्म-अंकित परिमाणीकरण और कम-परिशुद्धता संग्रहण का लाभ उठाता है।
12. DualPipe एल्गोरिदम: कम्प्यूटेशन और कम्युनिकेशन चरणों को ओवरलैप करता है, पाइपलाइन बबल्स को कम करता है और समानांतरता में सुधार करता है।
13. कुशल क्रॉस-नोड संचार: NVLink और InfiniBand बैंडविड्थ का उपयोग करते हुए, all-to-all ऑपरेशन के लिए अनुकूलित कर्नेल को नियोजित करता है।
14. कम-परिशुद्धता ऑप्टिमाइज़र स्टेट्स: प्रदर्शन हानि के बिना मेमोरी खपत को कम करने के लिए ऑप्टिमाइज़र स्टेट्स को BF16 में संग्रहीत करता है।
15. मेमोरी ऑप्टिमाइज़ेशन तकनीकें: मेमोरी बचाने के लिए बैक-प्रोपेगेशन के दौरान कुछ ऑपरेशन (जैसे, RMSNorm) की पुनर्गणना करता है।

---

### प्री-ट्रेनिंग विवरण
16. स्थिर प्रशिक्षण प्रक्रिया: प्री-ट्रेनिंग के दौरान कोई भी अपूरणीय लॉस स्पाइक या रोलबैक नहीं हुआ।
17. संदर्भ लंबाई विस्तार: संदर्भ लंबाई को दो चरणों में 32K और बाद में 128K तक बढ़ाया गया।
18. प्रशिक्षण लागत: प्री-ट्रेनिंग के लिए 2.664M GPU घंटे, संदर्भ विस्तार के लिए 119K GPU घंटे और पोस्ट-ट्रेनिंग के लिए 5K GPU घंटे की आवश्यकता थी।
19. टोकन दक्षता: प्रति ट्रिलियन टोकन GPU घंटों को कम से कम करके प्रशिक्षण दक्षता सुनिश्चित की गई।
20. उच्च-गुणवत्ता वाला डेटा: प्री-ट्रेनिंग डेटासेट विविधता और प्रासंगिकता के लिए तैयार किया गया।

---

### पोस्ट-ट्रेनिंग संवर्द्धन
21. Supervised Fine-Tuning (SFT): मॉडल आउटपुट को मानवीय प्राथमिकताओं के साथ संरेखित करता है।
22. Reinforcement Learning (RL): फाइन-ट्यूनिंग के लिए Group Relative Policy Optimization को नियोजित करता है।
23. Knowledge Distillation: DeepSeek-R1 मॉडल से तर्क क्षमताओं को एकीकृत करता है।
24. आउटपुट स्टाइल नियंत्रण: जनरेशन लंबाई और शैली के साथ सटीकता को संतुलित करता है।
25. प्रदर्शन परिष्करण: पोस्ट-ट्रेनिंग बेंचमार्क परिणामों को और सुधारती है।

---

### बेंचमार्क प्रदर्शन
26. MMLU (शैक्षिक बेंचमार्क): 88.5 प्राप्त करता है, अन्य ओपन-सोर्स मॉडलों को पार करता है।
27. GPQA (सामान्य ज्ञान): 59.1 स्कोर करता है, जो GPT-4o और Claude-3.5-Sonnet के बराबर है।
28. गणित बेंचमार्क: गणितीय तर्क कार्यों में अत्याधुनिक प्रदर्शन।
29. कोड प्रतियोगिताएं: LiveCodeBench जैसे कोडिंग बेंचमार्क में उत्कृष्ट प्रदर्शन।
30. तथ्यात्मक ज्ञान: अंग्रेजी और चीनी तथ्यात्मकता बेंचमार्क में श्रेष्ठ परिणाम प्रदर्शित करता है।

---

### इनफेरेंस और डिप्लॉयमेंट
31. प्रीफिलिंग स्टेज: दक्षता के लिए टेंसर समानांतरता (TP4), सीक्वेंस समानांतरता (SP), और विशेषज्ञ समानांतरता (EP32) को जोड़ता है।
32. डिकोडिंग स्टेज: कम-विलंबता संचार के लिए EP320 के साथ IBGDA का उपयोग करता है।
33. डायनामिक रिडंडेंसी: संसाधन उपयोग को अनुकूलित करने के लिए विशेषज्ञ लोड को गतिशील रूप से समायोजित करता है।
34. चरणों का पृथक्करण: थ्रूपुट बढ़ाने के लिए प्रीफिलिंग और डिकोडिंग चरणों को अलग किया गया है।
35. हार्डवेयर उपयोग: NVLink और InfiniBand इंटरकनेक्ट्स के साथ H800 GPU के लिए अनुकूलित।

---

### लोड बैलेंसिंग और डिकोडिंग में नवाचार
36. बायस-आधारित रूटिंग: गतिशील रूप से संतुलित विशेषज्ञ लोड सुनिश्चित करने के लिए बायस टर्म्स पेश करता है।
37. स्पेक्युलेटिव डिकोडिंग: MTP मॉड्यूल का उपयोग करके जनरेशन विलंबता को बढ़ाता है।
38. रिडंडेंट एक्सपर्ट्स: GPU वर्कलोड को संतुलित करने के लिए उच्च-लोड विशेषज्ञों को डुप्लिकेट करता है।
39. नोड-सीमित रूटिंग: संचार ओवरहेड को कम करने के लिए टोकन रूटिंग को अधिकतम 4 नोड्स तक सीमित करता है।
40. कोई टोकन ड्रॉपिंग नहीं: सुनिश्चित करता है कि प्रशिक्षण और इनफेरेंस के दौरान सभी टोकन बरकरार रखे जाएं।

---

### तकनीकी विवरण
41. क्लस्टर कॉन्फ़िगरेशन: 2048 NVIDIA H800 GPU वाले क्लस्टर पर प्रशिक्षित।
42. पाइपलाइन समानांतरता: स्केलेबिलिटी के लिए 16-तरफा समानांतरता योजना को नियोजित करता है।
43. मेमोरी फुटप्रिंट: मेमोरी उपयोग को अनुकूलित करके महंगी टेंसर समानांतरता से बचता है।
44. कस्टम कर्नेल: क्रॉस-नोड ऑपरेशन को कुशलतापूर्वक संभालने के लिए विशेष संचार कर्नेल विकसित करता है।
45. मिश्रित परिशुद्धता अनुकूलन: इष्टतम प्रशिक्षण गतिशीलता के लिए FP8 और BF16 फॉर्मेट को जोड़ता है।

---

### मूल्यांकन और परिणाम
46. व्यापक बेंचमार्क: शिक्षा, कोडिंग और तर्क सहित विविध डोमेन में मूल्यांकन किया गया।
47. ओपन-सोर्स नेतृत्व: अपनी श्रेणी में सबसे मजबूत ओपन-सोर्स बेस मॉडल के रूप में उभरता है।
48. क्लोज्ड-सोर्स मॉडल के साथ तुलना: प्रदर्शन GPT-4o और Claude-3.5-Sonnet के बराबर है।
49. चीनी ज्ञान में मजबूती: चीनी तथ्यात्मकता बेंचमार्क में अग्रणी मॉडलों से बेहतर प्रदर्शन करता है।
50. लंबे संदर्भ को संभालना: विस्तारित संदर्भ प्रसंस्करण वाले कार्यों में उत्कृष्ट प्रदर्शन।

---

### भविष्य की दिशाएं
51. डायनामिक रिडंडेंसी अन्वेषण: अधिक अनुकूली रिडंडेंसी रणनीतियों की जांच करना।
52. स्पेक्युलेटिव डिकोडिंग विस्तार: इनफेरेंस त्वरण के लिए MTP के और उपयोगों की खोज करना।
53. हार्डवेयर सह-डिजाइन: बेहतर प्रदर्शन के लिए अगली पीढ़ी के GPU के अनुकूल होना।
54. व्यापक बेंचमार्क कवरेज: मूल्यांकन को अधिक विविध कार्यों तक विस्तारित करना।
55. स्थिरता: एल्गोरिदम और हार्डवेयर अनुकूलन के माध्यम से प्रशिक्षण लागत को और कम करना।

---

यह दस्तावेज़ DeepSeek-V3 का एक व्यापक सारांश प्रदान करता है, जिसमें इसके आर्किटेक्चर, प्रशिक्षण पद्धतियों, बेंचमार्क प्रदर्शन और भविष्य की संभावनाओं को शामिल किया गया है। यदि आपको विशिष्ट अनुभागों या अतिरिक्त बिंदुओं पर और विस्तार की आवश्यकता है, तो मुझे बताएं!