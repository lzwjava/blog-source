---
audio: false
generated: true
image: false
lang: hi
layout: post
model: x-ai/grok-4-fast
title: ट्रांसफॉर्मर परत संख्या का प्रदर्शन पर प्रभाव
translated: true
type: note
---

NanoGPT में, ट्रांसफॉर्मर ब्लॉक्स की संख्या (`n_layer` पैरामीटर द्वारा नियंत्रित) मॉडल की गहराई तय करती है, जो भाषा डेटा में पदानुक्रमित प्रस्तुतिकरण सीखने की इसकी क्षमता को सीधे प्रभावित करती है। मानक कॉन्फ़िगरेशन हल्के-फुल्के शेक्सपियर कैरेक्टर-लेवल उदाहरण (मिनटों में सिंगल GPU पर फिट होने के लिए) के लिए 6 लेयर्स और GPT-2 124M मॉडल को रिप्रोड्यूस करने (दिनों तक मल्टी-GPU सेटअप पर) के लिए 12 लेयर्स का उपयोग करते हैं। इसे घटाकर 4 या 1 लेयर करने से एक उथला मॉडल बनता है जो ट्रेन करने में तेज़ और कम मेमोरी का उपयोग करता है, लेकिन इसकी कीमत परफॉर्मेंस से चुकानी पड़ती है—आमतौर पर इसके परिणामस्वरूप हायर वैलिडेशन लॉस, अंडरफिटिंग और लो-क्वालिटी टेक्स्ट जनरेशन होता है।

### कम लेयर्स के मुख्य प्रभाव
- **मॉडल क्षमता और प्रदर्शन**: प्रत्येक ट्रांसफॉर्मर ब्लॉक सेल्फ-अटेंशन और फीडफॉरवर्ड लेयर्स जोड़ता है जो तेजी से अमूर्त फीचर्स (जैसे, टोकन से सिंटैक्स से सेमेंटिक्स तक) का निर्माण करते हैं। कम ब्लॉक इस स्टैकिंग को सीमित करते हैं, इसलिए मॉडल को जटिल पैटर्न से जूझना पड़ता है। शेक्सपियर डेटासेट पर:
  - 6 लेयर्स (डिफॉल्ट): A100 GPU पर ~3 मिनट के बाद ~1.47 वैलिडेशन लॉस; सुसंगत लेकिन अपूर्ण शेक्सपियर-जैसा टेक्स्ट जनरेट करता है (जैसे, "To be or not to be...")।
  - 4 लेयर्स: CPU पर ~3 मिनट के बाद ~1.88 वैलिडेशन लॉस (संभव बनाने के लिए स्केल-डाउन एम्बेडिंग/हेड्स के साथ); सैंपल अधिक शोरयुक्त और कम संरचित होते हैं (जैसे, "GLEORKEN VINGHARD III: Whell's the couse..."), जो "सही कैरेक्टर जेस्टाल्ट का एक संकेत" दिखाते हैं लेकिन आउटपुट अधिक गड़बड़ होता है।
  - 1 लेयर: NanoGPT डॉक्स या सामान्य प्रयोगों में कोई सीधे बेंचमार्क नहीं, लेकिन स्केलिंग ट्रेंड्स के आधार पर और भी अधिक लॉस (~2.0+) और आदिम जनरेशन की उम्मीद करें—अनिवार्य रूप से एक सिंगल अटेंशन + MLP पास, बेसिक n-gram-जैसी प्रेडिक्शन के टॉय डेमो के लिए अच्छा लेकिन सूक्ष्म भाषा मॉडलिंग पर फेल होना। यह शॉर्ट सीक्वेंस पर जल्दी ओवरफिट हो सकता है लेकिन खराब जनरलाइज करता है।

- **प्रशिक्षण और संसाधन प्रभाव**:
  - **गति/मेमोरी**: 4 लेयर्स, समान हार्डवेयर पर 6 लेयर्स की तुलना में ट्रेनिंग टाइम को ~20-30% काटती है और मेमोरी उपयोग को आधा कर देती है (जैसे, आसानी से CPU या लो-एंड GPU पर फिट हो जाती है)। 1 लेयर इससे भी ~50-70% तेज होगी, सेकंडों में CPU पर ट्रेन करने योग्य।
  - **हाइपरपैरामीटर्स**: आप अक्सर कम लेयर्स को छोटे `n_embd` (जैसे, 128-256) और `n_head` (जैसे, 4) के साथ जोड़ेंगे ताकि अंडरयूटिलाइजेशन से बचा जा सके, साथ ही कम इटरेशन (जैसे, 1000-2000) क्योंकि मॉडल तेजी से कन्वर्ज होता है लेकिन पहले पठार पर पहुंच जाता है।
  - **स्केलिंग व्यवहार**: प्रयोग दिखाते हैं कि लॉस लेयर्स के साथ लॉगरिदमिक रूप से सुधरता है (जैसे, 6 लेयर्स: ~1.78 वैल लॉस; 8: ~1.73; 12: ~1.73 समान सेटअप पर)। गहरे मॉडल (12+ लेयर्स) OpenWebText जैसे बड़े डेटासेट पर बेहतर प्रदर्शन करते हैं, GPT-2 रिप्रोडक्शन के लिए ~2.85 वैल लॉस हिट करते हैं।

### NanoGPT में प्रयोग कैसे करें
इसका परीक्षण करने के लिए, `config/train_shakespeare_char.py` (या अपनी कॉन्फ़िग फ़ाइल) को संशोधित करें और चलाएँ:
```
python train.py config/train_shakespeare_char.py --n_layer=4 --n_head=4 --n_embd=128 --max_iters=2000
```
1 लेयर के लिए: `--n_layer=1 --n_head=2 --n_embd=64` (इसे हल्का रखने के लिए और स्केल किया गया)। टेक्स्ट जनरेट करने और तुलना करने के लिए आउटपुट चेकपॉइंट पर `sample.py` का उपयोग करें।

संक्षेप में, 4 लेयर्स त्वरित प्रोटोटाइपिंग या कम-संसाधन रन (अभी भी उपयोगी आउटपुट) के लिए एक व्यावहारिक कटौती है, जबकि 1 लेयर प्रभावी होने से अधिक शैक्षिक है—ब्लॉक की भूमिका को समझने के लिए बढ़िया लेकिन गंभीर मॉडलिंग के लिए नहीं। यदि आप GPT-2 की 12 लेयर्स को टारगेट कर रहे हैं, तो 4 पर गिरने से बेंचमार्क पर लॉस लगभग दोगुना और सुसंगतता आधी हो जाएगी।

[karpathy/nanoGPT GitHub](https://github.com/karpathy/nanoGPT)  
[VizuaraAI/nano-gpt-oss GitHub](https://github.com/VizuaraAI/nano-gpt-oss)