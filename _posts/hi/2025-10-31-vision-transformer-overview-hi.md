---
audio: false
generated: true
image: false
lang: hi
layout: post
model: x-ai/grok-4-fast
title: विज़न ट्रांसफॉर्मर का अवलोकन
translated: true
type: note
---

### विजन ट्रांसफॉर्मर (ViT) का परिचय

विजन ट्रांसफॉर्मर (ViT) एक न्यूरल नेटवर्क आर्किटेक्चर है जो ट्रांसफॉर्मर मॉडल—जो मूल रूप से नेचुरल लैंग्वेज प्रोसेसिंग (NLP) के लिए डिज़ाइन किया गया था—को कंप्यूटर विज़न टास्क्स, विशेष रूप से इमेज क्लासिफिकेशन के लिए अनुकूलित करता है। 2020 में एलेक्सी डोसोवित्सकी और गूगल के उनके सहयोगियों द्वारा पेपर *An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale* में पेश किया गया, ViT इमेज को पारंपरिक कन्वोल्यूशनल न्यूरल नेटवर्क (CNN) से प्रोसेस करने के बजाय पैच के अनुक्रम के रूप में मानता है। यह इसे लंबी दूरी के डिपेंडेंसी और समानांतर कम्प्यूटेशन को हैंडल करने में ट्रांसफॉर्मर की ताकत का लाभ उठाने की अनुमति देता है।

ViT ने बड़े पैमाने के डेटासेट जैसे ImageNet पर CNN के मुकाबले प्रतिस्पर्धी या बेहतर प्रदर्शन दिखाया है, खासकर जब बड़ी मात्रा में डेटा (जैसे, JFT-300M) पर प्री-ट्रेन किया गया हो। DeiT (डेटा-एफिशिएंट इमेज ट्रांसफॉर्मर्स) जैसे वेरिएंट इसे छोटे डेटासेट के लिए अधिक कुशल बनाते हैं। आज, ViT से प्रेरित मॉडल DALL-E, स्टेबल डिफ्यूज़न, और आधुनिक क्लासिफायर जैसे मॉडलों में कई विज़न टास्क्स को पावर देते हैं।

### ViT कैसे काम करता है: समग्र आर्किटेक्चर और वर्कफ़्लो

ViT का मूल विचार एक इमेज को फिक्स्ड-साइज़ पैच के अनुक्रम में "टोकनाइज़" करना है, ठीक उसी तरह जैसे टेक्स्ट को शब्दों या टोकन में तोड़ा जाता है। इस अनुक्रम को फिर एक स्टैंडर्ड ट्रांसफॉर्मर एनकोडर द्वारा प्रोसेस किया जाता है (जनरेटिव टेक्स्ट मॉडल के विपरीत, कोई डिकोडर नहीं)। यहां बताया गया है कि यह कैसे काम करता है:

1.  **इमेज प्रीप्रोसेसिंग और पैच एक्सट्रैक्शन**:
    *   साइज़ \\(H \times W \times C\\) (ऊंचाई × चौड़ाई × चैनल, उदा. RGB के लिए 224 × 224 × 3) की एक इनपुट इमेज से शुरुआत करें।
    *   इमेज को फिक्स्ड साइज़ \\(P \times P\\) (उदा. 16 × 16 पिक्सेल) के नॉन-ओवरलैपिंग पैच में विभाजित करें। इससे \\(N = \frac{HW}{P^2}\\) पैच प्राप्त होते हैं (उदा. 224×224 इमेज और 16×16 पैच के लिए 196 पैच)।
    *   प्रत्येक पैच को लंबाई \\(P^2 \cdot C\\) (उदा. 16×16×3 के लिए 768 डायमेंशन) के 1D वेक्टर में बदल दिया जाता है।
    *   पैच क्यों? रॉ पिक्सेल एक अव्यावहारिक रूप से लंबा अनुक्रम बना देंगे (उदा. हाई-रेस इमेज के लिए लाखों), इसलिए पैच डायमेंशनैलिटी कम करने के लिए "विजुअल वर्ड्स" का काम करते हैं।

2.  **पैच एम्बेडिंग**:
    *   प्रत्येक फ्लैटन किए गए पैच वेक्टर पर एक लर्नेबल लीनियर प्रोजेक्शन (एक साधारण फुली कनेक्टेड लेयर) लागू करें, जो इसे एक फिक्स्ड एम्बेडिंग डायमेंशन \\(D\\) (उदा. 768, BERT जैसे ट्रांसफॉर्मर से मेल खाता हुआ) पर मैप करता है।
    *   यह \\(N\\) एम्बेडिंग वेक्टर उत्पन्न करता है, प्रत्येक का आकार \\(D\\)।
    *   वैकल्पिक रूप से, क्लासिफिकेशन टास्क के लिए BERT की तरह, अनुक्रम की शुरुआत में एक विशेष [CLS] टोकन एम्बेडिंग (आकार \\(D\\) का एक लर्नेबल वेक्टर) जोड़ें।

3.  **पोजिशनल एम्बेडिंग्स**:
    *   स्पेशियल इनफॉर्मेशन को एनकोड करने के लिए पैच एम्बेडिंग में लर्नेबल 1D पोजिशनल एम्बेडिंग जोड़ें (इसके बिना ट्रांसफॉर्मर पर्म्यूटेशन-इनवेरिएंट होते हैं)।
    *   अब पूरा इनपुट अनुक्रम है: \\([ \text{[CLS]}, \text{patch}_1, \text{patch}_2, \dots, \text{patch}_N ] + \text{positions}\\), आकार \\((N+1) \times D\\) का एक मैट्रिक्स।

4.  **ट्रांसफॉर्मर एनकोडर ब्लॉक्स**:
    *   अनुक्रम को \\(L\\) स्टैक्ड ट्रांसफॉर्मर एनकोडर लेयर्स (उदा. 12 लेयर्स) में फीड करें।
    *   प्रत्येक लेयर में शामिल हैं:
        *   **मल्टी-हेड सेल्फ-अटेंशन (MSA)**: सभी जोड़े पैच (जिसमें [CLS] शामिल है) के बीच अटेंशन स्कोर की गणना करता है। यह मॉडल को ग्लोबल रिलेशनशिप कैप्चर करने की अनुमति देता है, जैसे "इस बिल्ली के कान का संबंध 100 पैच दूर व्हिस्कर से है," जो कि CNN के लोकल रिसेप्टिव फील्ड के विपरीत है।
            *   फॉर्मूला: Attention(Q, K, V) = \\(\text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right) V\\), जहां Q, K, V इनपुट के प्रोजेक्शन हैं।
        *   **मल्टी-लेयर पर्सेप्ट्रॉन (MLP)**: एक फीड-फॉरवर्ड नेटवर्क (GELU एक्टिवेशन के साथ दो लीनियर लेयर्स) जिसे पोजिशन-वाइज लागू किया जाता है।
        *   लेयर नॉर्मलाइज़ेशन और रेज़िडुअल कनेक्शन: इनपुट + MSA → Norm → MLP → Norm + इनपुट।
    *   आउटपुट: परिष्कृत एम्बेडिंग का एक अनुक्रम, फिर भी \\((N+1) \times D\\)।

5.  **क्लासिफिकेशन हेड**:
    *   इमेज क्लासिफिकेशन के लिए, [CLS] टोकन के आउटपुट को निकालें (या सभी पैच एम्बेडिंग का माध्य लें)।
    *   इसे क्लास लॉजिट आउटपुट करने के लिए एक साधारण MLP हेड (उदा. एक या दो लीनियर लेयर्स) से पास करें।
    *   ट्रेनिंग के दौरान, लेबल्ड डेटा पर क्रॉस-एन्ट्रॉपी लॉस का उपयोग करें। प्री-ट्रेनिंग में अक्सर मास्क्ड पैच प्रिडिक्शन या अन्य सेल्फ-सुपरवाइज्ड टास्क शामिल होते हैं।

**मुख्य हाइपरपैरामीटर्स** (मूल ViT-Base मॉडल से):
*   पैच साइज़ \\(P\\): 16
*   एम्बेडिंग डायमेंशन \\(D\\): 768
*   लेयर्स \\(L\\): 12
*   हेड्स: 12
*   पैरामीटर्स: ~86M

ViT अच्छी तरह से स्केल करता है: बड़े मॉडल (उदा. ViT-Large with \\(D=1024\\), \\(L=24\\)) बेहतर प्रदर्शन करते हैं लेकिन अधिक डेटा/कम्प्यूट की आवश्यकता होती है।

**ट्रेनिंग और इनफेरेंस**:
*   **ट्रेनिंग**: लेबल्ड डेटा पर एंड-टू-एंड; अरबों इमेज पर प्री-ट्रेनिंग से बहुत लाभ मिलता है।
*   **इनफेरेंस**: एनकोडर के माध्यम से फॉरवर्ड पास (अटेंशन के कारण ~O(N²) टाइम, लेकिन FlashAttention जैसे ऑप्टिमाइज़ेशन के साथ कुशल)।
*   CNN के विपरीत, ViT में ट्रांसलेशन इनवेरिएंस जैसे कोई इंडक्टिव बायस नहीं हैं—सब कुछ सीखा जाता है।

### टेक्स्ट ट्रांसफॉर्मर्स से तुलना: समानताएं और अंतर

ViT मौलिक रूप से टेक्स्ट ट्रांसफॉर्मर्स (जैसे, BERT) के एनकोडर भाग के *समान आर्किटेक्चर* है, लेकिन 2D विजुअल डेटा के लिए अनुकूलित। यहां एक साइड-बाय-साइड तुलना दी गई है:

| पहलू                 | टेक्स्ट ट्रांसफॉर्मर (जैसे, BERT)                  | विजन ट्रांसफॉर्मर (ViT)                           |
|----------------------|-----------------------------------------------------|----------------------------------------------------|
| **इनपुट रिप्रेजेंटेशन**  | वेक्टर में एम्बेड किए गए टोकन (शब्द/उपशब्द) का अनुक्रम। | वेक्टर में एम्बेड किए गए इमेज पैच का अनुक्रम। पैच "विजुअल टोकन" की तरह हैं। |
| **अनुक्रम लंबाई**    | परिवर्तनशील (उदा. एक वाक्य के लिए 512 टोकन)।        | इमेज साइज़/पैच साइज़ के आधार पर फिक्स्ड (उदा. [CLS] के साथ 197)। |
| **पोजिशनल एनकोडिंग** | शब्द क्रम के लिए 1D (निरपेक्ष या सापेक्ष)।          | पैच क्रम के लिए 1D (लर्नेबल) (उदा. रो-मेजर फ्लैटनिंग)। कोई अंतर्निहित 2D संरचना नहीं। |
| **मूल मैकेनिज्म**    | डिपेंडेंसी को मॉडल करने के लिए टोकन पर सेल्फ-अटेंशन। | पैच पर सेल्फ-अटेंशन—समान गणित, लेकिन वाक्य रचनात्मक के बजाय स्थानिक "रिश्तों" पर ध्यान देता है। |
| **आउटपुट/टास्क्स**   | क्लासिफिकेशन/मास्क्ड LM के लिए एनकोडर; जनरेशन के लिए डिकोडर। | क्लासिफिकेशन के लिए केवल एनकोडर; डिटेक्शन/सेगमेंटेशन के लिए विस्तारित किया जा सकता है। |
| **ताकत**             | लंबी दूरी की टेक्स्ट डिपेंडेंसी को संभालता है।      | इमेज में ग्लोबल कॉन्टेक्स्ट (उदा. पूरे दृश्य की समझ)। |
| **कमजोरियां**        | विशाल टेक्स्ट कॉर्पोरा की आवश्यकता।                 | डेटा-हंग्री; CNN प्री-ट्रेनिंग के बिना छोटे डेटासेट पर संघर्ष करता है। |
| **प्रिडिक्शन स्टाइल** | डिकोडर में नेक्स्ट-टोकन प्रिडिक्शन (ऑटोरेग्रेसिव)।   | कोई अंतर्निहित "नेक्स्ट" प्रिडिक्शन नहीं—इमेज को समग्र रूप से वर्गीकृत करता है। |

सार रूप में, ViT एक "प्लग-एंड-प्ले" स्वैप है: टोकन एम्बेडिंग को पैच एम्बेडिंग से बदलें, और आपको एक विजन मॉडल मिल जाता है। दोनों एक अनुक्रम में रिश्तों को तौलने के लिए अटेंशन पर निर्भर करते हैं, लेकिन टेक्स्ट स्वाभाविक रूप से अनुक्रमिक/रैखिक है, जबकि इमेज स्थानिक हैं (ViT इसे अटेंशन के माध्यम से सीखता है)।

### ViT में "नेक्स्ट टोकन" बनाम "नेक्स्ट पिक्सेल" को संबोधित करना

नहीं, ViT टेक्स्ट ट्रांसफॉर्मर द्वारा "नेक्स्ट टोकन" की प्रिडिक्शन करने की तरह "नेक्स्ट पिक्सेल" की प्रिडिक्शन *नहीं* करता है (जैसे, GPT में ऑटोरेग्रेसिव जनरेशन)। यहां कारण बताया गया है:

*   **टेक्स्ट ट्रांसफॉर्मर (ऑटोरेग्रेसिव)**: GPT जैसे मॉडल में, डिकोडर अनुक्रमिक रूप से उत्पन्न करता है—एक समय में एक टोकन, सभी पिछले टोकन पर कंडीशनिंग करते हुए। यह कुछ जनरेटिव मॉडल (जैसे, PixelRNN) में इमेज के लिए पिक्सेल-बाय-पिक्सेल हो सकता है, लेकिन अकुशल है।

*   **ViT का दृष्टिकोण**: ViT *नॉन-ऑटोरेग्रेसिव* और समग्र है। यह *संपूर्ण* इमेज (सभी पैच) को एनकोडर के माध्यम से समानांतर रूप से प्रोसेस करता है। इनफेरेंस के दौरान कोई अनुक्रमिक "नेक्स्ट" प्रिडिक्शन नहीं होता—अटेंशन *सभी* पैच में एक साथ रिश्तों की गणना करता है। जनरेशन टास्क (जैसे, इनपेंटिंग) के लिए, ViT वेरिएंट ऑटोरेग्रेसिव तरीके से मास्क्ड पैच की प्रिडिक्शन करने के लिए डिकोडर का उपयोग कर सकते हैं, लेकिन आधार ViT जनरेशन के लिए नहीं, बल्कि क्लासिफिकेशन के लिए है।

*   **पिक्सेल क्यों नहीं?** अलग-अलग पिक्सेल को टोकन के रूप में मानने से अनुक्रम की लंबाई बहुत बढ़ जाएगी (उदा. 224×224×3 = ~150K टोकन), जिससे क्वाड्रैटिक अटेंशन कम्प्यूटेशनल रूप से असंभव हो जाएगा (O(N²) = ट्रिलियन ऑपरेशन)। पैच ~256 पिक्सेल (16×16) को एक "टोकन" में समूहित करते हैं, जिससे N प्रबंधनीय (~200) बना रहता है।

यदि आप जनरेटिव विजन (जैसे नेक्स्ट-पिक्सेल या नेक्स्ट-पैच) चाहते हैं, तो VQ-VAE + ट्रांसफॉर्मर या Parti जैसे मॉडल देखें, जो ViT के विचारों पर आधारित हैं।

अधिक जानकारी के लिए, मूल पेपर देखें: [An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale](https://arxiv.org/abs/2010.11929)। यदि आप कोड उदाहरण (जैसे, PyTorch में) या विशिष्ट वेरिएंट से तुलना चाहते हैं, तो बताएं!