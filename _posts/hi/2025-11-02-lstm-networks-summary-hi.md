---
audio: false
generated: true
image: false
lang: hi
layout: post
model: x-ai/grok-4-fast
title: एलएसटीएम नेटवर्क्स की समझ सारांश
translated: true
type: note
---

क्रिस्टोफर ओलाह का 2015 का ब्लॉग पोस्ट लॉन्ग शॉर्ट-टर्म मेमोरी (LSTM) नेटवर्क्स पर गहराई से चर्चा करता है, जो रिकरंट न्यूरल नेटवर्क (RNN) का एक शक्तिशाली प्रकार है जो क्रमिक डेटा को हैंडल करने के लिए डिज़ाइन किया गया है, जहाँ अतीत का संदर्भ मायने रखता है। यह इस बात से शुरुआत करता है कि कैसे मनुष्य समय के साथ समझ बनाते हैं (जैसे किसी वाक्य को पढ़ना) और पारंपरिक न्यूरल नेटवर्क, जो इनपुट्स को स्वतंत्र रूप से मानते हैं। RNN इसे लूप्स जोड़कर ठीक करते हैं जो जानकारी को बनाए रखने देते हैं, और भाषा मॉडलिंग या वीडियो विश्लेषण जैसे कार्यों के लिए मॉड्यूल्स की एक श्रृंखला में अनरोल होते हैं।

## वैनिला RNN की सीमाएँ
हालांकि RNN छोटे अनुक्रमों पर बेहतर प्रदर्शन करते हैं—जैसे "the clouds are in the" के बाद "sky" की भविष्यवाणी करना—लेकिन वे दीर्घकालिक निर्भरताओं के साथ संघर्ष करते हैं। उदाहरण के लिए, "I grew up in France… I speak fluent French" में, "France" का जल्दी उल्लेख "French" के लिए संकेत होना चाहिए, लेकिन वैनिला RNN अक्सर प्रशिक्षण के दौरान वैनिशिंग ग्रेडिएंट्स के कारण भूल जाते हैं। इस सीमा को, जिसे शुरुआती शोध में उजागर किया गया था, ने LSTM का रास्ता साफ किया।

## LSTM का मूल: सेल स्टेट और गेट्स
LSTM एक **सेल स्टेट** पेश करते हैं—एक "कन्वेयर बेल्ट" जो जानकारी को समय चरणों में बिना किसी बड़े बदलाव के सीधे आगे ले जाती है, जिससे दीर्घकालिक स्मृति संभव होती है। इस प्रवाह को नियंत्रित करते हैं तीन **गेट्स**, प्रत्येक एक सिग्मॉइड लेयर (0-1 आउटपुट देती है) जो यह तय करने के लिए पॉइंटवाइज गुणा की जाती है कि क्या रखना है और क्या छोड़ना है:

- **फॉरगेट गेट**: पिछली हिडन स्टेट और वर्तमान इनपुट को देखता है ताकि सेल स्टेट से अप्रासंगिक पुरानी जानकारी को मिटा सके। उदा., किसी वाक्य में नया विषय आने पर पुराने विषय के लिंग को भूल जाना।
- **इनपुट गेट**: तय करता है कि कौन सी नई जानकारी जोड़नी है, जो एक tanh लेयर के साथ जोड़ी बनाती है जो उम्मीदवार मान बनाती है। साथ में, वे स्केलिंग करके और ताज़ा डेटा जोड़कर सेल स्टेट को अपडेट करते हैं।
- **आउटपुट गेट**: सेल स्टेट (tanh स्केलिंग के बाद) को फ़िल्टर करता है ताकि हिडन स्टेट आउटपुट उत्पन्न हो सके, जो अगले चरण को प्रभावित करता है।

गणित इस प्रकार सरल होती है:  
नया सेल स्टेट \\( C_t = f_t \odot C_{t-1} + i_t \odot \tilde{C}_t \\)  
(जहाँ \\( f_t \\), \\( i_t \\) गेट आउटपुट हैं, \\( \odot \\) एलिमेंट-वाइज़ गुणा है)।

इसे दर्शाने वाले आरेख पूरे पोस्ट में हैं: पीले बक्से न्यूरल लेयर्स के लिए, गुलाबी वृत्त ऑपरेशन्स के लिए, और लाइनें डेटा प्रवाह के लिए।

## वेरिएंट और एक्सटेंशन
यह पोस्ट ट्वीक्स को कवर करता है जैसे **पीपहोल कनेक्शन** (गेट्स को सेल स्टेट पर नज़र डालने देना) और **कपल्ड गेट्स** (दक्षता के लिए फॉरगेट/इनपुट गेट को जोड़ना)। यह गेटेड रिकरंट यूनिट (GRU) का भी जिक्र करता है, जो एक सरल चचेरा भाई है जो गेट्स और स्टेट्स को मर्ज करता है। अन्य उल्लेखों में डेप्थ गेटेड RNN और क्लॉकवर्क RNN शामिल हैं, जिनके अध्ययन दिखाते हैं कि वेरिएंट्स तुलनीय प्रदर्शन करते हैं।

## प्रशिक्षण और परे
LSTM अनरोल की गई श्रृंखला पर बैकप्रोपगेशन थ्रू टाइम के माध्यम से प्रशिक्षण लेते हैं, वैनिला RNN की समस्याओं से बचते हुए। यह पोस्ट हॉकरेटर और श्मिडहबर जैसे अग्रणों को श्रेय देकर समाप्त होती है, भविष्य के विचारों जैसे अटेंशन मैकेनिज्म (इनपुट्स के महत्वपूर्ण हिस्सों पर ध्यान केंद्रित करने के लिए) की ओर इशारा करती है, और जेनरेटिव RNN और ग्रिड LSTM से लिंक करती है।

कुल मिलाकर, यह एक दृश्य, सहज मार्गदर्शिका है जो LSTM को ब्लैक बॉक्स जैसा कम महसूस कराती है—डीप लर्निंग में अनुक्रमों पर काम करने वाले किसी के लिए भी आवश्यक पठन।

[Understanding LSTM Networks](https://colah.github.io/posts/2015-08-Understanding-LSTMs/)