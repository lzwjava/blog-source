---
audio: false
generated: true
image: false
lang: hi
layout: post
model: x-ai/grok-4-fast
title: 'संरचित व्याख्या: पीईएफटी और लोरा'
translated: true
type: note
---

नीचे, मैं प्रदान किए गए पाठ की एक गहन, संरचित व्याख्या प्रस्तुत करूंगा। मैं इसे चरण दर चरण तोड़ूंगा, जिसकी शुरुआत मूलभूत अवधारणाओं से होगी और फिर Parameter Efficient Fine-Tuning (PEFT) और Low-Rank Adaptation (LoRA) की विशिष्टताओं तक बढ़ूंगा। मैं इसे सुलभ बनाने के लिए सादृश्यों, गणितीय अंतर्ज्ञान और वास्तविक दुनिया के संदर्भों का उपयोग करूंगा, फिर भी गहन बनाए रखूंगा। यह व्याख्या सीधे तौर पर पाठ से ली गई है, साथ ही मशीन लर्निंग (ML) में बड़े भाषा मॉडल (LLM) के लिए अंतर्निहित विचारों, प्रेरणाओं और निहितार्थों का विस्तार करती है।

### 1. आधुनिक भाषा मॉडल का पैमाना: प्रीट्रेनिंग और इसका महत्व
पाठ आज के अग्रणी LLM के विशाल पैमाने को उजागर करते हुए शुरू होता है: "आज के अग्रणी भाषा मॉडल में एक ट्रिलियन से अधिक पैरामीटर होते हैं, जिन्हें दसियों ट्रिलियन टोकन पर प्रीट्रेन किया गया है। बेस मॉडल प्रदर्शन पैमाने के साथ लगातार सुधरता रहता है, क्योंकि ये ट्रिलियन लिखित मानव ज्ञान में सभी पैटर्न को सीखने और प्रस्तुत करने के लिए आवश्यक हैं।"

#### पैरामीटर और टोकन क्या हैं?
- **पैरामीटर** न्यूरल नेटवर्क में "वेट" होते हैं - संख्यात्मक मान जो मॉडल प्रशिक्षण के दौरान सीखता है। उन्हें मॉडल की "मेमोरी" या "ज्ञान के नॉब" के रूप में सोचें। एक ट्रिलियन-पैरामीटर मॉडल (जैसे, GPT-4 या PaLM) में लगभग 1,000 बिलियन ऐसे मान होते हैं, जो लगभग लाखों उच्च-रिज़ॉल्यूशन छवियों के डेटा संग्रहण के बराबर होते हैं।
- **टोकन** टेक्स्ट की मूल इकाइयाँ होते हैं जिन्हें मॉडल प्रोसेस करता है (जैसे, शब्द या उपशब्द)। प्रीट्रेनिंग में मॉडल को इनमें से **दसियों ट्रिलियन** (जैसे, किताबों, वेबसाइटों और कोड रिपॉजिटरी से) फीड करना शामिल होता है ताकि यह व्याकरण, तथ्यों और तर्क जैसे सामान्य पैटर्न सीख सके।

#### पैमाना प्रदर्शन को क्यों सुधारता है?
- LLM ट्रांसफॉर्मर-आधारित आर्किटेक्चर हैं (जिनकी शुरुआत 2017 के पेपर "Attention is All You Need" में हुई थी), जो ध्यान मैकेनिज्म और फीड-फॉरवर्ड नेटवर्क की परतों के माध्यम से जटिल पैटर्न को पकड़ने में माहिर हैं।
- अनुभवजन्य स्केलिंग नियम (जैसे, OpenAI के Kaplan et al., 2020 से) दिखाते हैं कि प्रदर्शन (जैसे, प्रश्न-उत्तर जैसे कार्यों पर सटीकता) अधिक पैरामीटर, डेटा और कम्प्यूट के साथ अनुमानित रूप से सुधरता है। पैरामीटर को दोगुना करने से अक्सर "उभरती क्षमताओं" (जैसे, मॉडल अचानक गणित या अनुवाद में अच्छा हो जाता है) में लघुगणकीय लाभ मिलते हैं।
- **अंतर्ज्ञान**: मानव ज्ञान विशाल और परस्पर जुड़ा हुआ है। इसे पूरी तरह से प्रस्तुत करने के लिए (जैसे, प्रत्येक भाषा की वाक्य रचना, ऐतिहासिक तथ्य, वैज्ञानिक सिद्धांत), मॉडल को इन निम्न-स्तरीय सहसंबंधों के रूप में एनकोड करने के लिए एक विशाल "पैरामीटर स्पेस" की आवश्यकता होती है। छोटे मॉडल (जैसे, 1 बिलियन पैरामीटर) सतही पैटर्न पर ओवरफिट हो जाते हैं और सूक्ष्म कार्यों में विफल हो जाते हैं, जबकि ट्रिलियन-स्केल मॉडल बेहतर सामान्यीकरण करते हैं।
- **ट्रेड-ऑफ**: इस पैमाने के लिए भारी कम्प्यूट (जैसे, हफ्तों के लिए हजारों GPU) और ऊर्जा की आवश्यकता होती है, लेकिन यह Llama या GPT श्रृंखला जैसे "बेस मॉडल" की नींव है।

संक्षेप में, प्रीट्रेनिंग मानवता के लिखित कोष से पैटर्न को बलपूर्वक लागू करके एक सामान्य-उद्देश्य वाला "दिमाग" बनाती है। पाठ किसी भी विशेषज्ञता से पहले इसे आधार रेखा के रूप में महत्व देता है।

### 2. पोस्ट-ट्रेनिंग (फाइन-ट्यूनिंग): संकीर्ण फोकस और दक्षता की चुनौतियाँ
पाठ प्रीट्रेनिंग की तुलना "पोस्ट-ट्रेनिंग" से करता है, जिसमें "छोटे डेटासेट शामिल होते हैं और आम तौर पर ज्ञान के संकीर्ण डोमेन और व्यवहार की सीमा पर ध्यान केंद्रित करता है। एक टेराबिट वेट का उपयोग एक गीगाबिट या मेगाबिट प्रशिक्षण डेटा से अपडेट का प्रतिनिधित्व करने के लिए करना बेकार लगता है।"

#### पोस्ट-ट्रेनिंग/फाइन-ट्यूनिंग क्या है?
- प्रीट्रेनिंग के बाद, बेस मॉडल को छोटे, कार्य-विशिष्ट डेटासेट (जैसे, ट्रिलियन टोकन के मुकाबले 1-10 मिलियन उदाहरण) पर "फाइन-ट्यून" किया जाता है। यह इसे चैटबॉट (जैसे, निर्देश-अनुसरण), सेंटीमेंट विश्लेषण, या मेडिकल प्रश्नोत्तर जैसे अनुप्रयोगों के लिए अनुकूलित करता है।
- उदाहरण: GPT-3 को ग्राहक सहायता लॉग पर एक सहायक असिस्टेंट बनाने के लिए, या कानूनी ग्रंथों पर अनुबंध समीक्षा के लिए फाइन-ट्यून करना।
- **छोटे डेटासेट क्यों?** फाइन-ट्यूनिंग बेस ज्ञान में "अपडेट" या "ओवरराइड" को लक्षित करता है - जैसे, विनम्रता या डोमेन-विशिष्ट शब्दजाल सिखाना - बिना सामान्य भाषा की समझ को दोबारा बनाए।

#### बेकारपन का अंतर्ज्ञान
- **डेटा बनाम मॉडल आकार बेमेल**: यदि बेस मॉडल में ~1 ट्रिलियन पैरामीटर (टेराबिट-स्केल, क्योंकि प्रति पैरामीटर लगभग 1 बिट) हैं, लेकिन फाइन-ट्यूनिंग डेटा छोटा (गीगाबिट या मेगाबिट-स्केल) है, तो *सभी* पैरामीटर अपडेट करना एक फुटनोट के लिए पूरी विश्वकोश को फिर से लिखने जैसा है। मॉडल के अधिकांश वेट नए कार्य के लिए अप्रासंगिक रहते हैं।
- **पूर्ण फाइन-ट्यूनिंग (FullFT) की समस्याएं**:
  - **कम्प्यूट ओवरहेड**: सभी पैरामीटर अपडेट करने के लिए प्रत्येक प्रशिक्षण चरण के दौरान पूरे मॉडल के लिए ग्रेडिएंट (त्रुटि संकेत) की पुनर्गणना की आवश्यकता होती है। यह मेमोरी और समय लागत को 10-100x गुणा कर देता है।
  - **विनाशकारी विस्मृति**: FullFT मॉडल की सामान्य क्षमताओं को कम कर सकता है (जैसे, एक गणित-ट्यून किया गया मॉडल कविता भूल जाता है)।
  - **संग्रहण सूजन**: फाइन-ट्यून किए गए मॉडल बेस जितने ही बड़े (ट्रिलियन पैरामीटर) होते हैं, जिससे तैनाती महंगी हो जाती है (जैसे, क्लाउड लागत आकार के साथ बढ़ती है)।
- **सादृश्य**: एक एकल एकल प्रदर्शन के लिए एक विशाल ऑर्केस्ट्रा को ट्यून करने की कल्पना करें, जहां हर संगीतकार को फिर से प्रशिक्षित किया जाए। यह ओवरकिल है जब आप सिर्फ एकल कलाकार को कोच कर सकते हैं।

इस अक्षमता ने **Parameter Efficient Fine-Tuning (PEFT)** को प्रेरित किया: पैरामीटर के एक छोटे अंश (जैसे, 0.1-1%) को अपडेट करने के तरीके, जबकि FullFT के प्रदर्शन लाभ का 90-100% प्राप्त करना।

### 3. Parameter Efficient Fine-Tuning (PEFT): मुख्य विचार
"PEFT... एक बड़े नेटवर्क को पैरामीटर के एक बहुत छोटे सेट को अपडेट करके समायोजित करता है।"

- **मुख्य प्रेरणा**: बेस मॉडल की ताकत को संरक्षित रखें जबकि न्यूनतम परिवर्तनों के साथ कार्य-विशिष्ट अपडेट को इंजेक्ट करें। यह कम्प्यूट, मेमोरी और संग्रहण को कम करता है - AI को लोकतांत्रिक बनाने (जैसे, छोटी टीमों को सुपरकंप्यूटर के बिना Llama 2 जैसे मॉडल को फाइन-ट्यून करने देने) के लिए महत्वपूर्ण।
- **सामान्य PEFT तकनीकें** (LoRA के अलावा, जिसका बाद में उल्लेख किया गया है):
  - **एडाप्टर**: ट्रांसफॉर्मर परतों के बीच छोटे "प्लग-इन" मॉड्यूल (जैसे, बॉटलनेक लेयर) डालें, केवल उन्हें प्रशिक्षित करें।
  - **प्रॉम्प्ट ट्यूनिंग**: सॉफ्ट प्रॉम्प्ट (जैसे, वर्चुअल टोकन) सीखें जो इनपुट से पहले जोड़े जाते हैं, केवल ~0.01% पैरामीटर अपडेट करते हैं।
  - **प्रीफिक्स ट्यूनिंग**: समान, लेकिन ध्यान परतों के लिए प्रीफिक्स को ट्यून करता है।
- **यह क्यों काम करता है**: फाइन-ट्यूनिंग अपडेट अक्सर "लो-डायमेंशनल" होते हैं - वे पूर्ण पैरामीटर स्पेस के एक सबस्पेस में स्थित होते हैं। आपको सब कुछ ट्वीक करने की आवश्यकता नहीं है; कुछ लक्षित परिवर्तन नेटवर्क के माध्यम से प्रसारित होते हैं।
- **अनुभवजन्य सफलता**: PEFT तरीके GLUE (प्राकृतिक भाषा समझ) जैसे बेंचमार्क पर FullFT से मेल खाते हैं या उसे पार करते हैं, साथ ही 10-100x कम कम्प्यूट के साथ। Hugging Face के PETF जैसे लाइब्रेरी इसे प्लग-एंड-प्ले बनाते हैं।

PEFT "सब कुछ प्रशिक्षित करने" के प्रतिमान को "सर्जिकल रूप से संपादित करने" में बदल देता है, पाठ की दक्षता थीम के साथ संरेखित करता है।

### 4. Low-Rank Adaptation (LoRA): अग्रणी PEFT विधि
"अग्रणी PEFT विधि low-rank adaptation, या LoRA है। LoRA मूल मॉडल से प्रत्येक वेट मैट्रिक्स W को एक संशोधित संस्करण W′ = W + γ B A से बदल देता है, जहाँ B और A मैट्रिक्स हैं जिनमें एक साथ W की तुलना में बहुत कम पैरामीटर होते हैं, और γ एक स्थिर स्केलिंग फैक्टर है। प्रभाव में, LoRA फाइन-ट्यूनिंग द्वारा दिए गए अपडेट का एक low-dimensional प्रतिनिधित्व बनाता है।"

#### गणितीय विवरण
LoRA ट्रांसफॉर्मर में वेट मैट्रिक्स **W** को लक्षित करता है (जैसे, ध्यान या फीड-फॉरवर्ड लेयर के लिए क्वेरी/की/वैल्यू प्रोजेक्शन में)। ये आम तौर पर d × k मैट्रिक्स (जैसे, 4096 × 4096, प्रत्येक में लाखों पैरामीटर) होते हैं।

- **सूत्र**: फाइन-ट्यूनिंग के दौरान, सीधे W को अपडेट करने के बजाय, LoRA आउटपुट की गणना इस प्रकार करता है:
  ```
  h = W x + γ (B A) x  (जहाँ x इनपुट है)
  ```
  - **W**: फ्रोजन मूल वेट (अपरिवर्तित)।
  - **A**: एक लो-रैंक मैट्रिक्स, यादृच्छिक रूप से इनिशियलाइज़ किया गया (जैसे, r × k, जहाँ r << d, जैसे r=8-64)।
  - **B**: एक अन्य लो-रैंक मैट्रिक्स (d × r), शून्य पर इनिशियलाइज़ किया गया (ताकि प्रारंभिक अपडेट शून्य हो, व्यवधान से बचने के लिए)।
  - **γ (गामा)**: स्केलिंग फैक्टर (जैसे, γ = α / r, जहाँ α एक हाइपरपैराम जैसे 16 है) अपडेट परिमाण को नियंत्रित करने और प्रशिक्षण को स्थिर करने के लिए।
  - पूर्ण अपडेट किया गया वेट: **W' = W + γ B A**।

- **"लो-रैंक" क्यों?**
  - मैट्रिक्स को सिंगुलर वैल्यू डिकम्पोजिशन (SVD) के माध्यम से विघटित किया जा सकता है: कोई भी मैट्रिक्स ≈ U Σ V^T, जहाँ "रैंक" महत्वपूर्ण सिंगुलर वैल्यू की संख्या होती है।
  - फाइन-ट्यूनिंग अपडेट ΔW = W' - W अक्सर **लो-रैंक** (r << min(d,k)) होते हैं, जिसका अर्थ है कि वे एक संपीड़ित सबस्पेस में परिवर्तनों को कैप्चर करते हैं (जैसे, कुछ दिशाएँ जैसे "सुरक्षा पर जोर दें" या "कोड पर ध्यान केंद्रित करें")।
  - **B A** ΔW को रैंक-r के साथ अनुमानित करता है (पैराम: d*r + r*k बनाम पूर्ण W के लिए d*k)। 4096×4096 W में r=8 के लिए, LoRA ~65k पैराम बनाम 16M का उपयोग करता है - 99.6% की कमी!
  - **अंतर्ज्ञान**: अपडेट एक उच्च-आयामी स्पेस में वैक्टर की तरह होते हैं; LoRA उन्हें एक low-dimensional "हाईवे" (रैंक r) पर प्रोजेक्ट करता है, विशाल पैरामीटर स्पेस में शोर को नजरअंदाज करता है।

- **प्रशिक्षण कैसे काम करता है**:
  1. फॉरवर्ड पास: W + γ B A का उपयोग करके h की गणना करें, लेकिन केवल A और B को प्रशिक्षित करें (W फ्रोजन)।
  2. बैकप्रॉप: ग्रेडिएंट केवल A/B में प्रवाहित होते हैं, मेमोरी को कम रखते हुए।
  3. इनफेरेंस: या तो मर्ज करें (W' = W + B A) एक एकल मॉडल के लिए या मॉड्यूलरिटी के लिए अलग रखें।
- **पेपर से (Hu et al., 2021)**: LoRA को विजन/भाषा मॉडल के लिए पेश किया गया था लेकिन NLP में विस्फोट हुआ। यह सारांशीकरण जैसे कार्यों पर एडाप्टर से बेहतर प्रदर्शन करता है, साथ ही कम मेमोरी का उपयोग करता है। QLoRA जैसे वेरिएंट आगे और छोटे फुटप्रिंट के लिए बेस मॉडल को क्वांटाइज़ करते हैं।

संक्षेप में, LoRA मॉडल को एक लाइटवेट "डेल्टा" (B A) जोड़कर "हैक" करता है जो फाइन-ट्यूनिंग को एक कॉम्पैक्ट रैखिक परिवर्तन के रूप में दर्शाता है।

### 5. पूर्ण फाइन-ट्यूनिंग (FullFT) पर LoRA के लाभ
पाठ कच्ची दक्षता से परे व्यावहारिक लाभों को सूचीबद्ध करता है, जो व्यावहारिकता पर जोर देता है। मैं प्रत्येक का विस्तार करूंगा।

#### a. पोस्ट-ट्रेनिंग की लागत और गति
- LoRA ~0.1% पैरामीटर अपडेट करने के कारण 100-1000x तेज/सस्ता प्रशिक्षित होता है। उदा., एकल A100 GPU पर Llama-7B को फाइन-ट्यून करना (FullFT को 8+ GPU की आवश्यकता होती है) दिनों के मुकाबले घंटे लेता है।
- कम परिशुद्धता (जैसे, bfloat16) पर्याप्त होती है, ऊर्जा उपयोग को कम करती है।

#### b. मल्टी-टेनेंट सर्विंग
"चूंकि LoRA मूल वेट को अपरिवर्तित रखते हुए एक एडाप्टर (यानी, A और B मैट्रिक्स) को प्रशिक्षित करता है, एक एकल इनफेरेंस सर्वर मेमोरी में कई एडाप्टर (विभिन्न मॉडल संस्करण) रख सकता है और उन्हें एक बैच्ड तरीके से एक साथ उनमें से सैंपल ले सकता है। Punica: Multi-Tenant LoRA Serving (Chen, Ye, et al, 2023) आधुनिक इनफेरेंस इंजन जैसे vLLM और SGLang इस सुविधा को लागू करते हैं।"

- **इसका क्या अर्थ है**: बेस W साझा किया जाता है; एडाप्टर छोटे (पूर्ण मॉडल के लिए GB बनाम MB) होते हैं। एक सर्वर एक W + N एडाप्टर (जैसे, कोडिंग, लेखन, अनुवाद के लिए) लोड करता है।
- **मल्टी-टेनेंसी**: बेस को पुनः लोड किए बिना एक साथ कई उपयोगकर्ताओं/मॉडलों को सर्व करें। दक्षता के लिए एडाप्टर में बैच अनुरोध।
- **वास्तविक-विश्व प्रभाव**: प्रोडक्शन में (जैसे, Hugging Face Spaces या Azure ML), यह "मॉडल सूप" या ऑन-द-फ्लाई व्यक्तित्व बदलने को सक्षम बनाता है। Punica (2023) पेजिंग के माध्यम से मेमोरी को ऑप्टिमाइज़ करता है; vLLM/SGLang 10x थ्रूपुट के लिए पेज्ड अटेंशन का उपयोग करते हैं।
- **सादृश्य**: एक नए ट्यून प्रति नई कार खरीदने बनाम स्वैप करने योग्य टर्बो किट (एडाप्टर) वाले एकल इंजन (W) की तरह।

#### c. प्रशिक्षण के लिए लेआउट आकार
"जब पूरे मॉडल को फाइन-ट्यून किया जाता है, तो ऑप्टिमाइज़र स्टेट को मूल वेट के साथ संग्रहीत करने की आवश्यकता होती है, अक्सर उच्च परिशुद्धता पर। परिणामस्वरूप, FullFT को आम तौर पर उसी मॉडल से सैंपल लेने की तुलना में एक ऑर्डर ऑफ मैग्नीट्यूड अधिक एक्सेलेरेटर की आवश्यकता होती है... प्रशिक्षण के लिए, वेट संग्रहीत करने के अलावा, हमें आम तौर पर सभी वेट के लिए ग्रेडिएंट और ऑप्टिमाइज़र मोमेंट्स संग्रहीत करने की आवश्यकता होती है; इसके अलावा, ये वेरिएबल्स अक्सर उच्च परिशुद्धता (float32) में संग्रहीत होते हैं, जो इनफेरेंस के लिए वेट संग्रहीत करने में उपयोग की जाने वाली (bfloat16 या निम्न) से अधिक होती है। चूंकि LoRA बहुत कम वेट को प्रशिक्षित करता है और बहुत कम मेमोरी का उपयोग करता है, इसे सैंपल लेने के लिए उपयोग किए जाने वाले लेआउट से केवल थोड़ा बड़े लेआउट पर प्रशिक्षित किया जा सकता है।"

- **प्रशिक्षण मेमोरी विवरण**:
  - FullFT: वेट (1T पैराम @ bfloat16 = ~2TB) + ग्रेडिएंट (समान) + ऑप्टिमाइज़र स्टेट्स (जैसे, Adam: प्रति पैराम 2 मोमेंट @ float32 = ~8TB कुल)। वितरित "लेआउट" (जैसे, डेटा/मॉडल समानांतरता) में 100s GPU की आवश्यकता होती है।
  - LoRA: केवल A/B (~0.1% पैराम) को ग्रेडिएंट/स्टेट्स (~2-10GB अतिरिक्त) मिलते हैं। 1-2 GPU पर प्रशिक्षित करें, इनफेरेंस लेआउट के समान।
- **परिशुद्धता विवरण**: इनफेरेंस गति के लिए कम-परिशुद्धता (bfloat16/float16) का उपयोग करता है; प्रशिक्षण को ग्रेडिएंट स्थिरता के लिए float32 की आवश्यकता होती है। LoRA इस ओवरहेड को कम करता है।
- **सुलभता**: शौकिया/स्टार्टअप उपभोक्ता हार्डवेयर (जैसे, RTX 4090) पर फाइन-ट्यून कर सकते हैं, बनाम FullFT जिसके लिए एंटरप्राइज क्लस्टर की आवश्यकता होती है। दक्षता: LoRA अक्सर कम वेरिएबल के कारण तेजी से अभिसरण करता है।

#### d. लोडिंग और ट्रांसफर में आसानी
"कम वेट संग्रहीत करने के साथ, LoRA एडाप्टर सेट अप या मशीनों के बीच स्थानांतरित करने के लिए तेज़ और आसान होते हैं।"

- एडाप्टर <100MB फाइलें (पूर्ण मॉडल के लिए 100s GB बनाम) होते हैं, ज़िप करने योग्य और साझा करने योग्य (जैसे, GitHub या टोरेंट के माध्यम से)।
- तैनाती: बेस को एक बार लोड करें, सेकंडों में एडाप्टर स्वैप करें। स्थायित्व के लिए मर्ज करना वैकल्पिक है।
- **एज केस**: वर्जन कंट्रोल (जैसे, एडाप्टर के लिए Git), फेडरेटेड लर्निंग (ऑन-डिवाइस एडाप्टर प्रशिक्षित करें, केवल डेल्टा भेजें), या पुनरावृत्त ट्यूनिंग (एकाधिक LoRA स्टैक करें)।

### व्यापक निहितार्थ और सीमाएँ
- **LoRA का वर्चस्व क्यों**: यह सरल, प्रभावी और सहजता से एकीकृत होता है (जैसे, PEFT लाइब्रेरी में)। Stable Diffusion फाइन-ट्यून या Alpaca (निर्देश-ट्यून किया गया Llama) जैसे मॉडल में उपयोग किया जाता है।
- **सीमाएँ**: बहुत ही विशिष्ट कार्यों पर हमेशा FullFT जितना प्रदर्शनकारी नहीं होता; रैंक r का चुनाव मायने रखता है (बहुत कम अंडरफिट, बहुत अधिक दक्षता बर्बाद करता है)। सभी अपडेट प्रकार (जैसे, आर्किटेक्चरल परिवर्तन) को हैंडल नहीं करता है।
- **भविष्य के रुझान**: DoRA (वेट को और विघटित करें) या VeRA (वेक्टर-आधारित, और भी स्पार्स) जैसे एक्सटेंशन। जैसे-जैसे मॉडल 10T+ पैराम तक स्केल होते हैं, PEFT/LoRA स्थिरता के लिए आवश्यक हो जाता है।

यह पाठ AI में एक महत्वपूर्ण बदलाव को समाहित करता है: संसाधन-भूखे पूर्ण पुनःप्रशिक्षण से लेकर सुरुचिपूर्ण, मॉड्यूलर अपडेट तक। यदि आप गणितीय व्युत्पत्तियों, कोड उदाहरणों (जैसे, PyTorch में), या संबंधित पेपर में गोता लगाना चाहते हैं, तो मुझे बताएं!