---
audio: false
generated: true
image: false
lang: hi
layout: post
model: x-ai/grok-4-fast:free
title: लोरा के साथ कुशल फाइन-ट्यूनिंग
translated: true
type: note
---

लो-रैंक एडाप्टेशन (LoRA) बड़े भाषा मॉडल के लिए एक पैरामीटर-कुशल फाइन-ट्यूनिंग तकनीक है। एक प्री-ट्रेन्ड मॉडल के सभी पैरामीटर्स को अपडेट करने के बजाय, LoRA विशिष्ट परतों, आमतौर पर अटेंशन या फीडफॉरवर्ड परतों, को एडाप्ट करने के लिए छोटे, लो-रैंक मैट्रिक्स पेश करता है। ये मैट्रिक्स मूल मॉडल पैरामीटर्स को फ्रोजन रखते हुए टास्क-विशिष्ट अपडेट्स को कैप्चर करते हैं, जिससे मेमोरी और कम्प्यूटेशनल लागत में काफी कमी आती है। उदाहरण के लिए, GPT जैसे बड़े मॉडल को फाइन-ट्यून करने के लिए अरबों पैरामीटर्स को अपडेट करने की आवश्यकता हो सकती है, लेकिन LoRA कुल पैरामीटर्स के अक्सर 1% से भी कम को समायोजित करके तुलनीय प्रदर्शन प्राप्त कर सकता है। यह LoRA को संसाधन-सीमित वातावरणों के लिए आदर्श बनाता है, जो बिना व्यापक रीट्रेनिंग के विशिष्ट कार्यों के लिए बड़े मॉडल के कुशल कस्टमाइजेशन को सक्षम करता है।

[LoRA: Low-Rank Adaptation of Large Language Models](https://arxiv.org/abs/2106.09685)