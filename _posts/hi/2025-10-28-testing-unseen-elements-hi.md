---
audio: false
generated: true
image: false
lang: hi
layout: post
model: x-ai/grok-4-fast
title: अदृश्य तत्वों के साथ सामान्यीकरण का परीक्षण
translated: true
type: note
---

इल्या सुत्स्केवर के अपने 2014 के "सीक्वेंस टू सीक्वेंस लर्निंग विथ न्यूरल नेटवर्क्स" पेपर (और संबंधित शुरुआती कार्य जैसे आरएनएन लैंग्वेज मॉडलिंग पर उनकी पीएचडी थीसिस) पर अपने न्यूरआईपीएस 2024 टेस्ट ऑफ टाइम अवार्ड टॉक में, उन्होंने सामान्यीकरण करने वाले न्यूरल लैंग्वेज मॉडल बनाने के शुरुआती प्रयासों पर चर्चा की। एक प्रमुख चुनौती यह सुनिश्चित करना था कि मॉडल केवल ट्रेनिंग डेटा को रटे नहीं बल्कि नए इनपुट को संभाल सकें—यानी, ओवरफिटिंग से बच सकें।

इसका पता लगाने के लिए उन्होंने जिस विशिष्ट "अनाड़ी तरीके" का जिक्र किया, उसमें मॉडल का परीक्षण **अनदेखे शब्दों या एन-ग्राम (बहु-शब्द अनुक्रम) पर करना शामिल है जो ट्रेनिंग कॉर्पस (जिसे अक्सर "डेटाबेस" कहा जाता है) में मौजूद नहीं होते**।

### यह दृष्टिकोण क्यों?
- **शुरुआती लैंग्वेज मॉडल में ओवरफिटिंग का जोखिम**: सरल बेसलाइन जैसे एन-ग्राम मॉडल (उदाहरण के लिए, बाइग्राम या ट्राइग्राम) अक्सर केवल तभी सही भविष्यवाणी करते हैं जब सटीक अनुक्रम प्रशिक्षण में कई बार आया हो, इस तरह से "ओवरफिट" हो जाते हैं। वे किसी भी नई चीज़ को लगभग शून्य संभावना देते हैं, और सामान्यीकरण करने में विफल रहते हैं।
- **अनाड़ी पहचान परीक्षण**: सच्चे सामान्यीकरण (ओवरफिटिंग नहीं) की जांच के लिए, एक हेल्ड-आउट वैलिडेशन/टेस्ट सेट पर प्रशिक्षण दें जिसमें जानबूझकर "अनदेखे" तत्व शामिल हों:
    - आम वाक्यांशों को आविष्कारित लेकिन प्रशंसनीय वाक्यांशों से बदलें (उदाहरण के लिए, उनकी थीसिस में, एक नकली सिटेशन जैसे "(ABC एट अल., 2003)" पर वाक्य पूरा करने का परीक्षण करना—एक ऐसी स्ट्रिंग जिसका सामना मॉडल ने अपने अप्राकृतिक कैपिटलाइजेशन और लेखक नाम के कारण कभी नहीं किया था)।
    - मापें कि क्या मॉडल उचित संभावनाएं निर्दिष्ट करता है, सुसंगत पूरक जनरेट करता है, या नवीनता के बावजूद कम पर्प्लेक्सिटी/ब्लू स्कोर बनाए रखता है।
- यदि मॉडल इन अनदेखी वस्तुओं पर विफल रहता है (उदाहरण के लिए, उच्च पर्प्लेक्सिटी या असंगत आउटपुट) लेकिन देखे गए ट्रेनिंग डेटा पर उत्कृष्ट प्रदर्शन करता है, तो यह ओवरफिटिंग कर रहा है (पैटर्न सीखने के बजाय विशिष्टताओं को याद कर रहा है)। यदि यह सफल होता है, तो यह सीखे गए प्रतिनिधित्व (उदाहरण के लिए, एलएसटीएम स्टेट्स जो सिंटैक्स/सिमेंटिक्स कैप्चर करते हैं) के माध्यम से सामान्यीकरण कर रहा है।

### उनके कार्य से उदाहरण
- **seq2seq पेपर (2014) में**: उन्होंने एक निश्चित शब्दावली (80k फ्रेंच शब्द, 160k अंग्रेजी) का उपयोग किया, शब्दावली से बाहर (OOV) के शब्दों को "UNK" से बदल दिया। सामान्यीकरण समस्याओं का पता लगाने के लिए, उन्होंने OOV घटनाओं के लिए ब्लू स्कोर पर पेनल्टी लगाई और "औसत शब्द आवृत्ति रैंक" (दुर्लभ = अधिक अनदेखे जैसा) बनाम प्रदर्शन प्लॉट किया। एलएसटीएम ने बेसलाइन के विपरीत, गिरावट के बिना दुर्लभ/अनदेखे शब्दों को अच्छी तरह से संभाला।
- **उनकी पीएचडी थीसिस (2013, आरएनएन लैंग्वेज मॉडलिंग पर) में**: संरचित वाक्य पूर्णता के लिए, उन्होंने पूरी तरह से नई स्ट्रिंग्स जैसे "(ABC एट अल., 2003)" (दुर्लभता के कारण प्रशिक्षण में अनदेखा) पर परीक्षण किया। मल्टीप्लिकेटिव आरएनएन (एमआरएनएन) ने मजबूती से सामान्यीकरण किया (उदाहरण के लिए, वर्ष/संदर्भ को सही ढंग से भविष्यवाणी करना), जबकि एन-ग्राम बेसलाइन तब तक नहीं कर सके जब तक कि सटीक मेल मौजूद नहीं थे—जो उनकी ओवरफिटिंग को उजागर करता है।

यह "अनदेखे शब्दों/एन-ग्राम की जांच" परीक्षण ड्रॉपआउट या वैलिडेशन कर्व्स जैसी उन्नत तकनीकों से पहले एक सीधा, शुरुआती अनुमानित तरीका (ह्यूरिस्टिक) था। सुत्स्केवर ने टॉक में बताया कि कैसे ऐसी अंतर्दृष्टि ने आधुनिक एलएलएम में स्केलिंग को प्रेरित किया, जहां विशाल अनदेखे डेटा पर सामान्यीकरण अब भी मूलभूत है।

[सीक्वेंस टू सीक्वेंस लर्निंग विथ न्यूरल नेटवर्क्स (arXiv PDF)](https://arxiv.org/pdf/1409.3215.pdf)  
[इल्या सुत्स्केवर पीएचडी थीसिस (PDF)](https://www.cs.utoronto.ca/~ilya/pubs/ilya_sutskever_phd_thesis.pdf)  
[इल्या सुत्स्केवर न्यूरआईपीएस 2024 टॉक वीडियो](https://www.youtube.com/watch?v=YD-9NG1Ke5Y)