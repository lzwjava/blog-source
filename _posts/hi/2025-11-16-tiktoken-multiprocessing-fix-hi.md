---
audio: false
generated: true
image: false
lang: hi
layout: post
model: x-ai/grok-4-fast
title: टिकटोकन मल्टीप्रोसेसिंग पिकल त्रुटि को ठीक करना
translated: true
type: note
---

आपकी त्रुटि `datasets.map(..., num_proc>1)` का एक कस्टम टोकनाइज़र (जैसे nanoGPT के FineWeb स्क्रिप्ट में इस्तेमाल होने वाला Tiktoken एनकोडर) के साथ उपयोग करते समय बहुत आम है — टोकनाइज़र BPE फ़ाइल (`cl100k_base.tiktoken` या इसी तरह) लोड करता है और यह फ़ाइल हैंडल **पिकल (pickled) नहीं किया जा सकता** जब इसे Linux पर सब-प्रोसेस में भेजा जाता है (fork-आधारित मल्टीप्रोसेसिंग)।

चाइल्ड प्रोसेस तुरंत क्रैश हो जाती हैं → "One of the subprocesses has abruptly died"।

### त्वरित समाधान (99% मामलों में काम करता है)

`data/fineweb/prepare.py` को एडिट करें और टोकनाइज़ेशन को **केवल मुख्य प्रोसेस** में चलाने के लिए मजबूर करें:

```python
# लगभग लाइन 80-85 के आसपास, .map() कॉल ढूंढें और num_proc=8 → num_proc=1 में बदलें

tokenized = raw_datasets.map(
    tokenize,
    num_proc=1,               # ← इस लाइन को बदलें (या बस argument को हटा दें)
    desc="tokenizing the splits",
)
```

यह धीमा होगा (सिंगल-कोर), लेकिन यह विश्वसनीय रूप से काम करेगा। FineWeb-10B पर यह आपके CPU/disk के आधार पर ~30-60 मिनट लेता है।

### बेहतर समाधान (मल्टीप्रोसेसिंग रखें, फिर भी तेज़)

टोकनाइज़र को ग्लोबल स्कोप के बजाय वर्कर प्रोसेस के अंदर आलसी तरीके से (lazily) लोड होने दें।

मौजूदा टोकनाइज़र क्रिएशन (लगभग लाइन 40-50) को इस पैटर्न से बदलें:

```python
import tiktoken

# ग्लोबल encoder = tiktoken.get_encoding("cl100k_base") को हटा दें

def get_encoder():
    """प्रत्येक सबप्रोसेस में टोकनाइज़र को आलसी तरीके से लोड करें"""
    return tiktoken.get_encoding("cl100k_base")

def tokenize(example):
    enc = get_encoder()                                   # ← प्रति-प्रोसेस लोड होगा
    ids = enc.encode_ordinary(example["text"])
    ids.append(enc.eot_token)  # या जो भी स्क्रिप्ट करती है
    return {"ids": ids, "len": len(ids)}
```

फिर `num_proc=8` (या कोई भी नंबर) रखें। इस तरह प्रत्येक वर्कर forking के बाद .tiktoken फ़ाइल की अपनी खुद की कॉपी लोड करता है → कोई पिकलिंग समस्या नहीं।

### और भी तेज़ विकल्प (बार-बार रन के लिए अनुशंसित)

Hugging Face से 10B/100B FineWeb-Edu शार्ड्स को पहले से डाउनलोड करें और Karpathy के अपडेटेड स्क्रिप्ट का उपयोग करें जो सीधे parquet/arrow फ़ाइलों पर काम करती है, विशाल .txt फ़ाइल के बजाय:

```bash
# बहुत तेज़ और इस बग से पूरी तरह बचाता है
python data/fineweb/prepare_fineweb.py --version 10B
# या 100B
```

नई स्क्रिप्ट (~2024 में जोड़ी गई) ऑफिशियल HF डेटासेट का उपयोग करती है और मल्टीप्रोसेसिंग-सुरक्षित है।

### सारांश

| समाधान                           | गति       | जटिलता    | सिफारिश         |
|----------------------------------|-----------|-----------|------------------|
| `num_proc=1`                     | धीमी      | 1 लाइन    | त्वरित समाधान   |
| tokenize() के अंदर लेज़ी एनकोडर | तेज़      | छोटा एडिट | .txt के लिए सबसे अच्छा |
| नई prepare_fineweb.py स्क्रिप्ट का उपयोग | सबसे तेज़ | नया कमांड | अनुशंसित       |

एक को चुनें और आप इस त्रुटि को तुरंत पार कर जाएंगे। मुझे बताएं कि आपने कौन सा रास्ता अपनाया और अगर जरूरत पड़ी तो मैं आपको सटीक पैच दूंगा!