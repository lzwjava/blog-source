---
audio: false
generated: true
lang: hi
layout: post
title: ट्रांसफॉर्मर्स में के, क्यू, वी मैकेनिज्म
translated: true
type: note
---

**की (K), क्वेरी (Q), और वैल्यू (V)** तंत्र ट्रांसफॉर्मर-आधारित बड़े भाषा मॉडल (LLM) में **सेल्फ-अटेंशन** तंत्र का एक मौलिक घटक है। यह तंत्र, जिसकी शुरुआत शोध पत्र *"Attention is All You Need"* (Vaswani et al., 2017) में हुई थी, मॉडल को यह तय करने में सक्षम बनाता है कि टेक्स्ट को प्रोसेस या जनरेट करते समय एक अनुक्रम में विभिन्न शब्दों के महत्व को कैसे तौला जाए। नीचे, मैं ट्रांसफॉर्मर LLM के संदर्भ में **K, Q, V** तंत्र के काम करने के तरीके का एक व्यापक परिचय प्रदान करता हूं, जिसमें इसकी अंतर्दृष्टि, गणितीय सूत्रीकरण, सेल्फ-अटेंशन में कार्यान्वयन और व्यापक आर्किटेक्चर में इसकी भूमिका शामिल है।

---

### 1. **सेल्फ-अटेंशन में K, Q, V के पीछे की अंतर्दृष्टि**
सेल्फ-अटेंशन तंत्र एक ट्रांसफॉर्मर मॉडल को प्रत्येक शब्द (या टोकन) के लिए अनुक्रम के प्रासंगिक भागों पर ध्यान केंद्रित करके एक इनपुट अनुक्रम को प्रोसेस करने की अनुमति देता है। **K, Q, V** घटक इस प्रक्रिया के मूलभूत निर्माण खंड हैं, जो मॉडल को गतिशील रूप से यह निर्धारित करने में सक्षम बनाते हैं कि इनपुट के कौन से भाग एक दूसरे के लिए सबसे प्रासंगिक हैं।

- **क्वेरी (Q):** एक टोकन द्वारा अनुक्रम में अन्य टोकनों से पूछे गए "प्रश्न" का प्रतिनिधित्व करती है। प्रत्येक टोकन के लिए, क्वेरी वेक्टर एनकोड करता है कि टोकन अनुक्रम के बाकी हिस्सों से कौन सी जानकारी ढूंढ रहा है।
- **की (K):** अनुक्रम में प्रत्येक टोकन के "विवरण" का प्रतिनिधित्व करती है। की वेक्टर एनकोड करता है कि एक टोकन दूसरों को कौन सी जानकारी प्रदान कर सकता है।
- **वैल्यू (V):** वास्तविक सामग्री या जानकारी का प्रतिनिधित्व करती है जो एक टोकन रखता है। एक बार मॉडल यह निर्धारित कर लेता है कि कौन से टोकन प्रासंगिक हैं (Q और K इंटरैक्शन के माध्यम से), यह आउटपुट बनाने के लिए संबंधित वैल्यू वेक्टर्स को पुनः प्राप्त करता है।

**Q** और **K** के बीच की परस्पर क्रिया यह निर्धारित करती है कि प्रत्येक टोकन को हर दूसरे टोकन पर कितना ध्यान देना चाहिए, और फिर **V** वेक्टर्स को इस ध्यान के आधार पर भारित किया जाता है और संयोजित किया जाता है ताकि प्रत्येक टोकन के लिए आउटपुट तैयार किया जा सके।

इसे एक लाइब्रेरी खोज की तरह समझें:
- **क्वेरी**: आपकी खोज क्वेरी (उदाहरण के लिए, "मशीन लर्निंग")।
- **की**: लाइब्रेरी में किताबों के शीर्षक या मेटाडेटा, जिनकी तुलना आप प्रासंगिक किताबें खोजने के लिए अपनी क्वेरी से करते हैं।
- **वैल्यू**: प्रासंगिक किताबों की पहचान करने के बाद आपके द्वारा पुनः प्राप्त की गई किताबों की वास्तविक सामग्री।

---

### 2. **सेल्फ-अटेंशन में K, Q, V कैसे काम करते हैं**
सेल्फ-अटेंशन तंत्र **वैल्यू** वेक्टर्स का एक भारित योग गणना करता है, जहां भार **क्वेरी** और **की** वेक्टर्स के बीच समानता द्वारा निर्धारित किए जाते हैं। यहां इस प्रक्रिया का चरण-दर-चरण विवरण दिया गया है:

#### चरण 1: इनपुट प्रतिनिधित्व
- एक ट्रांसफॉर्मर लेयर का इनपुट टोकन (जैसे, शब्द या उपशब्द) का एक अनुक्रम होता है, जिनमें से प्रत्येक को एक उच्च-आयामी एम्बेडिंग वेक्टर (उदाहरण के लिए, आयाम \\( d_{\text{model}} = 512 \\)) के रूप में दर्शाया जाता है।
- \\( n \\) टोकन के एक अनुक्रम के लिए, इनपुट एक मैट्रिक्स \\( X \in \mathbb{R}^{n \times d_{\text{model}}} \\) होता है, जहां प्रत्येक पंक्ति एक टोकन की एम्बेडिंग है।

#### चरण 2: K, Q, V जनरेट करने के लिए रैखिक परिवर्तन
- प्रत्येक टोकन के लिए, तीन वेक्टर्स की गणना की जाती है: **क्वेरी (Q)**, **की (K)**, और **वैल्यू (V)**। इन्हें इनपुट एम्बेडिंग पर सीखे गए रैखिक परिवर्तन लागू करके प्राप्त किया जाता है:
  \\[
  Q = X W_Q, \quad K = X W_K, \quad V = X W_V
  \\]
  - \\( W_Q, W_K, W_V \in \mathbb{R}^{d_{\text{model}} \times d_k} \\) सीखे गए वजन मैट्रिक्स हैं।
  - आमतौर पर, \\( d_k = d_v \\), और उन्हें अक्सर \\( d_{\text{model}} / h \\) पर सेट किया जाता है (जहां \\( h \\) अटेंशन हेड्स की संख्या है, जिसकी व्याख्या बाद में की गई है)।
  - परिणाम है:
    - \\( Q \in \mathbb{R}^{n \times d_k} \\): सभी टोकन के लिए क्वेरी मैट्रिक्स।
    - \\( K \in \mathbb{R}^{n \times d_k} \\): सभी टोकन के लिए की मैट्रिक्स।
    - \\( V \in \mathbb{R}^{n \times d_v} \\): सभी टोकन के लिए वैल्यू मैट्रिक्स।

#### चरण 3: अटेंशन स्कोर की गणना करें
- अटेंशन तंत्र गणना करता है कि प्रत्येक टोकन को हर दूसरे टोकन पर कितना ध्यान देना चाहिए, एक टोकन के क्वेरी वेक्टर और सभी टोकन के की वेक्टर्स के बीच **डॉट प्रोडक्ट** की गणना करके:
  \\[
  \text{Attention Scores} = Q K^T
  \\]
  - यह एक मैट्रिक्स \\( \in \mathbb{R}^{n \times n} \\) उत्पन्न करता है, जहां प्रत्येक प्रविष्टि \\( (i, j) \\) टोकन \\( i \\) की क्वेरी और टोकन \\( j \\) की की के बीच असामान्यीकृत समानता का प्रतिनिधित्व करती है।
- ग्रेडिएंट को स्थिर करने और बड़े मूल्यों को रोकने के लिए, स्कोर को की आयाम के वर्गमूल से स्केल किया जाता है:
  \\[
  \text{Scaled Scores} = \frac{Q K^T}{\sqrt{d_k}}
  \\]
  - इसे **स्केल्ड डॉट-प्रोडक्ट अटेंशन** कहा जाता है।

#### चरण 4: अटेंशन वेट प्राप्त करने के लिए सॉफ्टमैक्स लागू करें
- स्केल्ड स्कोर को संभावनाओं (अटेंशन वेट) में बदलने के लिए एक **सॉफ्टमैक्स** फ़ंक्शन के माध्यम से पारित किया जाता है जो प्रत्येक टोकन के लिए 1 तक जुड़ते हैं:
  \\[
  \text{Attention Weights} = \text{softmax}\left( \frac{Q K^T}{\sqrt{d_k}} \right)
  \\]
  - परिणाम एक मैट्रिक्स \\( \in \mathbb{R}^{n \times n} \\) होता है, जहां प्रत्येक पंक्ति अनुक्रम में सभी टोकन पर एक टोकन के अटेंशन वितरण का प्रतिनिधित्व करती है।
  - उच्च अटेंशन वेट इंगित करते हैं कि संबंधित टोकन एक दूसरे से अत्यधिक प्रासंगिक हैं।

#### चरण 5: आउटपुट की गणना करें
- अटेंशन वेट का उपयोग **वैल्यू** वेक्टर्स के भारित योग की गणना करने के लिए किया जाता है:
  \\[
  \text{Attention Output} = \text{softmax}\left( \frac{Q K^T}{\sqrt{d_k}} \right) V
  \\]
  - आउटपुट एक मैट्रिक्स \\( \in \mathbb{R}^{n \times d_v} \\) होता है, जहां प्रत्येक पंक्ति एक टोकन का एक नया प्रतिनिधित्व है, जो उनकी प्रासंगिकता के आधार पर अन्य सभी टोकन से जानकारी शामिल करता है।

#### चरण 6: मल्टी-हेड अटेंशन
- व्यवहार में, ट्रांसफॉर्मर **मल्टी-हेड अटेंशन** का उपयोग करते हैं, जहां उपरोक्त प्रक्रिया समानांतर में कई बार की जाती है (अलग-अलग \\( W_Q, W_K, W_V \\) के साथ) विभिन्न प्रकार के रिश्तों को पकड़ने के लिए:
  - इनपुट को \\( h \\) हेड्स में विभाजित किया जाता है, प्रत्येक में छोटे \\( Q, K, V \\) वेक्टर्स आयाम \\( d_k = d_{\text{model}} / h \\) के साथ।
  - प्रत्येक हेड अपना स्वयं का अटेंशन आउटपुट गणना करता है।
  - सभी हेड्स से आउटपुट को संयोजित किया जाता है और एक अंतिम रैखिक परिवर्तन के माध्यम से पारित किया जाता है:
    \\[
    \text{MultiHead}(Q, K, V) = \text{Concat}(\text{head}_1, \text{head}_2, \dots, \text{head}_h) W_O
    \\]
    जहां \\( W_O \in \mathbb{R}^{h \cdot d_v \times d_{\text{model}}} \\) एक सीखा हुआ आउटपुट प्रोजेक्शन मैट्रिक्स है।

---

### 3. **ट्रांसफॉर्मर LLM में K, Q, V की भूमिका**
**K, Q, V** तंत्र का उपयोग ट्रांसफॉर्मर आर्किटेक्चर के विभिन्न भागों में किया जाता है, जो अटेंशन के प्रकार पर निर्भर करता है:

- **एनकोडर में सेल्फ-अटेंशन (उदाहरण के लिए, BERT):**
  - सभी टोकन इनपुट अनुक्रम में अन्य सभी टोकन पर ध्यान देते हैं (द्विदिश अटेंशन)।
  - \\( Q, K, V \\) सभी एक ही इनपुट अनुक्रम \\( X \\) से प्राप्त होते हैं।
  - यह मॉडल को पूर्ववर्ती और अनुवर्ती दोनों टोकन से संदर्भ प्राप्त करने की अनुमति देता है, जो टेक्स्ट वर्गीकरण या प्रश्नोत्तर जैसे कार्यों के लिए उपयोगी है।

- **डिकोडर में सेल्फ-अटेंशन (उदाहरण के लिए, GPT):**
  - ऑटोरेग्रेसिव मॉडल जैसे GPT में, डिकोडर भविष्य के टोकन पर ध्यान देने से रोकने के लिए **मास्क्ड सेल्फ-अटेंशन** का उपयोग करता है (क्योंकि मॉडल टेक्स्ट को क्रमिक रूप से जनरेट करता है)।
  - मास्क यह सुनिश्चित करता है कि प्रत्येक टोकन \\( i \\) के लिए, टोकन \\( j > i \\) के लिए अटेंशन स्कोर सॉफ्टमैक्स से पहले \\(-\infty\\) पर सेट किए जाते हैं, जिससे उनका वजन प्रभावी रूप से शून्य हो जाता है।
  - \\( Q, K, V \\) अभी भी इनपुट अनुक्रम से प्राप्त होते हैं, लेकिन अटेंशन कारणात्मक होता है (केवल पिछले टोकन पर ध्यान देना)।

- **एनकोडर-डिकोडर मॉडल में क्रॉस-अटेंशन (उदाहरण के लिए, T5):**
  - एनकोडर-डिकोडर आर्किटेक्चर में, डिकोडर एनकोडर के आउटपुट पर ध्यान देने के लिए क्रॉस-अटेंशन का उपयोग करता है।
  - यहां, \\( Q \\) डिकोडर के इनपुट से प्राप्त होता है, जबकि \\( K \\) और \\( V \\) एनकोडर के आउटपुट से आते हैं, जिससे डिकोडर को आउटपुट जनरेट करते समय इनपुट अनुक्रम के प्रासंगिक भागों पर ध्यान केंद्रित करने की अनुमति मिलती है।

---

### 4. **K, Q, V इतने प्रभावी क्यों हैं**
**K, Q, V** तंत्र कई कारणों से शक्तिशाली है:
- **गतिशील संदर्भीकरण**: यह प्रत्येक टोकन को उनकी सामग्री के आधार पर अन्य टोकन से जानकारी एकत्र करने की अनुमति देता है, न कि निश्चित पैटर्न पर निर्भर रहने की (जैसे, RNN या CNN में)।
- **समानांतरीकरण**: रिकरंट न्यूरल नेटवर्क के विपरीत, सेल्फ-अटेंशन सभी टोकन को एक साथ प्रोसेस करता है, जिससे यह GPU जैसे आधुनिक हार्डवेयर पर अत्यधिक कुशल बन जाता है।
- **लचीलापन**: मल्टी-हेड अटेंशन मॉडल को \\( Q, K, V \\) के लिए विभिन्न प्रोजेक्शन सीखकर विविध संबंधों (जैसे, वाक्य रचनात्मक, अर्थ संबंधी) को पकड़ने में सक्षम बनाता है।
- **मापनीयता**: यह तंत्र लंबे अनुक्रमों के लिए अच्छी तरह से काम करता है (हालांकि कम्प्यूटेशनल लागत अनुक्रम लंबाई के साथ द्विघात रूप से बढ़ती है, जिसे स्पार्स अटेंशन या कुशल ट्रांसफॉर्मर जैसी तकनीकों से कम किया जाता है)।

---

### 5. **गणितीय सारांश**
स्केल्ड डॉट-प्रोडक्ट अटेंशन फॉर्मूला है:
\\[
\text{Attention}(Q, K, V) = \text{softmax}\left( \frac{Q K^T}{\sqrt{d_k}} \right) V
\\]
मल्टी-हेड अटेंशन के लिए:
\\[
\text{MultiHead}(Q, K, V) = \text{Concat}(\text{head}_1, \dots, \text{head}_h) W_O
\\]
जहां:
\\[
\text{head}_i = \text{Attention}(Q W_{Q_i}, K W_{K_i}, V W_{V_i})
\\]

---

### 6. **व्यावहारिक उदाहरण**
वाक्य पर विचार करें: *"The cat sat on the mat."*
- **इनपुट**: प्रत्येक शब्द को एक एम्बेडिंग वेक्टर में परिवर्तित किया जाता है (उदाहरण के लिए, एक वर्ड एम्बेडिंग लेयर के माध्यम से)।
- **Q, K, V गणना**: प्रत्येक टोकन के लिए, मॉडल सीखे गए वजन का उपयोग करके क्वेरी, की और वैल्यू वेक्टर्स की गणना करता है।
- **अटेंशन स्कोर**: "cat" शब्द के लिए, क्वेरी वेक्टर "sat" और "mat" को उच्च अटेंशन स्कोर निर्दिष्ट कर सकता है क्योंकि वे संदर्भ में प्रासंगिक हैं, जबकि "the" को कम स्कोर दे सकता है।
- **आउटपुट**: अटेंशन तंत्र "cat" के लिए एक नया प्रतिनिधित्व उत्पन्न करता है जो "sat" और "mat" से जानकारी को शामिल करता है, जो उनके अटेंशन स्कोर द्वारा भारित होता है।

---

### 7. **एक्सटेंशन और ऑप्टिमाइजेशन**
- **कुशल अटेंशन**: लंबे अनुक्रमों को संभालने के लिए, Performer, Linformer, या Longformer जैसे वेरिएंट अटेंशन की कम्प्यूटेशनल जटिलता को कम करते हैं।
- **पोजिशनल एनकोडिंग**: चूंकि सेल्फ-अटेंशन क्रमचय-अपरिवर्तनीय है, ट्रांसफॉर्मर शब्द क्रम को पकड़ने के लिए इनपुट एम्बेडिंग में पोजिशनल एनकोडिंग जोड़ते हैं, जो अप्रत्यक्ष रूप से \\( Q, K, V \\) को प्रभावित करता है।
- **लेयर स्टैकिंग**: ट्रांसफॉर्मर सेल्फ-अटेंशन की कई परतों को स्टैक करते हैं, जिससे \\( Q, K, V \\) को तेजी से अमूर्त प्रतिनिधित्व पर काम करने की अनुमति मिलती है।

---

### 8. **निष्कर्ष**
**K, Q, V** तंत्र ट्रांसफॉर्मर की सेल्फ-अटेंशन का हृदय है, जो LLM को टेक्स्ट में जटिल संबंधों को कुशलतापूर्वक और प्रभावी ढंग से मॉडल करने में सक्षम बनाता है। क्वेरी और की के आधार पर अटेंशन स्कोर की गणना करके, और वैल्यू वेक्टर्स से जानकारी एकत्रित करके, ट्रांसफॉर्मर भाषा मॉडलिंग, अनुवाद और अन्य कार्यों में अत्याधुनिक प्रदर्शन प्राप्त करते हैं। इस तंत्र को समझना आधुनिक LLM के मानव-जैसे टेक्स्ट को प्रोसेस और जनरेट करने के तरीके को ग्रहण करने की कुंजी है।

यदि आप किसी विशिष्ट पहलू (जैसे, मल्टी-हेड अटेंशन, कोड कार्यान्वयन, या विशिष्ट मॉडल में अनुप्रयोग) में गहराई से उतरना चाहते हैं, तो मुझे बताएं!