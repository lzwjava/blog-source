---
audio: false
generated: true
image: false
lang: hi
layout: post
model: anthropic/claude-4.5-sonnet
title: डीपसीक के विरल ध्यान दक्षता में क्रांति
translated: true
type: note
---

DeepSeek का नेटिव स्पार्स अटेंशन (NSA) लार्ज लैंग्वेज मॉडल्स के लिए कुशल लॉन्ग-कॉन्टेक्स्ट मॉडलिंग में एक क्रांतिकारी अविष्कार है। पारंपरिक फुल अटेंशन मैकेनिज्म की द्विघात कम्प्यूटेशनल जटिलता के विपरीत, NSA एक परिष्कृत हायरार्किकल स्पार्स अटेंशन रणनीति के माध्यम से कम्प्यूटेशनल लागत को बुद्धिमानी से कम करते हुए मॉडल परफॉर्मेंस को बनाए रखती है या उससे भी अधिक करती है।[1][2]

## कोर आर्किटेक्चर और डिजाइन फिलॉसफी

NSA लॉन्ग-कॉन्टेक्स्ट मॉडलिंग की मौलिक चुनौती को संबोधित करता है: स्टैंडर्ड अटेंशन मैकेनिज्म को O(n²) कम्प्यूटेशन की आवश्यकता होती है, जहाँ n सीक्वेंस लंबाई है, जो हजारों टोकन से अधिक के कॉन्टेक्स्ट के लिए उन्हें अत्यधिक महंगा बना देती है। **NSA एक डायनामिक हायरार्किकल स्पार्स रणनीति का उपयोग करता है, जो ग्लोबल कॉन्टेक्स्ट जागरूकता और लोकल प्रिसिजन दोनों को बनाए रखने के लिए कोर्स-ग्रेन्ड टोकन कम्प्रेशन के साथ फाइन-ग्रेन्ड टोकन सिलेक्शन को जोड़ता है**[3]

यह मैकेनिज्म दो मुख्य सिद्धांतों पर काम करता है:

1. **सभी टोकन को समान अटेंशन की आवश्यकता नहीं होती** - कुछ को संपीड़ित या सारांशित किया जा सकता है
2. **हार्डवेयर ऑप्टिमाइजेशन आवश्यक है** - एल्गोरिदम दक्षता का वास्तविक दुनिया में तेजी से निष्पादन के बिना कोई मतलब नहीं है

## थ्री-ब्रांच आर्किटेक्चर

NSA अटेंशन को तीन समानांतर शाखाओं के माध्यम से संसाधित करता है जो एक कुशल स्पार्स अटेंशन पैटर्न बनाने के लिए मिलकर काम करती हैं:[4]

### 1. **कम्प्रेशन ब्रांच**
यह शाखा लगातार टोकन को ब्लॉकों में समूहित करके और उन्हें प्रतिनिधि टोकन में संपीड़ित करके कोर्स-ग्रेन्ड कॉन्टेक्स्ट एकत्रीकरण को संभालती है। कम्प्रेशन मैकेनिज्म टोकन समूहों के सारांशित प्रतिनिधित्व बनाकर उन टोकन की संख्या को कम कर देता है जिन पर मॉडल को ध्यान देना चाहिए। उदाहरण के लिए, 32,768-टोकन वाली सीक्वेंस को लगभग 2,046 कम्प्रेशन टोकन में संपीड़ित किया जा सकता है।[5]

कम्प्रेशन यह निर्धारित करने के लिए सीखे गए गेटिंग मैकेनिज्म का उपयोग करता है कि कई टोकन से जानकारी को एकल प्रतिनिधि टोकन में कैसे एकत्र किया जाना चाहिए, जिससे पूरी कम्प्यूटेशनल लागत के बिना वैश्विक संदर्भ जागरूकता संरक्षित रहे।

### 2. **सिलेक्शन ब्रांच**
यह शाखा वर्तमान क्वेरी के लिए सबसे महत्वपूर्ण टोकन की गतिशील रूप से पहचान करके फाइन-ग्रेन्ड टोकन चयन को लागू करती है। सभी टोकन पर ध्यान देने के बजाय, मॉडल महत्व स्कोर की गणना करता है और केवल उन टोकन पर चुनिंदा रूप से ध्यान देता है जो वर्तमान क्वेरी के लिए सबसे प्रासंगिक हैं। यह स्थानीय सटीकता बनाए रखता है और उन महत्वपूर्ण विवरणों को कैप्चर करता है जो केवल कम्प्रेशन के माध्यम से खो सकते हैं।

चयन प्रक्रिया प्रशिक्षण के दौरान सीखी जाती है, जो मॉडल को विभिन्न संदर्भों और कार्यों के लिए यह अनुकूली रूप से निर्धारित करने की अनुमति देती है कि कौन से टोकन सबसे अधिक सूचना मूल्य रखते हैं।[6]

### 3. **स्लाइडिंग विंडो ब्रांच**
यह शाखा प्रत्येक टोकन को एक निश्चित विंडो के भीतर अपने तत्काल पड़ोसियों पर ध्यान देने की अनुमति देकर स्थानीय संदर्भ बनाए रखती है। यह सुनिश्चित करता है कि कम्प्रेशन या सिलेक्शन निर्णयों की परवाह किए बिना शॉर्ट-रेंज निर्भरताएं हमेशा कैप्चर की जाती हैं। स्लाइडिंग विंडो आमतौर पर एक परिभाषित त्रिज्या के भीतर हाल के टोकन को कवर करती है।

## गणितीय आधार

NSA में अटेंशन कम्प्यूटेशन को तीन अलग-अलग की-वैल्यू सेट पर संचालित होने के रूप में व्यक्त किया जा सकता है:

- कम्प्रेशन ब्रांच से **कम्प्रेस्ड KV पेयर्स**
- सिलेक्शन ब्रांच से **सिलेक्टेड KV पेयर्स**
- स्लाइडिंग विंडो से **लोकल KV पेयर्स**

सभी n टोकन पर अटेंशन की गणना करने के बजाय, NSA इन तीन स्रोतों को जोड़ने वाले बहुत छोटे प्रभावी सेट पर अटेंशन की गणना करता है। **हायरार्किकल टोकन कम्प्रेशन को ब्लॉकवाइज टोकन सिलेक्शन के साथ एकीकृत करके**[3], यह मैकेनिज्म द्विघात जटिलता को लगभग रैखिक या निकट-रैखिक स्केलिंग तक कम कर देता है।

## हार्डवेयर-अलाइंड ऑप्टिमाइजेशन

NSA का एक महत्वपूर्ण नवाचार इसकी हार्डवेयर-कॉन्शियस डिजाइन है। पिछले स्पार्स अटेंशन तरीके अक्सर वास्तविक दुनिया की गति नहीं दे पाते थे क्योंकि वे आधुनिक GPU आर्किटेक्चर के लिए अनुकूलित नहीं थे।[1]

NSA निम्नलिखित के माध्यम से substantial स्पीडअप प्राप्त करता है:

### **ब्लॉकवाइज मेमोरी एक्सेस पैटर्न**
एल्गोरिथ्म डेटा को ब्लॉकों में व्यवस्थित करता है जो GPU मेमोरी हायरार्की और Tensor Core ऑपरेशंस के साथ संरेखित होते हैं। यह कॉलेस्ड मेमोरी लोड को अधिकतम करता है और GPU कम्प्यूट यूनिट के कुशल उपयोग को सक्षम बनाता है।[3]

### **अंकगणितीय तीव्रता संतुलन**
एल्गोरिथ्म को उच्च अंकगणितीय तीव्रता बनाए रखने के लिए डिज़ाइन किया गया है - मेमोरी एक्सेस के लिए कम्प्यूटेशन का अनुपात। यह सुनिश्चित करता है कि GPU मेमोरी-बाउंड के बजाय कम्प्यूट-बाउंड रहें, जिससे हार्डवेयर उपयोग अधिकतम हो।

### **फ्यूज्ड कर्नेल इम्प्लीमेंटेशन**
NSA कई ऑपरेशंस को सिंगल फ्यूज्ड कर्नेल में जोड़ता है, जिससे रेडंडेंट KV कैशे ट्रांसफर और इंटरमीडिएट टेंसर मटेरियलाइजेशन को खत्म किया जाता है।[5] इससे मेमोरी बैंडविड्थ आवश्यकताएं काफी कम हो जाती हैं।

### **ऑप्टिमाइज्ड लूप शेड्यूलिंग**
सावधानीपूर्वक कर्नेल-लेवल ऑप्टिमाइजेशन रेडंडेंट मेमोरी ऑपरेशंस को खत्म करता है और रजिस्टर रीयूज को अधिकतम करता है।

## परफॉर्मेंस गेन

दक्षता में सुधार substantial हैं:[7]

- ट्रेनिंग के दौरान FlashAttention-2 की तुलना में **फॉरवर्ड कम्प्यूटेशन में 9.0× तक तेज**
- **बैकवर्ड पास 6.0× तेज**
- 64k-लंबाई वाली सीक्वेंस के लिए डिकोडिंग के दौरान **11.6× स्पीडअप**
- बेंचमार्क में **फुल अटेंशन परफॉर्मेंस को बनाए रखता है या उससे अधिक करता है**

लंबी सीक्वेंस के लिए स्पीडअप विशेष रूप से dramatic है। 64k-टोकन सीक्वेंस के लिए, NSA लगभग 11.6× तेज डिकोडिंग हासिल करता है क्योंकि यह मेमोरी से बहुत कम KV कैश डेटा लोड करता है।[3]

## नेटिव ट्रेनएबिलिटी - एक महत्वपूर्ण उन्नति

पिछले कई स्पार्स अटेंशन तरीकों के विपरीत जो केवल इनफरेंस को तेज करते थे, **NSA एंड-टू-एंड ट्रेनिंग को सक्षम बनाता है, जिससे मॉडल परफॉर्मेंस को त्यागे बिना प्रीट्रेनिंग कम्प्यूटेशन कम हो जाता है**[1]. स्पार्सिटी पैटर्न प्रशिक्षण के दौरान सीखा जाता है न कि फिक्स्ड या ह्यूरिस्टिक-आधारित होता है।

इसका मतलब है:
- मॉडल सीखता है कि किन टोकन को कम्प्रेस करना है और किन का चयन करना है
- ग्रेडिएंट्स स्पार्स अटेंशन निर्णयों के माध्यम से फ्लो होते हैं
- कम्प्रेशन और सिलेक्शन रणनीतियाँ विशिष्ट कार्य और डेटा वितरण के अनुकूल हो जाती हैं

यह नेटिव ट्रेनएबिलिटी महत्वपूर्ण है क्योंकि यह मॉडल को हैंड-क्राफ्टेड नियमों पर निर्भर रहने के बजाय इष्टतम स्पार्सिटी पैटर्न खोजने की अनुमति देती है।

## पारंपरिक अटेंशन पर फायदे

**कम्प्यूटेशनल दक्षता**: द्विघात जटिलता को निकट-रैखिक तक कम करती है, जिससे 100k+ टोकन कॉन्टेक्स्ट का व्यावहारिक प्रोसेसिंग संभव होता है।

**मेमोरी दक्षता**: ट्रेनिंग और इनफरेंस दोनों के दौरान KV कैश मेमोरी आवश्यकताओं को नाटकीय रूप से कम करती है।

**परफॉर्मेंस संरक्षण**: प्रायोगिक परिणाम दिखाते हैं कि NSA-प्रशिक्षित मॉडल सामान्य बेंचमार्क, लॉन्ग-कॉन्टेक्स्ट कार्यों और इंस्ट्रक्शन-आधारित रीजनिंग में फुल अटेंशन मॉडल्स के बराबर या उनसे बेहतर प्रदर्शन करते हैं।[3]

**हार्डवेयर स्पीडअप**: कुछ स्पार्स तरीकों के विपरीत जो सैद्धांतिक लाभ दिखाते हैं लेकिन सीमित वास्तविक दुनिया में सुधार, NSA वास्तविक GPU हार्डवेयर पर substantial मापा गया स्पीडअप प्रदान करता है।

**अनुकूली स्पार्सिटी**: सीखे गए अटेंशन पैटर्न फिक्स्ड पैटर्न का उपयोग करने के बजाय कार्य आवश्यकताओं के अनुकूल होते हैं।

## तकनीकी कार्यान्वयन विवरण

कार्यान्वयन कई परिष्कृत तकनीकों का लाभ उठाता है:

- **डायनामिक हायरार्किकल कम्प्रेशन** जो सामग्री के आधार पर कम्प्रेशन अनुपात को अनुकूलित करता है
- इंटेलिजेंट टोकन मर्जिंग के लिए **गेटेड एग्रीगेशन मैकेनिज्म**
- सीखे गए महत्व मेट्रिक्स का उपयोग करके **स्कोर-आधारित टोकन सिलेक्शन**
- GPU कैश हायरार्की के लिए अनुकूलित **ब्लॉक-अलाइंड मेमोरी ऑपरेशंस**
- **Triton-आधारित कस्टम कर्नेल** जो स्टैंडर्ड इम्प्लीमेंटेशन से बेहतर प्रदर्शन करते हैं[8]

## हाल के विकास

DeepSeek ने हाल ही में DeepSeek-V3.2-Exp की घोषणा की, जो DeepSeek Sparse Attention (DSA) नामक एक उन्नत संस्करण को लागू करता है। यह नया वेरिएंट आउटपुट क्वालिटी पर न्यूनतम प्रभाव के साथ फाइन-ग्रेन्ड स्पार्स अटेंशन प्राप्त करता है, जिससे कम्प्यूटेशनल लागत को कम करते हुए लॉन्ग-कॉन्टेक्स्ट परफॉर्मेंस को और बढ़ावा मिलता है।[9]

## निष्कर्ष

NSA एल्गोरिदम दक्षता, हार्डवेयर उपयोग और मॉडल ट्रेनएबिलिटी का एक साथ अनुकूलन करके अटेंशन मैकेनिज्म डिजाइन में एक पैराडाइम शिफ्ट का प्रतिनिधित्व करता है। हायरार्किकल कम्प्रेशन, डायनामिक सिलेक्शन और स्लाइडिंग विंडो को हार्डवेयर-अलाइंड इम्प्लीमेंटेशन के साथ जोड़कर, यह लॉन्ग-कॉन्टेक्स्ट मॉडलिंग को व्यावहारिक और कुशल बनाता है। यह मैकेनिज्म प्रदर्शित करता है कि एल्गोरिदम और हार्डवेयर ऑप्टिमाइजेशन के सावधानीपूर्वक सह-डिजाइन से मॉडल गुणवत्ता से समझौता किए बिना गति और मेमोरी दक्षता दोनों में आर्डर-ऑफ-मैग्निट्यूड सुधार दिया जा सकता है।

उद्धरण:
[1] https://arxiv.org/abs/2502.11089
[2] https://medium.com/data-science-in-your-pocket/deepseek-native-sparse-attention-advanced-attention-mechanism-for-llms-6ac68fc014ff
[3] https://arxiv.org/pdf/2502.11089
[4] https://shchegrikovich.substack.com/p/attention-vs-attention
[5] https://medium.com/@nanda.yugandhar/the-illustrated-guid-to-native-sparse-attention-b657b5e76bbc
[6] https://www.marktechpost.com/2025/02/18/deepseek-ai-introduces-nsa-a-hardware-aligned-and-natively-trainable-sparse-attention-mechanism-for-ultra-fast-long-context-training-and-inference/
[7] https://medium.com/foundation-models-deep-dive/deepseeks-nsa-for-efficient-attention-14b6f01486d5
[8] https://arxiv.org/html/2502.11089v1
[9] https://api-docs.deepseek.com/news/news250929