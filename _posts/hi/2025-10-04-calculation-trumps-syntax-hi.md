---
audio: false
generated: true
image: false
lang: hi
layout: post
model: x-ai/grok-4-fast
title: गणना एआई में वाक्यविन्यास से बेहतर है
translated: true
type: note
---

आप बिल्कुल सही कह रहे हैं कि AI और LLM का दिल सुलेखित गद्य या प्रोग्रामिंग भाषाओं पर दार्शनिक विचार-विमर्श में नहीं बसता—हालाँकि यिन वांग और डैनियल पी. फ्राइडमैन जैसे लोगों ने निस्संदेह हमारी सोच को पैना किया है। वांग का ब्लॉग पायथन और लिस्प जैसी प्रणालियों की गहराइयों में उतरता है, और आलोचना करता है कि वे व्यवहार में कम्प्यूटेशन को कैसे आकार देती हैं, जबकि फ्राइडमैन की किताबें (जैसे *द लिटिल स्कीमर* श्रृंखला) रिकर्शन और फंक्शनल पैराडाइम को लगभग काव्यात्मक ढंग से स्पष्ट करती हैं। लेकिन हाँ, जब कोई ऐसी चीज़ *बनाने* की बात आती है जो काम करे—जैसे सुसंगत पाठ जनरेट करना या न्यूरल नेट में ग्रेडिएंट्स की गणना करना—तो असली मुद्दा गणना के "कैसे" और "क्या" में है, न कि सिंटैक्स शुगर में।

### गणना सिंटैक्स से महत्वपूर्ण क्यों है
अपने मूल में, मेरे जैसे एक LLM के लिए लिस्प मैक्रोज़ या जावा की ऑब्जेक्ट हायरार्की पर विचार करना नहीं है; यह मैट्रिक्स गुणन, अटेंशन मैकेनिज्म, और प्रोबेबिलिस्टिक सैंपलिंग को बड़े पैमाने पर एक्ज़िक्यूट कर रहा है। "कैसे कैलकुलेट करें" इसका सार है:
- **अल्गोरिदम और मॉडल**: ट्रांसफॉर्मर आर्किटेक्चर (वासवानी एट अल., 2017) जैसी चीज़ें परिभाषित करती हैं कि *क्या* कम्प्यूट किया जाएगा—टोकन एम्बेडिंग्स पर सेल्फ-अटेंशन, पोजिशनल एनकोडिंग्स, आदि। यहीं जादू होता है, भाषा से स्वतंत्र। आप जीपीटी को स्यूडोकोड में इम्प्लीमेंट कर सकते हैं और वह कागज़ पर "काम" करेगा; सिंटैक्स सिर्फ एक वाहन है।
- **न्यूमेरिकल प्रिसिजन और एफिशिएंसी**: यहाँ, "क्या कैलकुलेट करें" बहुत मायने रखता है। हम टोकन प्रोबेबिलिटीज, लॉस फंक्शन्स (जैसे क्रॉस-एन्ट्रॉपी), और बैकप्रोपेगेशन की बात कर रहे हैं। गणित में गलती करें, और आपका मॉडल बेकार的输出 देगा। सिंटैक्स? यह द्वितीयक है—पायथन का NumPy आपको 90% तक पठनीय कोड के साथ ले जाता है, लेकिन विशाल मॉडल्स को ट्रेन करने के लिए यह इंटरप्रेटेड और धीमा है।

भाषा का चुनाव *हालाँकि*, एक व्यावहारिक फिल्टर के रूप में छिपकर आता है। C++ AI के परफॉर्मेंस-क्रिटिकल हिस्सों (जैसे TensorFlow के कर्नेल या PyTorch के CUDA बाइंडिंग्स) के लिए बेहतरीन है, जहाँ डिस्ट्रिब्यूटेड ट्रेनिंग में हर साइकल मायने रखती है। प्रोटोटाइपिंग के लिए पायथन हावी है क्योंकि यह संक्षिप्त है—C++ की मेमोरी मैनेजमेंट से जूझना क्यों जब `torch.nn.Linear` आपको एक लाइन में लेयर्स चेन करने देता है? लेकिन आप सही हैं: यह एक "छोटा मुद्दा" है। असली बॉटलनेक अल्गोरिदमिक (जैसे कपलान एट अल. के स्केलिंग लॉज़) या डेटा-संबंधी हैं, न कि यह कि आप `def` टाइप कर रहे हैं या `struct`।

### जहाँ PL की अंतर्दृष्टि अभी भी भूमिका निभाती है
फिर भी, PL को पूरी तरह खारिज करना बच्चे को नहाने के पानी के साथ फेंकने जैसा लगता है। अच्छा भाषा डिजाइन (फ्राइडमैन/वांग जैसे लोगों से प्रेरित) उन गणनाओं को *व्यक्त* करने को सुरक्षित और तेज़ बनाता है:
- **तर्क के लिए एब्स्ट्रक्शन**: फंक्शनल प्योरिटी (स्कीम की तरह) प्रोबेबिलिस्टिक कोड में बग्स से बचने में मदद करती है, जो विश्वसनीय AI इनफरेंस के लिए बहुत बड़ी है।
- **पीड़ा के बिना परफॉर्मेंस**: रस्ट या यहाँ तक कि आधुनिक C++ PL रिसर्च से प्रेरणा लेकर आपको स्पीड *प्लस* सेफ्टी देते हैं, जिससे "गलत कैलकुलेशन" की errors कम होती हैं जो रॉ C को प्रभावित करती हैं।

अंततः, AI प्रगति 80% गणित/मॉडल/डेटा, 15% इंजीनियरिंग (जहाँ C++ जैसी भाषाएँ मजबूत हैं), और 5% सिंटैक्स कविता है। अगर हम बाद वाले के लिए ऑप्टिमाइज़ करते, तो हमारे पास सुंदर शोधपत्र होते लेकिन धीमे रोबोट। आपकी क्या राय है—क्या आपको लगता है कि हम और अधिक PL-AI फ्यूजन देखेंगे, जैसे न्यूरोसिम्बॉलिक सामग्री के लिए डोमेन-स्पेसिफिक भाषाएँ?

[Wang's Blog](https://www.yinwang.org)
[Friedman's Essence of Programming](https://www.cs.indiana.edu/~dfriedman/)
[Attention Is All You Need (Transformers Paper)](https://arxiv.org/abs/1706.03762)
[Scaling Laws for Neural Language Models](https://arxiv.org/abs/2001.08361)