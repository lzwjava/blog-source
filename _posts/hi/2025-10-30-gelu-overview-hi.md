---
audio: false
generated: true
image: false
lang: hi
layout: post
model: x-ai/grok-4-fast
title: जीईएलयू एक्टिवेशन फंक्शन का अवलोकन
translated: true
type: note
---

### जीईएलयू क्या है?

गाउसियन एरर लीनियर यूनिट (GELU) एक स्मूथ एक्टिवेशन फंक्शन है जो न्यूरल नेटवर्क्स में उपयोग किया जाता है, खासकर BERT और GPT श्रृंखला जैसे ट्रांसफॉर्मर-आधारित मॉडल्स में। इसे 2016 में शोधकर्ताओं डैन हेंड्रिक्स और केविन गिम्पेल द्वारा पेश किया गया था। यह लीनियर ट्रांसफॉर्मेशन के फायदों को गाउसियन (सामान्य) वितरण से प्रेरित प्रोबेबिलिस्टिक वेटिंग के साथ जोड़ता है। ReLU जैसे सरल एक्टिवेशन फंक्शंस (जो नेगेटिव इनपुट के लिए शून्य आउटपुट देता है) के विपरीत, GELU नेगेटिव वैल्यूज के लिए एक छोटे ग्रेडिएंट फ्लो की अनुमति देता है, जिससे वैनिशिंग ग्रेडिएंट्स जैसी समस्याओं को कम करके और कन्वर्जेंस में सुधार करके डीप नेटवर्क्स में बेहतर प्रदर्शन होता है।

#### गणितीय परिभाषा
GELU का मुख्य सूत्र है:

\\[
\text{GELU}(x) = x \cdot \Phi(x)
\\]

जहाँ \\(\Phi(x)\\) स्टैंडर्ड नॉर्मल डिस्ट्रीब्यूशन का क्यूमुलेटिव डिस्ट्रीब्यूशन फंक्शन (CDF) है:

\\[
\Phi(x) = \frac{1}{2} \left[ 1 + \erf\left( \frac{x}{\sqrt{2}} \right) \right]
\\]

यहाँ, \\(\erf\\) एरर फंक्शन है, जो मापता है कि एक गाउसियन स्टेप फंक्शन का कितना अच्छा अनुमान लगाता है। यह GELU को प्रोबेबिलिस्टिक बनाता है: यह नेगेटिव इनपुट को ReLU की तुलना में कम कठोरता से "ड्रॉप" करता है जबकि पॉजिटिव इनपुट को बरकरार रखता है।

कम्प्यूटेशनल दक्षता के लिए, अनुमानित सूत्र अक्सर उपयोग किए जाते हैं, जैसे कि सिग्मॉइड-आधारित संस्करण:

\\[
\text{GELU}(x) \approx x \cdot \sigma(1.702x)
\\]

जहाँ \\(\sigma\\) सिग्मॉइड फंक्शन है।

#### GELU का उपयोग क्यों करें?
- **स्मूथनेस**: यह हर जगह डिफरेंशिएबल है, जो ऑप्टिमाइजेशन में सहायता करता है।
- **नॉन-मोनोटोनिसिटी**: नेगेटिव वैल्यूज को थोड़ा दबाता है लेकिन उन्हें शून्य नहीं करता है, जिससे जानकारी सुरक्षित रहती है।
- **अनुभवजन्य फायदे**: नेचुरल लैंग्वेज प्रोसेसिंग जैसे कार्यों में ReLU और ELU से बेहतर प्रदर्शन करता है, जिससे बड़े मॉडल्स में तेज ट्रेनिंग और उच्च सटीकता मिलती है।
- **नुकसान**: `erf` या `sigmoid` कम्प्यूटेशन के कारण ReLU की तुलना में थोड़ा अधिक कम्प्यूटेशनली महंगा है।

व्यवहार में, PyTorch और TensorFlow जैसी लाइब्रेरीज में GELU के बिल्ट-इन इम्प्लीमेंटेशन हैं (उदाहरण के लिए, `torch.nn.GELU()`)।

#### संदर्भ
- [Gaussian Error Linear Units (GELUs)](https://arxiv.org/abs/1606.08415)
- [GELU Explained](https://www.baeldung.com/cs/gelu-activation-function)
- [GELU Activation](https://medium.com/@shauryagoel/gelu-gaussian-error-linear-unit-4ec59fb2e47c)