---
audio: false
generated: true
image: false
lang: hi
layout: post
model: x-ai/grok-4-fast
title: गहरी न्यूरल नेटवर्क प्रशिक्षण का अनुकूलन
translated: true
type: note
---

### अध्याय 8: गहन मॉडलों के प्रशिक्षण के लिए अनुकूलन

यह अध्याय इयान गुडफेलो, योशुआ बेंजियो और एरोन कौरविले द्वारा लिखित *डीप लर्निंग* में गहन न्यूरल नेटवर्क को प्रशिक्षित करने के लिए तैयार की गई ग्रेडिएंट-आधारित अनुकूलन तकनीकों पर केंद्रित है। यह अध्याय 4 के बुनियादी अवधारणाओं पर आधारित है और इष्टतम पैरामीटर \\( \theta \\) खोजने के लिए एक लागत फ़ंक्शन \\( J(\theta) \\) को कम करने पर जोर देता है, जहां \\( J(\theta) \\) आमतौर पर प्रशिक्षण डेटा पर एक लॉस और रेगुलराइजेशन शर्तों को जोड़ता है। लक्ष्य वास्तविक जोखिम \\( J^*(\theta) = \mathbb{E}_{(x,y) \sim p_{data}} L(f(x;\theta), y) \\) का अनुमान लगाना है, लेकिन व्यवहार में, यह प्रशिक्षण सेट पर अनुभवजन्य जोखिम के माध्यम से किया जाता है।

#### शुद्ध अनुकूलन से सीखना कैसे भिन्न है
मशीन लर्निंग अनुकूलन सीधे लागत फ़ंक्शन को कम करने के बारे में नहीं है बल्कि अदृश्य डेटा (जैसे, टेस्ट सेट सटीकता) पर प्रदर्शन में अप्रत्यक्ष रूप से सुधार करने के बारे में है। प्रमुख अंतरों में शामिल हैं:
- **अप्रत्यक्ष लक्ष्य**: लागत \\( J(\theta) \\) एक अंतर्निहित माप (जैसे 0-1 लॉस) के लिए एक प्रॉक्सी के रूप में कार्य करती है। सरोगेट लॉस (उदाहरण के लिए, वर्गीकरण के लिए नकारात्मक लॉग-संभावना) का उपयोग किया जाता है क्योंकि वास्तविक लॉस में अक्सर उपयोगी ग्रेडिएंट्स का अभाव होता है।
- **विघटनशीलता**: \\( J(\theta) \\) उदाहरणों पर औसत लेता है, जो अनुभवजन्य जोखिम न्यूनीकरण (ERM) को सक्षम बनाता है: \\( J(\theta) \approx \frac{1}{m} \sum_{i=1}^m L(f(x^{(i)};\theta), y^{(i)}) \\)।
- **ओवरफिटिंग के जोखिम**: उच्च-क्षमता वाले मॉडल प्रशिक्षण डेटा को याद कर सकते हैं, इसलिए अर्ली स्टॉपिंग (वैलिडेशन प्रदर्शन के आधार पर) महत्वपूर्ण है, भले ही प्रशिक्षण लॉस कम होता रहे।
- **बैच रणनीतियाँ**:
  - **बैच विधियाँ**: सटीक ग्रेडिएंट्स के लिए पूर्ण डेटासेट का उपयोग करती हैं (नियतात्मक लेकिन बड़े डेटा के लिए धीमी)।
  - **स्टोकेस्टिक ग्रेडिएंट डिसेंट (SGD)**: एकल उदाहरणों का उपयोग करती है (शोरयुक्त लेकिन तेज़ अपडेट)।
  - **मिनीबैच विधियाँ**: दोनों का संतुलन, डीप लर्निंग में आम (आकार जैसे 32–256)। छोटे बैचों से शोर रेगुलराइजेशन में सहायता करता है; शफलिंग पूर्वाग्रह को रोकती है।

ऑनलाइन लर्निंग (स्ट्रीमिंग डेटा) दोहराव के बिना वास्तविक जोखिम ग्रेडिएंट्स का अनुमान लगाती है।

#### डीप लर्निंग अनुकूलन में चुनौतियाँ
गहन मॉडलों को प्रशिक्षित करना कम्प्यूटेशनल रूप से गहन (क्लस्टर पर दिनों से लेकर महीनों तक) होता है और शास्त्रीय अनुकूलन की तुलना में कठिन होता है, इसके कारण:
- **दुर्गमता**: गैर-अवकलनीय लॉस और ERM में ओवरफिटिंग।
- **पैमाना**: बड़े डेटासेट पूर्ण-बैच ग्रेडिएंट्स को अव्यवहार्य बना देते हैं; सैंपलिंग विचरण का परिचय देती है (त्रुटि \\( 1/\sqrt{n} \\) के रूप में स्केल करती है)।
- **डेटा समस्याएं**: अतिरेक, सहसंबंध (शफलिंग द्वारा ठीक), और पुनः सैंपलिंग से पूर्वाग्रह।
- **हार्डवेयर सीमाएं**: बैच आकार मेमोरी द्वारा सीमित; अतुल्यकालिक समानांतरता मदद करती है लेकिन असंगतताएं पैदा कर सकती है।
- न्यूरल-विशिष्ट बाधाएं (बाद में विस्तृत): खराब कंडीशनिंग, स्थानीय मिनीमा, पठार, और विलुप्त/विस्फोटक ग्रेडिएंट्स।

प्रथम-कोटि विधियाँ (केवल ग्रेडिएंट) द्वितीय-कोटि (हेसियन-आधारित) की तुलना में शोर को बेहतर सहन करती हैं, जो छोटे बैचों में त्रुटियों को बढ़ाती हैं।

#### अनुकूलन एल्गोरिदम
यह अध्याय \\( J(\theta) \\) को कम करने के लिए एल्गोरिदम की समीक्षा करता है, जो कैनोनिकल SGD से शुरू होकर वेरिएंट तक फैली हुई है:
- **स्टोकेस्टिक ग्रेडिएंट डिसेंट (SGD)**: कोर मिनीबैच अपडेट: \\( \theta \leftarrow \theta - \epsilon \hat{g} \\), जहां \\( \hat{g} \\) मिनीबैच ग्रेडिएंट अनुमान है और \\( \epsilon \\) लर्निंग रेट है। खराब स्थानीय मिनीमा से बचने के लिए शोर के कारण बैच विधियों की तुलना में तेजी से अभिसरण करता है।
- **मोमेंटम और वेरिएंट**: सपाट क्षेत्रों में त्वरण और दोलनों को कम करने के लिए वेग शब्द जोड़ते हैं।
- **अनुकूली विधियाँ**: विरल ग्रेडिएंट्स और विभिन्न स्केलों को संभालने के लिए प्रति-पैरामीटर लर्निंग रेट्स को समायोजित करती हैं (जैसे, AdaGrad, RMSProp, Adam)।
- **द्वितीय-कोटि सन्निकटन**: बेहतर कंडीशनिंग के लिए वक्रता जानकारी (जैसे, संयुग्म ग्रेडिएंट, प्राकृतिक ग्रेडिएंट) का उपयोग करते हैं, लेकिन महंगे हैं।
- **निर्देशांक अवतरण और अन्य**: विशिष्ट आर्किटेक्चर के लिए।

व्यावहारिक सुझावों में लर्निंग रेट शेड्यूलिंग, इनिशियलाइजेशन (जैसे, ज़ेवियर), और विधियों को संयोजित करना (जैसे, प्रीकंडीशनिंग) शामिल हैं।

#### अन्य प्रमुख भाग
- **पैरामीटर इनिशियलाइजेशन**: विलुप्त ग्रेडिएंट्स से बचने के लिए महत्वपूर्ण; सममित/ऑर्थोगोनल इनिट और लेयर-वाइज प्रीट्रेनिंग पर चर्चा करता है।
- **ट्रेडऑफ**: अभिसरण गति, स्थिरता और सामान्यीकरण के बीच संतुलन बनाता है; वैलिडेशन-संचालित ट्यूनिंग पर जोर देता है।
- **उन्नत विषय**: एसिंक सेटिंग्स में विलंबित ग्रेडिएंट्स, वितरित अनुकूलन, और विकासवादी विधियों से कनेक्शन को कवर करता है।

कुल मिलाकर, अध्याय इस बात पर जोर देता है कि कोई एक एल्गोरिदम हावी नहीं है—सफलता समस्या-विशिष्ट ट्यूनिंग पर निर्भर करती है, जिसमें SGD और इसके एक्सटेंशन डीप लर्निंग के लिए मुख्य साधन के रूप में कार्य करते हैं।

[डीप लर्निंग बुक - अध्याय 8: अनुकूलन](https://www.deeplearningbook.org/contents/optimization.html)