---
audio: false
generated: true
lang: hi
layout: post
title: डीप लर्निंग, मशीन लर्निंग और जीपीटी प्रगति (2010-2025)
translated: true
type: note
---

यहाँ 2010 से अक्टूबर 2024 तक की मेरी जानकारी के अनुसार, डीप लर्निंग, मशीन लर्निंग और जीपीटी में प्रमुख सफलताओं का विस्तृत समयरेखा दिया गया है:

## 2010 के प्रारंभिक वर्ष: नींव

- **2012: एलेक्सनेट** - क्रिज़ेव्स्की, सुत्सकेवर और हिंटन के सीएनएन आर्किटेक्चर ने इमेजनेट प्रतियोगिता जीती, त्रुटि दरों को 26% से घटाकर 15.3% किया, जिसने कंप्यूटर विजन में डीप लर्निंग क्रांति की शुरुआत की।

- **2013: वर्ड2वेक** - मिकोलोव ने वर्ड एम्बेडिंग तकनीकों का परिचय दिया, जिसने शब्दों को संदर्भ के आधार पर वेक्टर के रूप में प्रस्तुत किया, जिससे शब्दार्थ समझ संभव हुई।

- **2014: जीएएन (जनरेटिव एडवरसैरियल नेटवर्क)** - गुडफेलो ने एक ऐसा फ्रेमवर्क पेश किया जहां जनरेटर और डिस्क्रिमिनेटर नेटवर्क एक दूसरे के साथ प्रतिस्पर्धा करते हैं, जिससे यथार्थवादी छवि निर्माण संभव हुआ।

- **2014: सीक्वेंस-टू-सीक्वेंस मॉडल** - सुत्सकेवर, विनयल्स और ले ने मशीन अनुवाद के लिए ऐसे मॉडल विकसित किए जो इनपुट अनुक्रमों को आउटपुट अनुक्रमों में मैप कर सकते थे।

## 2010 के मध्य वर्ष: फाउंडेशन मॉडल्स का उदय

- **2015: रेज़नेट** - हे एट अल. ने रेज़िडुअल कनेक्शन पेश किए, जिससे बहुत गहरे नेटवर्क (152+ लेयर्स) के प्रशिक्षण को सक्षम किया गया और 3.57% त्रुटि दर के साथ इमेजनेट जीता।

- **2015: बैच नॉर्मलाइज़ेशन** - इओफ़े और स्ज़ेगेडी ने न्यूरल नेटवर्क प्रशिक्षण को स्थिर और तेज करने के लिए एक तकनीक विकसित की।

- **2015: अटेंशन मैकेनिज्म** - बहदानौ ने न्यूरल मशीन अनुवाद के लिए अटेंशन का परिचय दिया, जिससे मॉडल इनपुट अनुक्रमों के प्रासंगिक भागों पर ध्यान केंद्रित कर सके।

- **2016: अल्फागो** - डीपमाइंड की प्रणाली ने गो के खेल में विश्व चैंपियन ली सेडोल को हराया, जिसमें डीप रीइन्फोर्समेंट लर्निंग को मोंटे कार्लो ट्री सर्च के साथ जोड़ा गया था।

## 2010 के अंतिम वर्ष: ट्रांसफॉर्मर क्रांति

- **2017: ट्रांसफॉर्मर आर्किटेक्चर** - वासवानी एट अल. ने "अटेंशन इज़ ऑल यू नीड" पेपर पेश किया, जिसने आरएनएन को सेल्फ-अटेंशन मैकेनिज्म से बदल दिया।

- **2018: बर्ट** - गूगल के बायडायरेक्शनल एनकोडर रिप्रेजेंटेशन्स फ्रॉम ट्रांसफॉर्मर्स ने प्राकृतिक भाषा समझ में अत्याधुनिक परिणाम हासिल किए।

- **2018: जीपीटी-1** - ओपनएआई ने पहला जनरेटिव प्री-ट्रेंड ट्रांसफॉर्मर जारी किया, जिसमें 117M पैरामीटर थे और इसे बुककॉर्पस पर प्रशिक्षित किया गया था।

- **2019: जीपीटी-2** - ओपनएआई ने 1.5B पैरामीटर तक स्केल किया, जिसमें आश्चर्यजनक जीरो-शॉट क्षमताएं दिखाईं, लेकिन दुरुपयोग की चिंताओं के कारण शुरू में पूर्ण रिलीज रोक दी गई।

## 2020 के प्रारंभिक वर्ष: स्केलिंग और मल्टीमॉडैलिटी

- **2020: जीपीटी-3** - ओपनएआई ने 175B पैरामीटर वाला मॉडल जारी किया, जिसने बिना फाइन-ट्यूनिंग के कार्यों में उल्लेखनीय फ्यू-शॉट लर्निंग क्षमताएं दिखाईं।

- **2021: डीएएल-ई** - ओपनएआई ने प्रदर्शित किया कि ट्रांसफॉर्मर टेक्स्ट विवरणों से छवियां जनरेट कर सकते हैं।

- **2021: कोडेक्स** - ओपनएआई के कोड जनरेशन मॉडल, जो गिटहब कोपिलट को पावर देता है, ने प्रोग्रामिंग क्षमताएं दिखाईं।

- **2021: डिफ्यूज़न मॉडल** - ग्लाइड, डीएएल-ई 2, और स्टेबल डिफ्यूज़न ने बेहतर छवि जनरेशन क्वालिटी पेश की।

- **2022: चैटजीपीटी** - ओपनएआई के जीपीटी मॉडल के लिए कन्वर्सेशनल इंटरफेस ने अभूतपूर्व सार्वजनिक अपनान (2 महीने में 100M उपयोगकर्ता) हासिल किया।

- **2022: पाल्म** - गूगल के 540B पैरामीटर मॉडल ने रीजनिंग क्षमताएं प्रदर्शित कीं।

- **2022: चिंचिला** - डीपमाइंड ने इष्टतम स्केलिंग नियम दिखाए, जिससे पता चला कि अधिक डेटा वाले छोटे मॉडल बड़े मॉडलों से बेहतर प्रदर्शन कर सकते हैं।

## 2023-2024: मल्टीमॉडल एलएलएम और रीजनिंग

- **2023: जीपीटी-4** - ओपनएआई का मल्टीमॉडल मॉडल जिसमें बेहतर रीजनिंग, सुरक्षा और इमेज समझ क्षमताएं हैं।

- **2023: क्लॉड** - एन्थ्रोपिक ने संवैधानिक एआई जारी किया जो सहायकता, हानिरहितता और ईमानदारी पर केंद्रित है।

- **2023: ल्लामा** - मेटा ने ओपन-वेट लार्ज लैंग्वेज मॉडल जारी किए, जिसने ओपन-सोर्स नवाचार को बढ़ावा दिया।

- **2023: मिश्चर-ऑफ-एक्सपर्ट्स (एमओई)** - मिक्सट्राल 8x7B जैसे मॉडलों ने प्रत्येक इनपुट के लिए नेटवर्क के केवल प्रासंगिक भागों को सक्रिय करके दक्षता लाभ दिखाया।

- **2023-2024: मल्टीमॉडल मॉडल** - जीपीटी-4वी, जेमिनी, क्लॉड 3 और अन्य ने विजन, ऑडियो और भाषा में क्षमताएं हासिल कीं।

- **2024: सोरा** - ओपनएआई के टेक्स्ट-टू-वीडियो मॉडल ने जटिल दृश्यों के फोटोरियलिस्टिक वीडियो जनरेट किए।

- **2024: रीजनिंग एनहांसमेंट्स** - मॉडलों में चेन-ऑफ-थॉट क्षमताओं में सुधार हुआ, जिसमें गणित, कोडिंग और वैज्ञानिक रीजनिंग के लिए विशेष मॉडल शामिल हैं।

- **2024: एजेंट फ्रेमवर्क** - प्लानिंग, टूल यूज और मेमोरी को शामिल करने वाली प्रणालियों ने मॉडलों को जटिल मल्टी-स्टेप कार्यों को पूरा करने की अनुमति दी।

## इस पूरी अवधि के प्रमुख रुझान:

1.  **स्केलिंग नियम**: मॉडल आकार, डेटासेट आकार और कम्प्यूटेशन बढ़ाकर लगातार प्रदर्शन में सुधार।

2.  **उभरती क्षमताएं**: इन-कॉन्टेक्स्ट लर्निंग जैसी क्षमताएं कुछ निश्चित स्केल थ्रेशोल्ड पर अप्रत्याशित रूप से प्रकट होना।

3.  **मल्टीमॉडैलिटी**: टेक्स्ट, इमेज, ऑडियो और वीडियो का एकीकृत मॉडलों में एकीकरण।

4.  **रीइन्फोर्समेंट लर्निंग फ्रॉम ह्यूमन फीडबैक (आरएलएचएफ)**: मानवीय प्राथमिकताओं के साथ मॉडलों का संरेखण।

5.  **लोकतंत्रीकरण**: ओपन-सोर्स मॉडल और सुलभ फ्रेमवर्क की वृद्धि ने एमएल विकास की बाधाओं को कम किया।

यह अवधि मानव इतिहास में किसी भी क्षेत्र में सबसे तेज तकनीकी प्रगति का प्रतिनिधित्व करती है, जिसने एमएल को एक विशेष शैक्षणिक अनुशासन से वैश्विक प्रभाव वाली सामान्य-उद्देश्य तकनीक में बदल दिया।