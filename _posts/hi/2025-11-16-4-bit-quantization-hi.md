---
audio: false
generated: true
image: false
lang: hi
layout: post
model: x-ai/grok-4-fast
title: कुशल एलएलएम के लिए ४-बिट क्वांटीकरण
translated: true
type: note
---

### 4-बिट क्वांटिज़ेशन क्या है (QLoRA या GPTQ के संदर्भ में)?

4-बिट क्वांटिज़ेशन एक ऐसी तकनीक है जो लार्ज लैंग्वेज मॉडल्स (LLMs) की मेमोरी फुटप्रिंट को काफी कम कर देती है, जहाँ प्रत्येक वेट को सामान्य 16-बिट (FP16/BF16) या 32-बिट (FP32) प्रिसिजन के बजाय केवल 4 बिट्स में स्टोर किया जाता है।

सामान्य (पूर्ण-प्रिसिजन) मॉडल:
- FP32: 4 बाइट्स प्रति पैरामीटर → 7B मॉडल ≈ 28 GB
- FP16/BF16: 2 बाइट्स प्रति पैरामीटर → 7B मॉडल ≈ 14 GB

4-बिट क्वांटिज़ेशन:
- ~0.5 बाइट्स प्रति पैरामीटर → 7B मॉडल ≈ 3.5–4 GB (मूल आकार का लगभग 1/4 से 1/8, सटीक विधि पर निर्भर करता है)

आजकल आमतौर पर दो प्रमुख 4-बिट दृष्टिकोण देखने को मिलते हैं:

| विधि     | पूरा नाम                      | मुख्य पेपर / वर्ष | सामान्य उपयोग के मामले                | मुख्य विशेषताएं                                                                                                                              |
|----------|--------------------------------|-------------------|---------------------------------------|----------------------------------------------------------------------------------------------------------------------------------------------|
| GPTQ     | GPTQ                           | 2023              | पोस्ट-ट्रेनिंग क्वांटिज़ेशन (केवल इनफेरेंस) | वन-शॉट, बहुत सटीक, रीट्रेनिंग की आवश्यकता नहीं। ट्रेनिंग के बाद वेट्स को 4-बिट में राउंड करता है।                                                |
| QLoRA    | Quantized Low-Rank Adaptation  | 2023 (Jun)        | कुशल फाइन-ट्यूनिंग / इंस्ट्रक्शन ट्यूनिंग | 4-बिट स्टोरेज + LoRA एडाप्टर्स + पेज्ड ऑप्टिमाइज़र्स को जोड़ता है। सिंगल 24–48 GB GPU पर 65B+ मॉडल्स के फाइन-ट्यूनिंग की अनुमति देता है। |

#### QLoRA अधिक विस्तार से (आमतौर पर लोगों का मतलब जब वे "4-बिट QLoRA" कहते हैं)
QLoRA एक साथ चार चतुर चीजें करता है:

1. 4-बिट नॉर्मलफ्लोट (NF4) क्वांटिज़ेशन
   - वेट्स के सामान्य रूप से वितरित होने (अधिकांश LLM वेट्स ट्रेनिंग के बाद ≈ गाऊसियन होते हैं) के लिए अनुकूलित एक विशेष 4-बिट डेटा टाइप।
   - सादे INT4 से बेहतर; सैद्धांतिक रूप से सामान्य रूप से वितरित डेटा के लिए इष्टतम।

2. डबल क्वांटिज़ेशन
   - क्वांटिज़ेशन कॉन्स्टेंट्स (स्केलिंग फैक्टर्स) को भी FP16 → 8-बिट में क्वांटाइज़ किया जाता है, जिससे कुछ और MB बचत होती है।

3. पेज्ड ऑप्टिमाइज़र्स
   - ऑप्टिमाइज़र स्टेट्स (AdamW मोमेंट्स) को CPU RAM में स्टोर किया जाता है और NVIDIA यूनिफाइड मेमोरी के साथ GPU में पेज किया जाता है। ट्रेनिंग के दौरान OOM को रोकता है।

4. LoRA एडाप्टर्स
   - केवल छोटे लो-रैंक मैट्रिक्स (r=64 या उससे कम) को ट्रेन किया जाता है जबकि बेस 4-बिट मॉडल फ्रोजन रहता है।

परिणाम: आप एक 48 GB RTX A6000 पर पूरी तरह से 65B Llama/Mistral मॉडल या एक 80 GB A100 पर 70B मॉडल को QLoRA के साथ फाइन-ट्यून कर सकते हैं, जबकि सामान्य फुल फाइन-ट्यूनिंग के लिए 8×A100s या अधिक की आवश्यकता होगी।

#### GPTQ (इनफेरेंस-केंद्रित वाला)
- ट्रेनिंग पूरी होने के बाद किया जाता है।
- वेट्स को 4-बिट में कम्प्रेस करते समय राउंडिंग एरर को कम करने के लिए सेकंड-ऑर्डर (हेसियन) इनफॉर्मेशन का उपयोग करता है।
- अत्यधिक सटीक — आमतौर पर FP16 की तुलना में <0.1 पर्प्लेक्सिटी डिग्रेडेशन।
- AutoGPTQ, ExLlama, vLLM, और llama.cpp (GGUF में भी GPTQ-स्टाइल मोड हैं) जैसे टूल्स के साथ लोकप्रिय।

### ग्रेडिएंट चेकपॉइंटिंग (जिसे "एक्टिवेशन चेकपॉइंटिंग" भी कहा जाता है)

ट्रेनिंग के दौरान इस्तेमाल की जाने वाली एक पूरी तरह से अलग मेमोरी-बचत तकनीक।

यह कैसे काम करता है:
- सामान्य रूप से, बैकप्रोपगेशन को फॉरवर्ड पास से सभी इंटरमीडिएट एक्टिवेशन्स को स्टोर करने की आवश्यकता होती है → मेमोरी मॉडल की गहराई/बैच साइज/सीक्वेंस लेंथ के साथ रैखिक रूप से बढ़ती है।
- ग्रेडिएंट चेकपॉइंटिंग मेमोरी के बदले कम्प्यूटेशनल लागत लेती है:
  - फॉरवर्ड पास के दौरान, यह केवल कुछ "चेकपॉइंट" एक्टिवेशन्स को सेव करता है।
  - बैकवर्ड पास के दौरान, यह गायब एक्टिवेशन्स की गणना ऑन-द-फ्लाई दोबारा करता है।

सामान्य मेमोरी बचत: एक्टिवेशन मेमोरी का 50–80%, लगभग 20–30% धीमी ट्रेनिंग की लागत पर।

इसका उपयोग लगभग हमेशा QLoRA या किसी भी बड़े-मॉडल फाइन-ट्यूनिंग के साथ किया जाता है क्योंकि 4-बिट वेट्स के साथ भी, एक्टिवेशन्स अभी भी GPU मेमोरी को बहुत अधिक उपयोग में लेते हैं।

### TL;DR सारांश

"4-बिट क्वांट (QLoRA-स्टाइल या GPTQ) + ग्रेडिएंट चेकपॉइंटिंग" आधुनिक रेसिपी है:

- कंज्यूमर या सिंगल-एंटरप्राइज़ GPUs पर विशाल मॉडल्स (33B, 70B, 405B) को चलाने या फाइन-ट्यून करने के लिए।
- QLoRA-स्टाइल 4-बिट → बहुत कम GPU मेमोरी के साथ फाइन-ट्यूनिंग।
- GPTQ-स्टाइल 4-बिट → बहुत कम GPU मेमोरी के साथ तेज इनफेरेंस।
- ग्रेडिएंट चेकपॉइंटिंग → लंबी सीक्वेंस पर OOM के बिना वास्तविक बैकवर्ड पास को संभव बनाता है।

इन सबके मिलाजुला प्रभाव के कारण ही आज लोग एक सिंगल 3090/4090 पर Llama-3-70B को फाइन-ट्यून करते देखे जा रहे हैं।