---
audio: false
generated: true
image: false
lang: hi
layout: post
model: x-ai/grok-4-fast
title: जीपीटी मॉडल प्रशिक्षण में वेट डिके
translated: true
type: note
---

वेट डिके एक सामान्य रेगुलराइजेशन तकनीक है जिसका उपयोग GPT मॉडल (और लगभग सभी बड़े भाषा मॉडल) को ट्रेन करते समय ओवरफिटिंग को रोकने और जनरलाइजेशन को सुधारने के लिए किया जाता है।

### यह वास्तव में क्या करता है
वेट डिके लॉस फंक्शन में एक पेनल्टी टर्म जोड़ता है जो मॉडल के वेट को छोटे मूल्यों की ओर धकेलता है। गणितीय रूप से, यदि आपका मूल लॉस ℒ (उदाहरण के लिए, क्रॉस-एन्ट्रॉपी) है, तो वेट डिके के साथ आप ऑप्टिमाइज़ करते हैं:

ℒ_कुल = ℒ + (λ / 2) × ||w||²

जहाँ:
- w मॉडल पैरामीटर (वेट) हैं
- ||w||² वेट का स्क्वायर्ड L2 नॉर्म है
- λ (लैम्डा) वेट डिके गुणांक है (आमतौर पर GPT-स्टाइल ट्रेनिंग में 0.01 ~ 0.1)

यह अतिरिक्त टर्म मॉडल को बहुत बड़े वेट सीखने से हतोत्साहित करता है, जब तक कि वे मूल लॉस को काफी कम नहीं करते।

### व्यवहार में इसे कैसे लागू किया जाता है (AdamW)
अधिकांश GPT ट्रेनिंग रन (OpenAI का GPT-2, GPT-3, LLaMA, Mistral, आदि) में, लोग स्टैंडर्ड L2 रेगुलराइजेशन के साथ सादे Adam का उपयोग नहीं करते हैं। इसके बजाय वे AdamW (डिकपल्ड वेट डिके के साथ Adam) का उपयोग करते हैं।

मुख्य अंतर:
- Adam में स्टैंडर्ड L2 रेगुलराइजेशन सीधे ग्रेडिएंट में पेनल्टी जोड़ता है।
- AdamW इसे डिकपल करता है: यह वेट डिके को एक अलग अपडेट स्टेप के रूप में लागू करता है, जो Adam जैसे एडाप्टिव ऑप्टिमाइज़र के साथ बहुत बेहतर काम करता है।

इसलिए जब आप ट्रेनिंग कॉन्फ़िग में कुछ इस तरह देखते हैं:

```python
optimizer = AdamW(model.parameters(), lr=5e-5, weight_decay=0.1)
```

वह 0.1 ऊपर बताया गया λ है, और यह सही डिकपल्ड वर्जन का उपयोग कर रहा है।

### वास्तविक GPT-स्टाइल मॉडल में विशिष्ट मान

| मॉडल / पेपर           | वेट डिके    | नोट्स                                      |
|----------------------|-------------|--------------------------------------------|
| GPT-2 (OpenAI)       | 0.01        |                                            |
| GPT-3 (OpenAI)       | 0.1         | Brown et al. 2020                          |
| LLaMA 1 & 2 (Meta)   | 0.1         | अक्सर नॉर्म/बायस पर कोई डिके नहीं के साथ संयुक्त |
| PaLM (Google)        | 0.1         |                                            |
| Grok-1 (xAI)         | 0.1         | ओपन-रिलीज़ कॉन्फ़िग से                     |
| Mistral / Mixtral    | 0.1         |                                            |

### महत्वपूर्ण बारीकी: किन पैरामीटर पर वेट डिके लागू होता है?
आधुनिक प्रथा (LLaMA के बाद से) वेट डिके को केवल कुछ विशिष्ट लेयर्स पर लागू करना है:
- हाँ: लीनियर लेयर्स (वेट मैट्रिक्स), एम्बेडिंग मैट्रिक्स
- नहीं: बायस टर्म्स, LayerNorm वेट, कोई भी 1D पैरामीटर

यह आमतौर पर PyTorch पैरामीटर ग्रुप के साथ किया जाता है:

```python
decay = set()
no_decay = set()
for name, param in model.named_parameters():
    if "bias" in name or "layer_norm" in name or param.ndim == 1:
        no_decay.add(param)
    else:
        decay.add(param)

optimizer = AdamW([
    {"params": list(decay),   "weight_decay": 0.1},
    {"params": list(no_decay), "weight_decay": 0.0}
], lr=...)
```

### यह GPT-स्केल मॉडल के लिए क्यों महत्वपूर्ण है
अरबों या सैकड़ों अरबों पैरामीटर पर, मॉडल आसानी से ट्रेनिंग डेटा को याद कर सकते हैं। वेट डिके सबसे सरल और प्रभावी उपकरणों में से एक है (ड्रॉपआउट, स्टोकैस्टिक डेप्थ, आदि के साथ) जो प्रभावी क्षमता को नियंत्रित रखता है और डाउनस्ट्रीम परफॉर्मेंस को सुधारता है।

संक्षेप में: वेट डिके ≈ L2 रेगुलराइजेशन, लेकिन ठीक से AdamW के माध्यम से लागू किया गया और चुनिंदा रूप से लागू किया गया, वर्चुअली सभी आधुनिक GPT-स्टाइल प्रीट्रेनिंग रेसिपी में एक मानक घटक है।