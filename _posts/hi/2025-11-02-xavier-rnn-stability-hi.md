---
audio: false
generated: true
image: false
lang: hi
layout: post
model: x-ai/grok-4-fast
title: आरएनएन स्थिरता के लिए जेवियर आरंभीकरण
translated: true
type: note
---

### आरएनएन में ज़ेवियर इनिशियलाइज़ेशन क्यों?

आरएनएन (रिकरंट न्यूरल नेटवर्क्स) अपने वेट मैट्रिक्स—जैसे `Wxh` (इनपुट-टू-हिडन), `Whh` (हिडन-टू-हिडन, रिकरंट वेट), और `Why` (हिडन-टू-आउटपुट)—को स्थिर ट्रेनिंग डायनामिक्स बनाए रखने के लिए ज़ेवियर (जिसे ग्लोरोट भी कहते हैं) इनिशियलाइज़ेशन पर निर्भर करते हैं। आपके कोड में इस्तेमाल होने वाला विशिष्ट रूप एक गॉसियन (सामान्य) डिस्ट्रीब्यूशन का उपयोग करता है जिसे `sqrt(1 / fan_in)` से स्केल किया जाता है, जहाँ `fan_in` उस लेयर में इनपुट न्यूरॉन्स की संख्या है। यह वेट के वेरिएंस को लगभग `1 / fan_in` पर बनाए रखता है, यह सुनिश्चित करते हुए कि आने वाले सिग्नल बहुत अधिक प्रवर्धित या दबाए न जाएँ।

यहाँ बताया गया है कि यह आरएनएन के लिए क्यों महत्वपूर्ण है, और साधारण यूनिफ़ॉर्म रैंडम ड्रा [0, 1] से क्यों समस्याएँ आएँगी:

#### 1. **लेयर्स और टाइम स्टेप्स में सिग्नल वेरिएंस को बनाए रखना**
   - फीडफॉरवर्ड नेटवर्क्स में, ज़ेवियर *एक्टिवेशन्स के वेरिएंस* को लगभग स्थिर रखने में मदद करता है जब सिग्नल आगे प्रसारित होते हैं (और ग्रेडिएंट्स पीछे की ओर)। इसके बिना, डीप लेयर्स में एक्टिवेशन विस्फोटित (बहुत बड़े हो सकते हैं) या लुप्त (लगभग शून्य तक गिर सकते हैं) हो सकते हैं, जिससे ट्रेनिंग असंभव हो जाती है।
   - आरएनएन *समय के साथ "अनरोल" किए गए* डीप नेटवर्क्स की तरह होते हैं: रिकरंट वेट `Whh` प्रत्येक टाइम स्टेप पर हिडन स्टेट को गुणा करता है, जिससे गुणन की एक श्रृंखला बनती है (उदाहरण के लिए, सीक्वेंस लंबाई *T* के लिए, यह *T* लेयर्स डीप की तरह है)। यदि `Whh` में वेट का वेरिएंस >1 है, तो ग्रेडिएंट्स पीछे की ओर घातीय रूप से विस्फोटित होते हैं (लंबी सीक्वेंस के लिए खराब)। यदि <1 है, तो वे लुप्त हो जाते हैं।
   - ज़ेवियर का स्केलिंग (उदाहरण के लिए, `Whh` के लिए `* sqrt(1. / hidden_size)`) यह सुनिश्चित करता है कि हिडन स्टेट का अपेक्षित वेरिएंस ~1 बना रहे, इसे रोकते हुए। [0,1] यूनिफ़ॉर्म इनिट के लिए:
     - माध्य ~0.5 (सकारात्मक पूर्वाग्रहित, ड्रिफ्ट का कारण बनता है)।
     - वेरिएंस ~1/12 ≈ 0.083—बड़े `hidden_size` (उदाहरण के लिए, 512) के लिए बहुत छोटा, जिससे सिग्नल जल्दी लुप्त हो जाते हैं।

#### 2. **लेयर डायमेंशन के अनुकूल होना**
   - ज़ेवियर *fan_in* (लेयर में इनपुट्स) और कभी-कभी *fan_out* (आउटपुट्स) को ध्यान में रखता है। `Wxh` के लिए, `sqrt(1 / input_size)` द्वारा स्केलिंग वोकैबुलरी साइज या एम्बेडिंग डायमेंशन के आधार पर नॉर्मलाइज़ करती है। `Whh` के लिए, यह रिकरंट लूप से मेल खाने के लिए `hidden_size` पर आधारित होती है।
   - [0,1] डायमेंशन्स को नज़रअंदाज करता है: एक बड़ी हिडन लेयर (उदाहरण के लिए, 1000 यूनिट्स) में, बिना स्केलिंग के कई [0,1] इनपुट्स का योग अगली लेयर को अभिभूत कर देगा। छोटी लेयर्स में, यह अंडरफ़्लो करेगा।

#### 3. **बैकप्रोपेगेशन थ्रू टाइम (BPTT) में ग्रेडिएंट फ्लो**
   - आरएनएन ट्रेनिंग BPTT का उपयोग करती है, जहाँ ग्रेडिएंट्स *T* स्टेप्स पर पीछे की ओर प्रवाहित होते हैं। खराब इनिट "वैनिशिंग ग्रेडिएंट प्रॉब्लम" (वैनिला आरएनएन में आम, यही कारण है कि एलएसटीएम/जीआरयू मौजूद हैं) की ओर ले जाता है। ज़ेवियर संतुलित मैग्निट्यूड के साथ शुरुआत करके इसे कम करता है।
   - अनुभवजन्य अध्ययन (उदाहरण के लिए, ग्लोरोट और बेंजियो का मूल पेपर) दिखाते हैं कि ज़ेवियर डीप/रिकरंट नेट्स में कन्वर्जेंस के लिए आवश्यक एपॉक्स को 2-10x तक कम कर देता है।

#### विकल्प और कब [0,1] "काम" कर सकता है
- [0,1] छोटे टॉय मॉडल्स (उदाहरण के लिए, सिंगल-लेयर, छोटी सीक्वेंस) के लिए "काम" कर सकता है लेकिन बड़े स्केल पर विफल हो जाता है—जैसे विस्फोटित लॉस या NaN।
- आधुनिक ट्वीक्स: He इनिट (ReLUs के लिए, `sqrt(2 / fan_in)` से स्केल करता है) या ऑर्थोगोनल इनिट (आरएनएन के लिए, नॉर्म्स को ठीक से बनाए रखता है)। लेकिन आरएनएन में आम सिग्मॉइड/टैनह एक्टिवेशन्स के लिए ज़ेवियर एक सुरक्षित डिफ़ॉल्ट है।

संक्षेप में, ज़ेवियर *स्थिरता पहले* के बारे में है: यह वेट्स को छोटा-लेकिन-बहुत-छोटा-नहीं, डायमेंशन-अवेयर सेट करता है, ताकि ग्रेडिएंट्स स्वतंत्र रूप से प्रवाहित हो सकें। रैंडम [0,1] बहुत अपरिष्कृत है—जैसे बिना टायर संतुलित किए कार को फ्लोर करना।

गहराई से जानने के लिए:  
[Understanding the Difficulty of Training Deep Feedforward Neural Networks](http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf) (Glorot & Bengio, 2010)  
[Recurrent Neural Network Regularization](https://arxiv.org/abs/1409.2329) (Zaremba et al., on RNN init issues)