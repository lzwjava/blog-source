---
audio: false
generated: true
lang: hi
layout: post
title: एनकोडर-डिकोडर आर्किटेक्चर
translated: true
type: note
---

आइए Encoder-Decoder आर्किटेक्चर को विस्तार से समझते हैं और इसी तरह की अन्य अवधारणाओं का पता लगाते हैं।

**Encoder-Decoder आर्किटेक्चर क्या है?**

Encoder-Decoder आर्किटेक्चर एक न्यूरल नेटवर्क डिज़ाइन है जो आमतौर पर sequence-to-sequence कार्यों के लिए उपयोग किया जाता है। इसका मूल विचार एक इनपुट अनुक्रम को एक निश्चित-लंबाई वाले मध्यवर्ती प्रतिनिधित्व ("एनकोडेड" स्थिति) में संसाधित करना है और फिर इस प्रतिनिधित्व का उपयोग आउटपुट अनुक्रम उत्पन्न करने के लिए करना है।

यहां इसके घटकों पर एक विस्तृत नज़र है:

*   **एनकोडर:**
    *   एक इनपुट अनुक्रम (जैसे, अंग्रेजी में एक वाक्य, स्टॉक की कीमतों का समय श्रृंखला डेटा) को इनपुट के रूप में लेता है।
    *   इनपुट अनुक्रम को चरण दर चरण संसाधित करता है।
    *   इनपुट अनुक्रम को एक निश्चित आकार के वेक्टर या वेक्टरों के एक सेट में बदल देता है। यह वेक्टर (या वेक्टरों का सेट) पूरे इनपुट अनुक्रम से आवश्यक जानकारी को कैप्चर करने के लिए डिज़ाइन किया गया है। यह इनपुट का एक सारांश या प्रतिनिधित्व के रूप में कार्य करता है।
    *   सामान्य एनकोडर नेटवर्क में Recurrent Neural Networks (RNNs) जैसे LSTM और GRU, और Transformer एनकोडर (जैसे BERT जैसे मॉडल में उपयोग किए जाते हैं) शामिल हैं।

*   **डिकोडर:**
    *   एनकोड किए गए प्रतिनिधित्व (एनकोडर से) को इनपुट के रूप में लेता है।
    *   आउटपुट अनुक्रम को चरण दर चरण उत्पन्न करता है।
    *   प्रत्येक चरण पर, यह एनकोड किए गए प्रतिनिधित्व और पहले से उत्पन्न किए गए तत्वों के आधार पर आउटपुट अनुक्रम में अगले तत्व की भविष्यवाणी करता है।
    *   डिकोडिंग प्रक्रिया तब तक जारी रहती है जब तक कि एक विशेष "अनुक्रम-समाप्ति" टोकन उत्पन्न नहीं हो जाता या एक पूर्वनिर्धारित लंबाई सीमा तक नहीं पहुंच जाती।
    *   एनकोडर के समान, सामान्य डिकोडर नेटवर्क में RNNs (LSTM, GRU) और Transformer डिकोडर (जैसे GPT मॉडल में देखे जाते हैं) भी शामिल हैं।

**वे एक साथ कैसे काम करते हैं:**

1.  इनपुट अनुक्रम को एनकोडर में फीड किया जाता है।
2.  एनकोडर इनपुट को संसाधित करता है और एक निश्चित-लंबाई वाला संदर्भ वेक्टर (या संदर्भ वेक्टरों का एक सेट) उत्पन्न करता है।
3.  यह संदर्भ वेक्टर फिर प्रारंभिक अवस्था के रूप में डिकोडर को पास किया जाता है।
4.  डिकोडर आउटपुट अनुक्रम उत्पन्न करने के लिए इस संदर्भ वेक्टर का उपयोग करता है, एक समय में एक तत्व।

**विशिष्ट अनुप्रयोग:**

Encoder-Decoder आर्किटेक्चर उन कार्यों के लिए अत्यधिक प्रभावी हैं जहां इनपुट और आउटपुट संभावित रूप से अलग-अलग लंबाई के अनुक्रम होते हैं। कुछ सामान्य अनुप्रयोगों में शामिल हैं:

*   **मशीन अनुवाद:** एक भाषा से दूसरी भाषा में पाठ का अनुवाद करना।
*   **पाठ सारांशीकरण:** लंबे पाठ का एक छोटा सारांश उत्पन्न करना।
*   **स्पीच रिकग्निशन:** बोले गए ऑडियो को टेक्स्ट में बदलना।
*   **इमेज कैप्शनिंग:** एक छवि का पाठ्य विवरण उत्पन्न करना।
*   **कोड जनरेशन:** एक विवरण के आधार पर कोड स्निपेट उत्पन्न करना।
*   **प्रश्नोत्तर:** एक संदर्भ दिए जाने पर एक प्रश्न का उत्तर उत्पन्न करना।

**इसी तरह की अन्य आर्किटेक्चर कौन सी हैं?**

जबकि Encoder-Decoder एक विशिष्ट और व्यापक रूप से उपयोग की जाने वाली आर्किटेक्चर है, कई अन्य आर्किटेक्चर भी हैं जो इनपुट को संसाधित करने और आउटपुट उत्पन्न करने की समान अवधारणाओं को साझा करते हैं, अक्सर इन प्रक्रियाओं के लिए अलग-अलग चरणों या घटकों के साथ। यहां कुछ उदाहरण दिए गए हैं:

1.  **ट्रांसफॉर्मर आर्किटेक्चर (कुछ संदर्भों में स्पष्ट रूप से एनकोडर और डिकोडर को अलग किए बिना):** जबकि प्रॉम्प्ट T5 का उल्लेख करता है जो स्पष्ट रूप से एक एनकोडर और डिकोडर का उपयोग करता है, मूल ट्रांसफॉर्मर आर्किटेक्चर को स्वयं अलग-अलग एनकोडर और डिकोडर स्टैक वाले के रूप में देखा जा सकता है। एनकोडर स्टैक इनपुट अनुक्रम को संसाधित करता है, और डिकोडर स्टैक आउटपुट अनुक्रम उत्पन्न करता है, उन्हें जोड़ने के लिए अटेंशन मैकेनिज्म का उपयोग करता है। BERT जैसे मॉडल मुख्य रूप से एनकोडर भाग का उपयोग करते हैं, जबकि GPT जैसे मॉडल मुख्य रूप से डिकोडर भाग का उपयोग करते हैं। T5 और अन्य sequence-to-sequence ट्रांसफॉर्मर दोनों का उपयोग करते हैं।

2.  **अटेंशन मैकेनिज्म वाले Sequence-to-Sequence मॉडल:** बुनियादी Encoder-Decoder आर्किटेक्चर लंबे इनपुट अनुक्रमों के साथ संघर्ष कर सकता है क्योंकि पूरे इनपुट को एक एकल निश्चित-लंबाई वाले वेक्टर में संपीड़ित किया जाता है। इसके समाधान के लिए **अटेंशन मैकेनिज्म** पेश किया गया था। यह डिकोडर को आउटपुट जनरेशन के प्रत्येक चरण पर इनपुट अनुक्रम के विभिन्न भागों पर "ध्यान" देने की अनुमति देता है। यह विशेष रूप से लंबे अनुक्रमों के लिए प्रदर्शन में काफी सुधार करता है। आर्किटेक्चरल रूप से, इसमें अभी भी एक एनकोडर और एक डिकोडर है, लेकिन उन्हें जोड़ने वाली एक अतिरिक्त अटेंशन लेयर के साथ।

3.  **ऑटोरेग्रेसिव मॉडल:** ये मॉडल आउटपुट अनुक्रमों को एक समय में एक तत्व उत्पन्न करते हैं, जहां अगले तत्व की भविष्यवाणी पहले से उत्पन्न तत्वों पर निर्भर करती है। जबकि समान तरीके से एक अलग "एनकोडर" नहीं होता है, उन्हें एक प्रारंभिक संदर्भ (जो एक एनकोडेड इनपुट हो सकता है या बस एक प्रारंभिक टोकन) को संसाधित करने और फिर पुनरावृत्त रूप से आउटपुट अनुक्रम को "डिकोड" करने के रूप में देखा जा सकता है। उदाहरणों में GPT जैसे लैंग्वेज मॉडल शामिल हैं।

4.  **जनरेटिव एडवरसैरियल नेटवर्क (GANs):** जबकि मुख्य रूप से छवियों जैसे डेटा उत्पन्न करने के लिए उपयोग किया जाता है, GANs में एक **जनरेटर** नेटवर्क शामिल होता है जो यथार्थवादी आउटपुट उत्पन्न करना सीखता है और एक **डिस्क्रिमिनेटर** नेटवर्क होता है जो वास्तविक और उत्पन्न डेटा के बीच अंतर करने की कोशिश करता है। जनरेटर को एक अव्यक्त इनपुट (एनकोडेड प्रतिनिधित्व के समान) लेने और इसे एक डेटा नमूने में "डिकोड" करने के रूप में देखा जा सकता है।

5.  **वेरिएशनल ऑटोएनकोडर (VAEs):** VAEs में एक **एनकोडर** होता है जो एक इनपुट को एक अव्यक्त संभाव्यता वितरण में मैप करता है और एक **डिकोडर** होता है जो आउटपुट उत्पन्न करने के लिए इस वितरण से सैंपल लेता है। जबकि प्राथमिक लक्ष्य पारंपरिक अर्थ में sequence-to-sequence कार्यों के बजाय जनरेटिव मॉडलिंग है, एक अव्यक्त स्थान में एनकोडिंग और फिर डिकोडिंग की अवधारणा केंद्रीय है।

6.  **विशिष्ट फीचर एक्सट्रैक्शन और प्रेडिक्शन चरणों वाले मॉडल:** विभिन्न डोमेन में, आपको ऐसी आर्किटेक्चर मिल सकती हैं जहां नेटवर्क का एक हिस्सा इनपुट से प्रासंगिक फीचर्स निकालने के लिए जिम्मेदार होता है (एनकोडर के समान), और दूसरा हिस्सा भविष्यवाणियां करने या आउटपुट उत्पन्न करने के लिए इन फीचर्स का उपयोग करता है (डिकोडर के समान)। यह एक अधिक सामान्य अवधारणा है और इमेज रिकग्निशन जैसे क्षेत्रों में क्लासिफिकेशन हेड्स या ऑब्जेक्ट डिटेक्शन मॉडल के साथ पाई जा सकती है।

संक्षेप में, Encoder-Decoder आर्किटेक्चर sequence-to-sequence कार्यों के लिए एक मौलिक डिज़ाइन है। जबकि इसके विशिष्ट घटक हैं, एक इनपुट को एक मध्यवर्ती प्रतिनिधित्व में संसाधित करने और फिर उस प्रतिनिधित्व के आधार पर एक आउटपुट उत्पन्न करने का अंतर्निहित विचार विभिन्न कार्यों के लिए डिज़ाइन किए गए कई न्यूरल नेटवर्क आर्किटेक्चर में एक सामान्य विषय है।