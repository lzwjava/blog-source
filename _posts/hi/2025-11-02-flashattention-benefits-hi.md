---
audio: false
generated: true
image: false
lang: hi
layout: post
model: openai/gpt-5
title: फ्लैशअटेंशन एल्गोरिदम और लाभ
translated: true
type: note
---

# FlashAttention का गहन विवरण (त्रि डाओ एट अल., 2022)

संक्षिप्त सारांश: FlashAttention, GPU DRAM (HBM) में पूर्ण \\(N\times N\\) अटेंशन मैट्रिक्स आवंटित करने की आवश्यकता को समाप्त कर देता है, ऑन-चिप SRAM (रजिस्टर / शेयर्ड मेमोरी) के अंदर अटेंशन की गणना **ब्लॉक-दर-ब्लॉक** करके, पूरे अटेंशन पाइपलाइन को एक सिंगल कर्नेल में फ्यूज़ करके और संख्यात्मक रूप से सही ब्लॉकवाइज सॉफ्टमैक्स एक्यूमुलेशन का उपयोग करके। इससे HBM ट्रैफिक और मेमोरी का उपयोग \\(O(N^2)\\) से प्रभावी रूप से \\(O(N)\\) हो जाता है, और व्यवहार में लंबे सीक्वेंस के लिए GPUs पर बड़ी वॉल-क्लॉक स्पीडअप मिलती है। citeturn0search0turn0search9

---

## समस्या: स्टैंडर्ड अटेंशन IO-बाउंड क्यों है
ट्रांसफॉर्मर सेल्फ-अटेंशन (स्केल्ड डॉट-प्रोडक्ट) आमतौर पर तीन चरणों के साथ लागू किया जाता है:

1. स्कोर की गणना करें \\(S = Q K^\top\\) (आकार \\(N\times N\\));
2. रोवाइज सॉफ्टमैक्स की गणना करें \\(P = \mathrm{softmax}(S)\\);
3. आउटपुट की गणना करें \\(O = P V\\)।

सरलता से आप \\(S\\) (और अक्सर \\(P\\)) को GPU DRAM में मटेरियलाइज़ करते हैं। सीक्वेंस लंबाई \\(N\\) के लिए यह \\(O(N^2)\\) मेमोरी का उपयोग करता है और दो IO समस्याओं की ओर जाता है:
- बड़ा DRAM फुटप्रिंट (अक्सर पहली चीज जो GPU मेमोरी को फुलाती है), और
- DRAM (HBM) और ऑन-चिप SRAM/रजिस्टरों के बीच बहुत सारे रीड/राइट — और ये HBM↔SRAM ट्रांसफर आधुनिक GPUs पर वास्तविक बॉटलनेक हैं।

FlashAttention अटेंशन को एक **IO समस्या** के रूप में पुनः परिभाषित करता है, न कि केवल एक FLOP समस्या, और HBM एक्सेस को कम करने को लक्षित करता है। citeturn0search0

---

## मुख्य विचार (उच्च स्तर पर)
1. **मैट्रिक्स को टाइल करें** \\(Q, K, V\\) को ऐसे ब्लॉक्स में जो ऑन-चिप SRAM (शेयर्ड मेमोरी / रजिस्टर) में फिट हो जाएं।
2. **अटेंशन को ब्लॉक-दर-ब्लॉक प्रोसेस करें**: किसी दिए गए \\(Q\\)-टाइल और \\(K,V\\)-टाइल्स के स्ट्रीमिंग सेट के लिए, आउटपुट में आंशिक योगदान की गणना करें और तुरंत उन्हें एक्यूमुलेट करें — पूर्ण \\(N\times N\\) स्कोर मैट्रिक्स को DRAM में कभी भी मटेरियलाइज़ न करें।
3. **सब कुछ एक कर्नेल में फ्यूज़ करें**: कर्नेल टाइल्स को SRAM में लोड करता है, उस टाइल पेयर के लिए \\(QK^\top\\) की गणना करता है, सॉफ्टमैक्स लॉजिक लागू करता है और \\(V\\)-टाइल से गुणा करता है, और आंशिक आउटपुट लिखता है — बीच के बड़े मैट्रिक्स के DRAM में राउंड-ट्रिप के बिना। कर्नेल फ्यूजन इंस्ट्रक्शन और मेमोरी ओवरहेड को कम करता है।
4. **ब्लॉकवाइज न्यूमेरिकली स्टेबल सॉफ्टमैक्स एक्यूमुलेशन**: क्योंकि पूरी पंक्ति में सॉफ्टमैक्स को ग्लोबल मैक्स और सुम की आवश्यकता होती है, FlashAttention मल्टीपल \\(K\\)-टाइल्स से सॉफ्टमैक्स योगदान को सटीक और स्थिर रूप से संयोजित करने के लिए एक रनिंग मैक्स / रनिंग सुम (लॉग-सम-एक्सप स्टाइल) का उपयोग करता है, बिना स्कोर की पूरी पंक्ति को स्टोर किए।
5. **रीकम्प्यूटेशन के माध्यम से बैकवर्ड**: बैकवर्ड के लिए बड़े इंटरमीडिएट्स को स्टोर करने के बजाय, बैकवर्ड पास के दौरान प्रत्येक ब्लॉक के लिए फॉरवर्ड अटेंशन की पुनर्गणना करें (अतिरिक्त FLOPs के बदले बहुत कम DRAM IO)। बचाया गया DRAM IO आमतौर पर नेट स्पीडअप देता है क्योंकि DRAM IO प्रभावी होता है। citeturn0search2turn0search10

ये विचार मिलकर मेमोरी रिडक्शन और वॉल-क्लॉक स्पीड सुधार दोनों प्रदान करते हैं। citeturn0search0

---

## ब्लॉकवाइज एल्गोरिदम — चरण दर चरण (फॉरवर्ड)
सीक्वेंस लंबाई \\(N\\) और हेड डाइम \\(d\\) वाले एक सिंगल अटेंशन हेड पर विचार करें। एक टाइल साइज \\(B\\) चुनें ताकि एक \\(B\times B\\) स्कोर ब्लॉक और संबंधित \\(Q\\), \\(K\\), \\(V\\) टाइल्स SRAM में फिट हो जाएं।

प्रत्येक क्वेरी टाइल \\(Q_{i}\\) (पंक्तियाँ \\(iB:(i+1)B\\)) के लिए:

1. एक आउटपुट एक्यूमुलेटर \\(O_i \leftarrow 0\\) इनिशियलाइज़ करें।
2. रनिंग नॉर्मलाइज़ेशन स्टेट इनिशियलाइज़ करें: `row_max` (प्रति क्वेरी पंक्ति) \\(-\infty\\) पर, `row_sum` 0 पर। ये मल्टीपल K-टाइल्स में सॉफ्टमैक्स के लिए संख्यात्मक रूप से स्थिर डिनॉम को ट्रैक करते हैं।
3. प्रत्येक की/वैल्यू टाइल \\(K_{j}, V_{j}\\) (कॉलम \\(jB:(j+1)B\\)) के लिए:
   - \\(Q_i\\), \\(K_j\\), \\(V_j\\) को SRAM में लोड करें।
   - रॉ स्कोर के टाइल की गणना करें \\(S_{ij} = Q_i K_j^\top / \sqrt{d}\\) (आकार \\(B\times B\\) वेक्टराइज्ड फॉर्म में)।
   - \\(S_{ij}\\) में प्रत्येक पंक्ति के लिए, लोकल रो मैक्स \\(m_{ij}\\) और एक्सपोनेंशिएटेड वैल्यूज \\(\exp(S_{ij} - m_{ij})\\) की गणना करें।
   - इस टाइल के एक्सपोनेंशियल्स को लॉग-सम-एक्सप ट्रिक का उपयोग करके रनिंग रो नॉर्मलाइज़ेशन में मर्ज करें:
     - मान लें \\(M = \max(\text{row\_max}, m_{ij})\\)।
     - अपडेट करें `row_sum` := `row_sum` · exp(row_max − M) + local_sum · exp(m_{ij} − M)।
     - सेट करें `row_max` := \\(M\\)।
   - उचित रूप से स्केल किए गए एक्सपोनेंशियल्स के साथ एक्यूमुलेटर में टाइल के योगदान की गणना करें: एक्यूमुलेट \\(O_i \mathrel{+}= \text{(tile-softmax)} \times V_j\\)। (सब SRAM के अंदर किया जाता है।)
4. सभी K-टाइल्स को स्ट्रीम करने के बाद, row_sum और row_max का उपयोग करके सही सॉफ्टमैक्स आउटपुट उत्पन्न करने के लिए अंतिम नॉर्मलाइज़ेशन करें; \\(O_i\\) को DRAM में लिखें।

मुख्य बिंदु: कोई भी \\(N\times N\\) मैट्रिक्स कभी भी DRAM में नहीं लिखा जाता है; केवल छोटे टाइल्स और अंतिम आउटपुट हैं। रनिंग मैक्स + सुम का उपयोग करके संख्यात्मक-रूप से सही एक्यूमुलेशन वह है जो प्रति-टाइल सॉफ्टमैक्स टुकड़ों को पंक्ति पर पूर्ण सॉफ्टमैक्स के समान परिणाम में सटीक रूप से संयोजित करने देता है। citeturn0search2turn0search10

---

## व्यवहार में कर्नेल फ्यूजन और SRAM टाइलिंग क्यों जीतती है
- **कम HBM एक्सेस:** स्टैंडर्ड अटेंशन DRAM में \\(O(N^2)\\) एलिमेंट्स को रीड/राइट करता है (स्कोर, सॉफ्टमैक्स)। FlashAttention प्रत्येक \\(Q,K,V\\) एलिमेंट को एक स्थिर संख्या में बार पढ़ता है, और सभी अस्थायी स्कोर/सॉफ्टमैक्स वैल्यू केवल SRAM में रहते हैं। पेपर में IO विश्लेषण कम HBM एक्सेस और उन रेंजों को दिखाता है जहां SRAM साइज को देखते हुए FlashAttention IO-ऑप्टिमल है। citeturn0search0
- **लेटेंसी और बैंडविड्थ सीमाएं FLOPs से अधिक मायने रखती हैं:** GPUs FP मल्टीप्लाई-एक्यूमुलेट में अत्यंत तेज़ हैं; जब DRAM ट्रैफिक रनटाइम पर हावी होता है, तो DRAM ट्रांसफर को कम करना FLOPs को कम करने से अधिक मायने रखता है। कर्नेल फ्यूजन इंटरमीडिएट DRAM ट्रैफिक को हटाता है और कर्नेल लॉन्च ओवरहेड को कम करता है। citeturn0search0
- **बैकवर्ड पास ट्रेडऑफ:** बैकवर्ड के दौरान फॉरवर्ड ब्लॉक्स की पुनर्गणना FLOPs को बढ़ाती है लेकिन DRAM में बड़े इंटरमीडिएट्स को स्टोर करने से बचाती है। क्योंकि रीकम्प्यूटेशन SRAM में होता है और DRAM ट्रैफिक को सीमित करता है, कई मामलों में यह वॉल-क्लॉक टाइम के लिए नेट जीत होती है। citeturn0search10

पेपर और फॉलो-अप से अनुभवजन्य परिणाम मल्टीपल× स्पीडअप (उदाहरण के लिए, उनके रिपोर्ट किए गए बेंचमार्क में मॉडल और सीक्वेंस लंबाई के आधार पर 2–7×) और पीक मेमोरी में बड़ी कमी दिखाते हैं। citeturn0search0turn0search10

---

## महत्वपूर्ण इम्प्लीमेंटेशन विवरण और ट्रेडऑफ

- **टाइल साइज चयन:** टाइल \\(B\\) को इस तरह चुना जाना चाहिए कि वर्किंग सेट (Q, K, V के टाइल्स, स्कोर बफर्स, आंशिक एक्यूमुलेटर्स, प्लस अतिरिक्त स्क्रैच) प्रति थ्रेडब्लॉक ऑन-चिप SRAM में फिट हो जाए। इष्टतम \\(B\\) हेड डाइमेंशन, डेटाटाइप्स (FP16/FP32/FP8), और GPU आर्किटेक्चर (शेयर्ड मेमोरी / रजिस्टरों की मात्रा) पर निर्भर करता है। बहुत छोटा कम्प्यूट एफिशिएंसी को कम करता है; बहुत बड़ा SRAM में फिट नहीं होगा। citeturn0search2

- **संख्यात्मक स्थिरता:** एल्गोरिदम यह सुनिश्चित करने के लिए प्रति-पंक्ति रनिंग मैक्स और सुम (लॉग-सम-एक्सप मर्जिंग) का उपयोग करता है कि अंतिम सॉफ्टमैक्स फुल-मैट्रिक्स सॉफ्टमैक्स के बराबर हो। यह महत्वपूर्ण है: FlashAttention **सटीक अटेंशन** (एक सन्निकटन नहीं) है उस स्थिर एक्यूमुलेशन के कारण। citeturn0search0

- **मास्किंग और कॉजलिटी:** कॉजल मास्किंग (ऑटोरेग्रेसिव) को स्ट्रीम्ड टाइल्स में मास्क्ड पोजीशन से योगदान को छोड़कर या शून्य करके और रनिंग नॉर्मलाइज़ेशन को तदनुसार अपडेट करके संभाला जाता है। ब्लॉकवाइज लॉजिक अभी भी काम करती है लेकिन यह सुनिश्चित करने के लिए सावधानीपूर्वक टाइल ऑर्डरिंग की आवश्यकता हो सकती है कि मास्क्ड एलिमेंट्स एक्यूमुलेटर्स को दूषित न करें। citeturn0search2

- **बैकवर्ड पास और मेमोरी लेआउट:** FlashAttention केवल न्यूनतम मेटाडेटा (जैसे, प्रति ब्लॉक row_max/row_sum) स्टोर करता है और बैकवर्ड के दौरान फॉरवर्ड टाइल उत्पादों की पुनर्गणना करता है। इम्प्लीमेंटेशन रीयूज को अधिकतम करने और रजिस्टर प्रेशर को कम करने के लिए कार्य को सावधानीपूर्वक पुन: क्रमबद्ध करते हैं। citeturn0search10

- **प्रिसिजन और डेटाटाइप्स:** FP16/FP8 का उपयोग टाइल बफरिंग और एक्यूमुलेशन विकल्पों को प्रभावित करता है। कुछ बाद के कार्य (FlashAttention-2 / FlashAttention-3) यूटिलाइजेशन और FP थ्रूपुट को और आगे बढ़ाने के लिए मिक्स्ड प्रिसिजन और नए GPU फीचर्स (हॉपर, H100) के लिए ऑप्टिमाइजेशन जोड़ते हैं। citeturn0search4turn0search11

- **पैरेललिज्म मैपिंग:** कर्नेल वार्प्स/CTA ब्लॉक्स को क्वेरी टाइल्स पर मैप करता है; एक CTA के भीतर, वार्प्स K/V टाइल्स को लोड करने और टाइल मैटमल और रिडक्शन की गणना करने में सहयोग करते हैं। पीक थ्रूपुट के लिए कुशल वार्प-लेवल रिडक्शन और फ्यूज्ड मल्टीप्लाई-ऐड इंस्ट्रक्शन का उपयोग महत्वपूर्ण है। citeturn0search2

---

## FlashAttention बनाम अनुमानित लॉन्ग-अटेंशन विधियां
FlashAttention **सटीक** अटेंशन सेमांटिक्स बनाए रखता है (फुल अटेंशन के समान संख्यात्मक परिणाम, फ्लोटिंग-पॉइंट राउंडिंग तक), जबकि कई लॉन्ग-अटेंशन विधियां अटेंशन का अनुमान लगाती हैं (स्पार्सिटी, लो-रैंक, FAVOR+, आदि) और गुणवत्ता के बदले मेमोरी/समय का व्यापार करती हैं। FlashAttention इसके बजाय मेमोरी/IO लागत को कम करता है जबकि सटीक कम्प्यूटेशन को संरक्षित रखता है, इसलिए मॉडल गुणवत्ता अपरिवर्तित रहती है जबकि थ्रूपुट/मेमोरी में सुधार होता है। इसीलिए यह व्यापक रूप से आकर्षक है: कोई सटीकता ट्रेडऑफ नहीं, बस एक बेहतर लो-लेवल कर्नेल। citeturn0search0

---

## प्रैक्टिकल उपलब्धता और इकोसिस्टम
- लेखकों ने एक इम्प्लीमेंटेशन (CUDA) और FlashAttention और बाद में FlashAttention-2 के साथ एक मेंटेन रिपो जारी किया है। कई फ्रेमवर्क (Hugging Face Transformers, XLA/PyTorch फोर्क्स, Triton-आधारित इम्प्लीमेंटेशन) या तो flash_attn ऑपरेटर को कॉल करते हैं या समान फ्यूज्ड कर्नेल प्रदान करते हैं। आप `flash_attn` ऑपरेटर या ऐसी लाइब्रेरीज का उपयोग कर सकते हैं जो इसे एक्सपोज़ करती हैं; PyTorch में, हाल के संस्करणों में मेमोरी-एफिशिएंट अटेंशन प्रिमिटिव्स भी शामिल हैं, और तीसरे पक्ष के `flash_attn` पैकेज कई वर्कलोड के लिए ड्रॉप-इन स्पीड/मेमोरी सुधार देते हैं। इंस्टॉलर और API उदाहरणों के लिए आधिकारिक रिपो को चेक करें। citeturn0search9turn0search4

चेतावनी: "कस्टम कर्नेल की कोई आवश्यकता नहीं" केवल आंशिक रूप से सत्य है — FlashAttention *एक कस्टम फ्यूज्ड कर्नेल है (रिपो में काम) जिसे फ्रेमवर्क कॉल करते हैं। आधुनिक PyTorch संस्करण आंतरिक रूप से तुलनीय फ्यूज्ड कर्नेल शिप कर सकते हैं या वेंडर लाइब्रेरीज को डेलिगेट कर सकते हैं, लेकिन मुख्य विचार के लिए एक फ्यूज्ड कर्नेल इम्प्लीमेंटेशन (चाहे CUDA, Triton, या वेंडर कोड में) की आवश्यकता होती है। महत्वपूर्ण सबक: आपको (एक मॉडल उपयोगकर्ता के रूप में) उन कर्नेल्स को स्वयं लिखने की आवश्यकता नहीं है — प्रदान किए गए ऑपरेटर का उपयोग करें। citeturn0search9turn0search7

---

## एक्सटेंशन और फॉलो-अप
- **FlashAttention-2 (2023):** और भी बेहतर GPU यूटिलाइजेशन और थ्रूपुट प्राप्त करने के लिए पैरेललिज्म, वर्क पार्टीशनिंग और मल्टीकोर स्केलिंग में सुधार करता है। citeturn0search4
- **FlashAttention-3 और अन्य इंजीनियरिंग कार्य (2024+):** नए हार्डवेयर (हॉपर/H100), FP8, और यहां तक कि उच्च TFLOP यूटिलाइजेशन के लिए और अधिक ट्वीक। ये हार्डवेयर-अवेयर फ्यूज्ड अटेंशन कर्नेल्स के ट्रेंड को जारी रखते हैं। citeturn0search11

---

## जब FlashAttention सबसे अधिक मदद करता है (अनुमानित नियम)
- **लंबे सीक्वेंस** (कई हज़ारों) या बड़े बैच/हेड साइज — सबसे अधिक मेमोरी बचाता है और सबसे बड़ा स्पीडअप देता है।
- **जब DRAM बैंडविड्थ बॉटलनेक हो** — उदाहरण के लिए, बड़े मॉडल जिनमें बड़ा \\(N\\) हो जहां नाइव अटेंशन DRAM को थ्रैश करेगा।
- **बड़े कॉन्टेक्स्ट के साथ ट्रेनिंग** क्योंकि रीकम्प्यूटेशन-फ्रेंडली बैकवर्ड पीक मेमोरी को कम करता है (बड़े बैच/कॉन्टेक्स्ट की अनुमति देता है)। citeturn0search0

---

## क्विक स्यूडोकोड (कॉन्सेप्चुअल)
```
प्रत्येक क्वेरी टाइल Qi के लिए:
    row_max = -inf (प्रति पंक्ति)
    row_sum = 0    (प्रति पंक्ति)
    out_acc = 0    (B x d_v)
    प्रत्येक की/वैल्यू टाइल Kj, Vj के लिए:
        Qi, Kj, Vj को SRAM में लोड करें
        S = Qi @ Kj.T / sqrt(d)
        m = max_row(S)
        expS = exp(S - m)      # लोकल एक्सपोनेंशियल्स
        local_sum = sum_row(expS)
        # नॉर्मलाइज़र्स को स्थिर रूप से मर्ज करें:
        M = max(row_max, m)
        row_sum = row_sum * exp(row_max - M) + local_sum * exp(m - M)
        row_max = M
        # स्केल किए गए expS के साथ आंशिक आउटपुट को एक्यूमुलेट करें:
        out_acc += (expS / (मर्ज के बाद row_sum)) @ Vj
    out_acc को DRAM में लिखें
```
(इम्प्लीमेंटेशन अधिक मेटाडेटा स्टोर करता है और सटीक स्केलिंग की सावधानीपूर्वक व्यवस्था करता है; ऊपर कॉन्सेप्चुअल है।) citeturn0search2

---

## संदर्भ / आगे कहाँ पढ़ें
- मूल FlashAttention पेपर (त्रि डाओ एट अल., 2022) — एल्गोरिदम, IO विश्लेषण, बेंचमार्क। citeturn0search0turn0search2
- आधिकारिक GitHub जिसमें CUDA कर्नेल और PyTorch बाइंडिंग्स हैं। citeturn0search9
- FlashAttention-2 पेपर (त्रि डाओ, 2023) और बाद के संस्करणों पर इंजीनियरिंग नोट्स। citeturn0search4turn0search11

---

यदि आप चाहें, तो मैं यह कर सकता हूं:
- एक नाइव अटेंशन लूप बनाम ब्लॉकवाइज FlashAttention स्यूडोकोड की लाइन-बाय-लाइन तुलना दिखाएं (वाइड टेबल्स के बिना),
- एक ठोस उदाहरण (जैसे, हेड डाइम 64, टाइल 128) के लिए CUDA/Triton कर्नेल ऑर्गनाइजेशन और रजिस्टर/शेयर्ड-मेमोरी बजटिंग का स्केच बनाएं, या
- आपको सटीक PyTorch API कॉल और एक उदाहरण स्निपेट दिखाएं ताकि `torch.nn.functional.scaled_dot_product_attention` को FlashAttention से बदला जा सके। आगे कौन सा सबसे उपयोगी होगा?