---
audio: false
generated: true
lang: hi
layout: post
title: рд▓рд╛рдорд╛.рд╕реАрдкреАрдкреА рдореЙрдбрд▓ рд▓реЛрдбрд┐рдВрдЧ рддреНрд░реБрдЯрд┐ рдХреЛ рдареАрдХ рдХрд░реЗрдВ
translated: true
type: note
---

рддреНрд░реБрдЯрд┐ `gguf_init_from_file_impl: key-value pairs рдХреЛ рдкрдврд╝рдиреЗ рдореЗрдВ рд╡рд┐рдлрд▓` рдХреЗ рдЖрдзрд╛рд░ рдкрд░, рдЖрдкрдХреА рдореЙрдбрд▓ рдлрд╝рд╛рдЗрд▓ рдпрд╛ рддреЛ **рдЦрд░рд╛рдм (рдХрд░рдкреНрдЯреЗрдб)**, **рдЕрдзреВрд░реА**, рдпрд╛ рдЖрдкрдХреЗ llama.cpp рд╡рд░реНрдЬрди рдХреЗ рд╕рд╛рде **рдЕрд╕рдВрдЧрдд** рд╣реИред рдЗрд╕реЗ рдареАрдХ рдХрд░рдиреЗ рдХрд╛ рддрд░реАрдХрд╛ рдпрд╣рд╛рдВ рдмрддрд╛рдпрд╛ рдЧрдпрд╛ рд╣реИ:

### 1. **рдореЙрдбрд▓ рдХреЛ рд╕рддреНрдпрд╛рдкрд┐рдд рдХрд░реЗрдВ рдФрд░ рдлрд┐рд░ рд╕реЗ рдбрд╛рдЙрдирд▓реЛрдб рдХрд░реЗрдВ**
   ```bash
   # рд╡рд░реНрддрдорд╛рди рдлрд╝рд╛рдЗрд▓ рдХрд╛ рдЖрдХрд╛рд░ рдЬрд╛рдВрдЪреЗрдВ (Q6_K рдХреНрд╡рд╛рдВрдЯрд╛рдЗрдЬреЗрд╢рди 8B рдореЙрдбрд▓ рдХреЗ рд▓рд┐рдП ~6.1GB рд╣реЛрдирд╛ рдЪрд╛рд╣рд┐рдП):
   ls -lh /home/lzw/Projects/llama.cpp/models/DeepSeek-R1-0528-Qwen3-8B-UD-Q6_K_XL.gguf

   # рдЕрдкреЗрдХреНрд╖рд┐рдд рдЖрдЙрдЯрдкреБрдЯ:
   # -rw-r--r-- 1 lzw lzw 6.1G Jun  2 10:00 ...Q6_K_XL.gguf
   ```
   - рдпрджрд┐ рдЖрдХрд╛рд░ < 6GB рд╣реИ, рддреЛ **рдореЙрдбрд▓ рдХреЛ рдлрд┐рд░ рд╕реЗ рдбрд╛рдЙрдирд▓реЛрдб рдХрд░реЗрдВ**:
     ```bash
     rm /home/lzw/Projects/llama.cpp/models/DeepSeek-R1-0528-Qwen3-8B-UD-Q6_K_XL.gguf
     # рдореВрд▓ рд╕реНрд░реЛрдд (рдЬреИрд╕реЗ, HuggingFace) рд╕реЗ рдлрд┐рд░ рд╕реЗ рдбрд╛рдЙрдирд▓реЛрдб рдХрд░реЗрдВ
     ```

### 2. **llama.cpp рдЕрдкрдбреЗрдЯ рдХрд░реЗрдВ**
рдЖрдкрдХрд╛ рдмрд┐рд▓реНрдб (рдХрдорд┐рдЯ `7675c555`) рдкреБрд░рд╛рдирд╛ рд╣реЛ рд╕рдХрддрд╛ рд╣реИ:
```bash
cd ~/Projects/llama.cpp
git pull origin master          # рдХреЛрдб рдЕрдкрдбреЗрдЯ рдХрд░реЗрдВ
rm -rf build && mkdir build     # рдмрд┐рд▓реНрдб рд╕рд╛рдлрд╝ рдХрд░реЗрдВ
cd build
cmake .. -DLLAMA_CUBLAS=ON      # CUDA рд╕рдХреНрд╖рдо рдХрд░реЗрдВ
make -j                         # рдлрд┐рд░ рд╕реЗ рдХрдВрдкрд╛рдЗрд▓ рдХрд░реЗрдВ
```

### 3. **рдореЙрдбрд▓ рдХреА рдЕрдЦрдВрдбрддрд╛ рд╕рддреНрдпрд╛рдкрд┐рдд рдХрд░реЗрдВ**
рдПрдХ рд╕рд╛рдзрд╛рд░рдг рдЗрдирдлрд╝рд░реЗрдВрд╕ рд╕реЗ рдореЙрдбрд▓ рдХрд╛ рдкрд░реАрдХреНрд╖рдг рдХрд░реЗрдВ:
```bash
~/Projects/llama.cpp/build/bin/main \
  -m /home/lzw/Projects/llama.cpp/models/DeepSeek-R1-0528-Qwen3-8B-UD-Q6_K_XL.gguf \
  -p "Hello" -n 10
```
- рдпрджрд┐ рдпрд╣ рд╡рд┐рдлрд▓ рд╣реЛрддрд╛ рд╣реИ, рддреЛ рдореЙрдбрд▓ рдлрд╝рд╛рдЗрд▓ рдирд┐рд╢реНрдЪрд┐рдд рд░реВрдк рд╕реЗ рдЦрд░рд╛рдм рд╣реИред

### 4. **рдПрдХ рдЕрд▓рдЧ рдореЙрдбрд▓ рдЖрдЬрд╝рдорд╛рдПрдВ (рд╕реИрдирд┐рдЯреА рдЪреЗрдХ)**
рдПрдХ рдЬреНрдЮрд╛рдд рдХрд╛рд░реНрдпрд╢реАрд▓ рдореЙрдбрд▓ (рдЬреИрд╕реЗ, [TinyLlama](https://huggingface.co/TheBloke/TinyLlama-1.1B-1T-OpenOrca-GGUF)) рд╕реЗ рдкрд░реАрдХреНрд╖рдг рдХрд░реЗрдВ:
```bash
~/Projects/llama.cpp/build/bin/llama-server \
  -m /path/to/tinyllama.Q8_0.gguf \
  --port 8000
```
- рдпрджрд┐ рдпрд╣ рдХрд╛рдо рдХрд░рддрд╛ рд╣реИ, рддреЛ рдЖрдкрдХреА рдореВрд▓ рдореЙрдбрд▓ рдлрд╝рд╛рдЗрд▓ рд╣реА рд╕рдорд╕реНрдпрд╛ рд╣реИред

### 5. **рдореЙрдбрд▓ рдХреЛ рдлрд┐рд░ рд╕реЗ рдХрдиреНрд╡рд░реНрдЯ рдХрд░реЗрдВ (рдПрдбрд╡рд╛рдВрд╕реНрдб)**
рдпрджрд┐ рдлрд┐рд░ рд╕реЗ рдбрд╛рдЙрдирд▓реЛрдб рдХрд░рдиреЗ рдореЗрдВ рд╡рд┐рдлрд▓ рд╣реЛ:
1. рдореВрд▓ Hugging Face рдореЙрдбрд▓ рдкреНрд░рд╛рдкреНрдд рдХрд░реЗрдВ:
   ```bash
   huggingface-cli download deepseek-ai/DeepSeek-R1-0528-Qwen3-8B --local-dir ./deepseek-original
   ```
2. GGUF рдореЗрдВ рдлрд┐рд░ рд╕реЗ рдХрдиреНрд╡рд░реНрдЯ рдХрд░реЗрдВ:
   ```bash
   python3 ~/Projects/llama.cpp/convert-hf-to-gguf.py \
     ./deepseek-original \
     --outtype q6_k  # рдЕрдкрдиреЗ рдХреНрд╡рд╛рдВрдЯрд╛рдЗрдЬреЗрд╢рди рд╕реЗ рдореЗрд▓ рдЦрд╛рдПрдВ
   ```

### рд╕рд╛рдорд╛рдиреНрдп рд╕рдорд╛рдзрд╛рди рд╕рд╛рд░рд╛рдВрд╢:

| рд╕рдорд╕реНрдпрд╛ | рд╕рдорд╛рдзрд╛рди |
|-------|----------|
| **рдЦрд░рд╛рдм рдбрд╛рдЙрдирд▓реЛрдб** | рдореЙрдбрд▓ рдлрд┐рд░ рд╕реЗ рдбрд╛рдЙрдирд▓реЛрдб рдХрд░реЗрдВ |
| **рдкреБрд░рд╛рдирд╛ llama.cpp** | `git pull` + рдлрд┐рд░ рд╕реЗ рдХрдВрдкрд╛рдЗрд▓ рдХрд░реЗрдВ |
| **рдЕрд╕рдВрдЧрдд GGUF** | рдирд╡реАрдирддрдо `convert-hf-to-gguf.py` рдХрд╛ рдЙрдкрдпреЛрдЧ рдХрд░реЗрдВ |
| **рдлрд╝рд╛рдЗрд▓ рдЕрдиреБрдорддрд┐рдпрд╛рдБ** | `chmod +r /path/to/model.gguf` |

> ЁЯТб **рдиреЛрдЯ:** DeepSeek-R1 рдореЙрдбрд▓ рдирдпрд╛ рд╣реИ (28 рдордИ, 2024 рдХреЛ рдЬрд╛рд░реА)ред рд╕рдВрдЧрддрддрд╛ рдХреЗ рд▓рд┐рдП рд╕реБрдирд┐рд╢реНрдЪрд┐рдд рдХрд░реЗрдВ рдХрд┐ рдЖрдкрдХрд╛ llama.cpp **рдкрд┐рдЫрд▓реЗ рд╕рдкреНрддрд╛рд╣ рдХреЗ рднреАрддрд░ рдЕрдкрдбреЗрдЯ рдХрд┐рдпрд╛ рдЧрдпрд╛** рд╣реИред

рдореЙрдбрд▓ рдХреЛ рдлрд┐рд░ рд╕реЗ рдбрд╛рдЙрдирд▓реЛрдб рдХрд░рдиреЗ рдФрд░ llama.cpp рдХреЛ рдЕрдкрдбреЗрдЯ рдХрд░рдиреЗ рд╕реЗ рд╢реБрд░реБрдЖрдд рдХрд░реЗрдВ тАУ рдЗрд╕рд╕реЗ 90% рдРрд╕реЗ рдореБрджреНрджреЗ рд╣рд▓ рд╣реЛ рдЬрд╛рддреЗ рд╣реИрдВред рдпрджрд┐ рдЖрдкрдХреЛ рдФрд░ рд╕рд╣рд╛рдпрддрд╛ рдЪрд╛рд╣рд┐рдП рддреЛ рдореБрдЭреЗ рдмрддрд╛рдПрдВ!