---
audio: false
generated: true
image: false
lang: hi
layout: post
model: x-ai/grok-4-fast
title: न्यूरल नेटवर्क के लिए अधिकतम अद्यतन पैरामीट्रीकरण
translated: true
type: note
---

### μP (मैक्सिमल अपडेट पैरामीटराइजेशन) क्या है?

μP, जिसका पूरा नाम मैक्सिमल अपडेट पैरामीटराइजेशन है, एक न्यूरल नेटवर्क पैरामीटराइजेशन तकनीक है जिसे ग्रेग यांग, एक गणितज्ञ और एआई शोधकर्ता (जो अब xAI में हैं, पहले माइक्रोसॉफ्ट रिसर्च में थे), द्वारा विकसित किया गया था। यह उनके 2022 के शोध पत्र "टेंसर प्रोग्राम्स V: ट्यूनिंग लार्ज न्यूरल नेटवर्क्स वाया जीरो-शॉट हाइपरपैरामीटर ट्रांसफर" में पेश की गई थी। यह डीप लर्निंग मॉडल्स के वजन को इस तरह से री-पैरामीटराइज करती है कि वे प्रशिक्षण के लिए अधिक स्केलेबल और कुशल बन जाएं।

#### उद्देश्य
μP का मुख्य लक्ष्य अलग-अलग मॉडल आकारों में हाइपरपैरामीटर (HP) ट्यूनिंग को स्थिर करना है। पारंपरिक सेटअप (जैसे स्टैंडर्ड पैरामीटराइजेशन, या SP) में, हाइपरपैरामीटर (जैसे लर्निंग रेट) को हर बार दोबारा ट्यून करने की आवश्यकता होती है जब आप मॉडल का आकार बढ़ाते हैं—उदाहरण के लिए, लाखों पैरामीटर्स से अरबों पैरामीटर्स तक—क्योंकि ग्रेडिएंट और अपडेट अस्थिर हो जाते हैं (जो अक्सर मॉडल की चौड़ाई या गहराई के साथ द्विघात रूप से बढ़ते हैं)। μP इस समस्या को पैरामीटर्स को इस तरह रूपांतरित करके ठीक करता है कि "अधिकतम अपडेट" (संभव सबसे बड़ा ग्रेडिएंट स्टेप) पैमाने की परवाह किए बिना सुसंगत बना रहता है। यह **μTransfer** को सक्षम बनाता है, एक वर्कफ़्लो जहां आप एक छोटे "प्रॉक्सी" मॉडल पर HP को ट्यून करते हैं और उन्हें बिना किसी और समायोजन के सीधे एक विशाल लक्ष्य मॉडल पर लागू करते हैं।

#### मुख्य लाभ
- **लागत में भारी बचत**: छोटे मॉडल्स पर ट्यूनिंग करना सस्ता होता है। उदाहरण के लिए, 13M-पैरामीटर वाले प्रॉक्सी से HP ट्रांसफर करने पर प्रकाशित BERT-लार्ज (350M पैरामीटर्स) के परिणामों से बेहतर प्रदर्शन मिला, जिसकी कुल ट्यूनिंग लागत BERT-लार्ज के सिर्फ एक प्रीट्रेनिंग रन के बराबर थी। GPT-3 (6.7B पैरामीटर्स) के लिए, एक 40M-प्रॉक्सी ट्रांसफ़र ने पूर्ण प्रीट्रेनिंग लागत के केवल 7% पर बेसलाइन को हरा दिया।
- **बड़े मॉडल्स के लिए स्केलेबिलिटी**: यह ट्रांसफॉर्मर्स और रेसनेट्स जैसी आर्किटेक्चर पर अच्छी तरह से काम करता है, जिससे यह विशाल न्यूरल नेटवर्क (जैसे, xAI में उपयोग किए जाने वाले) को प्रशिक्षित करने के लिए आदर्श बन जाता है। यह "स्केल-इनवेरिएंट ऑप्टिमा" सुनिश्चित करता है, जिसका अर्थ है कि मॉडल्स के बढ़ने के साथ लॉस लैंडस्केप अप्रत्याशित रूप से विकृत नहीं होता है।
- **उपयोग में आसानी**: यह एक PyTorch लाइब्रेरी के रूप में उपलब्ध है (`pip install mup`), और इसे बड़े AI मॉडल्स के लिए प्रोडक्शन ट्रेनिंग पाइपलाइन में एकीकृत किया जा चुका है।

#### संक्षिप्त गणितीय अंतर्ज्ञान
SP में, वजन का प्रसरण अस्थिर रूप से बढ़ता है, जिससे ग्रेडिएंट में विस्फोट या लुप्त होने की समस्या होती है। μP वजन को री-पैरामीटराइज करता है (उदाहरण के लिए, एक रैखिक परत के लिए, फैन-इन द्वारा स्केलिंग करके) फिशर सूचना मैट्रिक्स को सामान्य करने के लिए, यह सुनिश्चित करते हुए कि ग्रेडिएंट प्रवाह संतुलित रहे। इससे इष्टतम HP (जैसे लर्निंग रेट) पैमाने में लगभग स्थिर बने रहते हैं, जिससे प्रति-आकार पुन: ट्यूनिंग की आवश्यकता नहीं रह जाती है।

μP, न्यूरल नेटवर्क्स में अनंत-चौड़ाई/गहराई की सीमाओं को समझने के लिए यांग के व्यापक "टेंसर प्रोग्राम्स" फ्रेमवर्क पर आधारित है, और यह अभी भी विकसित हो रहा है—हालिया कार्य (2025 तक) HP ट्रांसफर से परे अधिकतम फीचर लर्निंग में इसकी भूमिका पर जोर देता है।

**संदर्भ**
- [टेंसर प्रोग्राम्स V: ट्यूनिंग लार्ज न्यूरल नेटवर्क्स वाया जीरो-शॉट हाइपरपैरामीटर ट्रांसफर](https://arxiv.org/abs/2203.03466)
- [द प्रैक्टिशनर्स गाइड टू द मैक्सिमल अपडेट पैरामीटराइजेशन](https://www.cerebras.ai/blog/the-practitioners-guide-to-the-maximal-update-parameterization)
- [हाउ टू स्केल (μP और स्केलिंग पर ब्लॉग)](https://howtoscalenn.github.io/)
- [ग्रेग यांग का प्रोफेशनल पेज](https://thegregyang.com/)