---
audio: false
date: 2025-09-11
generated: true
image: false
lang: hi
layout: post
model: openai/gpt-5
title: एलएलएम एपीआई के प्रमुख पैरामीटर
translated: true
type: note
---

बिल्कुल—कोई टेबल नहीं, सिर्फ स्पष्ट और साफ व्याख्या। यहाँ LLM APIs (जैसे OpenAI या Copilot जैसी प्रणालियों) के साथ काम करते समय उपलब्ध मुख्य पैरामीटरों का सरल विवरण दिया गया है:

---

## LLM "चैट कंप्लीशन" APIs में सामान्य पैरामीटर

ये वे मुख्य घटक हैं जो आप आमतौर पर एक अनुरोध करते समय पास करते हैं—चाहे वह OpenAI, Azure OpenAI, या उनसे प्राप्त प्रणालियों (कुछ संदर्भों में Copilot जैसी) के लिए हो:

### 1. **`model`**

निर्दिष्ट करता है कि किस भाषा मॉडल का उपयोग करना है (उदा. `gpt-4`, `gpt-3.5-turbo`, आदि)। यह गुणवत्ता, गति और लागत तय करता है।

### 2. **`messages`**

चैट संदेशों की एक सरणी जो इस तरह संरचित होती है:

```json
[
  {"role": "user", "content": "एक छोटी कहानी लिखें"}
```

प्रत्येक संदेश में एक `role` (`user`, `assistant`, या `system`) और `content` होता है।

### 3. **`temperature`**

यादृच्छिकता को नियंत्रित करता है:

* **कम (0–0.3)**: बहुत नियतात्मक; तथ्यात्मक या सटीक प्रतिक्रियाओं के लिए सुरक्षित।
* **मध्यम (0.4–0.7)**: संतुलित—सामान्य लेखन या कोड कार्यों के लिए उपयोगी।
* **उच्च (0.8–1.2)**: अधिक रचनात्मक; ब्रेनस्टॉर्मिंग या कहानियों के लिए आदर्श।
  अक्सर लगभग 0.7 पर डिफ़ॉल्ट होता है। ([Microsoft Learn][1])

### 4. **`top_p` (न्यूक्लियस सैंपलिंग)**

यादृच्छिकता को प्रबंधित करने का एक और तरीका। सभी टोकनों को देखने के बजाय, मॉडल गतिशील उपसमुच्चय से सैंपल लेता है जो संचयी संभाव्यता द्रव्यमान का प्रतिनिधित्व करता है। आमतौर पर, आप **या तो** `temperature` **या** `top_p` को समायोजित करते हैं, दोनों को एक साथ नहीं। ([Microsoft Learn][2])

---

## अतिरिक्त अक्सर देखे जाने वाले पैरामीटर

API और आपके उपयोग के मामले के आधार पर, आप इनसे भी सामना कर सकते हैं:

* **`n`**: उत्पन्न करने के लिए प्रतिक्रियाओं की संख्या (उदा. 2-5 विकल्प लौटाएं)।
* **`stop`**: अधिकतम चार स्ट्रिंग्स जहां मॉडल जनरेशन बंद कर देगा यदि उनका सामना होता है।
* **`max_tokens`**: उत्पन्न प्रतिक्रिया की लंबाई को सीमित करता है।
* **`stream`**: यदि true पर सेट किया जाता है, तो रिजल्ट रीयल-टाइम फ्लो के लिए टोकन-दर-टोकन आते हैं।
* **`user`**: यह पहचानने के लिए एक स्ट्रिंग कि कौन सा उपयोगकर्ता अनुरोध कर रहा है, अक्सर ट्रैकिंग या उपयोग एनालिटिक्स के लिए।

---

## GitHub Copilot "API" के बारे में

GitHub Copilot वर्तमान में डेवलपर्स को कोड कंप्लीशन के लिए `temperature`, `messages` आदि जैसे पैरामीटर को नियंत्रित करने के लिए एक सीधी API पेश नहीं करता है (OpenAI के एंडपॉइंट्स की तरह) ([Stack Overflow][3])। इसलिए यदि आप "Copilot API" की बात कर रहे हैं, तो संभवतः आप सामान्य LLM "चैट कंप्लीशन" एंडपॉइंट्स द्वारा उपयोग किए जाने वाले सामान्य पैटर्न की बात कर रहे हैं—Copilot के लिए सार्वजनिक रूप से उपलब्ध कुछ नहीं।

---

## एकत्रित पैरामीटर (कोई टेबल नहीं, सिर्फ बुलेट पॉइंट)

* **आवश्यक**:

  * `model`
  * `messages`

* **सामान्य ट्यूनिंग नॉब्स**:

  * `temperature`
  * `top_p`
  * (एक समय में एक का उपयोग करें)

* **अतिरिक्त नियंत्रण**:

  * `n`
  * `stop`
  * `max_tokens`
  * `stream`
  * `user`

---

क्या आप इन्हें विशिष्ट उद्देश्यों—जैसे कोड जनरेशन, दस्तावेज़ सारांशन, या रचनात्मक लेखन—के लिए कैसे ट्यून करें, इसके बारे में गहराई से जानना चाहेंगे? या यह देखना चाहेंगे कि इनका उपयोग विभिन्न प्लेटफॉर्मों (उदा. OpenAI बनाम Azure) में कैसे किया जाता है?

[1]: https://learn.microsoft.com/en-us/answers/questions/1313865/recommended-openai-temperature-and-top-p?utm_source=chatgpt.com "Recommended openai temperature and top_p"
[2]: https://learn.microsoft.com/en-us/azure/ai-foundry/openai/concepts/prompt-engineering?utm_source=chatgpt.com "Prompt engineering techniques - Azure OpenAI"
[3]: https://stackoverflow.com/questions/76741410/how-to-invoke-github-copilot-programmatically?utm_source=chatgpt.com "How to invoke Github Copilot programmatically?"