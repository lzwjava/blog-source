---
audio: false
generated: true
image: false
lang: hi
layout: post
model: x-ai/grok-4-fast
title: जीपीटी-1 आर्किटेक्चर और प्रशिक्षण का अवलोकन
translated: true
type: note
---

### GPT-1 आर्किटेक्चर का अवलोकन

GPT-1 (जनरेटिव प्री-ट्रेन्ड ट्रांसफॉर्मर 1) 2018 में OpenAI द्वारा पेश किया गया एक आधारभूत मॉडल है। इसने डाउनस्ट्रीम टास्क के लिए फाइन-ट्यूनिंग के बाद, एक लैंग्वेज मॉडलिंग उद्देश्य का उपयोग करके अनलेबल्ड टेक्स्ट पर एक बड़े लैंग्वेज मॉडल को प्री-ट्रेन करने के दृष्टिकोण का बीड़ा उठाया। आर्किटेक्चर एक **डिकोडर-ओनली ट्रांसफॉर्मर** है, जो मूल ट्रांसफॉर्मर पेपर (वासवानी एट अल., 2017) से अनुकूलित है, लेकिन ऑटोरेग्रेसिव जनरेशन के लिए केवल डिकोडर स्टैक तक सीमित कर दिया गया है। यह डिज़ाइन मॉडल को एक अनुक्रम में अगले टोकन की भविष्यवाणी करने में सक्षम बनाता है, जिससे यह सतत टेक्स्ट से जुड़े कार्यों के लिए उपयुक्त हो जाता है।

BERT जैसे द्वि-दिशात्मक मॉडलों के विपरीत, GPT-1 कार्य-कारण सुनिश्चित करने के लिए **मास्क्ड सेल्फ-अटेंशन** का उपयोग करता है—प्रत्येक पोजीशन केवल पिछली पोजीशन पर ध्यान दे सकती है, जिससे भविष्य के टोकन से सूचना लीक होने से रोका जा सके।

### मुख्य घटक और हाइपरपैरामीटर्स

- **मॉडल प्रकार**: मास्क्ड मल्टी-हेड सेल्फ-अटेंशन और पोजीशन-वाइज फीड-फॉरवर्ड नेटवर्क वाला मल्टी-लेयर ट्रांसफॉर्मर डिकोडर।
- **लेयर की संख्या**: 12 ट्रांसफॉर्मर ब्लॉक (लेयर)।
- **अटेंशन मैकेनिज्म**: प्रति लेयर 12 अटेंशन हेड, प्रत्येक हेड 64-डायमेंशनल स्टेट्स को प्रोसेस करता है (कुल मॉडल डायमेंशन: 768)।
- **एम्बेडिंग डायमेंशन**:
  - हिडन साइज (d_model): 768.
  - फीड-फॉरवर्ड इनर डायमेंशन (d_ff): 3072 (हिडन साइज का 4x, ट्रांसफॉर्मर के लिए मानक)।
- **पोजिशनल एन्कोडिंग**: टोकन एम्बेडिंग में सीखे गए पोजिशनल एम्बेडिंग जोड़े गए (साइनसॉइडल एन्कोडिंग का उपयोग नहीं किया गया)।
- **एक्टिवेशन फंक्शन**: फीड-फॉरवर्ड लेयर में गॉसियन एरर लीनियर यूनिट्स (GELU)।
- **वोकैबुलरी और टोकनाइजेशन**: 40,000 मर्ज के साथ बाइट-पेयर एन्कोडिंग (BPE), कॉर्पस पर प्रशिक्षित।
- **कुल पैरामीटर्स**: लगभग 117 मिलियन।
- **अनुक्रम लंबाई**: 512 टोकन के अनुक्रमों पर प्रशिक्षित।
- **रेगुलराइजेशन**:
  - ड्रॉपआउट: रेजिडुअल, एम्बेडिंग और अटेंशन पर 0.1।
  - वेट डिके: नॉन-बायस/नॉन-लेयर-नॉर्म वेट पर संशोधित L2 रेगुलराइजेशन (0.01)।
- **इनिशियलाइजेशन**: वेट एक सामान्य वितरण N(0, 0.02) से इनिशियलाइज़ किए गए।

### प्रशिक्षण विवरण

- **प्री-ट्रेनिंग**:
  - **डेटासेट**: बुक्सकॉर्पस, ~7,000 अप्रकाशित पुस्तकों का संग्रह (कुल ~800 मिलियन शब्द) जिनमें फ़ैंटेसी, रोमांस और एडवेंचर जैसी शैलियाँ शामिल हैं। टेक्स्ट को साफ किया गया (जैसे, ftfy लाइब्रेरी के माध्यम से) और spaCy के साथ टोकनाइज़ किया गया।
  - **उद्देश्य**: अनसुपरवाइज्ड लैंग्वेज मॉडलिंग (अगले टोकन की भविष्यवाणी)।
  - **ऑप्टिमाइज़र**: एडम with β1=0.9, β2=0.999, ε=1e-8।
  - **लर्निंग रेट शेड्यूल**: 2,000 अपडेट पर लीनियर वार्मअप से अधिकतम 2.5×10⁻⁴ तक, फिर कोसाइन एनीलिंग से 0 तक।
  - **बैच साइज**: प्रति मिनीबैच 64 अनुक्रम।
  - **युग**: 100।
  - **प्रदर्शन मीट्रिक**: प्रशिक्षण कॉर्पस पर 18.4 की पर्प्लेक्सिटी हासिल की।
- **फाइन-ट्यूनिंग**:
  - न्यूनतम परिवर्तन: टास्क-विशिष्ट भविष्यवाणियों के लिए एक रैखिक आउटपुट लेयर जोड़ें और इनपुट को एकल अनुक्रम के रूप में संरचित करने के लिए डिलीमीटर टोकन (जैसे, <s>, <e>, $) का उपयोग करें।
  - सहायक लैंग्वेज मॉडलिंग लॉस को फाइन-ट्यूनिंग के दौरान 0.5 के वजन के साथ उपयोग किया गया।
  - हाइपरपैरामीटर्स: कम लर्निंग रेट (6.25×10⁻⁵), बैच साइज 32, 3 युग, 0.2% वार्मअप और लीनियर डिके के साथ।
  - इस ट्रांसफर लर्निंग दृष्टिकोण ने टास्क-विशिष्ट आर्किटेक्चर के बिना ही टेक्स्चुअल एंटेलमेंट और प्रश्नोत्तर जैसे कार्यों पर मजबूत प्रदर्शन की अनुमति दी।

### उल्लेखनीय नवाचार और सीमाएँ

GPT-1 ने प्रदर्शित किया कि एक एकल प्री-ट्रेन्ड मॉडल सरल इनपुट फॉर्मेटिंग के माध्यम से विविध NLP कार्यों के लिए प्रभावी ढंग से ट्रांसफर हो सकता है, जो उस समय के कुछ टास्क-ट्यून्ड मॉडलों से बेहतर प्रदर्शन करता था। हालाँकि, यह आधुनिक LLMs (जैसे, GPT-3 में 175B पैरामीटर हैं) की तुलना में अपेक्षाकृत छोटा था और 512-टोकन संदर्भों तक सीमित था। इस आर्किटेक्चर ने पूरी GPT श्रृंखला की नींव रखी, जिसमें प्री-ट्रेनिंग में स्केलेबिलिटी पर जोर दिया गया।

[Improving Language Understanding by Generative Pre-Training](https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf)  
[GPT-1 - Wikipedia](https://en.wikipedia.org/wiki/GPT-1)