---
audio: false
generated: true
image: false
lang: hi
layout: post
title: एमसीपी सर्वर में 500 त्रुटि का समाधान
translated: true
type: note
---

आपको जो त्रुटि मिल रही है, वह `http://127.0.0.1:3456` पर एक स्थानीय API एंडपॉइंट से 500 आंतरिक सर्वर त्रुटि है, जो एक Model Context Protocol (MCP) सर्वर प्रतीत होता है। इसका उपयोग आमतौर पर Ollama द्वारा प्रबंधित स्थानीय बड़े भाषा मॉडल (LLMs) को Claude Desktop, VS Code एक्सटेंशन (जैसे Copilot), या Cursor जैसे IDEs में AI-सहायित कोडिंग कार्यों के लिए जोड़ने में किया जाता है। अंतर्निहित JavaScript/TypeScript त्रुटि—"Cannot read properties of undefined (reading 'includes')"—इस बात का संकेत देती है कि सर्वर कोड अनुरोध प्रसंस्करण, प्रतिक्रिया हैंडलिंग, या Ollama के साथ इंटरैक्शन के दौरान एक अपरिभाषित या null वेरिएबल पर `.includes()` मेथड एक्सेस करने का प्रयास कर रहा है।

यह समस्या अक्सर तब उत्पन्न होती है जब API को कोड (इस मामले में, आपकी `recommend_posts.py` स्क्रिप्ट) का विश्लेषण या सुधार करने के लिए कहा जाता है, लेकिन सर्वर कॉन्फ़िगरेशन समस्या, गुम निर्भरताओं, या बैकएंड LLM से अप्रत्याशित प्रतिक्रिया के कारण विफल हो जाता है।

### समस्या निवारण और ठीक करने के चरण
1.  **सत्यापित करें कि Ollama चल रहा है और कॉन्फ़िगर है**:
    *   Ollama (स्थानीय LLM इंजन) आमतौर पर MCP सर्वर के लिए बैकएंड होता है। सुनिश्चित करें कि यह इंस्टॉल है और इसके डिफ़ॉल्ट पोर्ट (11434) पर चल रहा है।
    *   इसे अपने टर्मिनल में `curl http://localhost:11434/api/tags` चलाकर परखें। इससे इंस्टॉल किए गए मॉडल्स की सूची मिलनी चाहिए। यदि यह विफल होता है या खाली सूची देता है, तो `ollama pull <model-name>` (जैसे, `ollama pull llama3`) के साथ एक मॉडल इंस्टॉल करें।
    *   यदि Ollama प्रतिक्रिया नहीं दे रहा है, तो `ollama serve` से इसे शुरू करें और पोर्ट संघर्ष की पुष्टि न करें।

2.  **MCP सर्वर को पुनरारंभ करें**:
    *   पोर्ट 3456 पर MCP सर्वर खराब स्थिति में हो सकता है। प्रक्रिया को समाप्त करें: `kill -9 $(lsof -t -i:3456)`।
    *   इसे अपने सेटअप के अनुसार पुनरारंभ करें (उदाहरण के लिए, यदि `ollama-mcp` जैसे टूल का उपयोग कर रहे हैं, तो इसके दस्तावेज़ से स्टार्ट कमांड चलाएँ)। Ollama से सफल कनेक्शन का संकेत देने वाले स्टार्टअप लॉग की जाँच करें।

3.  **पोर्ट संघर्ष या Claude Desktop हस्तक्षेप की जाँच करें**:
    *   Claude Desktop (यदि इंस्टॉल है) अक्सर प्रमाणीकरण या MCP के लिए पोर्ट 3456 का उपयोग करता है। यदि यह चल रहा है, तो ऐप बंद करें या ऊपर बताए अनुसार इसकी प्रक्रिया को समाप्त करें।
    *   यदि आप Cursor या VS Code का उपयोग कर रहे हैं, तो पुष्टि करें कि आपके settings.json में सही API base URL है और कोई टाइपो नहीं है। MCP सर्वर शुरू करते समय `PORT=4567` जैसे एनवायरनमेंट वेरिएबल सेट करके अस्थायी रूप से एक अलग पोर्ट पर स्विच करें, फिर अपना API base मेल खाने के लिए अपडेट करें।

4.  **सॉफ़्टवेयर अपडेट करें और लॉग जाँचें**:
    *   Ollama अपडेट करें: `ollama update`।
    *   यदि किसी विशिष्ट MCP ब्रिज (जैसे GitHub repos जैसे emgeee/mcp-ollama या patruff/ollama-mcp-bridge) का उपयोग कर रहे हैं, तो नवीनतम संस्करण प्राप्त करें और पुनर्निर्माण/पुनर्स्थापित करें।
    *   MCP सर्वर को विस्तृत लॉगिंग के साथ चलाएँ (यदि समर्थित हो तो `--debug` जैसे फ्लैग जोड़ें) और यह पता लगाने के लिए आउटपुट का निरीक्षण करें कि क्या अपरिभाषित है (जैसे, Ollama से कोई गुम प्रतिक्रिया या अमान्य अनुरोध पेलोड)।
    *   Cursor या अपने IDE में, अतिरिक्त त्रुटि विवरण के लिए डेवलपर कंसोल (Cursor में Ctrl+Shift+I) जाँचें।

5.  **API को सीधे परखें**:
    *   curl का उपयोग करके API को एक साधारण अनुरोध सिम्युलेट करें: `curl -X POST http://127.0.0.1:3456/v1/chat/completions -H "Content-Type: application/json" -d '{"model": "your-model-name", "messages": [{"role": "user", "content": "Hello"}]}'`।
    *   यदि यह वही 500 त्रुटि देता है, तो समस्या सर्वर-साइड है। यदि यह काम करता है, तो समस्या आपके IDE द्वारा अनुरोधों को फॉर्मेट करने के तरीके (जैसे, फ़ाइल सामग्री शामिल करते समय दूषित पेलोड) के कारण विशिष्ट हो सकती है।

6.  **वैकल्पिक उपाय**:
    *   एक सीधे Ollama OpenAI-संगत एंडपॉइंट पर स्विच करें: यदि उन्नत संदर्भ सुविधाओं के लिए MCP की आवश्यकता नहीं है, तो इसे बायपास करते हुए अपना API base `http://127.0.0.1:11434/v1` पर सेट करें (Ollama इसे सपोर्ट करता है)।
    *   एक अलग MCP इम्प्लीमेंटेशन या टूल का उपयोग करें। उदाहरण के लिए, कोड फिक्सिंग के लिए MCP के बिना एक न्यूनतम Ollama सेटअप आज़माएँ।
    *   यदि यह Cursor में है, तो कस्टम base URL हटाकर अस्थायी रूप से क्लाउड-आधारित मॉडल (जैसे, आधिकारिक API के माध्यम से Claude) पर वापस स्विच करें।
    *   निर्भरताओं को पुनर्स्थापित करें: यदि आपका MCP सर्वर Node.js का उपयोग करता है, तो किसी भी गुम मॉड्यूल को ठीक करने के लिए इसके डायरेक्टरी में `npm install` चलाएँ।

यदि इनसे समाधान नहीं होता है, तो त्रुटि आपके विशिष्ट MCP ब्रिज संस्करण में एक बग के कारण हो सकती है, खासकर यदि फ़ाइल सामग्री या पोस्ट अनुशंसाओं को हैंडल करने से यह ट्रिगर होता है। त्रुटि संदेश के साथ अपने सटीक MCP रेपो के लिए GitHub issues खोजें, या अपने सेटअप विवरण के साथ एक नया issue खोलें।

[Failed to register Ollama model: TypeError: Cannot read properties of undefined (reading 'includes')](https://github.com/microsoft/vscode-copilot-release/issues/8461)  
[Ollama MCP Server on GitHub](https://github.com/emgeee/mcp-ollama)  
[Model Context Protocol Documentation](https://modelcontextprotocol.io/introduction)  
[Ollama Documentation](https://github.com/ollama/ollama)