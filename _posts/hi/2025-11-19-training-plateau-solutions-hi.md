---
audio: false
generated: true
image: false
lang: hi
layout: post
model: x-ai/grok-4-fast
title: प्रशिक्षण स्थिरता के कारण और समाधान
translated: true
type: note
---

आपके द्वारा पोस्ट किए गए ट्रेनिंग लॉग में FineWeb-edu डेटा पर चल रहे एक छोटे पैमाने के nanoGPT रन में एक क्लासिक **गंभीर पठार (severe plateau)** दिख रहा है। वैलिडेशन लॉस शुरुआत में अच्छी तरह गिरता है (~10.9 → ~3.57, लगभग iter 14k पर), लेकिन फिर पूरी तरह से सपाट हो जाता है और लगभग ~13-14k इटरेशन के बाद **बहुत धीरे-धीरे बढ़ना** भी शुरू कर देता है।

यहाँ वास्तविक कारण हैं कि आपका मॉडल सुधरना क्यों बंद कर दिया (इस एकदम सटीक सेटअप के लिए महत्व के क्रम में):

### 1. आप एक ही Parquet फ़ाइल पर बहुत अधिक ओवरफिटिंग कर रहे हैं
- आपके रन का नाम: `125M-single-parquet-4070`
- आप एक 40M–125M मॉडल (लॉग के अनुसार 40.56M पैरामीटर, संभवतः मूल GPT-2 small कॉन्फ़िगरेशन) को **एक ही FineWeb parquet** (संभवतः 10–15 GB → अधिकतम ~3–4 बिलियन टोकन, लेकिन फ़िल्टरिंग के बाद संभवतः इससे कहीं कम) पर ट्रेन कर रहे हैं।
- 14,000 इटरेशन तक, batch_size=16, grad_accum=32, block_size=1024 के साथ, आपने पहले ही देख लिया है:
  14,000 इटर × 524,288 टोकन/इटर ≈ **7.3 बिलियन टोकन**
- इसका मतलब है कि आपने उसी एकदम सटीक डेटा पर पहले ही **2–3 पूर्ण एपोक (epoch)** कर लिए हैं।
- FineWeb-edu उच्च-गुणवत्ता वाला है लेकिन फिर भी इसमें बहुत सारे नियर-डुप्लिकेट्स और बॉयलरप्लेट हैं। एक ही फ़ाइल(ों) पर 1.5–2 एपोक के बाद, एक 40M–125M मॉडल लगभग हर उपयोगी चीज को याद कर लेगा और लॉस मजबूती से पठार पर पहुँच जाता है।

### 2. पठार के बाद लर्निंग रेट अब बहुत अधिक है
- आप `learning_rate = 1e-3` का उपयोग कर रहे हैं, जो 20,000 इटरेशन में कोसाइन डिके के through `min_lr = 1e-4` तक कम होती है।
- इटरेशन 14,000 पर LR अभी भी ~2.5e-4 है (कोसाइन डिके शुरुआत में धीमी होती है)।
- एक बार मॉडल ने डेटा में मौजूद सब कुछ सीख लेने के बाद, LR को सैकड़ों माइक्रो-एलआर यूनिट्स पर बनाए रखना इसे और फाइन-ट्यून करने से रोकता है और वास्तव में जनरलाइजेशन को नुकसान पहुंचाना शुरू कर देता है → वैल लॉस धीरे-धीरे चढ़ता है।

### 3. मॉडल आकार बनाम डेटा विविधता का मिसमैच
एक ही parquet फ़ाइल पर 125M (या यहाँ तक कि 40M जो आपने वास्तव में इनिशियलाइज़ किया है) मॉडल, एक कॉलेज के छात्र को केवल एक टेक्स्टबुक देने और उससे वर्षों तक उसे पढ़ते रहने के बराबर है। एक समय बाद वह इसे परफेक्टली रीटाइट कर सकता है, लेकिन नए टेक्स्ट पर टेस्ट-टाइम परफॉर्मेंस सुधरना बंद कर देती है और पहले के पैटर्न को भूलने (catastrophic forgetting) के कारण थोड़ी खराब भी हो जाती है।

### 4. पर्याप्त मजबूत रेगुलराइजेशन का अभाव
- dropout = 0.1 ठीक है, लेकिन जब आप इतनी जोर से ओवरफिट कर रहे हों तो पर्याप्त नहीं है
- weight_decay = 0.1 मानक है, लेकिन छोटे डेटा पर 2+ एपोक के लिए अभी भी पर्याप्त नहीं है
- कोई ग्रेडिएंट क्लिपिंग नहीं (nanoGPT डिफॉल्ट 1.0 है, लेकिन कभी-कभी लोग छोटे डेटा के लिए 0.5 या नीचे सेट करते हैं)

### 5. देखे गए टोकन बनाम वास्तविक विविधता
भले ही आपने 7+ बिलियन टोकन देखे हैं, **प्रभावी विविधता** डीडप, बॉयलरप्लेट, लो-एडु-स्कोर सैंपल्स आदि के बाद शायद 1–1.5B अद्वितीय उच्च-गुणवत्ता वाले टोकन है। यह 125M मॉडल के लिए इस टेक्स्ट डिस्ट्रीब्यूशन पर ~3.5–3.6 लॉस से आगे सुधार जारी रखने के लिए पर्याप्त नहीं है।

### आपके अपने लॉग से सबूत

| स्टेप | वैल लॉस | टिप्पणी                                       |
|--------|----------|-----------------------------------------------|
| 0      | 10.87    | रैंडम                                        |
| 2000   | 4.03     | तेज प्रगति                                   |
| 5000   | 3.76     | अभी भी अच्छा                                 |
| 8000   | 3.65     | धीमा पड़ रहा है                              |
| 11000  | 3.61     | लगभग सपाट                                   |
| 13500  | 3.57     | **निरपेक्ष न्यूनतम (absolute lowest)**      |
| 14500  | 3.569    | फिर से बढ़ना शुरू → क्लासिक ओवरफिट का संकेत |

### वास्तव में कम लॉस पाने के लिए आपको क्या करना चाहिए

1. बहुत अधिक डेटा का उपयोग करें
   - पूर्ण 100+ GB FineWeb-edu सैंपल (या कम से कम 10–20 parquet फाइलें) का उपयोग करें
   - या हग्गिंगफेस डेटासेट्स से मल्टीपल शार्ड्स के साथ स्ट्रीम करें

2. विविध डेटा पर ठीक 1 एपोक (या थोड़ा कम) ट्रेन करें
   - कुल ~10B–15B टोकन FineWeb-edu पर 125M के लिए स्वीट स्पॉट है
   - Llama-3/Phi-3 स्टाइल: 15B टोकन → 125M-क्लास मॉडल के लिए ~3.0–3.2 वैल लॉस

3. अंतिम लर्निंग रेट को कम करें
   - `min_lr = 5e-5` या `3e-5` का उपयोग करें (हाल के छोटे मॉडल रन में आम)
   - या लीनियर डिके + लंबे जीरो-एलआर होल्ड में स्विच करें

4. थोड़ा बड़ा मॉडल या बेहतर आर्किटेक्चर ट्रिक्स
   - मूल nanoGPT, GPT-2 स्टाइल का है (ALiBi अनुपस्थित, पुराना RoPE, आदि)
   - आधुनिक 125M मॉडल (Qwen2-0.5B, Phi-3-mini, Llama-3 8B स्केल डाउन) SwiGLU, RMSNorm, रोटरी एम्बेडिंग्स आदि के कारण उसी डेटा पर ~2.8–3.0 तक पहुँच जाते हैं।

### संक्षेप में – आपका रन ~3.57 पर पठार पर क्यों पहुँच गया और कभी नीचे नहीं जाएगा

आप एक 40–125M मॉडल को एक ही FineWeb parquet फ़ाइल (~कुछ बिलियन टोकन कुल) पर कई एपोक के लिए ट्रेन कर रहे हैं। मॉडल ने लगभग ~13k इटरेशन तक पहले ही हर उपयोगी चीज को याद कर लिया है। अभी भी उच्च LR के साथ जारी ट्रेनिंग सिर्फ वेट्स को इधर-उधर करती है → पठार + थोड़ी सी ओवरफिट गिरावट।

यह वह नंबर 1 गलती है जो लोग पहली बार FineWeb पर nanoGPT आज़माते समय करते हैं: "मैं टेस्ट करने के लिए बस एक फाइल का उपयोग करूंगा" → इसी एकदम सटीक लॉस कर्व के साथ समाप्त होता है।

सुधार: या तो (a) 10–50× अधिक डेटा का उपयोग करें और कुल ~1 एपोक के लिए ट्रेन करें, या (b) स्वीकार करें कि ~3.55–3.6 इस मॉडल+डेटा कॉम्बिनेशन के लिए निरपेक्ष फ्लोर है और ट्रेनिंग रोक दें।

यदि आप एक ऐसा रन चाहते हैं जो वास्तव में nanoGPT-स्टाइल कोड के साथ ~3.0–3.2 लॉस तक पहुँचता है, तो आपको कम से कम पूर्ण 100GB FineWeb-edu सैंपल (या नए FineWeb-Edu 2.0) की आवश्यकता है और अधिकतम ~15B टोकन के लिए ट्रेन करना होगा।