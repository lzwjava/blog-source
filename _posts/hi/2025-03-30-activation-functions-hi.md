---
audio: false
generated: true
lang: hi
layout: post
title: न्यूरल नेटवर्क में सक्रियण फलन
translated: true
type: note
---

**सिग्मॉइड**, **ReLU**, और **Leaky ReLU** सभी न्यूरल नेटवर्क्स में उपयोग किए जाने वाले एक्टिवेशन फंक्शन हैं। इनमें से प्रत्येक की अपनी विशेषताएं हैं और ये हल किए जा रहे प्रश्न के प्रकार के आधार पर विभिन्न कार्यों के लिए उपयुक्त हैं। यहां अंतरों का विवरण दिया गया है:

### 1. **सिग्मॉइड**:
   - **फॉर्मूला**:  
     \\[
     \sigma(x) = \frac{1}{1 + e^{-x}}
     \\]
   - **रेंज**: (0, 1)
   - **फायदे**:
     - चिकना ग्रेडिएंट, जो ऑप्टिमाइज़ेशन में मदद करता है।
     - आउटपुट 0 और 1 के बीच बाउंडेड होता है, जिससे यह प्रोबेबिलिटीज या बाइनरी क्लासिफिकेशन के लिए अच्छा बनता है।
   - **नुकसान**:
     - **वैनिशिंग ग्रेडिएंट प्रॉब्लम**: बहुत बड़े या छोटे इनपुट वैल्यूज के लिए, ग्रेडिएंट बहुत छोटा (लगभग शून्य) हो जाता है, जो ट्रेनिंग को धीमा कर सकता है, खासकर डीप नेटवर्क्स में।
     - आउटपुट शून्य-केंद्रित नहीं होते हैं, जिससे समस्याएं हो सकती हैं जब ग्रेडिएंट अपडेट एक दिशा से प्रभावित होते हैं।
   - **उपयोग**: अक्सर बाइनरी क्लासिफिकेशन टास्क्स (जैसे लॉजिस्टिक रिग्रेशन में) के लिए आउटपुट लेयर में उपयोग किया जाता है।

### 2. **ReLU (रेक्टिफाइड लीनियर यूनिट)**:
   - **फॉर्मूला**:  
     \\[
     f(x) = \max(0, x)
     \\]
   - **रेंज**: [0, ∞)
   - **फायदे**:
     - **तेज़ और सरल**: कंप्यूट करने में आसान और प्रैक्टिस में कुशल।
     - ग्रेडिएंट्स को अच्छी तरह से प्रोपेगेट होने देकर वैनिशिंग ग्रेडिएंट प्रॉब्लम को हल करता है।
     - स्पैरसिटी को प्रोत्साहित करता है (कई न्यूरॉन्स निष्क्रिय हो सकते हैं)।
   - **नुकसान**:
     - **डाइंग ReLU प्रॉब्लम**: न्यूरॉन्स ट्रेनिंग के दौरान "मर" सकते हैं यदि उनका आउटपुट हमेशा शून्य रहता है (यानी, नेगेटिव इनपुट्स के लिए)। इससे कुछ न्यूरॉन्स फिर से कभी एक्टिवेट नहीं हो पाते।
   - **उपयोग**: डीप नेटवर्क्स की हिडन लेयर्स में बहुत आमतौर पर उपयोग किया जाता है, खासकर कन्वोल्यूशनल और डीप न्यूरल नेटवर्क्स में।

### 3. **Leaky ReLU**:
   - **फॉर्मूला**:  
     \\[
     f(x) = \max(\alpha x, x)
     \\]
     जहां \\( \alpha \\) एक छोटा सा कॉन्स्टेंट है (जैसे, 0.01)।
   - **रेंज**: (-∞, ∞)
   - **फायदे**:
     - \\( x \\) के नेगेटिव वैल्यूज के लिए एक छोटी नेगेटिव स्लोप की अनुमति देकर **डाइंग ReLU प्रॉब्लम** को रोकता है।
     - ReLU की तरह, यह वैनिशिंग ग्रेडिएंट प्रॉब्लम में मदद करता है और स्पैरसिटी को प्रोत्साहित करता है।
   - **नुकसान**:
     - अभी भी नेगेटिव आउटपुट्स की कुछ संभावना है, लेकिन स्टैंडर्ड ReLU की तुलना में बहुत कम समस्याग्रस्त।
     - \\( \alpha \\) का चुनाव मुश्किल हो सकता है, और कभी-कभी ट्यूनिंग की आवश्यकता होती है।
   - **उपयोग**: ReLU के साथ होने वाली समस्याओं से बचने के लिए कई डीप लर्निंग मॉडल्स में उपयोग किया जाता है, खासकर उन नेटवर्क्स में जहां डाइंग ReLU प्रॉब्लम हो सकती है।

---

### **तुलना सारांश**:

- **सिग्मॉइड**:  
  बाइनरी क्लासिफिकेशन टास्क्स में आउटपुट लेयर्स के लिए सबसे अच्छा, लेकिन वैनिशिंग ग्रेडिएंट प्रॉब्लम से ग्रस्त है।

- **ReLU**:  
  हिडन लेयर्स के लिए एक अच्छा सामान्य-उद्देश्य वाला एक्टिवेशन फंक्शन। यह ट्रेनिंग को तेज करता है और बड़े नेटवर्क्स के लिए अच्छा काम करता है, लेकिन इसमें डाइंग न्यूरॉन प्रॉब्लम हो सकती है।

- **Leaky ReLU**:  
  ReLU का एक मॉडिफिकेशन जो नेगेटिव इनपुट्स के लिए एक छोटे, नॉन-जीरो ग्रेडिएंट की अनुमति देकर डाइंग ReLU प्रॉब्लम को संबोधित करता है। यह डीपर मॉडल्स के लिए व्यापक रूप से उपयोग किया जाता है।

### **प्रैक्टिस में**:
- **सिग्मॉइड** का उपयोग अक्सर तब किया जाता है जब आउटपुट (0, 1) की रेंज में होना चाहिए, जैसे बाइनरी क्लासिफिकेशन टास्क्स की फाइनल लेयर में।
- **ReLU** अक्सर हिडन लेयर्स के लिए डिफॉल्ट चॉइस होता है क्योंकि यह सरल है और वैनिशिंग ग्रेडिएंट्स को अच्छी तरह हैंडल करने की क्षमता रखता है।
- **Leaky ReLU** का उपयोग ReLU के स्थान पर तब किया जाता है जब ट्रेनिंग के दौरान न्यूरॉन्स के "मरने" की चिंता होती है।

इनमें से प्रत्येक एक्टिवेशन फंक्शन का आपके द्वारा बनाए जा रहे न्यूरल नेटवर्क की संरचना और लक्ष्यों के आधार पर अपना स्थान है!