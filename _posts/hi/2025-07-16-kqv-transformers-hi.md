---
audio: false
generated: false
image: false
lang: hi
layout: post
title: तंत्रिका नेटवर्क, ट्रांसफार्मर और जीपीटी
translated: true
---

### विषय-सूची

1. [मैंने ट्रांसफॉर्मर में KQV मैकेनिज्म कैसे सीखा](#how-i-learned-the-kqv-mechanism-in-transformers)
   - क्वेरी, कुंजी, वैल्यू मैट्रिक्स टोकन इंटरैक्शन का प्रतिनिधित्व करते हैं
   - समझने के लिए आयामों और आकृतियों को जानना आवश्यक है
   - प्रारंभिक अवधारणाएँ समय के साथ स्पष्ट होती जाती हैं
   - AI युग प्रचुर मात्रा में सीखने के संसाधन प्रदान करता है
   - प्रेरणादायक कहानियाँ निरंतर सीखने के लिए प्रेरित करती हैं

2. [न्यूरल नेटवर्क से GPT तक](#from-neural-network-to-gpt)
   - समझने के लिए न्यूरल नेटवर्क को खरोंच से दोहराना
   - ट्रांसफॉर्मर एम्बेडिंग और एन्कोडिंग के माध्यम से टेक्स्ट को संसाधित करते हैं
   - सेल्फ-अटेंशन शब्दों के बीच समानता की गणना करता है
   - मूलभूत व्याख्यान देखें और कोड पढ़ें
   - परियोजनाओं और पत्रों के माध्यम से जिज्ञासा का पालन करें

3. [न्यूरल नेटवर्क कैसे काम करता है](#how-neural-network-works)
   - बैकप्रॉपगेशन एल्गोरिदम भार और पूर्वाग्रहों को अपडेट करता है
   - इनपुट डेटा नेटवर्क परतों के माध्यम से सक्रिय होता है
   - फीडफॉरवर्ड सिग्मॉइड के माध्यम से परत आउटपुट की गणना करता है
   - त्रुटि गणना सीखने के समायोजन को निर्देशित करती है
   - आयामी समझ समझ के लिए महत्वपूर्ण है


## मैंने ट्रांसफॉर्मर में KQV मैकेनिज्म कैसे सीखा

*2025.07.16*

[ट्रांसफॉर्मर में K, Q, V मैकेनिज्म](https://lzwjava.github.io/notes/2025-06-02-attention-kqv-en) पढ़ने के बाद, मैं किसी तरह समझ गया कि K, Q और V कैसे काम करते हैं।

Q का अर्थ है क्वेरी, K का अर्थ है कुंजी, और V का अर्थ है वैल्यू। एक वाक्य के लिए, क्वेरी एक मैट्रिक्स है जो एक टोकन का मान संग्रहीत करता है जिसे उसे अन्य टोकन के बारे में पूछने की आवश्यकता होती है। कुंजी टोकन के विवरण को दर्शाती है, और वैल्यू टोकन के वास्तविक अर्थ मैट्रिक्स को दर्शाती है।

उनके विशिष्ट आकार होते हैं, इसलिए उनके आयामों और विवरणों को जानने की आवश्यकता होती है।

मुझे यह लगभग जून 2025 की शुरुआत में समझ में आया। मैंने पहली बार इसके बारे में 2023 के अंत में सीखा था। उस समय, मैंने [द इलस्ट्रेटेड ट्रांसफॉर्मर](https://jalammar.github.io/illustrated-transformer/) जैसे लेख पढ़े थे, लेकिन मुझे ज्यादा समझ नहीं आया।

लगभग दो साल बाद, मुझे अब इसे समझना आसान लगा। इन दो वर्षों के दौरान, मैंने बैकएंड के काम पर और अपने एसोसिएट डिग्री की परीक्षाओं की तैयारी पर ध्यान दिया, और मैंने मशीन लर्निंग के बारे में ज्यादा पढ़ा या सीखा नहीं। हालांकि, मैं गाड़ी चलाते समय या अन्य काम करते समय समय-समय पर इन अवधारणाओं पर विचार करता रहा।

यह मुझे समय के प्रभाव की याद दिलाता है। हम पहली नज़र में बहुत सी चीजें सीख सकते हैं, भले ही हमें ज्यादा समझ न आए। लेकिन किसी तरह, यह हमारी सोच के लिए एक शुरुआती बिंदु को ट्रिगर करता है।

समय के साथ, मैंने पाया कि ज्ञान और खोज के लिए, पहली बार में चीजों के बारे में सोचना या समझना मुश्किल होता है। लेकिन बाद में, सीखना और जानना आसान लगता है।

एक कारण यह है कि AI युग में सीखना आसान है क्योंकि आप अपने संदेहों को दूर करने के लिए किसी भी विवरण या पहलू में गहराई से जा सकते हैं। अधिक संबंधित AI वीडियो भी उपलब्ध हैं। इससे भी महत्वपूर्ण बात यह है कि आप देखते हैं कि इतने सारे लोग इस पर सीख रहे हैं और प्रोजेक्ट बना रहे हैं, जैसे [llama.cpp](https://github.com/ggml-org/llama.cpp)।

जॉर्जी गर्गानोव की कहानी प्रेरणादायक है। 2021 के आसपास शुरू होने वाले एक नए मशीन लर्निंग सीखने वाले के रूप में, उन्होंने AI समुदाय में एक शक्तिशाली प्रभाव डाला।

इस तरह की चीजें बार-बार होंगी। तो, रीइन्फोर्समेंट लर्निंग और नवीनतम AI ज्ञान के लिए, भले ही मैं अभी भी उन पर ज्यादा समय नहीं दे पा रहा हूं, मुझे लगता है कि मैं कुछ समय निकाल सकता हूं ताकि उन्हें जल्दी से सीख सकूं और उनके बारे में बहुत कुछ सोचने की कोशिश कर सकूं। दिमाग अपना काम करेगा।


---

## न्यूरल नेटवर्क से GPT तक

*2023.09.28*

### YouTube वीडियो

एंड्रेज कारपाथी - आइए GPT बनाते हैं: खरोंच से, कोड में, विस्तार से बताया गया।

उमर जमील - अटेंशन इज़ ऑल यू नीड (ट्रांसफॉर्मर) - मॉडल स्पष्टीकरण (गणित सहित), इन्फरेंस और प्रशिक्षण

जॉश स्टारमर के साथ स्टेटक्वेस्ट - ट्रांसफॉर्मर न्यूरल नेटवर्क्स, ChatGPT की नींव, स्पष्ट रूप से समझाया गया!!!

पास्कल पाउपार्ट - CS480/680 लेक्चर 19: अटेंशन और ट्रांसफॉर्मर नेटवर्क्स

द ए.आई. हैकर - माइकल फी - ट्रांसफॉर्मर न्यूरल नेटवर्क का सचित्र गाइड: एक चरण-दर-चरण स्पष्टीकरण

### मैं कैसे सीखता हूँ

एक बार जब मैंने "न्यूरल नेटवर्क्स एंड डीप लर्निंग" किताब का आधा हिस्सा पढ़ लिया, तो मैंने हस्तलिखित अंकों को पहचानने वाले न्यूरल नेटवर्क के उदाहरण को दोहराना शुरू कर दिया। मैंने GitHub पर एक रिपॉजिटरी बनाई, https://github.com/lzwjava/neural-networks-and-zhiwei-learning।

यही असली मुश्किल हिस्सा है। यदि कोई बिना किसी कोड को कॉपी किए खरोंच से लिख सकता है, तो वह बहुत अच्छी तरह समझता है।

मेरे दोहराए गए कोड में अभी भी update_mini_batch और backprop का कार्यान्वयन नहीं है। हालांकि, डेटा लोड करने, फीड फॉरवर्डिंग और मूल्यांकन के चरण में चर का सावधानीपूर्वक अवलोकन करके, मुझे वेक्टर, आयामीता, मैट्रिक्स और वस्तुओं के आकार की कहीं बेहतर समझ मिली।

और मैंने GPT और ट्रांसफॉर्मर के कार्यान्वयन को सीखना शुरू कर दिया। वर्ड एम्बेडिंग और पोसिशनल एन्कोडिंग द्वारा, टेक्स्ट संख्याओं में बदल जाता है। फिर, संक्षेप में, इसमें हस्तलिखित अंकों को पहचानने के लिए साधारण न्यूरल नेटवर्क से कोई अंतर नहीं है।

एंड्रेज कारपाथी का व्याख्यान "आइए GPT बनाते हैं" बहुत अच्छा है। वह चीजों को अच्छी तरह से समझाते हैं।

पहला कारण यह है कि यह वास्तव में खरोंच से है। हम पहले देखते हैं कि टेक्स्ट कैसे उत्पन्न होता है। यह कुछ हद तक अस्पष्ट और यादृच्छिक है। दूसरा कारण यह है कि एंड्रेज चीजों को बहुत सहज रूप से कह सकते थे। एंड्रेज ने कई महीनों तक नैनोजीपीटी परियोजना पर काम किया।

मेरे पास व्याख्यान की गुणवत्ता का आकलन करने का एक नया विचार था। क्या लेखक वास्तव में इन कोडों को लिख सकता है? मुझे समझ क्यों नहीं आता और लेखक कौन सा विषय छोड़ देता है? इन सुंदर आरेखों और एनिमेशन के अलावा, उनकी क्या कमियां और दोष हैं?

मशीन लर्निंग के विषय पर वापस आते हैं। जैसा कि एंड्रेज बताते हैं, ड्रापआउट, अवशेष कनेक्शन, सेल्फ-अटेंशन, मल्टी-हेड अटेंशन, मास्क्ड अटेंशन।

अधिक उपरोक्त वीडियो देखकर, मुझे थोड़ा-थोड़ा समझ आने लगा।

sin और cos फ़ंक्शन के साथ पोसिशनल एन्कोडिंग द्वारा, हमें कुछ भार मिलते हैं। वर्ड एम्बेडिंग द्वारा, हम शब्दों को संख्याओं में बदलते हैं।

$$
    PE_{(pos,2i)} = sin(pos/10000^{2i/d_{model}}) \\
    PE_{(pos,2i+1)} = cos(pos/10000^{2i/d_{model}})
$$

> पिज्जा ओवन से निकला और उसका स्वाद अच्छा था।

इस वाक्य में, एल्गोरिथम को कैसे पता चलेगा कि यह पिज्जा या ओवन को संदर्भित करता है? हम वाक्य में प्रत्येक शब्द के लिए समानताओं की गणना कैसे करते हैं?

हम भार का एक सेट चाहते हैं। यदि हम अनुवाद के कार्य के लिए ट्रांसफॉर्मर नेटवर्क का उपयोग करते हैं, तो हर बार जब हम एक वाक्य इनपुट करते हैं, तो यह दूसरी भाषा का संबंधित वाक्य आउटपुट कर सकता है।

यहाँ डॉट उत्पाद के बारे में। एक कारण यह है कि हम यहाँ डॉट उत्पाद का उपयोग करते हैं कि डॉट उत्पाद वेक्टर में प्रत्येक संख्या पर विचार करेगा। क्या होगा यदि हम स्क्वेर्ड डॉट उत्पाद का उपयोग करें? हम पहले संख्याओं का वर्ग करते हैं, फिर उन्हें डॉट उत्पाद करने देते हैं। क्या होगा यदि हम कुछ उलटा डॉट उत्पाद करें?

यहाँ मास्क्ड के बारे में, हम आधे मैट्रिक्स की संख्याओं को नकारात्मक अनंत तक बदलते हैं। और फिर हम मानों को 0 से 1 तक रखने के लिए सॉफ्टमैक्स का उपयोग करते हैं। क्या होगा यदि हम नीचे-बाईं संख्याओं को नकारात्मक अनंत तक बदल दें?

### योजना

कोड और पेपर पढ़ना और वीडियो देखना जारी रखें। बस मजा करें और अपनी जिज्ञासा का पालन करें।

https://github.com/karpathy/nanoGPT

https://github.com/jadore801120/attention-is-all-you-need-pytorch

---

## न्यूरल नेटवर्क कैसे काम करता है

*2023.05.30*

आइए सीधे न्यूरल वर्क के केंद्र में चर्चा करें। यानी, बैकप्रॉपगेशन एल्गोरिथम:

1. इनपुट x: इनपुट लेयर के लिए संगत सक्रियण \$$a^{1}\$$ सेट करें।
2. फीडफॉरवर्ड: प्रत्येक l=2,3,…,L के लिए \$$z^{l} = w^l a^{l-1}+b^l\$$ और \$$a^{l} = \sigma(z^{l})\$$ की गणना करें।
3. आउटपुट त्रुटि \$$\delta^{L}\$$: वेक्टर \$$\delta^{L} = \nabla_a C \odot \sigma'(z^L)\$$ की गणना करें।
4. त्रुटि को बैकप्रॉपगेट करें: प्रत्येक l=L−1,L−2,…,2 के लिए, \$$\delta^{l} = ((w^{l+1})^T \delta^{l+1}) \odot \sigma'(z^{l})\$$ की गणना करें।
5. आउटपुट: कॉस्ट फंक्शन का ग्रेडिएंट \$$\frac{\partial C}{\partial w^l_{jk}} = a^{l-1}_k \delta^l_j\$$ और \$$\frac{\partial C}{\partial b^l_j} = \delta^l_j\$$ द्वारा दिया गया है।

यह माइकल नेल्सन की पुस्तक *न्यूरल नेटवर्क्स एंड डीप लर्निंग* से लिया गया है। क्या यह बहुत भारी लग रहा है? पहली बार देखने पर ऐसा लग सकता है। लेकिन इसके आसपास एक महीने के अध्ययन के बाद ऐसा नहीं है। मुझे समझाने दें।

### इनपुट

इसमें 5 चरण होते हैं। पहला चरण इनपुट है। यहाँ हम हस्तलिखित अंकों का इनपुट के रूप में उपयोग करते हैं। हमारा कार्य उन्हें पहचानना है। एक हस्तलिखित अंक में 784 पिक्सल होते हैं, जो कि 28*28 है। प्रत्येक पिक्सल में एक ग्रेस्केल मान होता है जो 0 से 255 तक होता है। इसलिए सक्रियण का अर्थ है कि हम इसे सक्रिय करने के लिए कुछ फ़ंक्शन का उपयोग करते हैं, इसके मूल मान को प्रसंस्करण की सुविधा के लिए एक नए मान में बदलते हैं।

कहिए, हमारे पास अब 784 पिक्सल की 1000 तस्वीरें हैं। अब हम इसे यह पहचानने के लिए प्रशिक्षित करते हैं कि वे कौन सा अंक दिखाते हैं। हमारे पास अब उस सीखने के प्रभाव का परीक्षण करने के लिए 100 तस्वीरें हैं। यदि कार्यक्रम 97 तस्वीरों के अंकों को पहचान सकता है, तो हम कहते हैं कि इसकी सटीकता 97% है।

तो हम 1000 तस्वीरों को लूप करेंगे, ताकि भार और पूर्वाग्रहों को प्रशिक्षित कर सकें। हर बार जब हम इसे सीखने के लिए एक नई तस्वीर देते हैं तो हम भार और पूर्वाग्रहों को अधिक सही बनाते हैं।

एक बैच प्रशिक्षण परिणाम 10 न्यूरॉन्स में परिलक्षित होना है। यहाँ, 10 न्यूरॉन्स 0 से 9 तक का प्रतिनिधित्व करते हैं और इसका मान 0 से 1 तक होता है ताकि इसकी सटीकता के बारे में उनके आत्मविश्वास को इंगित किया जा सके।

और इनपुट 784 न्यूरॉन्स है। हम 784 न्यूरॉन्स को 10 न्यूरॉन्स तक कैसे कम कर सकते हैं? यहाँ यही बात है। मान लीजिए हमारे पास दो परतें हैं। परत का क्या अर्थ है? वह पहली परत है, हमारे पास 784 न्यूरॉन्स हैं। दूसरी परत में, हमारे पास 10 न्यूरॉन्स हैं।

हम 784 न्यूरॉन्स में प्रत्येक न्यूरॉन को एक भार देते हैं, जैसे,

$$w_1, w_2, w_3, w_4, ... , w_{784}$$

और पहली परत को एक पूर्वाग्रह देते हैं, यानी, \$$b_1\$$।

और इसलिए दूसरी परत में पहले न्यूरॉन के लिए, इसका मान है:

$$w_1*a_1 + w_2*a_2+...+ w_{784}*a_{784}+b_1$$

लेकिन ये भार और एक पूर्वाग्रह \$$neuron^2_{1}\$$(दूसरी परत में पहला) के लिए हैं। \$$neuron^2_{2}\$$ के लिए, हमें भार और पूर्वाग्रह का एक और सेट चाहिए।

सिग्मॉइड फ़ंक्शन के बारे में क्या? हम उपरोक्त मान को 0 से 1 तक मैप करने के लिए सिग्मॉइड फ़ंक्शन का उपयोग करते हैं।

$$
\begin{eqnarray}
  \sigma(z) \equiv \frac{1}{1+e^{-z}}
\end{eqnarray}
$$

$$
\begin{eqnarray}
  \frac{1}{1+\exp(-\sum_j w_j x_j-b)}
\end{eqnarray}
$$

हम पहली परत को सक्रिय करने के लिए सिग्मॉइड फ़ंक्शन का भी उपयोग करते हैं। यानी, हम उस ग्रेस्केल मान को 0 से 1 तक की सीमा में बदलते हैं। तो अब, प्रत्येक परत में प्रत्येक न्यूरॉन का मान 0 से 1 तक है।

तो अब हमारे दो-परत नेटवर्क के लिए, पहली परत में 784 न्यूरॉन्स हैं, और दूसरी परत में 10 न्यूरॉन्स हैं। हम भार और पूर्वाग्रहों को प्राप्त करने के लिए इसे प्रशिक्षित करते हैं।

हमारे पास 784 * 10 भार और 10 पूर्वाग्रह हैं। दूसरी परत में, प्रत्येक न्यूरॉन के लिए, हम इसके मान की गणना के लिए 784 भार और 1 पूर्वाग्रह का उपयोग करेंगे। यहाँ कोड ऐसा है,

```python
    def __init__(self, sizes):
        self.num_layers = len(sizes)
        self.sizes = sizes
        self.biases = [np.random.randn(y, 1) for y in sizes[1:]]
        self.weights = [np.random.randn(y, x)
                        for x, y in zip(sizes[:-1], sizes[1:])]
```

### फीडफॉरवर्ड

> फीडफॉरवर्ड: प्रत्येक l=2,3,…,L के लिए \$$z^{l} = w^l a^{l-1}+b^l\$$ और \$$a^{l} = \sigma(z^{l})\$$ की गणना करें।

ध्यान दें कि यहाँ, हम पिछली परत के मान, यानी \$$a^{l-1}\$$ और वर्तमान परत के भार, \$$w^l\$$ और उसके पूर्वाग्रह \$$b^l\$$ का उपयोग वर्तमान परत के मान, \$$a^{l}\$$ को प्राप्त करने के लिए सिग्मॉइड करने के लिए करते हैं।

कोड:

```python
        nabla_b = [np.zeros(b.shape) for b in self.biases]
        nabla_w = [np.zeros(w.shape) for w in self.weights]
        # फीडफॉरवर्ड
        activation = x
        activations = [x]
        zs = []
        for b, w in zip(self.biases, self.weights):
            z = np.dot(w, activation)+b
            zs.append(z)
            activation = sigmoid(z)
            activations.append(activation)
```
### आउटपुट त्रुटि

> आउटपुट त्रुटि \$$\delta^{L}\$$: वेक्टर \$$\delta^{L} = \nabla_a C \odot \sigma'(z^L)\$$ की गणना करें।

आइए देखें कि \$$\nabla\$$ का क्या अर्थ है।

> डेल, या नेबला, गणित (विशेषकर वेक्टर कैलकुलस में) में एक ऑपरेटर है जिसका उपयोग वेक्टर डिफरेंशियल ऑपरेटर के रूप में किया जाता है, जिसे आमतौर पर नेबला प्रतीक ∇ द्वारा दर्शाया जाता है।

$$
\begin{eqnarray}
  w_k & \rightarrow & w_k' = w_k-\eta \frac{\partial C}{\partial w_k} \\
  b_l & \rightarrow & b_l' = b_l-\eta \frac{\partial C}{\partial b_l}
\end{eqnarray}
$$

यहाँ \$$\eta\$$ सीखने की दर है। हम व्युत्पन्न का उपयोग करते हैं कि C भार और पूर्वाग्रह के संबंध में है, यानी उनके बीच की दर परिवर्तन। यही नीचे `sigmoid_prime` है।

कोड:

```python
        delta = self.cost_derivative(activations[-1], y) * \
            sigmoid_prime(zs[-1])
        nabla_b[-1] = delta
        nabla_w[-1] = np.dot(delta, activations[-2].transpose())
```

```python
    def cost_derivative(self, output_activations, y):
        return (output_activations-y)
```

### त्रुटि को बैकप्रॉपगेट करें

> त्रुटि को बैकप्रॉपगेट करें: प्रत्येक l=L−1,L−2,…,2 के लिए, \$$\delta^{l} = ((w^{l+1})^T \delta^{l+1}) \odot \sigma'(z^{l})\$$ की गणना करें।

```python
     for l in range(2, self.num_layers):
            z = zs[-l]
            sp = sigmoid_prime(z)
            delta = np.dot(self.weights[-l+1].transpose(), delta) * sp
            nabla_b[-l] = delta
            nabla_w[-l] = np.dot(delta, activations[-l-1].transpose())
        return (nabla_b, nabla_w)
```

### आउटपुट

> आउटपुट: कॉस्ट फंक्शन का ग्रेडिएंट \$$\frac{\partial C}{\partial w^l_{jk}} = a^{l-1}_k \delta^l_j\$$
और \$$\frac{\partial C}{\partial b^l_j} = \delta^l_j\$$ द्वारा दिया गया है।

```python
    def update_mini_batch(self, mini_batch, eta):
        nabla_b = [np.zeros(b.shape) for b in self.biases]
        nabla_w = [np.zeros(w.shape) for w in self.weights]
        for x, y in mini_batch:
            delta_nabla_b, delta_nabla_w = self.backprop(x, y)
            nabla_b = [nb+dnb for nb, dnb in zip(nabla_b, delta_nabla_b)]
            nabla_w = [nw+dnw for nw, dnw in zip(nabla_w, delta_nabla_w)]
        self.weights = [w-(eta/len(mini_batch))*nw
                        for w, nw in zip(self.weights, nabla_w)]
        self.biases = [b-(eta/len(mini_batch))*nb
                       for b, nb in zip(self.biases, nabla_b)]
```

### अंतिम

यह एक छोटा लेख है। और ज्यादातर हिस्से में, यह सिर्फ कोड और गणितीय सूत्र दिखाता है। लेकिन मुझे यह ठीक लगता है। इसे लिखने से पहले, मुझे स्पष्ट रूप से समझ नहीं आया था। कोड और किताब से स्निपेट्स लिखने या सिर्फ कॉपी करने के बाद, मुझे इसका अधिकांश हिस्सा समझ में आ गया। शिक्षक यिन वांग से आत्मविश्वास प्राप्त करने के बाद, *न्यूरल नेटवर्क्स एंड डीप लर्निंग* किताब का लगभग 30% पढ़ने के बाद, एंड्रेज कारपाथी के स्टैनफोर्ड व्याख्यान और एंड्रयू एनजी के पाठ्यक्रमों को सुनने के बाद, अपने दोस्त क्यूई के साथ चर्चा करने के बाद, और एनाकोंडा, नंपी और थीनो लाइब्रेरी के साथ छेड़छाड़ करके वर्षों पहले के कोड को काम करने के बाद, अब मैं इसे समझ गया हूँ।

मुख्य बिंदुओं में से एक आयाम हैं। हमें प्रत्येक प्रतीक और चर के आयामों को जानना चाहिए। और यह सिर्फ भिन्नता योग्य संगणना करता है। आइए यिन वांग के उद्धरणों के साथ समाप्त करें:

> मशीन लर्निंग वास्तव में उपयोगी है, कोई तो इसे सुंदर सिद्धांत भी कह सकता है, क्योंकि यह केवल मेकअप के बाद कैलकुलस है! यह न्यूटन, लाइबनिज़ का पुराना और महान सिद्धांत है, एक सरल, सुरुचिपूर्ण और शक्तिशाली रूप में। मशीन लर्निंग मूल रूप से कुछ फ़ंक्शन प्राप्त करने और उन्हें फिट करने के लिए कैलकुलस का उपयोग करना है, और डीप लर्निंग अधिक जटिल फ़ंक्शन को फिट करना है।

> आर्टिफिशियल इंटेलिजेंस में कोई 'बुद्धिमत्ता' नहीं है, न्यूरल नेटवर्क में कोई 'न्यूरल' नहीं है, मशीन लर्निंग में कोई 'सीखना' नहीं है, और डीप लर्निंग में कोई 'गहराई' नहीं है। डीप लर्निंग में कोई 'गहराई' नहीं है। इस क्षेत्र में जो वास्तव में काम करता है उसे 'कैलकुलस' कहा जाता है। इसलिए मैं इस क्षेत्र को 'डिफरेंशियल कंप्यूटिंग' कहना पसंद करता हूं, और मॉडल बनाने की प्रक्रिया को 'डिफरेंशियल प्रोग्रामिंग' कहा जाता है।