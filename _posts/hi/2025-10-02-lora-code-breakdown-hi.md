---
audio: false
generated: true
image: false
lang: hi
layout: post
model: x-ai/grok-4-fast:free
title: लोरा कोड विश्लेषण और व्याख्या
translated: true
type: note
---

मैं LoRA डेमो कोड को मुख्य भागों में तोड़कर, प्रत्येक भाग को विस्तार से समझाऊंगा। यह LoRA के काम करने के तरीके को चरणबद्ध तरीके से स्पष्ट करने में मदद करेगा। यह कोड पिछले उदाहरण पर आधारित है, जो एक साधारण लीनियर लेयर के लिए LoRA को लागू करने के लिए PyTorch का उपयोग करता है।

### कोड भाग 1: LoRA लेयर की परिभाषा
```python
import torch
import torch.nn as nn

class LoRALayer(nn.Module):
    def __init__(self, in_features, out_features, rank=4):
        super(LoRALayer, self).__init__()
        # मूल फ्रीज किए गए वेट
        self.linear = nn.Linear(in_features, out_features)
        self.linear.weight.requires_grad = False  # मूल वेट को फ्रीज करें
        # LoRA पैरामीटर: लो-रैंक मैट्रिक्स A और B
        self.lora_A = nn.Parameter(torch.randn(in_features, rank))
        self.lora_B = nn.Parameter(torch.randn(rank, out_features))
        self.scaling = 1.0  # LoRA अपडेट के लिए स्केलिंग फैक्टर
```

#### स्पष्टीकरण
यह भाग `LoRALayer` क्लास को परिभाषित करता है, जो LoRA तकनीक को लागू करती है। यहां क्या हो रहा है:

- **इम्पोर्ट और क्लास सेटअप**: हम PyTorch (`torch`) और इसके न्यूरल नेटवर्क मॉड्यूल (`nn`) को इम्पोर्ट करते हैं। `LoRALayer` क्लास `nn.Module` से इनहेरिट करती है, जिससे यह एक PyTorch मॉड्यूल बन जाती है जिसे बड़े मॉडल में इंटीग्रेट किया जा सकता है।
- **मूल लीनियर लेयर**: `self.linear = nn.Linear(in_features, out_features)` एक स्टैंडर्ड लीनियर लेयर (जैसे न्यूरल नेटवर्क में फुली कनेक्टेड लेयर) बनाता है जिसमें `in_features` इनपुट और `out_features` आउटपुट होते हैं। यह प्री-ट्रेंड वेट को दर्शाता है जिसे हम एडाप्ट करना चाहते हैं।
- **वेट फ्रीज करना**: `self.linear.weight.requires_grad = False` लीनियर लेयर के मूल वेट को फ्रीज कर देता है, यह सुनिश्चित करते हुए कि ट्रेनिंग के दौरान उन्हें अपडेट नहीं किया जाता। यह LoRA की दक्षता की कुंजी है, क्योंकि यह बड़े प्री-ट्रेंड मॉडल को मॉडिफाई करने से बचाता है।
- **LoRA पैरामीटर**: `self.lora_A` और `self.lora_B` लो-रैंक मैट्रिक्स हैं। `lora_A` का आकार `(in_features, rank)` है, और `lora_B` का आकार `(rank, out_features)` है। `rank` पैरामीटर (डिफॉल्ट=4) इन मैट्रिक्स के आकार को नियंत्रित करता है, जिससे वे मूल वेट मैट्रिक्स (आकार `in_features x out_features`) की तुलना में काफी छोटे रहते हैं। ये मैट्रिक्स ट्रेनएबल (`nn.Parameter`) होते हैं और रैंडम वैल्यू (`torch.randn`) के साथ इनिशियलाइज़ किए जाते हैं।
- **स्केलिंग फैक्टर**: `self.scaling = 1.0` एक हाइपरपैरामीटर है जो LoRA एडजस्टमेंट को स्केल करने के लिए है, जो एडाप्टेशन की स्ट्रेंथ को फाइन-ट्यून करने की अनुमति देता है।

यह सेटअप यह सुनिश्चित करता है कि ट्रेनिंग के दौरान केवल छोटे `lora_A` और `lora_B` मैट्रिक्स अपडेट होते हैं, जिससे ट्रेनएबल पैरामीटर की संख्या में भारी कमी आती है।

---

### कोड भाग 2: LoRA फॉरवर्ड पास
```python
    def forward(self, x):
        # मूल लीनियर ट्रांसफॉर्मेशन + LoRA एडजस्टमेंट
        original = self.linear(x)
        lora_adjustment = self.scaling * torch.matmul(torch.matmul(x, self.lora_A), self.lora_B)
        return original + lora_adjustment
```

#### स्पष्टीकरण
यह भाग `LoRALayer` के फॉरवर्ड पास को परिभाषित करता है, जो लेयर के आउटपुट की गणना करता है:

- **इनपुट**: इनपुट `x` आकार `(batch_size, in_features)` का एक टेंसर है, जो इनपुट डेटा के एक बैच को दर्शाता है।
- **मूल आउटपुट**: `original = self.linear(x)` फ्रीज लीनियर लेयर के आउटपुट की गणना करता है, जो इनपुट पर प्री-ट्रेंड वेट लागू करता है।
- **LoRA एडजस्टमेंट**: `torch.matmul(torch.matmul(x, self.lora_A), self.lora_B)` टर्म लो-रैंक एडाप्टेशन की गणना करती है। पहले, `x` को `lora_A` (आकार `in_features x rank`) से गुणा किया जाता है, जो आकार `(batch_size, rank)` का एक टेंसर उत्पन्न करता है। फिर, इसे `lora_B` (आकार `rank x out_features`) से गुणा किया जाता है, जो आकार `(batch_size, out_features)` का एक टेंसर देता है - जो मूल आउटपुट के समान आकार है। यह एडजस्टमेंट टास्क-स्पेसिफिक अपडेट को दर्शाता है।
- **स्केलिंग और कॉम्बिनेशन**: एडजस्टमेंट को `self.scaling` द्वारा स्केल किया जाता है और मूल आउटपुट में जोड़ा जाता है, जिससे अंतिम आउटपुट प्राप्त होता है। यह सुनिश्चित करता है कि मॉडल प्री-ट्रेंड नॉलेज को बरकरार रखते हुए टास्क-स्पेसिफिक एडाप्टेशन को शामिल कर ले।

लो-रैंक स्ट्रक्चर (`rank` छोटा है, जैसे 4) यह सुनिश्चित करता है कि एडजस्टमेंट फुल वेट मैट्रिक्स को अपडेट करने की तुलना में कम्प्यूटेशनल रूप से सस्ता और पैरामीटर-एफिशिएंट है।

---

### कोड भाग 3: टॉय डेटासेट और ट्रेनिंग
```python
def create_toy_dataset(n_samples=1000):
    X = torch.randn(n_samples, 64)  # रैंडम इनपुट फीचर्स
    y = torch.randn(n_samples, 10)  # रैंडम टार्गेट आउटपुट
    return X, y

def train_model(model, X, y, epochs=10, lr=0.01):
    criterion = nn.MSELoss()
    optimizer = optim.Adam([param for param in model.parameters() if param.requires_grad], lr=lr)
    
    for epoch in range(epochs):
        model.train()
        optimizer.zero_grad()
        outputs = model(X)
        loss = criterion(outputs, y)
        loss.backward()
        optimizer.step()
        print(f"Epoch {epoch+1}/{epochs}, Loss: {loss.item():.4f}")
```

#### स्पष्टीकरण
यह भाग एक टॉय डेटासेट बनाता है और LoRA-एडाप्टेड मॉडल को ट्रेन करता है:

- **टॉय डेटासेट**: `create_toy_dataset` फंक्शन डेमो के लिए सिंथेटिक डेटा जनरेट करता है। `X` आकार `(1000, 64)` का एक टेंसर है (1000 सैंपल, 64 फीचर्स), और `y` आकार `(1000, 10)` का एक टेंसर है (1000 सैंपल, 10 आउटपुट डायमेंशन)। ये रैंडम टेंसर हैं जो इनपुट-आउटपुट पेयर का अनुकरण करते हैं।
- **ट्रेनिंग फंक्शन**: `train_model` फंक्शन एक साधारण ट्रेनिंग लूप सेट करता है:
  - **लॉस फंक्शन**: `nn.MSELoss()` मीन स्क्वेर्ड एरर को लॉस के रूप में परिभाषित करता है, जो इस रिग्रेशन-जैसे टॉय टास्क के लिए उपयुक्त है।
  - **ऑप्टिमाइज़र**: `optim.Adam` केवल ट्रेनएबल पैरामीटर (`param.requires_grad` `True` है) को ऑप्टिमाइज़ करता है, जो `lora_A` और `lora_B` हैं। फ्रोजन `linear.weight` को बाहर रखा जाता है, जिससे दक्षता सुनिश्चित होती है।
  - **ट्रेनिंग लूप**: प्रत्येक एपोक के लिए, मॉडल आउटपुट की गणना करता है, लॉस की गणना करता है, बैकप्रोपेगेशन (`loss.backward()`) करता है, और LoRA पैरामीटर को अपडेट करता है (`optimizer.step()`)। ट्रेनिंग प्रोग्रेस को मॉनिटर करने के लिए लॉस प्रिंट किया जाता है।

यह सेटअप दर्शाता है कि कैसे LoRA केवल लो-रैंक मैट्रिक्स को फाइन-ट्यून करता है, जिससे प्रक्रिया हल्की-फुल्की रहती है।

---

### कोड भाग 4: मेन एक्जिक्यूशन और पैरामीटर काउंट
```python
def main():
    # रिप्रोड्यूसिबिलिटी के लिए रैंडम सीड सेट करें
    torch.manual_seed(42)
    
    # टॉय डेटासेट बनाएं
    X, y = create_toy_dataset()
    
    # LoRA के साथ मॉडल इनिशियलाइज़ करें
    model = LoRALayer(in_features=64, out_features=10, rank=4)
    
    # ट्रेनएबल पैरामीटर गिनें
    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
    total_params = sum(p.numel() for p in model.parameters())
    print(f"Trainable parameters: {trainable_params}")
    print(f"Total parameters: {total_params}")
    
    # मॉडल को ट्रेन करें
    train_model(model, X, y)

if __name__ == "__main__":
    main()
```

#### स्पष्टीकरण
यह भाग सब कुछ एक साथ जोड़ता है और LoRA की दक्षता को उजागर करता है:

- **रैंडम सीड**: `torch.manual_seed(42)` रैंडम इनिशियलाइज़ेशन की रिप्रोड्यूसिबिलिटी सुनिश्चित करता है।
- **डेटासेट और मॉडल**: टॉय डेटासेट बनाया जाता है, और एक `LoRALayer` को `in_features=64`, `out_features=10`, और `rank=4` के साथ इनिशियलाइज़ किया जाता है।
- **पैरामीटर काउंट**: कोड गणना करता है:
  - **ट्रेनएबल पैरामीटर**: केवल `lora_A` (64 × 4 = 256) और `lora_B` (4 × 10 = 40), कुल 296 पैरामीटर।
  - **कुल पैरामीटर**: इसमें फ्रोजन `linear.weight` (64 × 10 = 640) और `linear.bias` (10), प्लस LoRA पैरामीटर शामिल हैं, कुल 946।
  - यह LoRA की दक्षता दिखाता है: केवल ~31% (296/946) पैरामीटर ट्रेनएबल हैं, जबकि फुल फाइन-ट्यूनिंग में 100% होते हैं।
- **ट्रेनिंग**: टॉय डेटासेट पर LoRA पैरामीटर को फाइन-ट्यून करने के लिए `train_model` फंक्शन को कॉल किया जाता है।

जब आप इसे रन करेंगे, तो आप देखेंगे कि ट्रेनएबल पैरामीटर की संख्या कुल पैरामीटर से बहुत छोटी है, और लो-रैंक अपडेट के कारण मॉडल तेजी से ट्रेन होता है।

---

### कोड को रन करना
इसे रन करने के लिए, आपको PyTorch इंस्टॉल करना होगा (`pip install torch`)। पूरे कोड को एक `.py` फाइल के रूप में सेव करें और उसे एक्जिक्यूट करें। आउटपुट पैरामीटर की संख्या और 10 एपोक के दौरान ट्रेनिंग लॉस दिखाएगा, जो फाइन-ट्यूनिंग में LoRA की दक्षता को प्रदर्शित करेगा।

[LoRA: Low-Rank Adaptation of Large Language Models](https://arxiv.org/abs/2106.09685)

अगर आप किसी भाग में और गहराई में जाना चाहते हैं या उदाहरण को संशोधित करना चाहते हैं तो मुझे बताएं!