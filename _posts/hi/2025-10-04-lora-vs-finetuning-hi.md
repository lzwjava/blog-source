---
audio: false
generated: true
image: false
lang: hi
layout: post
model: x-ai/grok-4-fast
title: लोरा बनाम पूर्ण फाइन-ट्यूनिंग प्रदर्शन
translated: true
type: note
---

### LoRA बनाम पूर्ण फाइन-ट्यूनिंग प्रदर्शन का अवलोकन

थिंकिंग मशीन्स लैब के अनुभवजन्य विश्लेषण के आधार पर, LoRA (लो-रैंक एडाप्टेशन) पैरामीटर-कुशल फाइन-ट्यूनिंग प्रदान करता है जो अक्सर छोटे डेटासेट या रीइन्फोर्समेंट लर्निंग (आरएल) जैसी क्षमता-सीमित परिदृश्यों में पूर्ण फाइन-ट्यूनिंग (FullFT) के बराबर प्रदर्शन करता है, लेकिन अंतर्निहित प्रशिक्षण गतिशीलता सीमाओं के कारण बड़े डेटासेट पर इसका प्रदर्शन घट जाता है।[1] यह विस्तार मॉडल डेवलपर्स के लिए तंत्र, साक्ष्य और व्यावहारिक निहितार्थों को समझाते हुए प्रत्येक निष्कर्ष में गहराई से उतरता है।

### छोटे-से-मध्यम इंस्ट्रक्शन-ट्यूनिंग और रीजनिंग डेटासेट पर समानता

LoRA, मध्यम आकार तक के डेटासेट (जैसे कि इंस्ट्रक्शन-फॉलोइंग के लिए उपयोग किए जाने वाले अल्पाका-स्टाइल डेटासेट) या रीजनिंग टास्क (जैसे कि GSM8K मैथ प्रॉब्लम्स) पर फाइन-ट्यूनिंग करते समय FullFT के साथ प्रदर्शन समानता हासिल करता है। यह समानता इसलिए उत्पन्न होती है क्योंकि इन डेटासेट में आमतौर पर 10,000–100,000 उदाहरण होते हैं, जो LoRA की लो-रैंक पैरामीटराइजेशन क्षमता के साथ अच्छी तरह मेल खाते हैं। LoRA वेट अपडेट को एक लो-रैंक मैट्रिक्स डिकम्पोजिशन (ΔW = B A, जहाँ B और A लो-रैंक मैट्रिक्स हैं) के रूप में अनुमानित करता है, जो सभी पैरामीटर्स को अपडेट करने की पूर्ण एक्सप्रेसिविटी की आवश्यकता के बिना ऐसे टास्क के लिए आवश्यक संकीर्ण व्यवहारिक बदलावों को कैप्चर करने के लिए पर्याप्त होता है।

व्यवहार में, इसका मतलब है कि डेवलपर्स बड़े मॉडल्स (जैसे, 70B+ पैरामीटर्स) को कंज्यूमर हार्डवेयर या सीमित मेमोरी वाले क्लाउड इंस्टेंस पर फाइन-ट्यून करने के लिए LoRA का उपयोग कर सकते हैं, और FullFT जैसे ही डाउनस्ट्रीम मेट्रिक्स जैसे accuracy या perplexity प्राप्त कर सकते हैं। उदाहरण के लिए, इंस्ट्रक्शन के लिए डॉली-15k जैसे डेटासेट पर, रैंक 8–16 के साथ LoRA अविभाज्य परिणाम देता है, जिससे trainable parameters और training time में 99% तक की बचत होती है।[1] हालाँकि, यह तभी लागू होता है जब डेटासेट प्रशिक्षण वितरण से परे व्यापक सामान्यीकरण की मांग नहीं करता—ओवरफिटिंग के जोखिम FullFT के समान ही रहते हैं।

### LoRA क्षमता से अधिक बड़े डेटासेट पर कम प्रदर्शन

जब डेटासेट LoRA की प्रभावी क्षमता (जैसे, कोड जनरेशन के लिए द स्टैक पर डोमेन-विशिष्ट अनुकूलन के लिए लाखों उदाहरण) से आगे बढ़ जाते हैं, तो LoRA FullFT से पिछड़ जाता है। मुख्य मुद्दा एक कठोर "क्षमता सीमा" नहीं है जहाँ loss अचानक स्थिर हो जाती है; इसके बजाय, LoRA कम प्रशिक्षण दक्षता प्रदर्शित करता है, जिसमें loss convergence धीमी हो जाती है, जो लो-रैंक बॉटलनेक और डेटासेट स्केल के बीच बेमेलपन से जुड़ी होती है।

यह LoRA के इंडक्टिव बायस से उपजता है: मैट्रिक्स के उत्पाद का रूप (W' = W + γ B A) अपडेट्स को एक उप-स्थान तक सीमित करता है, जो विरल, निम्न-आयामी बदलावों के लिए तो काम करता है लेकिन बड़े डेटासेट में उच्च-विचरण सिग्नल के साथ संघर्ष करता है। अनुभवजन्य रूप से, loss कर्व्स दिखाते हैं कि LoRA को FullFT के स्तर के निकट पहुँचने के लिए 2–5x अधिक चरणों की आवश्यकता होती है, और तब भी, कोडिंग के लिए HumanEval जैसे बेंचमार्क पर अंतिम प्रदर्शन 5–10% worse हो सकता है।[1] संबंध पैरामेट्रिक है: दक्षता तब घटती है जब डेटासेट का आकार LoRA रैंक (r) की तुलना में तेजी से बढ़ता है, यह सुझाव देता है कि r बढ़ाने से सीमांत रूप से मदद मिलती है लेकिन कम-डेटा शासन में ओवरफिटिंग का जोखिम उठाए बिना पूरी तरह से क्षतिपूर्ति नहीं होती।

निहितार्थों में बड़े पैमाने के कॉर्पोरा के लिए FullFT (या QLoRA जैसे हाइब्रिड) को प्राथमिकता देना शामिल है, जबकि LoRA पुनरावृत्त प्रोटोटाइपिंग में बेहतर होता है। यह तरीके चुनने से पहले डेटासेट आकार के अनुमान की आवश्यकता को भी रेखांकित करता है—टोकन काउंट जैसे टूल इसमें मार्गदर्शन कर सकते हैं।

### बड़े बैच साइज और पैरामीटराइजेशन प्रभावों के प्रति संवेदनशीलता

LoRA, FullFT की तुलना में बड़े बैच साइज के प्रति अधिक असहिष्णुता दिखाता है, जिसमें इष्टतम बिंदुओं (जैसे, बैच साइज > 512) से परे तेजी से loss penalties उत्पन्न होती हैं। जबकि FullFT का gradient noise अधिक सहजता से स्केल करता है, LoRA का product-of-matrices सेटअप लो-रैंक अपडेट्स में विचरण को बढ़ा देता है, जिससे अस्थिर अनुकूलन होता है। यह दंड तब भी बना रहता है जब रैंक बढ़ाई जाती है, क्योंकि यह प्रत्यक्ष वेट ऑप्टिमाइजेशन बनाम द्विरेखीय रूप (bilinear form) के अलग हेसियन गुणों में निहित है।

उदाहरण के लिए, रीजनिंग डेटासेट पर प्रयोगों में, 1k से अधिक बैच साइज के साथ LoRA loss 20–30% तेजी से बढ़ती है, जबकि FullFT व्यापक पैरामीटर एवरेजिंग के माध्यम से स्थिर हो जाता है।[1] शमन रणनीतियों में छोटे प्रभावी बैचों का अनुकरण करने के लिए gradient accumulation या सावधानीपूर्वक learning rate scheduling के साथ AdamW जैसी तकनीकों का उपयोग शामिल है। यह गतिशीलता LoRA के ट्रेड-ऑफ को उजागर करती है: मेमोरी में दक्षता लेकिन कंप्यूट समानांतरता (compute parallelism) को स्केल करने में नाजुकता, जिससे यह उच्च-थ्रूपुट प्रशिक्षण क्लस्टर के लिए कम आदर्श बन जाता है।

### सभी लेयर्स, विशेष रूप से MLP और MoE पर LoRA लागू करने के लाभ

छोटे डेटासेट पर भी, LoRA को सार्वभौमिक रूप से (attention, MLP, और Mixture-of-Experts लेयर्स पर) लागू करना, केवल attention पर लागू करने वाले वेरिएंट से बेहतर प्रदर्शन करता है, खासकर जब पैरामीटर काउंट को उच्च रैंक के माध्यम से मेल खाया जाता है। शुरुआती कार्यान्वयन में आम, केवल attention वाला LoRA, मल्टी-हॉप रीजनिंग जैसे टास्क पर 3–7% तक कम प्रदर्शन करता है क्योंकि यह फीड-फॉरवर्ड लेयर्स (MLP/MoE) की उपेक्षा करता है, जो अधिकांश गैर-रैखिक परिवर्तनों और डोमेन-विशिष्ट ज्ञान एकीकरण को संभालते हैं।

फुल-लेयर LoRA मॉडल की आर्किटेक्चर का अधिक समग्र रूप से लाभ उठाता है: MLP लगभग ~70% पैरामीटर्स में योगदान करते हैं और टास्क-विशिष्ट गणनाओं को कैप्चर करते हैं, जबकि MoE (Mixtral जैसे मॉडल में) रूट-विशिष्ट अनुकूलन से लाभान्वित होते हैं। केवल attention रैंक को बढ़ाकर पैरामीटर्स का मिलान करना, attention हेड्स में redundancy के कारण विफल हो जाता है, जिससे अक्षम उप-स्थान बनते हैं। सर्वोत्तम अभ्यास: छोटे डेटा के लिए सभी लेयर्स में रैंक 16–64 का उपयोग करें, जिससे अतिरिक्त कंप्यूट के बिना ही दक्षता और मूल्यांकन (evals) में लाभ मिलता है।[1] यह खोज PEFT जैसी लाइब्रेरी में व्यापक अपनाव को प्रोत्साहित करती है, जिससे विशेष आर्किटेक्चर में "LoRA tax" कम होती है।

### कम रैंक के साथ रीइन्फोर्समेंट लर्निंग में समानता

LoRA, RL फाइन-ट्यूनिंग (जैसे, RLHF या प्रेफरेंस डेटासेट पर DPO) में FullFT के बराबर प्रदर्शन करता है, यहाँ तक कि बहुत कम रैंक (r=4–8) पर भी, RL की अंतर्निहित कम-क्षमता आवश्यकताओं के कारण। सूचना-सैद्धांतिक रूप से, RL अपडेट्स विरल trajectories पर reward modeling और policy gradients पर केंद्रित होते हैं, जो शायद ही कभी लो-रैंक डेल्टा की प्रतिनिधित्व शक्ति से अधिक होते हैं—SL के सघन टोकन पूर्वानुमानों के विपरीत।

सिद्धांत से प्रत्याशित, यह इसलिए मान्य है क्योंकि RL losses (जैसे, PPO clips) व्यवहार में दिशात्मक बदलाव पर जोर देते हैं, जिन्हें LoRA का उप-स्थान पूर्ण पैरामीटराइजेशन के बिना ही कुशलतापूर्वक एनकोड करता है। प्रयोग Anthropic के HH-RLHF जैसे बेंचमार्क पर समानता की पुष्टि करते हैं, जहाँ LoRA तेजी से converge होता है और 10x कम मेमोरी का उपयोग करता है।[1] व्यवहारकर्ताओं के लिए, यह LoRA को RL पाइपलाइन के लिए एक डिफ़ॉल्ट विकल्प बना देता है, जो मामूली हार्डवेयर पर ट्रिलियन-पैरामीटर मॉडल के स्केलेबल अलाइनमेंट को सक्षम बनाता है।

### व्यापक निहितार्थ और सिफारिशें

ये अंतर्दृष्टि LoRA को अधिकांश पोस्ट-ट्रेनिंग आवश्यकताओं, विशेष रूप से संसाधन-सीमित वातावरण में, एक व्यावहारिक उपकरण के रूप में प्रकट करती हैं, लेकिन विस्तृत डेटा पर FullFT का सर्वत्र प्रतिस्थापन नहीं। भविष्य का कार्य अंतराल को पाटने के लिए अनुकूली रैंक या लेयर-विशिष्ट स्केलिंग की खोज कर सकता है। डेवलपर्स को अपने डेटासेट स्केल पर बेंचमार्क करना चाहिए और बैच प्रभावों पर शीघ्र निगरानी रखनी चाहिए।[1] कुल मिलाकर, LoRA लक्षित परिदृश्यों में बिना किसी पछतावे के फाइन-ट्यूनिंग को लोकतांत्रिक बनाता है, जो कुशल AI विकास की ओर बदलाव के साथ संरेखित होता है।

उद्धरण:
[1] https://tinker-docs.thinkingmachines.ai/lora-primer