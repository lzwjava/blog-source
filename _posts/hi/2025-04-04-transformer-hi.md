---
audio: false
generated: true
lang: hi
layout: post
title: ट्रांसफॉर्मर
translated: true
type: note
---

**ट्रांसफॉर्मर** एक क्रांतिकारी न्यूरल नेटवर्क आर्किटेक्चर है जो अधिकांश आधुनिक लार्ज लैंग्वेज मॉडल्स (LLMs) की बुनियाद बन गया है। इसकी शुरुआत वासवानी एट अल. के 2017 के महत्वपूर्ण शोध पत्र "अटेंशन इज़ ऑल यू नीड" में हुई थी और तब से इसने नेचुरल लैंग्वेज प्रोसेसिंग (NLP) के क्षेत्र को मौलिक रूप से बदल दिया है।

पहले के प्रमुख आर्किटेक्चर जैसे रिकरंट न्यूरल नेटवर्क्स (RNNs), जो अनुक्रमिक डेटा को चरण-दर-चरण प्रोसेस करते हैं, के विपरीत, ट्रांसफॉर्मर पूरे इनपुट अनुक्रम को **समानांतर रूप से (पैरेलल)** प्रोसेस करता है। यह मुख्य अंतर ट्रेनिंग की गति में महत्वपूर्ण सुधार और टेक्स्ट के भीतर लंबी दूरी के संबंधों (डिपेंडेंसीज़) को पकड़ने की क्षमता प्रदान करता है।

यहाँ LLMs के संदर्भ में ट्रांसफॉर्मर आर्किटेक्चर के मुख्य घटकों और अवधारणाओं पर एक गहन नज़र है:

**1. मूल विचार: अटेंशन मैकेनिज्म**

ट्रांसफॉर्मर की केंद्रीय नवीनता **अटेंशन मैकेनिज्म** है, विशेष रूप से **सेल्फ-अटेंशन**। यह मैकेनिज्म मॉडल को किसी विशिष्ट शब्द को प्रोसेस करते समय इनपुट अनुक्रम के विभिन्न शब्दों (या टोकन्स) के महत्व को तौलने की अनुमति देता है। तुरंत पहले आने वाले शब्दों (जैसे RNNs) पर निर्भर रहने के बजाय, सेल्फ-अटेंशन मॉडल को शब्दों के अर्थ और उनके बीच संबंधों को समझने के लिए संपूर्ण संदर्भ पर विचार करने में सक्षम बनाता है।

इसे इस तरह समझें: जब आप एक वाक्य पढ़ते हैं, तो आप प्रत्येक शब्द को अलग-थलग प्रोसेस नहीं करते। आपका दिमाग समग्र अर्थ और यह समझने के लिए कि प्रत्येक शब्द उसमें कैसे योगदान देता है, सभी शब्दों पर एक साथ विचार करता है। सेल्फ-अटेंशन मैकेनिज्म इसी व्यवहार की नकल करता है।

**सेल्फ-अटेंशन कैसे काम करता है (सरलीकृत):**

इनपुट अनुक्रम के प्रत्येक शब्द के लिए, ट्रांसफॉर्मर तीन वेक्टर्स की गणना करता है:

*   **क्वेरी (Q):** यह दर्शाता है कि वर्तमान शब्द अन्य शब्दों में "क्या ढूंढ रहा है"।
*   **की (K):** यह दर्शाता है कि प्रत्येक अन्य शब्द में "क्या जानकारी निहित है"।
*   **वैल्यू (V):** यह वास्तविक जानकारी को दर्शाता है जो प्रत्येक अन्य शब्द में मौजूद है और प्रासंगिक हो सकती है।

सेल्फ-अटेंशन मैकेनिज्म फिर निम्नलिखित चरणों को अंजाम देता है:

1.  **अटेंशन स्कोर की गणना करना:** एक शब्द के क्वेरी वेक्टर और अनुक्रम के हर दूसरे शब्द के की वेक्टर के बीच डॉट प्रोडक्ट की गणना की जाती है। ये स्कोर दर्शाते हैं कि प्रत्येक अन्य शब्द की जानकारी वर्तमान शब्द के लिए कितनी प्रासंगिक है।
2.  **स्कोर को स्केल करना:** स्कोर को की वेक्टर्स के आयाम (`sqrt(d_k)`) के वर्गमूल से विभाजित किया जाता है। यह स्केलिंग ट्रेनिंग के दौरान ग्रेडिएंट्स को स्थिर करने में मदद करती है।
3.  **सॉफ्टमैक्स लागू करना:** स्केल किए गए स्कोर को सॉफ्टमैक्स फंक्शन से गुजारा जाता है, जो उन्हें 0 और 1 के बीच की संभावनाओं में बदल देता है। ये संभावनाएं **अटेंशन वेट** को दर्शाती हैं - कि वर्तमान शब्द को अन्य शब्दों पर कितना "ध्यान" देना चाहिए।
4.  **वेटेड वैल्यू की गणना करना:** प्रत्येक शब्द के वैल्यू वेक्टर को उसके संबंधित अटेंशन वेट से गुणा किया जाता है।
5.  **वेटेड वैल्यू को जोड़ना:** वेटेड वैल्यू वेक्टर्स को जोड़कर वर्तमान शब्द के लिए **आउटपुट वेक्टर** तैयार किया जाता है। यह आउटपुट वेक्टर अब इनपुट अनुक्रम के सभी अन्य प्रासंगिक शब्दों की जानकारी रखता है, जो उनके महत्व के अनुसार वेटेड है।

**2. मल्टी-हेड अटेंशन**

मॉडल की विभिन्न प्रकार के संबंधों को पकड़ने की क्षमता को और बढ़ाने के लिए, ट्रांसफॉर्मर **मल्टी-हेड अटेंशन** का उपयोग करता है। यह सेल्फ-अटेंशन मैकेनिज्म को केवल एक बार करने के बजाय, क्वेरी, की और वैल्यू वेट मैट्रिक्स के अलग-अलग सेट के साथ कई बार समानांतर रूप से करता है। प्रत्येक "हेड" शब्दों के बीच संबंधों के विभिन्न पहलुओं (जैसे, व्याकरणिक निर्भरता, अर्थ संबंधी कनेक्शन) पर ध्यान केंद्रित करना सीखता है। सभी अटेंशन हेड्स के आउटपुट को फिर एक साथ जोड़ा (कॉन्केटनेट) जाता है और मल्टी-हेड अटेंशन लेयर के अंतिम आउटपुट का उत्पादन करने के लिए रैखिक रूप से रूपांतरित (लीनियर ट्रांसफॉर्म) किया जाता है।

**3. पोजिशनल एन्कोडिंग**

चूंकि ट्रांसफॉर्मर सभी शब्दों को समानांतर में प्रोसेस करता है, इसलिए यह अनुक्रम में शब्दों के **क्रम** की जानकारी खो देता है। इसे हल करने के लिए, इनपुट एम्बेडिंग्स में एक **पोजिशनल एन्कोडिंग** जोड़ा जाता है। ये एन्कोडिंग वेक्टर्स होते हैं जो अनुक्रम में प्रत्येक शब्द की स्थिति को दर्शाते हैं। ये आमतौर पर फिक्स्ड पैटर्न (जैसे, साइनसॉइडल फंक्शन) या सीखी गई एम्बेडिंग्स होती हैं। पोजिशनल एन्कोडिंग जोड़ने से, ट्रांसफॉर्मर भाषा की अनुक्रमिक प्रकृति को समझ सकता है।

**4. एनकोडर और डिकोडर स्टैक्स**

ट्रांसफॉर्मर आर्किटेक्चर में आमतौर पर दो मुख्य भाग होते हैं: एक **एनकोडर** और एक **डिकोडर**, दोनों एक दूसरे के ऊपर स्टैक की गई कई समान परतों से बने होते हैं।

*   **एनकोडर:** एनकोडर की भूमिका इनपुट अनुक्रम को प्रोसेस करना और उसका एक समृद्ध प्रतिनिधित्व (रिप्रेजेंटेशन) बनाना है। प्रत्येक एनकोडर लेयर में आमतौर पर शामिल होते हैं:
    *   एक **मल्टी-हेड सेल्फ-अटेंशन** सब-लेयर।
    *   एक **फीड-फॉरवर्ड न्यूरल नेटवर्क** सब-लेयर।
    *   प्रत्येक सब-लेयर के चारों ओर **रेजिडुअल कनेक्शन**, जिसके बाद **लेयर नॉर्मलाइजेशन** होता है। रेजिडुअल कनेक्शन ट्रेनिंग के दौरान ग्रेडिएंट फ्लो में मदद करते हैं, और लेयर नॉर्मलाइजेशन एक्टिवेशन्स को स्थिर करता है।

*   **डिकोडर:** डिकोडर की भूमिका आउटपुट अनुक्रम (जैसे, मशीन अनुवाद या टेक्स्ट जनरेशन में) उत्पन्न करना है। प्रत्येक डिकोडर लेयर में आमतौर पर शामिल होते हैं:
    *   एक **मास्क्ड मल्टी-हेड सेल्फ-अटेंशन** सब-लेयर। "मास्किंग" डिकोडर को ट्रेनिंग के दौरान टार्गेट अनुक्रम में भविष्य के टोकन्स को आगे देखने से रोकती है, यह सुनिश्चित करते हुए कि यह अगले टोकन की भविष्यवाणी करने के लिए केवल पहले से जेनरेट किए गए टोकन्स का उपयोग करता है।
    *   एक **मल्टी-हेड अटेंशन** सब-लेयर जो एनकोडर के आउटपुट पर अटेंड करता है। यह डिकोडर को आउटपुट जेनरेट करते समय इनपुट अनुक्रम के प्रासंगिक हिस्सों पर ध्यान केंद्रित करने की अनुमति देता है।
    *   एक **फीड-फॉरवर्ड न्यूरल नेटवर्क** सब-लेयर।
    *   एनकोडर के समान **रेजिडुअल कनेक्शन** और **लेयर नॉर्मलाइजेशन**।

**5. फीड-फॉरवर्ड नेटवर्क्स**

प्रत्येक एनकोडर और डिकोडर लेयर में एक फीड-फॉरवर्ड न्यूरल नेटवर्क (FFN) होता है। यह नेटवर्क प्रत्येक टोकन पर स्वतंत्र रूप से लागू होता है और अटेंशन मैकेनिज्म द्वारा सीखे गए रिप्रेजेंटेशन को आगे प्रोसेस करने में मदद करता है। इसमें आमतौर पर एक गैर-रैखिक एक्टिवेशन फंक्शन (जैसे, ReLU) के बीच में दो रैखिक परिवर्तन (लीनियर ट्रांसफॉर्मेशन) शामिल होते हैं।

**LLMs में ट्रांसफॉर्मर का उपयोग कैसे किया जाता है:**

LLMs मुख्य रूप से **डिकोडर-ओनली** ट्रांसफॉर्मर आर्किटेक्चर (जैसे GPT मॉडल) या **एनकोडर-डिकोडर** आर्किटेक्चर (जैसे T5) पर आधारित होते हैं।

*   **डिकोडर-ओनली मॉडल:** ये मॉडल पिछले टोकन्स को देखते हुए अनुक्रम में अगले टोकन की भविष्यवाणी करने के लिए प्रशिक्षित होते हैं। वे कई डिकोडर लेयर्स को स्टैक करते हैं। इनपुट प्रॉम्प्ट को परतों के माध्यम से पारित किया जाता है, और अंतिम परत अगले टोकन के लिए शब्दावली (वोकैब्युलरी) पर संभावना वितरण (प्रोबेबिलिटी डिस्ट्रीब्यूशन) की भविष्यवाणी करती है। इस वितरण से ऑटोरिग्रेसिवली सैंपलिंग करके, मॉडल सुसंगत और प्रासंगिक रूप से प्रासंगिक पाठ उत्पन्न कर सकता है।

*   **एनकोडर-डिकोडर मॉडल:** ये मॉडल एक इनपुट अनुक्रम लेते हैं और एक आउटपुट अनुक्रम उत्पन्न करते हैं। वे पूर्ण एनकोडर-डिकोडर आर्किटेक्चर का उपयोग करते हैं। एनकोडर इनपुट को प्रोसेस करता है, और डिकोडर टार्गेट अनुक्रम उत्पन्न करने के लिए एनकोडर के आउटपुट का उपयोग करता है। ये अनुवाद, सारांशीकरण और प्रश्नोत्तर जैसे कार्यों के लिए उपयुक्त हैं।

**महत्व को गहराई से समझना:**

ट्रांसफॉर्मर आर्किटेक्चर का LLMs पर गहरा प्रभाव है:

*   **लंबी दूरी की निर्भरताओं को संभालना:** सेल्फ-अटेंशन मैकेनिज्म मॉडल को उन शब्दों को सीधे जोड़ने की अनुमति देता है जो अनुक्रम में एक दूसरे से बहुत दूर हैं, जिससे लंबी दूरी के संदर्भ को पकड़ने में RNNs की सीमाओं को दूर किया जा सकता है।
*   **समानांतर प्रसंस्करण:** पूरे अनुक्रम को समानांतर रूप से प्रोसेस करने से ट्रेनिंग का समय काफी कम हो जाता है और बहुत बड़े डेटासेट और मॉडल के उपयोग की अनुमति मिलती है।
*   **स्केलेबिलिटी:** ट्रांसफॉर्मर आर्किटेक्चर बढ़ते डेटा और मॉडल आकार के साथ अच्छी तरह से स्केल करता है, जिससे अरबों या खरबों पैरामीटर्स वाले अत्यंत शक्तिशाली LLMs का विकास हुआ है।
*   **संदर्भगत समझ:** अटेंशन मैकेनिज्म मॉडल को शब्दों के संदर्भ को अधिक सूक्ष्म तरीके से समझने में सक्षम बनाता है, जिससे बेहतर भाषा की समझ और जनरेशन होती है।
*   **नवाचार की नींव:** ट्रांसफॉर्मर आर्किटेक्चर कई उन्नति और विविधताओं के लिए एक आधार के रूप में कार्य कर चुका है, जिससे आधुनिक LLMs के विविध परिदृश्य का निर्माण हुआ है।

संक्षेप में, ट्रांसफॉर्मर आर्किटेक्चर, अपने मुख्य सेल्फ-अटेंशन मैकेनिज्म, समानांतर प्रोसेसिंग क्षमताओं और एनकोडर-डिकोडर संरचना (या डिकोडर-ओनली विविधताओं) के साथ, NLP के क्षेत्र में क्रांति ला चुका है और आधुनिक लार्ज लैंग्वेज मॉडल्स की उल्लेखनीय क्षमताओं के पीछे प्रेरक शक्ति है।