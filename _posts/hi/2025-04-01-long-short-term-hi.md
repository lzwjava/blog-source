---
audio: false
generated: true
lang: hi
layout: post
title: लाँग शॉर्ट टर्म मेमोरी नेटवर्क्स
translated: true
type: note
---

लॉन्ग शॉर्ट-टर्म मेमोरी (एलएसटीएम) नेटवर्क एक प्रकार का रिकरंट न्यूरल नेटवर्क आर्किटेक्चर है जो विशेष रूप से वैनिशिंग ग्रेडिएंट प्रॉब्लम को हल करने और सीक्वेंशियल डेटा में लॉन्ग-टर्म डिपेंडेंसीज सीखने में सक्षम बनाने के लिए डिज़ाइन किया गया है। यह एक अधिक परिष्कृत आंतरिक संरचना के माध्यम से इसे प्राप्त करते हैं, जिसे **एलएसटीएम सेल** कहा जाता है।

यहां बताया गया है कि एक एलएसटीएम सेल कैसे काम करता है:

**मुख्य विचार: सेल स्टेट**

एलएसटीएम में केंद्रीय अवधारणा **सेल स्टेट** (जिसे अक्सर 'C<sub>t</sub>' से दर्शाया जाता है) है। सेल स्टेट को पूरे अनुक्रम में चलने वाली एक कन्वेयर बेल्ट के रूप में समझें। यह अनुक्रम के दीर्घकालिक इतिहास से संबंधित जानकारी को ले जाता है। **गेट्स** नामक संरचनाओं के माध्यम से नेटवर्क में प्रवाहित होते समय सेल स्टेट में जानकारी जोड़ी या हटाई जा सकती है।

**गेट्स**

एलएसटीएम सेल में तीन मुख्य गेट होते हैं जो सूचना के प्रवाह को नियंत्रित करते हैं:

1.  **फॉरगेट गेट:** यह गेट तय करता है कि पिछले सेल स्टेट की किस जानकारी को छोड़ देना चाहिए।
    * यह पिछली हिडन स्टेट (h<sub>t-1</sub>) और वर्तमान इनपुट (x<sub>t</sub>) प्राप्त करता है।
    * इन्हें एक न्यूरल नेटवर्क लेयर के बाद **सिग्मॉइड एक्टिवेशन फंक्शन** से गुजारा जाता है।
    * सिग्मॉइड फंक्शन 0 और 1 के बीच के मान आउटपुट करता है। 0 के करीब एक मान का मतलब है "इस जानकारी को पूरी तरह से भूल जाओ," जबकि 1 के करीब एक मान का मतलब है "इस जानकारी को पूरी तरह से रखो।"
    * गणितीय रूप से, फॉरगेट गेट का आउटपुट (f<sub>t</sub>) इस प्रकार गणना की जाती है:
        ```
        f_t = σ(W_f * [h_{t-1}, x_t] + b_f)
        ```
        जहाँ:
        * σ सिग्मॉइड फंक्शन है।
        * W<sub>f</sub> फॉरगेट गेट के लिए वेट मैट्रिक्स है।
        * [h<sub>t-1</sub>, x_t] पिछली हिडन स्टेट और वर्तमान इनपुट का संयोजन है।
        * b<sub>f</sub> फॉरगेट गेट के लिए बायस वेक्टर है।

2.  **इनपुट गेट:** यह गेट तय करता है कि वर्तमान इनपुट से कौन सी नई जानकारी सेल स्टेट में जोड़ी जानी चाहिए। इस प्रक्रिया में दो चरण शामिल हैं:
    * **इनपुट गेट लेयर:** एक सिग्मॉइड लेयर तय करती है कि हम किन मानों को अपडेट करेंगे।
        ```
        i_t = σ(W_i * [h_{t-1}, x_t] + b_i)
        ```
        जहाँ:
        * σ सिग्मॉइड फंक्शन है।
        * W<sub>i</sub> इनपुट गेट के लिए वेट मैट्रिक्स है।
        * [h<sub>t-1</sub>, x_t] पिछली हिडन स्टेट और वर्तमान इनपुट का संयोजन है।
        * b<sub>i</sub> इनपुट गेट के लिए बायस वेक्टर है।
    * **कैंडिडेट वैल्यूज लेयर:** एक tanh लेयर नए कैंडिडेट मानों (कैंडिडेट सेल स्टेट, जिसे 'C̃<sub>t</sub>' से दर्शाया जाता है) का एक वेक्टर बनाती है जिसे सेल स्टेट में जोड़ा जा सकता है। tanh फंक्शन -1 और 1 के बीच के मान आउटपुट करता है, जो नेटवर्क को विनियमित करने में मदद करता है।
        ```
        C̃_t = tanh(W_C * [h_{t-1}, x_t] + b_C)
        ```
        जहाँ:
        * tanh हाइपरबोलिक टेंजेंट फंक्शन है।
        * W<sub>C</sub> कैंडिडेट सेल स्टेट के लिए वेट मैट्रिक्स है।
        * [h<sub>t-1</sub>, x_t] पिछली हिडन स्टेट और वर्तमान इनपुट का संयोजन है।
        * b<sub>C</sub> कैंडिडेट सेल स्टेट के लिए बायस वेक्टर है।

3.  **आउटपुट गेट:** यह गेट तय करता है कि वर्तमान सेल स्टेट की कौन सी जानकारी वर्तमान टाइम स्टेप के लिए हिडन स्टेट के रूप में आउटपुट की जानी चाहिए।
    * यह पिछली हिडन स्टेट (h<sub>t-1</sub>) और वर्तमान इनपुट (x<sub>t</sub>) प्राप्त करता है।
    * इन्हें यह तय करने के लिए एक न्यूरल नेटवर्क लेयर के बाद **सिग्मॉइड एक्टिवेशन फंक्शन** से गुजारा जाता है कि सेल स्टेट के किन हिस्सों को आउटपुट करना है।
        ```
        o_t = σ(W_o * [h_{t-1}, x_t] + b_o)
        ```
        जहाँ:
        * σ सिग्मॉइड फंक्शन है।
        * W<sub>o</sub> आउटपुट गेट के लिए वेट मैट्रिक्स है।
        * [h<sub>t-1</sub>, x_t] पिछली हिडन स्टेट और वर्तमान इनपुट का संयोजन है।
        * b<sub>o</sub> आउटपुट गेट के लिए बायस वेक्टर है।
    * इसके बाद सेल स्टेट को एक **tanh फंक्शन** से गुजारा जाता है ताकि मानों को -1 और 1 के बीच सिकोड़ा जा सके।
    * अंत में, सिग्मॉइड गेट के आउटपुट को सेल स्टेट पर लागू tanh फंक्शन के आउटपुट के साथ एलिमेंट-वाइज गुणा किया जाता है। यह नई हिडन स्टेट (h<sub>t</sub>) बन जाती है जिसे अगले टाइम स्टेप पर पास किया जाता है और भविष्यवाणियाँ करने के लिए भी इस्तेमाल किया जा सकता है।
        ```
        h_t = o_t * tanh(C_t)
        ```

**सेल स्टेट को अपडेट करना**

सेल स्टेट को फॉरगेट और इनपुट गेट्स द्वारा लिए गए निर्णयों के आधार पर अपडेट किया जाता है:

```
C_t = f_t * C_{t-1} + i_t * C̃_t
```

* फॉरगेट गेट (f<sub>t</sub>) तय करता है कि पिछले सेल स्टेट (C<sub>t-1</sub>) को कितना रखना है। यदि f<sub>t</sub> 0 के करीब है, तो जानकारी ज्यादातर भूल जाती है। यदि यह 1 के करीब है, तो जानकारी ज्यादातर रख ली जाती है।
* इनपुट गेट (i<sub>t</sub>) तय करता है कि नए कैंडिडेट मानों (C̃<sub>t</sub>) को सेल स्टेट में कितना जोड़ा जाना चाहिए। यदि i<sub>t</sub> 0 के करीब है, तो नई जानकारी को ज्यादातर नजरअंदाज कर दिया जाता है। यदि यह 1 के करीब है, तो नई जानकारी को ज्यादातर जोड़ दिया जाता है।

**एलएसटीएम वैनिशिंग ग्रेडिएंट प्रॉब्लम को कैसे हल करते हैं:**

लॉन्ग-टर्म डिपेंडेंसीज को हैंडल करने की एलएसटीएम की क्षमता की कुंजी सेल स्टेट और गेटिंग मैकेनिज्म में निहित है। सेल स्टेट कई टाइम स्टेप्स में वजन से बार-बार गुणा किए बिना सूचना के प्रवाह के लिए एक सीधा मार्ग प्रदान करता है, जो पारंपरिक आरएनएन में वैनिशिंग ग्रेडिएंट का प्राथमिक कारण है।

गेट्स एलएसटीएम को यह सीखने की अनुमति देते हैं कि सेल स्टेट में जानकारी को कब संग्रहीत करना है, एक्सेस करना है और भूलना है। यह चयनात्मक मेमोरी मैकेनिज्म नेटवर्क को लंबी अनुक्रमों पर प्रासंगिक जानकारी बनाए रखने और आवश्यकता पड़ने पर इसका उपयोग करने में सक्षम बनाता है, जिससे वैनिशिंग ग्रेडिएंट प्रॉब्लम प्रभावी ढंग से कम हो जाती है और लॉन्ग-टर्म डिपेंडेंसीज सीखना संभव हो पाता है।

मूल रूप से, एलएसटीएम नेटवर्क को लंबी अवधि तक "मेमोरी" बनाए रखने का एक तरीका प्रदान करते हैं, जिससे वे लॉन्ग-रेंज डिपेंडेंसीज वाले सीक्वेंशियल डेटा से जुड़े कार्यों के लिए अत्यधिक प्रभावी साबित होते हैं।