---
audio: false
lang: hi
layout: post
title: Apache Kafka का उपयोग Java में
translated: true
---

Apache Kafka का Java में उपयोग करने के लिए, आपko apne Java application mein Kafka client setup karna padta hai taaki messages produce aur consume kiya ja sake. Kafka ek distributed streaming platform hai, aur uske Java client libraries isse integrate karna aasaan banate hain. Neeche, main aapko basic steps ke saath start karne ke liye guide karunga.

Pehle, aapko apne environment ko setup karna padta hai. Yakeen karo ki aapke system ya server par Kafka install aur running hai. Aap usse official Apache Kafka website se download kar sakte hain aur ZooKeeper aur Kafka server ko provide kiya gaya scripts ka use karke start kar sakte hain. Simplicity ke liye, main assume karunga ki aap locally default settings ke saath Kafka run kar rahe hain (jaise `localhost:9092` bootstrap server ke roop mein).

Phir, aapko apne Java project mein Kafka client dependency add karni padti hai. Agar aap Maven ka use kar rahe hain, to `pom.xml` mein yeh include karein:

```xml
<dependency>
    <groupId>org.apache.kafka</groupId>
    <artifactId>kafka-clients</artifactId>
    <version>3.6.0</version> <!-- Latest version ka use karein -->
</dependency>
```

Ab, kuch code likhte hain. Main aapko dikhata hoon ki ek simple producer aur consumer kaise banaya ja sakta hai.

### Kafka Producer Example
Producer ek message ko Kafka topic mein bhejta hai. Yeh ek basic example hai:

```java
import org.apache.kafka.clients.producer.*;
import java.util.Properties;

public class SimpleProducer {
    public static void main(String[] args) {
        // Producer properties ko configure karein
        Properties props = new Properties();
        props.put("bootstrap.servers", "localhost:9092"); // Kafka server address
        props.put("key.serializer", "org.apache.kafka.common.serialization.StringSerializer");
        props.put("value.serializer", "org.apache.kafka.common.serialization.StringSerializer");

        // Ek producer instance banayein
        try (Producer<String, String> producer = new KafkaProducer<>(props)) {
            // Ek topic "test-topic" mein ek message bhejein
            String topic = "test-topic";
            for (int i = 0; i < 10; i++) {
                String key = "key" + i;
                String value = "Hello, Kafka " + i;
                ProducerRecord<String, String> record = new ProducerRecord<>(topic, key, value);

                producer.send(record, (metadata, exception) -> {
                    if (exception == null) {
                        System.out.println("Sent message: " + value + " to partition " + metadata.partition());
                    } else {
                        exception.printStackTrace();
                    }
                });
            }
        }
    }
}
```

Is code mein:
- `bootstrap.servers` specify karta hai ki Kafka kahan run ho raha hai.
- Serializers define karte hain ki keys aur values (donon strings hain yahan) ko bytes mein kaise convert kiya jaaye.
- `ProducerRecord` message ko represent karta hai, aur `send()` usse asynchronously dispatch karta hai ek callback ke saath success ya failure ko handle karne ke liye.

### Kafka Consumer Example
Consumer ek topic ko subscribe karta hai aur messages ko read karta hai. Yeh ek example hai:

```java
import org.apache.kafka.clients.consumer.*;
import java.util.Collections;
import java.util.Properties;

public class SimpleConsumer {
    public static void main(String[] args) {
        // Consumer properties ko configure karein
        Properties props = new Properties();
        props.put("bootstrap.servers", "localhost:9092");
        props.put("group.id", "test-group"); // Consumer group ID
        props.put("key.deserializer", "org.apache.kafka.common.serialization.StringDeserializer");
        props.put("value.deserializer", "org.apache.kafka.common.serialization.StringDeserializer");
        props.put("auto.offset.reset", "earliest"); // Topic ke shuruv se start karein

        // Ek consumer instance banayein
        try (KafkaConsumer<String, String> consumer = new KafkaConsumer<>(props)) {
            // Topic ko subscribe karein
            consumer.subscribe(Collections.singletonList("test-topic"));

            // Messages ke liye poll karein
            while (true) {
                ConsumerRecords<String, String> records = consumer.poll(1000); // Timeout in ms
                for (ConsumerRecord<String, String> record : records) {
                    System.out.printf("Received: key=%s, value=%s, partition=%d, offset=%d%n",
                            record.key(), record.value(), record.partition(), record.offset());
                }
            }
        }
    }
}
```

Yahan:
- `group.id` consumer group ko identify karta hai coordination ke liye.
- Deserializers byte data ko strings mein convert karte hain.
- `poll()` messages ko ek loop mein fetch karta hai, aur `auto.offset.reset` ensure karta hai ki aap shuruv se se earliest message se start karein agar koi offset commit nahi hai.

### Code ko Run Karna
1. **Kafka ko Start Karein**: Yakeen karein ki ZooKeeper aur Kafka running hain (jaise `bin/zookeeper-server-start.sh config/zookeeper.properties` aur `bin/kafka-server-start.sh config/server.properties` Unix-like systems par).
2. **Ek Topic Banayein**: Agar `test-topic` nahi hai, to Kafka command-line tool ka use karke banayein: `bin/kafka-topics.sh --create --topic test-topic --bootstrap-server localhost:9092 --partitions 1 --replication-factor 1`.
3. **Producer ko Run Karein**: `SimpleProducer` class ko execute karein messages bhejne ke liye.
4. **Consumer ko Run Karein**: `SimpleConsumer` class ko run karein messages ko consume karne ke liye dekhein.

### Tips
- **Error Handling**: Production code ke liye try-catch blocks ya logging add karein.
- **Configuration**: Properties jaise `acks` (producer ke liye reliability) ya `max.poll.records` (consumer ke liye batch size) apne requirements ke hisaab se adjust karein.
- **Libraries**: Zyada complex applications ke liye, Spring Kafka ka sochiye jo configuration aur usage ko simplify karta hai.

Yeh ek basic setup hai aapko start karne ke liye. Mujhe batayein agar aap specific features jaise partitioning, error handling, ya ek larger application ke saath integrate karna chahte hain!