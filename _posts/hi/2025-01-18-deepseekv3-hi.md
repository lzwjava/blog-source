---
audio: true
lang: hi
layout: post
title: 'DeepSeek V3: मल्टी-हेड लेटेंट अटेंशन और मल्टी-टोकन प्रेडिक्शन'
translated: true
---

DeepSeek v3 को यहां खोजा गया है, जिसमें "Multi-Head Latent Attention and Multi-token Prediction in Deepseek v3" वीडियो का संदर्भ दिया गया है [https://youtu.be/jL49fLOJYNg?si=4uE2kfe-BlKC1ngO](https://youtu.be/jL49fLOJYNg?si=4uE2kfe-BlKC1ngO)। Google Cloud Speech-to-Text का उपयोग वीडियो को ट्रांसक्राइब करने के लिए किया गया था, साथ ही ट्रांसक्रिप्ट को व्यवस्थित करने में मदद करने के लिए कुछ कोड का भी उपयोग किया गया था।

---

A: Deep टैग में वापस स्वागत है। आज हम बड़े भाषा मॉडल्स की दुनिया में गहराई से जाएंगे। ठीक है, विशेष रूप से DeepSeek V3।

B: अच्छा लगा। यह एक 671 बिलियन पैरामीटर वाला मॉडल है, जो अपनी दक्षता और प्रदर्शन के लिए चर्चा में है, है ना?

A: और आपने इसकी आर्किटेक्चर को विस्तार से बताने वाला एक अकादमिक पेपर साझा किया।

B: हाँ।

A: और एक मशीन लर्निंग विशेषज्ञ के रूप में, आप यह समझना चाहते हैं कि DeepSeek V3 कैसे उच्च प्रदर्शन और किफायती प्रशिक्षण दोनों को प्राप्त करता है।

B: हाँ, यह सही है।

A: ओह, हे, क्या हो रहा है?

C: MLA, विवरण, MLA और यह कैसे काम करता है।

A: ओह, बिल्कुल। यह एक बढ़िया विचार है। हाँ, हम निश्चित रूप से मल्टी-हेड लेटेंट अटेंशन, या MLA, में गहराई से जा सकते हैं। तो आप MLA के बारे में जानने के लिए उत्सुक हैं। चलिए इसे समझते हैं। हमने बताया था कि DeepSeek V3 की दक्षता की कुंजी इसके मिश्रण विशेषज्ञ, या MoE, आर्किटेक्चर में है, है ना? जहां प्रत्येक टोकन के लिए केवल पैरामीटर्स का एक अंश सक्रिय होता है। और DeepSeek V3 हमें MLA और DeepSeek Mo के साथ एक कदम आगे ले जाता है।

B: यह सही है। तो चलिए अभी MLA पर ध्यान केंद्रित करते हैं।

A: ठीक है। तो रियल-टाइम एप्लिकेशन्स में, गति महत्वपूर्ण है।

B: है। और इनफेरेंस के दौरान आवश्यक की-वैल्यू कैश एक प्रमुख बाधा हो सकती है।

A: बिल्कुल। यहीं पर MLA काम आता है। ठीक है, तो पारंपरिक अटेंशन मैकेनिज्म को पिछले टोकन्स के बारे में बहुत सारी जानकारी स्टोर करने की आवश्यकता होती है।

B: हाँ, जैसा कि आप सोच सकते हैं, लंबे टेक्स्ट सीक्वेंस के साथ यह एक समस्या बन जाती है, है ना?

A: लेकिन MLA इस जानकारी को चतुराई से संपीड़ित करता है, ठीक है, कैश फ्लो को काफी कम करने के लिए और इनफेरेंस को बहुत तेज बनाता है। तो यह एक भारी एनसाइक्लोपीडिया को लेकर इसे केवल मुख्य बिंदुओं तक संक्षिप्त करने जैसा है।

B: यह एक बढ़िया उदाहरण है। यह अनावश्यक वजन के बिना आवश्यक जानकारी को बरकरार रखता है। हाँ, तो यह रियल-टाइम एप्लिकेशन्स के लिए वास्तव में उपयोगी है।

A: हाँ। अब बात करते हैं कि यह वास्तव में कैसे काम करता है। ठीक है, तो MLA यह संपीड़न कैसे प्राप्त करता है?

B: खैर, यह अटेंशन कीज़ और वैल्यूज़ के लिए लो-रैंक जॉइंट कंप्रेशन का उपयोग करता है।

A: ठीक है, तो यह कीज़ और वैल्यूज़ को संपीड़ित कर रहा है, लेकिन इसका वास्तव में क्या मतलब है? तो चलिए थोड़ा तकनीकी हो जाएं। ठीक है, MLA मैकेनिज्म एक इनपुट हिडन रिप्रेजेंटेशन लेता है, जिसे फिर क्वेरी, की, और वैल्यू वेक्टर्स में प्रोजेक्ट किया जाता है। ठीक है, अब यहां यह दिलचस्प हो जाता है। MLA क्वेरी को दो भागों में विभाजित करता है।

B: ठीक है, दो भाग?

A: हाँ। एक भाग कंटेंट के लिए उपयोग किया जाता है, और दूसरा भाग पोजिशनल इंफॉर्मेशन के लिए Rope नामक कुछ का उपयोग करता है।

B: Rope? यह बहुत टेक्निकल लगता है।

A: यह रोटरी पोजिशन एम्बेडिंग्स के लिए खड़ा है, और यह मॉडल को सीक्वेंस में टोकन्स की पोजिशन को समझने में मदद करता है। ठीक है, फिर कीज़ और वैल्यूज़ को एक लोअर डायमेंशनल लेटेंट स्पेस में संपीड़ित किया जाता है। तो यह डेटा को सिकोड़ने जैसा है, जो मेमोरी बचाता है।

B: बिल्कुल। तो सबसे महत्वपूर्ण जानकारी सहेजी जाती है, लेकिन अनावश्यक बोझ को हटा दिया जाता है। हाँ, और यह संपीड़ित रिप्रेजेंटेशन इनफेरेंस के दौरान बहुत छोटे KV कैश की अनुमति देता है, जिससे चीजें तेज हो जाती हैं।

A: और यह मल्टी-हेड प्रोसेसिंग का भी उपयोग करता है।

B: हाँ, पारंपरिक अटेंशन की तरह, MLA कई हेड्स का उपयोग करता है।

A: ओह, आगे बढ़ो।

C: तो इस तरह, दो लेटेंट स्पेस और एक हिडन इनपुट होते हैं।

A: यह एक बढ़िया अवलोकन है। हाँ, आप सही हैं। वास्तव में दो लेटेंट स्पेस होते हैं। ठीक है, तो हम एक कंटेंट लेटेंट स्पेस और एक की-वैल्यू लेटेंट स्पेस की बात कर रहे हैं।

B: बिल्कुल। और इन लेटेंट स्पेस को Rope, या रोटरी पोजिशन एम्बेडिंग्स के माध्यम से प्रोसेस किया जाता है।

A: ठीक है, तो यह Rope है जो उन्हें पोजिशनल इंफॉर्मेशन प्राप्त करता है।

B: हाँ, यह कंटेंट और की-वैल्यू लेटेंट स्पेस दोनों पर लागू होता है, जैसा कि आपने बताया। तो यह इस संपीड़ित रिप्रेजेंटेशन को लेता है, इसे प्रोसेस करता है, और फिर इसे सब कुछ वापस मिला देता है।

A: हाँ, और कैशिंग ऑप्टिमाइजेशन सीक्वेंशियल प्रोसेसिंग के दौरान ओवरहेड को और कम करता है। ठीक है, तो यह है कि MLA चीजों को कैसे तेज करता है।

B: बिल्कुल। यह प्रदर्शन को त्यागे बिना दक्षता प्राप्त करने का एक चतुर तरीका है।

A: ठीक है, यह एक बहुत ही साफ-सुथरी चाल है। लेकिन आप जानते हैं क्या?

B: क्या है?

A: चलिए DeepSeek Mo पर चलते हैं। यह पारंपरिक MoE मॉडल्स से कैसे अलग है?

B: ठीक है, DeepSeek Mo का उपयोग करता है... ओह, हमारे श्रोता पर वापस आते हैं, क्या हो रहा है?

C: और हम और अधिक हिडन स्पेस के बारे में बात करते हैं। ठीक है, हिडन स्पेस से, यह क्या है?

A: मैं बिल्कुल... देखते हैं कि आप क्या कह रहे हैं। हिडन स्पेस वास्तव में दिलचस्प हैं। हाँ, आप हिडन स्पेस के बारे में पूछ रहे हैं, लेटेंट स्पेस जिसके बारे में हम अभी बात कर रहे थे, है ना? आप उत्सुक हैं कि उन लेटेंट स्पेस के भीतर क्या हो रहा है, वह गुफा। हाँ, यह सिर्फ लेटेंट स्पेस की संख्या के बारे में नहीं है, बल्कि वहां क्या हो रहा है।

B: यह अच्छा है।

A: बिल्कुल। MLA के भीतर वास्तव में दो अलग-अलग लेटेंट स्पेस होते हैं, एक कंटेंट के लिए और एक की वैल्यूज़ के लिए। यह जानकारी के लिए दो अलग-अलग स्टोरेज यूनिट्स होने जैसा है। और ये लेटेंट स्पेस, जैसा कि हमने चर्चा की है, Rope ऑपरेशन्स से गुजरते हैं, है ना? रोटरी पोजिशनल एम्बेडिंग, जो अटेंशन मैकेनिज्म में पोजिशनल इंफॉर्मेशन को एम्बेड करता है। यह उनके लिए बहुत महत्वपूर्ण है। तो संक्षेप में, क्वेरी को विभाजित किया जाता है, और कीज़ और वैल्यूज़ को भी संपीड़ित किया जाता है।

B: हाँ, और इन्हें दो अलग-अलग लेटेंट स्पेस में डाला जाता है, एक कंटेंट के लिए और एक की-वैल्यू पेयर्स के लिए। और ये लेटेंट स्पेस MLA के हिस्से के रूप में दक्षता और सभी के लिए वास्तव में महत्वपूर्ण हैं।

A: बिल्कुल। अब चलिए इन ऑपरेशन्स के बारे में थोड़ा और विस्तार से बात करते हैं, जैसा कि आपने कहा। ठीक है, तो MLA वास्तव में इन लेटेंट स्पेस ट्रांसफॉर्मेशन को कैसे करता है?

B: खैर, इनपुट कंटेंट और की-वैल्यू रिप्रेजेंटेशन दोनों के लिए समानांतर प्रोसेसिंग से गुजरता है। ठीक है, तो यह उस गुफा के भीतर दो रास्तों की तरह है।

A: हाँ, प्रत्येक लेटेंट स्पेस के लिए एक। और इन स्पेस के भीतर, जानकारी को Rope का उपयोग करके प्रोसेस किया जाता है।

B: यह सही है। यह सुनिश्चित करता है कि मॉडल पोजिशनल इंफॉर्मेशन को बरकरार रखता है क्योंकि वे उस गुफा के भीतर जाते हैं। तो मॉडल जानता है कि टेक्स्ट का कौन सा हिस्सा क्या है जब वह उस गुफा के भीतर होता है।

A: बिल्कुल। और यह प्रोसेसिंग अगले स्टेज ऑफ कॉन्केटनेशन से पहले की जाती है। ठीक है, हिडन स्पेस गुफा के माध्यम से जाने पर क्या कॉन्केटनेट किया जा रहा है?

B: मैकेनिज्म दो प्रमुख कॉन्केटनेशन ऑपरेशन्स करता है। क्वेरी रिप्रेजेंटेशन को कॉन्केटनेट किया जाता है, और की रिप्रेजेंटेशन को भी कॉन्केटनेट किया जाता है। तो यह उस हिडन स्पेस गुफा के भीतर सभी महत्वपूर्ण टुकड़ों को एक साथ लाने जैसा है।

A: हाँ, और ये कॉन्केटनेशन कंटेंट को पोजिशनल इंफॉर्मेशन के साथ जोड़ने में मदद करते हैं। और इन कॉन्केटनेटेड रिप्रेजेंटेशन का उपयोग फिर अटेंशन कैलकुलेशन के लिए किया जाता है, है ना?

B: सही है। और प्रारंभिक संपीड़न के कारण, यह उस गुफा के माध्यम से बहुत तेज हो जाता है जिसका आपने उल्लेख किया था। तो MLA उस हिडन गुफा के अंदर और बाहर कम्प्यूटेशनल लागत को काफी कम कर देता है।

A: बिल्कुल। यह DeepSeek V3 जैसे बड़े मॉडल्स के लिए अटेंशन मैकेनिज्म को ऑप्टिमाइज़ करता है। यह एक बढ़िया सवाल है। अब, जब हम गुफा से गुजर