---
audio: false
generated: true
image: false
lang: hi
layout: post
model: x-ai/grok-code-fast-1
title: टाइनीग्रेड लाइटवेट एमएल लाइब्रेरी
translated: true
type: note
---

### TinyGrad का परिचय
TinyGrad पायथन में एक हल्का, मिनिमलिस्ट मशीन लर्निंग और डीप लर्निंग लाइब्रेरी है, जिसे शिक्षा और सरलता के लिए डिज़ाइन किया गया है। यह PyTorch की तरह ऑटोमैटिक डिफरेंशिएशन (autograd) को लागू करता है लेकिन बहुत छोटा है—लगभग 1,000 लाइन्स कोड। यह ML कॉन्सेप्ट्स सीखने या बड़ी लाइब्रेरीज के ओवरहेड के बिना छोटे मॉडल बनाने के लिए बहुत अच्छा है। यह टेंसर्स, न्यूरल नेटवर्क्स, और बेसिक ऑपरेशंस को सपोर्ट करता है, जिसमें PyTorch या Metal के माध्यम से GPU एक्सेलेरेशन भी शामिल है।

आप ऑफिशियल रिपॉजिटरी यहाँ पा सकते हैं: [tinygrad GitHub](https://github.com/geohot/tinygrad)। नोट: यह एक्सपेरिमेंटल है और प्रोडक्शन यूज़ के लिए PyTorch या TensorFlow जितना रोबस्ट नहीं है।

### इंस्टालेशन
TinyGrad को pip के माध्यम से इंस्टॉल करें:

```bash
pip install tinygrad
```

इसकी डिपेंडेंसीज बहुत कम हैं लेकिन यह कुछ बैकेंड्स के लिए वैकल्पिक रूप से PyTorch का उपयोग करता है। GPU सपोर्ट के लिए, सुनिश्चित करें कि आपने PyTorch इंस्टॉल किया हुआ है।

### बेसिक यूसेज
इम्पोर्ट करके और कॉन्टेक्स्ट सेट करके शुरुआत करें (TinyGrad को स्पेसिफाई करने की आवश्यकता होती है कि आप ट्रेनिंग कर रहे हैं या इन्फरेंस, क्योंकि ग्रेडिएंट्स अलग तरह से कंप्यूट होते हैं)।

#### इम्पोर्टिंग और कॉन्टेक्स्ट
```python
from tinygrad import Tensor
from tinygrad.nn import Linear, BatchNorm2d  # न्यूरल नेट्स के लिए

# कॉन्टेक्स्ट सेट करें: ट्रेनिंग (ग्रेडिएंट ट्रैकिंग के लिए) या इन्फरेंस
Tensor.training = True  # ग्रेडिएंट ट्रैकिंग एनेबल करें
```

#### टेंसर्स बनाना और मैनिपुलेट करना
टेंसर्स कोर डेटा स्ट्रक्चर हैं, जो NumPy ऐरेज़ या PyTorch टेंसर्स के समान हैं।

```python
# लिस्ट्स, NumPy ऐरेज़, या शेप द्वारा टेंसर्स बनाएं
a = Tensor([1, 2, 3])          # लिस्ट से
b = Tensor.zeros(3)            # शेप (3,) का ज़ीरोज़ टेंसर
c = Tensor.rand(2, 3)          # शेप (2, 3) का रैंडम टेंसर

# बेसिक ऑपरेशंस
d = a + b                      # एलिमेंट-वाइज़ एडिशन
e = d * 2                      # स्केलर मल्टीप्लिकेशन
f = a @ Tensor([[1], [2], [3]])  # मैट्रिक्स मल्टीप्लिकेशन (a 1D है, इम्प्लिसिटली ट्रांसपोज़्ड)

print(e.numpy())               # प्रिंटिंग या आगे के उपयोग के लिए NumPy में कन्वर्ट करें
```

#### ऑटोमैटिक डिफरेंशिएशन (बैकप्रोपगेशन)
TinyGrad चेन रूल का उपयोग करके ग्रेडिएंट्स को ऑटोमैटिकली कंप्यूट करता है।

```python
# ग्रेडिएंट ट्रैकिंग एनेबल करें
Tensor.training = True

x = Tensor([1.0, 2.0, 3.0])
y = (x * 2).sum()             # कुछ ऑपरेशन; y एक स्केलर है

y.backward()                  # ग्रेडिएंट्स कंप्यूट करें
print(x.grad.numpy())         # x के संबंध में ग्रेडिएंट्स: [2, 2, 2] होने चाहिए
```

NumPy में एक्सपोर्ट करने के लिए, `.numpy()` का उपयोग करें—ग्रेडिएंट्स रीसेट होने तक एक्यूमुलेट होते रहते हैं।

#### न्यूरल नेटवर्क्स और ट्रेनिंग
TinyGrad में बेसिक लेयर्स और ऑप्टिमाइज़र्स शामिल हैं। यहाँ एक साधारण MLP उदाहरण है:

```python
from tinygrad.nn import Linear, optim

# एक साधारण मॉडल डिफाइन करें (जैसे, लीनियर लेयर)
model = Linear(3, 1)          # इनपुट 3, आउटपुट 1

# डमी डेटा
x = Tensor.rand(4, 3)         # 4 सैंपल्स का बैच, 3 फीचर्स
y_true = Tensor.rand(4, 1)    # टारगेट

# फॉरवर्ड पास
pred = model(x).sigmoid()      # बाइनरी क्लासिफिकेशन मानते हुए

# लॉस (जैसे, MSE)
loss = ((pred - y_true) ** 2).mean()

# बैकप्रोप और ऑप्टिमाइज़
loss.backward()
optim.Adam([model], lr=0.01).step()
```

कन्वोल्यूशन नेटवर्क्स के लिए, `tinygrad.nn` से `Conv2d` का उपयोग करें।

### एडवांस्ड फीचर्स
- **लॉस फंक्शंस और एक्टिवेशंस**: `tinygrad.nn` में उपलब्ध (जैसे, `sigmoid`, `relu`, `cross_entropy`)।
- **ऑप्टिमाइज़र्स**: `tinygrad.nn.optim` में `SGD`, `Adam`।
- **लेयर्स**: `Linear`, `Conv2d`, `BatchNorm`, आदि।
- **सेविंग/लोडिंग**: मॉडल्स को स्टेट डिक्शनरीज़ के रूप में सेव किया जा सकता है (PyTorch के समान)।
- **GPU/एक्सेलेरेशन**: TinyGrad PyTorch बैकेंड के माध्यम से GPU पर रन कर सकता है: `TESOR_SET_DEVICE='cuda:0'`। यह macOS पर Metal को भी सपोर्ट करता है।
- **विजन/डीनोइज़िंग उदाहरण**: रेपो में MNIST पर ResNet ट्रेन करने जैसे उदाहरण हैं।

पूरे उदाहरणों के लिए, GitHub रेपो में `examples/` फोल्डर चेक करें, जैसे इमेज क्लासिफिकेशन या रीइन्फोर्समेंट लर्निंग।

### उदाहरण: एक न्यूरल नेटवर्क को ट्रेन करना
यहाँ XOR पर एक नेटवर्क को ट्रेन करने के लिए एक कम्प्लीट, साधारण स्क्रिप्ट है:

```python
from tinygrad import Tensor
from tinygrad.nn import Linear, optim

# डेटा
X = Tensor([[0,0], [0,1], [1,0], [1,1]])
Y = Tensor([[0], [1], [1], [0]])

# मॉडल
model = Linear(2, 1)
opt = optim.Adam([model], lr=0.1)

Tensor.training = True

for epoch in range(1000):
    pred = model(X).sigmoid()
    loss = ((pred - Y) ** 2).mean()
    loss.backward()
    opt.step()
    opt.zero_grad()

# टेस्ट
print(model(Tensor([[0,0]])))  # ~0 होना चाहिए
```

### टिप्स और चेतावनियाँ
- **मोड स्विचिंग**: ट्रेनिंग के लिए हमेशा `Tensor.training = True` सेट करें; ग्रेडिएंट कंप्यूटेशन से बचने के लिए इन्फरेंस के लिए `False` पर स्विच करें।
- **शेप हैंडलिंग**: टेंसर्स NumPy-जैसी ब्रॉडकास्टिंग फॉलो करते हैं, लेकिन छोटे मॉडल्स में ओवरफ्लो के लिए ध्यान रखें।
- **डीबगिंग**: वैल्यूज़ इंस्पेक्ट करने के लिए `.numpy()` का उपयोग करें। अगर कुछ फेल होता है, तो शेप मिसमैच के लिए चेक करें।
- **परफॉर्मेंस**: यह बड़े मॉडल्स के लिए स्लो है—प्रोडक्शन के लिए PyTorch का उपयोग करें।
- **अपडेट्स**: TinyGrad तेजी से इवॉल्व होता है; लेटेस्ट वर्जन के लिए रेपो चेक करें।

अधिक डिटेल्स के लिए, [docs/README.md](https://github.com/geohot/tinygrad/blob/master/README.md) विजिट करें या उदाहरणों को लोकली रन करें। अगर आपका कोई स्पेसिफिक यूज़ केस है, तो टेलर्ड एडवाइस के लिए अधिक डिटेल्स प्रोवाइड करें!