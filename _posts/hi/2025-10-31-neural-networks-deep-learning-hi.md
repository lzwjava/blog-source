---
audio: false
generated: true
image: false
lang: hi
layout: post
model: x-ai/grok-4-fast
title: न्यूरल नेटवर्क और डीप लर्निंग का अवलोकन
translated: true
type: note
---

### अवलोकन
"न्यूरल नेटवर्क्स एंड डीप लर्निंग" माइकल नीलसन द्वारा लिखित एक निःशुल्क ऑनलाइन पाठ्यपुस्तक है जो न्यूरल नेटवर्क और डीप लर्निंग की मूलभूत अवधारणाओं से परिचय कराती है। यह समझ विकसित करने के लिए MNIST हस्तलिखित अंक पहचान डेटासेट को एक चलंत उदाहरण के रूप में उपयोग करती है, जो मूल अवधारणाओं से लेकर उन्नत तकनीकों तक आगे बढ़ती है। यह पुस्तक व्यावहारिक कार्यान्वयन (Python कोड उदाहरणों के साथ), गणितीय व्युत्पत्तियों और ऐतिहासिक संदर्भ पर जोर देती है, साथ ही यह भी जांच करती है कि छवि पहचान, स्पीच प्रोसेसिंग और प्राकृतिक भाषा समझ जैसे कार्यों के लिए न्यूरल नेटवर्क शक्तिशाली क्यों हैं। यह बैकप्रोपेगेशन और स्टोकेस्टिक ग्रेडिएंट डिसेंट जैसे मूल एल्गोरिदम को कवर करती है, डीप नेटवर्क को प्रशिक्षित करने में आने वाली चुनौतियों को संबोधित करती है, और कन्व्होल्यूशनल न्यूरल नेटवर्क (कॉन्वनेट्स) में हुई सफलताओं को प्रदर्शित करती है। इसकी शैली सहज लेकिन सटीक है, जिसमें अवधारणाओं को मजबूत करने के लिए अभ्यास और विज़ुअलाइज़ेशन शामिल हैं।

### अध्याय 1: हस्तलिखित अंकों को पहचानने के लिए न्यूरल नेट्स का उपयोग
यह प्रारंभिक अध्याय मानव दृष्टि की सहजता की तुलना पैटर्न पहचान में कंप्यूटरों के संघर्ष से करते हुए न्यूरल नेटवर्क को प्रेरित करता है। यह बिल्डिंग ब्लॉक्स के रूप में पर्सेप्ट्रॉन (बाइनरी निर्णय न्यूरॉन्स) और सिग्मॉइड न्यूरॉन्स (चिकने, संभाव्य आउटपुट) का परिचय देता है, और समझाता है कि इनपुट, हिडन और आउटपुट लेयर्स वाले फीडफॉरवर्ड नेटवर्क डेटा को पदानुक्रमित रूप से कैसे प्रोसेस करते हैं। MNIST (28x28 पिक्सेल के 60,000 ट्रेनिंग इमेज) का उपयोग करते हुए, यह एक तीन-लेयर नेटवर्क ([784 इनपुट, 30-100 हिडन, 10 आउटपुट]) को स्टोकेस्टिक ग्रेडिएंट डिसेंट (SGD) के माध्यम से क्वाड्रेटिक कॉस्ट को कम करने के लिए प्रशिक्षित करना दिखाता है, जो ~95-97% सटीकता प्राप्त करता है। मुख्य विचार: ग्रेडिएंट डिसेंट कॉस्ट सतह के ढलान का अनुसरण करके वज़न/बायस को ऑप्टिमाइज़ करता है; मिनी-बैच प्रशिक्षण की गति बढ़ाते हैं; सिग्मॉइड डिफरेंशिएबल लर्निंग सक्षम करता है। निष्कर्ष: न्यूरल नेट्स डेटा से नियम स्वचालित रूप से सीखते हैं, जो रैंडम गेसिंग (10%) या SVMs (~98% ट्यून्ड) जैसे बेसलाइन से बेहतर प्रदर्शन करते हैं, लेकिन इन्हें हाइपरपैरामीटर ट्यूनिंग (जैसे, लर्निंग रेट η) की आवश्यकता होती है।

### अध्याय 2: बैकप्रोपेगेशन एल्गोरिदम कैसे काम करता है
बैकप्रोपेगेशन को SGD के लिए ग्रेडिएंट की गणना करने का एक कुशल तरीका बताया गया है, जो चेन रूल का उपयोग करके त्रुटियों को लेयर्स के माध्यम से पीछे की ओर प्रसारित करता है। नोटेशन में वज़न मैट्रिक्स \\(w^l\\), बायस \\(b^l\\), और एक्टिवेशन \\(a^l = \sigma(z^l)\\) शामिल हैं, जहाँ \\(z^l = w^l a^{l-1} + b^l\\) है। इसे चार समीकरण परिभाषित करते हैं: आउटपुट त्रुटि \\(\delta^L = \nabla_a C \odot \sigma'(z^L)\\), बैकवर्ड प्रोपेगेशन \\(\delta^l = (w^{l+1})^T \delta^{l+1} \odot \sigma'(z^l)\\), और ग्रेडिएंट \\(\partial C / \partial b^l = \delta^l\\), \\(\partial C / \partial w^l = a^{l-1} (\delta^l)^T\\)। मिनी-बैच के लिए, उदाहरणों पर औसत निकालें। उदाहरण नाइव फाइनाइट डिफरेंसेज (जैसे, 2 पास बनाम लाखों) पर भारी स्पीडअप दिखाते हैं। अंतर्दृष्टि: सैचुरेशन वैनिशिंग ग्रेडिएंट (\\(\sigma' \approx 0\\)) का कारण बनती है; मैट्रिक्स फॉर्म तेज़ गणना सक्षम करते हैं। निष्कर्ष: बैकप्रोपेगेशन (1986 रमेलहार्ट एट अल.) न्यूरल लर्निंग का मुख्य आधार है, जो डिफरेंशिएबल कॉस्ट/एक्टिवेशन के लिए सामान्य है, लेकिन त्रुटि प्रवाह जैसी गतिशीलता को प्रकट करता है।

### अध्याय 3: न्यूरल नेटवर्क के सीखने के तरीके को सुधारना
क्वाड्रेटिक कॉस्ट की सैचुरेशन समस्याओं को संबोधित करते हुए, क्रॉस-एन्ट्रॉपी कॉस्ट \\(C = -\frac{1}{n} \sum [y \ln a + (1-y) \ln(1-a)]\\) \\(\sigma'\\) को कैंसल कर देती है, जिससे तेज़ डेरिवेटिव \\(\partial C / \partial w = \sigma(z) - y\\) प्राप्त होते हैं। सॉफ्टमैक्स आउटपुट संभाव्य वर्गीकरण सक्षम करते हैं। ओवरफिटिंग (उच्च ट्रेन/कम टेस्ट सटीकता) का निदान वैलिडेशन सेट के माध्यम से किया जाता है और L2 रेगुलराइज़ेशन (\\(C += \lambda/2n \sum w^2\\), वज़न सिकोड़ना) और ड्रॉपआउट (न्यूरॉन्स को यादृच्छिक रूप से शून्य करना) द्वारा इसे कम किया जाता है। डेटा विस्तार (जैसे, रोटेशन) विविधताओं का अनुकरण करता है। बेहतर इनिशियलाइज़ेशन (वज़न ~गॉसियन std \\(1/\sqrt{n_{in}}\\)) प्रारंभिक सैचुरेशन से बचाता है। हाइपरपैरामीटर ट्यूनिंग वैलिडेशन का उपयोग करती है: व्यापक रूप से शुरू करें (जैसे, η ट्रायल), अर्ली स्टॉपिंग के साथ परिष्कृत करें। अन्य विचार: मोमेंटम SGD को तेज करता है; ReLU/tanh एक्टिवेशन। MNIST उदाहरण 95% से 98%+ तक लाभ दिखाते हैं। निष्कर्ष: मजबूत सामान्यीकरण के लिए तकनीकों (क्रॉस-एन्ट्रॉपी + L2 + ड्रॉपआउट) को संयोजित करें; अक्सर अधिक डेटा एल्गोरिदमिक ट्वीक्स से बेहतर होता है।

### अध्याय 4: एक दृश्य प्रमाण कि न्यूरल नेट्स कोई भी फ़ंक्शन कम्प्यूट कर सकते हैं
एक रचनात्मक प्रमाण दिखाता है कि सिंगल-हिडन-लेयर सिग्मॉइड नेटवर्क पर्याप्त न्यूरॉन्स के साथ किसी भी सतत फ़ंक्शन \\(f(x)\\) को सटीकता \\(\epsilon > 0\\) तक अनुमानित कर सकते हैं, "बम्प" फ़ंक्शन (स्टेप जोड़े जो आयत बनाते हैं) और "टावर" (उच्च-आयामी एनालॉग) के माध्यम से। बड़े वज़न वाले स्टेप हेविसाइड जंप का अनुमान लगाते हैं; ओवरलैप्स खामियों को ठीक करते हैं। बहु-इनपुट/आउटपुट के लिए, पीसवाइज़-कॉन्स्टेंट लुकअप टेबल बनाएं। चेतावनी: केवल सन्निकटन (सटीक नहीं); सतत फ़ंक्शन। लीनियर एक्टिवेशन यूनिवर्सैलिटी में विफल होते हैं। निष्कर्ष: न्यूरल नेट्स NAND गेट्स की तरह ट्यूरिंग-कम्पलीट हैं, जो फोकस को "क्या वे कर सकते हैं?" से "उन्हें कुशलता से कैसे प्रशिक्षित करें?" की ओर स्थानांतरित करते हैं। व्यवहार में पदानुक्रम के लिए डीप नेट्स उत्कृष्ट प्रदर्शन करते हैं, भले ही सिद्धांत में उथले नेटवर्क पर्याप्त हों।

### अध्याय 5: डीप न्यूरल नेटवर्क को प्रशिक्षित करना कठिन क्यों है?
सैद्धांतिक लाभों (जैसे, कुशल पैरिटी कम्प्यूटेशन) के बावजूद, डीप नेट्स MNIST पर उथले नेटवर्क की तुलना में खराब प्रदर्शन करते हैं (~96.5% बनाम 96.9% 2 लेयर्स के लिए, 4 के लिए घटकर 96.5%)। सर्किट एनालॉजी गहराई की अमूर्त शक्ति को उजागर करते हैं, लेकिन वैनिशिंग ग्रेडिएंट विफलताओं की व्याख्या करते हैं: चेन-रूल उत्पाद \\(\partial C / \partial b^1 = \prod (w_j \sigma'(z_j)) \partial C / \partial a^L\\) घातीय रूप से सिकुड़ते हैं (\\(\sigma' \leq 0.25\\), |w| <1)। एक्सप्लोडिंग ग्रेडिएंट तब होते हैं यदि |w σ'| >1। अस्थिरता अंतर्निहित है; प्रारंभिक लेयर्स ~100x धीमी गति से सीखती हैं। अन्य मुद्दे: सैचुरेशन, खराब इनिशियलाइज़ेशन। निष्कर्ष: ग्रेडिएंट समस्याएं एल्गोरिदमिक हैं, आर्किटेक्चरल नहीं—बेहतर एक्टिवेशन/इनिशियलाइज़ेशन के माध्यम से हल करने योग्य, जो डीप सफलता का मार्ग प्रशस्त करती हैं।

### अध्याय 6: डीप लर्निंग
सुधारों को लागू करते हुए, कॉन्वनेट्स छवि संरचना का लाभ उठाते हैं: लोकल रिसेप्टिव फील्ड्स (जैसे, 5x5 कर्नेल), शेयर्ड वेट (ट्रांसलेशन इनवेरिएंस), और पूलिंग (जैसे, 2x2 मैक्स) पैरामीटर्स को कम करते हैं। MNIST का विकास: बेसलाइन फुली कनेक्टेड (97.8%) → कॉन्व-पूल (99.1%) → ReLU + विस्तार (99.4%) → ड्रॉपआउट/एन्सेंबल (99.7%, मानव के निकट)। KSH के 2012 इमेजनेट विजेता (7-लेयर कॉन्वनेट, 84.7% टॉप-5 सटीकता) ने ReLUs, ड्रॉपआउट, GPUs के माध्यम से क्रांतियां शुरू कीं। मानव बेंचमार्क पर मेल खाते/अधिक हैं लेकिन धीमे। RNNs (अनुक्रम, जैसे स्पीच) और DBNs (जनरेटिव) का सर्वेक्षण करता है। एडवरसैरियल उदाहरण अंतराल प्रकट करते हैं। निष्कर्ष: गहराई + कॉन्व्स + तकनीकें सफलताओं को सक्षम करती हैं; भविष्य: इरादा इंटरफेस, डेटा साइंस, लेकिन सामान्य AI को एकीकृत अंतर्दृष्टि की आवश्यकता है।

### परिशिष्ट: क्या इंटेलिजेंस के लिए कोई सरल एल्गोरिदम है?
नेट्स से परे अनुमान लगाते हुए, यह तर्क देता है कि बुद्धिमत्ता सरल सिद्धांतों (जैसे गति को एकीकृत करने वाला गुरुत्वाकर्षण) से उत्पन्न हो सकती है, मस्तिष्क के 100T कनेक्शन (70 क्वाड्रिलियन बिट्स) से नहीं बल्कि चिंपैंजी से जीनोम के ~125M बिट्स के अंतर से। फेरेट रिवायरिंग साझा संवेदी नियमों का सुझाव देती है; विकासवादी मनोविज्ञान विशेषज्ञता का संकेत देता है। मिंस्की का "सोसाइटी ऑफ माइंड" विविध एजेंटों के साथ प्रतिकार करता है। निष्कर्ष: आशावाद खोज को प्रेरित करता है—सरल एल्गोरिदम का पीछा करें, भले ही वे विस्तृत हों; AI को विचारों के "100 नोबेल पुरस्कार" की आवश्यकता हो सकती है।

[न्यूरल नेटवर्क्स एंड डीप लर्निंग](http://neuralnetworksanddeeplearning.com/)