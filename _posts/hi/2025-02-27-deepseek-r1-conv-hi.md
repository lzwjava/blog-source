---
audio: false
generated: true
lang: hi
layout: post
title: गहन संवाद
translated: true
---

A: नमस्ते, मैं DeepSeek-R1 मॉडलों और उनकी तर्क करने की क्षमता के बारे में बहुत सुन रहा हूँ। आप मुझे इसका विवरण दे सकते हैं?

B: बिल्कुल! हम शुरुआत के लिए बुनियादी बातों से करें। DeepSeek-R1 एक मॉडल श्रृंखला है जो DeepSeek-AI द्वारा विकसित की गई है और जो तर्क करने की क्षमता को मजबूत बनाने के लिए पुष्टि सीखने (RL) का उपयोग करती है। इसमें दो मुख्य संस्करण हैं: DeepSeek-R1-Zero और DeepSeek-R1।

A: DeepSeek-R1-Zero और DeepSeek-R1 में क्या अंतर है?

B: DeepSeek-R1-Zero को केवल RL के माध्यम से प्रशिक्षित किया जाता है, बिना किसी निर्देशित फाइन ट्यूनिंग (SFT) के। यह मजबूत तर्क करने की क्षमता दिखाती है, लेकिन इसमें पढ़ने की कठिनाई और भाषा मिश्रण जैसे समस्याएं होती हैं। दूसरी ओर, DeepSeek-R1 में बहु-स्तर प्रशिक्षण और cold-start डेटा को RL से पहले शामिल किया जाता है, जिससे इन समस्याओं को दूर किया जाता है और प्रदर्शन को और बेहतर बनाया जाता है।

A: यह रोचक है। इन मॉडलों में पुष्टि सीखने की प्रक्रिया कैसे काम करती है?

B: RL प्रक्रिया में एक इनाम प्रणाली का उपयोग किया जाता है ताकि मॉडल की सीखने को मार्गदर्शन किया जा सके। DeepSeek-R1-Zero के लिए, वे एक नियम आधारित इनाम प्रणाली का उपयोग करते हैं जो सटीकता और फॉर्मेट पर ध्यान केंद्रित करती है। मॉडल एक तर्क प्रक्रिया को जनरेट करने के लिए सीखता है, जिसके बाद अंतिम उत्तर आता है, जो समय के साथ बेहतर होता जाता है।

A: और DeepSeek-R1 में cold-start डेटा? यह कैसे मदद करता है?

B: Cold-start डेटा एक छोटी मात्रा में उच्च गुणवत्ता वाले, लंबे Chain-of-Thought (CoT) उदाहरण प्रदान करता है ताकि RL से पहले आधार मॉडल को फाइन ट्यून किया जा सके। यह पढ़ने की क्षमता को बेहतर बनाने और मॉडल को मानव पसंदों के साथ संरेखित करने में मदद करता है, जिससे तर्क प्रक्रियाएं अधिक स्पष्ट और उपयोगकर्ता अनुकूल बन जाती हैं।

A: वे कैसे सुनिश्चित करते हैं कि मॉडल के जवाब सटीक और अच्छे ढंग से फॉर्मेट किए गए हैं?

B: वे सटीकता इनाम और फॉर्मेट इनाम का एक संयोजन का उपयोग करते हैं। सटीकता इनाम सुनिश्चित करते हैं कि जवाब सही हैं, जबकि फॉर्मेट इनाम मॉडल को अपने सोचने की प्रक्रिया को विशेष टैग के बीच संरचित करने के लिए बाध्य करते हैं। यह एकरूपता और पढ़ने की क्षमता बनाए रखने में मदद करता है।

A: उन्होंने इन मॉडलों का मूल्यांकन करने के लिए कौन से बेंचमार्क का उपयोग किया?

B: उन्होंने मॉडलों का मूल्यांकन विभिन्न बेंचमार्कों पर किया है, जिसमें AIME 2024, MATH-500, GPQA Diamond, Codeforces और अन्य शामिल हैं। ये बेंचमार्क गणित, कोडिंग और सामान्य तर्क कार्य शामिल करते हैं, जिससे मॉडलों की क्षमताओं का व्यापक मूल्यांकन हो सकता है।

A: DeepSeek-R1 का प्रदर्शन OpenAI के o1 श्रृंखला जैसे अन्य मॉडलों के साथ कैसे है?

B: DeepSeek-R1 तर्क कार्य में OpenAI-o1-1217 के प्रदर्शन के समान है। उदाहरण के लिए, यह AIME 2024 पर 79.8% Pass@1 और MATH-500 पर 97.3% स्कोर करता है, कुछ मामलों में OpenAI के मॉडलों को मैच या उससे भी बेहतर कर देता है।

A: यह प्रभावशाली है। और संधारण प्रक्रिया? यह कैसे काम करती है?

B: संधारण में बड़े मॉडलों जैसे DeepSeek-R1 से छोटे और अधिक दक्ष मॉडलों में तर्क करने की क्षमता को स्थानांतरित करना शामिल है। वे DeepSeek-R1 द्वारा जनरेट किए गए डेटा का उपयोग करके खुले स्रोत मॉडलों जैसे Qwen और Llama को फाइन ट्यून करते हैं, जिससे छोटे मॉडल उत्पन्न होते हैं जो अपेक्षाकृत बेहतर प्रदर्शन करते हैं।

A: छोटे मॉडलों पर सीधे RL के मुकाबले संधारण के फायदे क्या हैं?

B: संधारण अधिक अर्थव्यवस्था और प्रभावी है। बड़े पैमाने पर सीधे RL के माध्यम से प्रशिक्षित छोटे मॉडल बड़े मॉडलों से संधारित मॉडलों के समान प्रदर्शन प्राप्त नहीं कर सकते। संधारण बड़े मॉडलों द्वारा खोजे गए उन्नत तर्क पैटर्नों का लाभ उठाता है, जिससे छोटे मॉडलों में बेहतर प्रदर्शन होता है।

A: संधारण प्रोसेस में कोई ट्रेड-ऑफ या सीमाएं हैं?

B: एक सीमा यह है कि संधारित मॉडलों को अपने पूर्ण क्षमता तक पहुंचने के लिए और भी RL की आवश्यकता हो सकती है। जबकि संधारण प्रदर्शन को काफी बेहतर बनाता है, इन मॉडलों पर RL लागू करने से और बेहतर परिणाम मिल सकते हैं। लेकिन इसके लिए अतिरिक्त गणनात्मक संसाधन की आवश्यकता होती है।

A: DeepSeek-R1-Zero में स्व-विकास प्रक्रिया कैसे काम करती है?

B: DeepSeek-R1-Zero में स्व-विकास प्रक्रिया रोचक है। मॉडल लंबे समय तक परीक्षण के दौरान गणना का उपयोग करके स्वतः जटिल तर्क कार्य को हल करने के लिए सीखता है। इससे विकसित व्यवहार जैसे प्रतिबिंब और विकल्पिक समस्या हल करने के तरीके निकलते हैं।

A: आप मॉडल के तर्क करने की क्षमता को समय के साथ कैसे विकसित होने का उदाहरण दे सकते हैं?

B: बिल्कुल! उदाहरण के लिए, मॉडल का औसत जवाब लंबाई समय के साथ बढ़ती है, जो यह दर्शाता है कि यह अपने समाधानों को सोचने और सुधारने में अधिक समय बिताने के लिए सीखता है। इससे AIME 2024 जैसे बेंचमार्क पर बेहतर प्रदर्शन होता है, जहां Pass@1 स्कोर 15.6% से 71.0% तक बढ़ जाता है।

A: पेपर में उल्लिखित 'aha moment' क्या है?

B: 'aha moment' एक ऐसा बिंदु है जब मॉडल अपने समस्या हल करने के प्रारंभिक तरीके को पुनः मूल्यांकन करने के लिए सीखता है, जिससे उसकी तर्क करने की क्षमता में महत्वपूर्ण सुधार होते हैं। यह मॉडल के स्वतः उन्नत समस्या हल करने के रणनीतियों को विकसित करने की क्षमता का प्रमाण है।

A: वे मॉडलों में भाषा मिश्रण की समस्या कैसे हल करते हैं?

B: भाषा मिश्रण को दूर करने के लिए, वे RL प्रशिक्षण के दौरान भाषा एकरूपता इनाम का परिचय देते हैं। यह इनाम मॉडल को मानव पसंदों के साथ संरेखित करता है, जिससे जवाब अधिक पढ़ने योग्य और स्पष्ट बन जाते हैं। हालांकि यह प्रदर्शन को थोड़ा कम करता है, लेकिन यह कुल उपयोगकर्ता अनुभव को बेहतर बनाता है।

A: पेपर में उल्लिखित कुछ असफल प्रयास क्या हैं?

B: उन्होंने प्रक्रिया इनाम मॉडल (PRM) और मॉन्टे कार्लो ट्री सर्च (MCTS) पर प्रयोग किया, लेकिन दोनों तरीकों में चुनौतियां आईं। PRM इनाम हैकिंग और पैमाने की समस्याओं से पीड़ित थी, जबकि MCTS टोकन जनरेशन में अति विशाल खोज स्थान से संघर्ष कर रही थी।

A: DeepSeek-R1 के लिए भविष्य की दिशाएं क्या हैं?

B: वे सामान्य क्षमताओं को बेहतर बनाने, भाषा मिश्रण को दूर करने, प्रोम्प्ट इंजीनियरिंग को बेहतर बनाने और सॉफ्टवेयर इंजीनियरिंग कार्य में प्रदर्शन को बेहतर बनाने का इरादा रखते हैं। वे संधारण की क्षमता को और अधिक जांचने और विभिन्न कार्यों के लिए लंबे CoT का उपयोग करने की संभावना का भी पता लगाने का इरादा रखते हैं।

A: वे सामान्य क्षमताओं को कैसे बेहतर बनाने का इरादा रखते हैं?

B: वे लंबे CoT का उपयोग करके कार्य जैसे फंक्शन कॉल, बहु-टर्न वार्तालाप, जटिल भूमिका निभाना और json आउटपुट को बेहतर बनाने का इरादा रखते हैं। इससे मॉडल अधिक बहुमुखी और अधिक व्यापक कार्य करने में सक्षम बन जाएगा।

A: भाषा मिश्रण की समस्या को कैसे दूर करने का इरादा है?

B: वे मॉडल को कई भाषाओं के लिए ऑप्टिमाइज करने का इरादा रखते हैं, सुनिश्चित करते हुए कि जब अन्य भाषाओं में पूछताछों को संभालते समय, यह तर्क और जवाब के लिए अंग्रेजी पर निर्भर नहीं करता। इससे मॉडल एक वैश्विक दर्शक के लिए अधिक पहुंच्य और उपयोगी बन जाएगा।

A: वे प्रोम्प्ट इंजीनियरिंग को कैसे बेहतर बनाने का इरादा रखते हैं?

B: वे उपयोगकर्ताओं को समस्या को सीधे वर्णित करने और आउटपुट फॉर्मेट को निर्दिष्ट करने के लिए zero-shot सेटिंग का उपयोग करने की सलाह देते हैं। यह तरीका few-shot प्रोम्प्टिंग से अधिक प्रभावी है, जो मॉडल के प्रदर्शन को कम कर सकता है।

A: सॉफ्टवेयर इंजीनियरिंग कार्यों के साथ उन्हें कौन सी चुनौतियां हैं?

B: लंबे मूल्यांकन समय RL प्रक्रिया की दक्षता को प्रभावित करते हैं, जिससे सॉफ्टवेयर इंजीनियरिंग कार्यों में बड़े पैमाने पर RL को व्यापक रूप से लागू करना मुश्किल हो जाता है। वे सॉफ्टवेयर इंजीनियरिंग डेटा पर अस्वीकार करने की नमूना लागू करने या असंयोजित मूल्यांकन को शामिल करने का इरादा रखते हैं ताकि दक्षता को बेहतर बनाया जा सके।

A: वे कैसे सुनिश्चित करते हैं कि मॉडल के जवाब उपयोगी और नुकसानदेह नहीं हैं?

B: वे मॉडल के उपयोगी और नुकसानदेह होने को बेहतर बनाने के लिए एक द्वितीयक पुष्टि सीखने का स्तर लागू करते हैं। इसमें मानव पसंदों के साथ मॉडल को संरेखित करने और संभावित खतरे को कम करने के लिए इनाम सिग्नल और विविध प्रोम्प्ट वितरण का संयोजन शामिल है।

A: LLMs के लिए पुष्टि सीखने में कुछ उभरते हुए प्रवृत्तियां क्या हैं?

B: कुछ उभरते हुए प्रवृत्तियां में उन्नत इनाम मॉडलों का उपयोग, नए RL एल्गोरिथम का पता लगाना और RL को अन्य प्रशिक्षण तकनीकों जैसे संधारण के साथ एकीकृत करना शामिल है। RL को बड़े मॉडलों के लिए अधिक दक्ष और पैमाने पर बनाना भी बढ़ता जा रहा है।

A: वे संधारित मॉडलों का प्रदर्शन अन्य तुलनात्मक मॉडलों के साथ कैसे तुलना करते हैं?

B: वे संधारित मॉडलों को GPT-4o-0513, Claude-3.5-Sonnet-1022 और QwQ-32B-Preview जैसे अन्य मॉडलों के साथ विभिन्न बेंचमार्कों पर तुलना करते हैं। संधारित मॉडल, जैसे DeepSeek-R1-Distill-Qwen-7B, इन सभी मॉडलों को पार कर जाते हैं, जिससे संधारण प्रोसेस की प्रभावशीलता का प्रमाण मिलता है।

A: DeepSeek-R1 पेपर से कुछ प्रमुख निष्कर्ष क्या हैं?

B: प्रमुख निष्कर्षों में RL द्वारा LLMs में तर्क करने की क्षमता को बढ़ाने की क्षमता, संधारण द्वारा इन क्षमताओं को छोटे मॉडलों में स्थानांतरित करने की प्रभावशीलता और भाषा मिश्रण और प्रोम्प्ट संवेदनशीलता जैसे मुद्दों को दूर करने की आवश्यकता शामिल है। पेपर में RL को अधिक दक्ष और पैमाने पर बनाने के लिए और अधिक अनुसंधान की आवश्यकता भी उल्लिखित है।

A: वे कैसे सुनिश्चित करते हैं कि मॉडल के जवाब सटीक और अच्छे ढंग से फॉर्मेट किए गए हैं?

B: वे सटीकता इनाम और फॉर्मेट इनाम का एक संयोजन का उपयोग करते हैं। सटीकता इनाम सुनिश्चित करते हैं कि जवाब सही हैं, जबकि फॉर्मेट इनाम मॉडल को अपने सोचने की प्रक्रिया को विशेष टैग के बीच संरचित करने के लिए बाध्य करते हैं। यह एकरूपता और पढ़ने की क्षमता बनाए रखने में मदद करता है।

A: उन्होंने इन मॉडलों का मूल्यांकन करने के लिए कौन से बेंचमार्क का उपयोग किया?

B: उन्होंने मॉडलों का मूल्यांकन विभिन्न बेंचमार्कों पर किया है, जिसमें AIME 2024, MATH-500, GPQA Diamond, Codeforces और अन्य शामिल हैं। ये बेंचमार्क गणित, कोडिंग और सामान्य तर्क कार्य शामिल करते हैं, जिससे मॉडलों की क्षमताओं का व्यापक मूल्यांकन हो सकता है।

A: DeepSeek-R1 का प्रदर्शन OpenAI के o1 श्रृंखला जैसे अन्य मॉडलों के साथ कैसे है?

B: DeepSeek-R1 तर्क कार्य में OpenAI-o1-1217 के प्रदर्शन के समान है। उदाहरण के लिए, यह AIME 2024 पर 79.8% Pass@1 और MATH-500 पर 97.3% स्कोर करता है, कुछ मामलों में OpenAI के मॉडलों को मैच या उससे भी बेहतर कर देता है।

A: यह प्रभावशाली है। और संधारण प्रक्रिया? यह कैसे काम करती है?

B: संधारण में बड़े मॉडलों जैसे DeepSeek-R1 से छोटे और अधिक दक्ष मॉडलों में तर्क करने की क्षमता को स्थानांतरित करना शामिल है। वे DeepSeek-R1 द्वारा जनरेट किए गए डेटा का उपयोग करके खुले स्रोत मॉडलों जैसे Qwen और Llama को फाइन ट्यून करते हैं, जिससे छोटे मॉडल उत्पन्न होते हैं जो अपेक्षाकृत बेहतर प्रदर्शन करते हैं।

A: छोटे मॉडलों पर सीधे RL के मुकाबले संधारण के फायदे क्या हैं?

B: संधारण अधिक अर्थव्यवस्था और प्रभावी है। बड़े पैमाने पर सीधे RL के माध्यम से प्रशिक्षित छोटे मॉडल बड़े मॉडलों से संधारित मॉडलों के समान प्रदर्शन प्राप्त नहीं कर सकते। संधारण बड़े मॉडलों द्वारा खोजे गए उन्नत तर्क पैटर्नों का लाभ उठाता है, जिससे छोटे मॉडलों में बेहतर प्रदर्शन होता है।

A: संधारण प्रोसेस में कोई ट्रेड-ऑफ या सीमाएं हैं?

B: एक सीमा यह है कि संधारित मॉडलों को अपने पूर्ण क्षमता तक पहुंचने के लिए और भी RL की आवश्यकता हो सकती है। जबकि संधारण प्रदर्शन को काफी बेहतर बनाता है, इन मॉडलों पर RL लागू करने से और बेहतर परिणाम मिल सकते हैं। लेकिन इसके लिए अतिरिक्त गणनात्मक संसाधन की आवश्यकता होती है।

A: DeepSeek-R1-Zero में स्व-विकास प्रक्रिया कैसे काम करती है?

B: DeepSeek-R1-Zero में स्व-विकास प्रक्रिया रोचक है। मॉडल लंबे समय तक परीक्षण के दौरान गणना का उपयोग करके स्वतः जटिल तर्क कार्य को हल करने के लिए सीखता है। इससे विकसित व्यवहार जैसे प्रतिबिंब और विकल्पिक समस्या हल करने के तरीके निकलते हैं।

A: आप मॉडल के तर्क करने की क्षमता को समय के साथ कैसे विकसित होने का उदाहरण दे सकते हैं?

B: बिल्कुल! उदाहरण के लिए, मॉडल का औसत जवाब लंबाई समय के साथ बढ़ती है, जो यह दर्शाता है कि यह अपने समाधानों को सोचने और सुधारने में अधिक समय बिताने के लिए सीखता है। इससे AIME 2024 जैसे बेंचमार्क पर बेहतर प्रदर्शन होता है, जहां Pass@1 स्कोर 15.6% से 71.0% तक बढ़ जाता है।

A: पेपर में उल्लिखित 'aha moment' क्या है?

B: 'aha moment' एक ऐसा बिंदु है जब मॉडल अपने समस्या हल करने के प्रारंभिक तरीके को पुनः मूल्यांकन करने के लिए सीखता है, जिससे उसकी तर्क करने की क्षमता में महत्वपूर्ण सुधार होते हैं। यह मॉडल के स्वतः उन्नत समस्या हल करने के रणनीतियों को विकसित करने की क्षमता का प्रमाण है।

A: वे मॉडलों में भाषा मिश्रण की समस्या कैसे हल करते हैं?

B: भाषा मिश्रण को दूर करने के लिए, वे RL प्रशिक्षण के दौरान भाषा एकरूपता इनाम का परिचय देते हैं। यह इनाम मॉडल को मानव पसंदों के साथ संरेखित करता है, जिससे जवाब अधिक पढ़ने योग्य और स्पष्ट बन जाते हैं। हालांकि यह प्रदर्शन को थोड़ा कम करता है, लेकिन यह कुल उपयोगकर्ता अनुभव को बेहतर बनाता है।

A: पेपर में उल्लिखित कुछ असफल प्रयास क्या हैं?

B: उन्होंने प्रक्रिया इनाम मॉडल (PRM) और मॉन्टे कार्लो ट्री सर्च (MCTS) पर प्रयोग किया, लेकिन दोनों तरीकों में चुनौतियां आईं। PRM इनाम हैकिंग और पैमाने की समस्याओं से पीड़ित थी, जबकि MCTS टोकन जनरेशन में अति विशाल खोज स्थान से संघर्ष कर रही थी।

A: DeepSeek-R1 के लिए भविष्य की दिशाएं क्या हैं?

B: वे सामान्य क्षमताओं को बेहतर बनाने, भाषा मिश्रण को दूर करने, प्रोम्प्ट इंजीनियरिंग को बेहतर बनाने और सॉफ्टवेयर इंजीनियरिंग कार्य में प्रदर्शन को बेहतर बनाने का इरादा रखते हैं। वे संधारण की क्षमता को और अधिक जांचने और विभिन्न कार्यों के लिए लंबे CoT का उपयोग करने की संभावना का भी पता लगाने का इरादा रखते हैं।

A: वे सामान्य क्षमताओं को कैसे बेहतर बनाने का इरादा रखते हैं?

B: वे लंबे CoT का उपयोग करके कार्य जैसे फंक्शन कॉल, बहु-टर्न वार्तालाप, जटिल भूमिका निभाना और json आउटपुट को बेहतर बनाने का इरादा रखते हैं। इससे मॉडल अधिक बहुमुखी और अधिक व्यापक कार्य करने में सक्षम बन जाएगा।

A: भाषा मिश्रण की समस्या को कैसे दूर करने का इरादा है?

B: वे मॉडल को कई भाषाओं के लिए ऑप्टिमाइज करने का इरादा रखते हैं, सुनिश्चित करते हुए कि जब अन्य भाषाओं में पूछताछों को संभालते समय, यह तर्क और जवाब के लिए अंग्रेजी पर निर्भर नहीं करता। इससे मॉडल एक वैश्विक दर्शक के लिए अधिक पहुंच्य और उपयोगी बन जाएगा।

A: वे प्रोम्प्ट इंजीनियरिंग को कैसे बेहतर बनाने का इरादा रखते हैं?

B: वे उपयोगकर्ताओं को समस्या को सीधे वर्णित करने और आउटपुट फॉर्मेट को निर्दिष्ट करने के लिए zero-shot सेटिंग का उपयोग करने की सलाह देते हैं। यह तरीका few-shot प्रोम्प्टिंग से अधिक प्रभावी है, जो मॉडल के प्रदर्शन को कम कर सकता है।

A: सॉफ्टवेयर इंजीनियरिंग कार्यों के साथ उन्हें कौन सी चुनौतियां हैं?

B: लंबे मूल्यांकन समय RL प्रक्रिया की दक्षता को प्रभावित करते हैं, जिससे सॉफ्टवेयर इंजीनियरिंग कार्यों में बड़े पैमाने पर RL को व्यापक रूप से लागू करना मुश्किल हो जाता है। वे सॉफ्टवेयर इंजीनियरिंग डेटा पर अस्वीकार करने की नमूना लागू करने या असंयोजित मूल्यांकन को शामिल करने का इरादा रखते हैं ताकि दक्षता को बेहतर बनाया जा सके।

A: वे कैसे सुनिश्चित करते हैं कि मॉडल के जवाब उपयोगी और नुकसानदेह नहीं हैं?

B: वे मॉडल के उपयोगी और नुकसानदेह होने को बेहतर बनाने के लिए एक द्वितीयक पुष्टि सीखने का स्तर लागू करते हैं। इसमें मानव पसंदों के साथ मॉडल को संरेखित करने और संभावित खतरे को कम करने के लिए इनाम सिग्नल और विविध प्रोम्प्ट वितरण का संयोजन शामिल है।