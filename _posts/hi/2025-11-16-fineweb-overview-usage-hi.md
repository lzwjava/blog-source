---
audio: false
generated: true
image: false
lang: hi
layout: post
model: x-ai/grok-4-fast
title: फाइनवेब डेटासेट अवलोकन और उपयोग
translated: true
type: note
---

### फाइनवेब डेटासेट का अवलोकन
फाइनवेब डेटासेट हगिंग फेस द्वारा विकसित एक बड़े पैमाने का, उच्च-गुणवत्ता वाला अंग्रेजी वेब टेक्स्ट कॉर्पस है, जो कॉमनक्रॉल स्नैपशॉट (2013–2024) से लिया गया है। फ़िल्टरिंग और डीडुप्लिकेशन के बाद इसमें 15 ट्रिलियन से अधिक टोकन हैं, जो इसे बड़े भाषा मॉडल (एलएलएम) के प्रीट्रेनिंग के लिए उपयुक्त बनाता है। इसे ओपन डेटा कॉमन्स अट्रिब्यूशन लाइसेंस (ODC-By) के तहत जारी किया गया है और हगिंग फेस डेटासेट्स पर होस्ट किया गया है।

इसके वेरिएंट हैं जैसे फाइनवेब-एडु (शैक्षिक सामग्री के लिए फ़िल्टर किया गया) और फाइनवेब2 (बहुभाषी विस्तार)। एलएलएम ट्रेनिंग के लिए, कोर `HuggingFaceFW/fineweb` शुरुआती बिंदु है।

### आवश्यक शर्तें
- **पायथन वातावरण**: पायथन 3.8+ के साथ हगिंग फेस की `datasets` लाइब्रेरी।
- **संग्रहण**: पूरा डेटासेट बहुत बड़ा है (~16TB संपीड़ित)। ट्रेनिंग के दौरान ऑन-द-फ्लाई प्रोसेसिंग के लिए स्ट्रीमिंग का उपयोग करें।
- **गति के लिए वैकल्पिक**: एचएफ ट्रांसफर सपोर्ट के साथ `huggingface_hub` इंस्टॉल करें:  
  ```
  pip install huggingface_hub[hf_transfer]
  ```
  फिर पर्यावरण चर सेट करें:  
  ```
  export HF_HUB_ENABLE_HF_TRANSFER=1
  ```
- **हगिंग फेस अकाउंट**: वैकल्पिक लेकिन गेटेड एक्सेस या तेज डाउनलोड के लिए अनुशंसित (एक मुफ्त खाता बनाएं और `huggingface-cli login` के माध्यम से लॉग इन करें)।

### डेटासेट को कैसे लोड करें
इसे सीधे एक्सेस करने के लिए `datasets` लाइब्रेरी का उपयोग करें। यहां कोड उदाहरणों के साथ एक चरण-दर-चरण मार्गदर्शिका दी गई है।

#### 1. निर्भरताएँ इंस्टॉल करें
```bash
pip install datasets
```

#### 2. पूर्ण डेटासेट लोड करें (ट्रेनिंग के लिए स्ट्रीमिंग मोड)
स्ट्रीमिंग पूरे डेटासेट को पहले से डाउनलोड किए बिना डेटा को बैचों में प्रदान करती है—सीमित संग्रहण पर ट्रेनिंग के लिए आदर्श।

```python
from datasets import load_dataset

# पूरे फाइनवेब डेटासेट को स्ट्रीमिंग मोड में लोड करें
dataset = load_dataset("HuggingFaceFW/fineweb", split="train", streaming=True)

# उदाहरण: पहले कुछ उदाहरणों पर पुनरावृति करें
for example in dataset.take(5):
    print(example)  # प्रत्येक उदाहरण में 'text', 'url', 'date' आदि फ़ील्ड होते हैं।
```

- **स्प्लिट्स**: मुख्य रूप से `train` (सभी डेटा)। अलग-अलग कॉमनक्रॉल डंप कॉन्फ़िग के रूप में उपलब्ध हैं जैसे `CC-MAIN-2015-11` (`load_dataset("HuggingFaceFW/fineweb", name="CC-MAIN-2015-11", split="train")` के माध्यम से लोड करें)।
- **डेटा फॉर्मेट**: `text` (साफ़ की गई सामग्री), `url`, `date`, `quality_score` आदि कॉलम वाली पार्केट फाइलें। टेक्स्ट टोकनाइजेशन-तैयार है।

#### 3. एक सबसेट या विशिष्ट कॉन्फ़िग लोड करें
परीक्षण या छोटे पैमाने की ट्रेनिंग के लिए:
```python
# एक विशिष्ट कॉमनक्रॉल डंप लोड करें (उदा., 2023 डेटा)
dataset = load_dataset("HuggingFaceFW/fineweb", name="CC-MAIN-2023-50", split="train")

# या शैक्षिक सबसेट लोड करें (फाइनवेब-एडु, ~0.5T टोकन)
edu_dataset = load_dataset("HuggingFaceFW/fineweb-edu", split="train", streaming=True)
```

#### 4. ट्रेनिंग पाइपलाइन के साथ एकीकृत करें
एलएलएम ट्रेनिंग (उदा., ट्रांसफॉर्मर्स या कस्टम लूप्स के साथ) के लिए, अपने डेटा लोडर में सीधे स्ट्रीमिंग इटरेटर का उपयोग करें:
```python
from transformers import DataCollatorForLanguageModeling, Trainer, TrainingArguments

# मान लें कि आपके पास एक टोकनाइज़र और मॉडल है
tokenizer = ...  # उदा., AutoTokenizer.from_pretrained("gpt2")

def tokenize_function(examples):
    return tokenizer(examples["text"], truncation=True, max_length=512)

# ऑन-द-फ्लाई टोकनाइज़ करें (दक्षता के लिए batched=True के साथ मैप में)
tokenized_dataset = dataset.map(tokenize_function, batched=True, remove_columns=dataset.column_names)

# ट्रेनर या कस्टम लूप के लिए आगे बढ़ें
data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)
# ... (tokenized_dataset के साथ Trainer सेट अप करें)
```

- **दक्षता टिप**: `.map()` में `batched=True` के साथ बैचों में प्रोसेस करें। वितरित ट्रेनिंग के लिए, हगिंग फेस एक्सेलेरेट का उपयोग करें।

#### 5. पूर्ण डेटासेट डाउनलोड करें (नॉन-स्ट्रीमिंग)
यदि आपको स्थानीय संग्रहण की आवश्यकता है (पूर्ण आकार के लिए अनुशंसित नहीं):
```python
dataset = load_dataset("HuggingFaceFW/fineweb", split="train")  # ~/.cache/huggingface/datasets पर डाउनलोड होता है
```
बाद में `dataset.save_to_disk("/path/to/save")` का उपयोग करें।

### ट्रेनिंग के लिए अतिरिक्त सुझाव
- **डीडुप्लिकेशन**: फाइनवेब पहले से ही डीडुप्लिकेटेड है, लेकिन यदि आवश्यक हो तो अपसैंपलिंग के लिए `minhash_cluster_size` जैसे मेटाडेटा की जांच करें।
- **गुणवत्ता फ़िल्टरिंग**: उच्च-गुणवत्ता वाले नमूनों को सबसैंपल करने के लिए अंतर्निहित `quality_score` (>0.5 थ्रेशोल्ड) का उपयोग करें।
- **वेरिएंट**:
  - **फाइनवेब-एडु**: शैक्षिक-केंद्रित ट्रेनिंग के लिए। ऊपर बताए अनुसार लोड करें।
  - **फाइनवेब2**: बहुभाषी डेटा (1000+ भाषाएँ) के लिए। `load_dataset("HuggingFaceFW/fineweb-2", name="eng_Latn", split="train", streaming=True)` के माध्यम से लोड करें।
- **मूल्यांकन**: हगिंग फेस एब्लेशन मॉडल और बेंचमार्क प्रदान करता है; तुलना करने के लिए सबसेट पर छोटे मॉडल (उदा., 1.8B पैरामीटर) को प्रशिक्षित करें।
- **संसाधन**: पूर्ण विवरण [रिसर्च पेपर](https://arxiv.org/abs/2402.06645) में उपलब्ध है। कस्टम फ़िल्टरिंग के लिए कोड [GitHub](https://github.com/huggingface/fineweb) पर है।

समस्याओं के लिए, [हगिंग फेस डेटासेट्स डॉक्स](https://huggingface.co/docs/datasets/loading) देखें। यदि आप किसी विशिष्ट फ्रेमवर्क (उदा., PyTorch Lightning) का उपयोग कर रहे हैं, तो इटरेटर को उसी के अनुसार एडाप्ट करें।