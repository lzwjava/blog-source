---
audio: false
generated: true
lang: hi
layout: post
title: कनव्होल्यूशनल न्यूरल नेटवर्क
translated: true
type: note
---

### सीएनएन (कन्व्होल्यूशनल न्यूरल नेटवर्क) क्या है?

**कन्व्होल्यूशनल न्यूरल नेटवर्क (CNN)** एक प्रकार का डीप न्यूरल नेटवर्क है जिसका प्रयोग मुख्य रूप से छवि-संबंधित कार्यों जैसे वर्गीकरण, ऑब्जेक्ट डिटेक्शन और सेगमेंटेशन के लिए किया जाता है। सीएनएन को इनपुट छवियों से स्थानिक विशेषताओं के पदानुक्रम को स्वचालित रूप से और अनुकूली तरीके से सीखने के लिए डिज़ाइन किया गया है। पारंपरिक पूर्ण रूप से जुड़े हुए लेयर्स की तुलना में, ये स्थानीय निर्भरताओं को पकड़ने और पैरामीटरों की संख्या को कम करने में अत्यधिक प्रभावी हैं।

### सीएनएन के प्रमुख घटक:
1. **कन्व्होल्यूशनल लेयर**:  
   यह लेयर इनपुट डेटा पर कन्व्होल्यूशन ऑपरेशन लागू करती है, जो इनपुट छवि से विशेषताएँ (जैसे किनारे, टेक्सचर, पैटर्न) निकालने में मदद करती है। कन्व्होल्यूशन ऑपरेशन फ़िल्टर (जिन्हें कर्नेल भी कहा जाता है) का उपयोग करता है जो इनपुट छवि पर स्लाइड करते हैं।

2. **पूलिंग लेयर**:  
   पूलिंग लेयर्स का उपयोग फीचर मैप्स को डाउनसैंपल करने के लिए किया जाता है, जिससे उनके स्थानिक आयाम कम हो जाते हैं और नेटवर्क अधिक कम्प्यूटेशनली कुशल बनता है, साथ ही यह ट्रांसलेशन इनवेरिएंस (छवि में ऑब्जेक्ट्स के घूम जाने पर भी उन्हें पहचानने की क्षमता) में भी मदद करता है।

3. **पूर्ण रूप से जुड़ी हुई लेयर**:  
   कन्व्होल्यूशन और पूलिंग लेयर्स के बाद, पिछली लेयर्स से निकाली गई विशेषताओं को वर्गीकृत करने के लिए पूर्ण रूप से जुड़ी हुई लेयर्स का उपयोग किया जाता है। अंतिम आउटपुट लेयर आमतौर पर वर्गीकरण कार्यों के लिए सॉफ्टमैक्स या सिग्मॉइड एक्टिवेशन फंक्शन का उपयोग करती है।

4. **एक्टिवेशन फंक्शन (ReLU)**:  
   प्रत्येक कन्व्होल्यूशन या पूर्ण रूप से जुड़ी हुई लेयर के बाद, **ReLU** (Rectified Linear Unit) जैसे एक्टिवेशन फंक्शन का उपयोग अक्सर मॉडल में गैर-रैखिकता लाने के लिए किया जाता है, जिससे यह अधिक जटिल पैटर्न सीख सकता है।

### सीएनएन आर्किटेक्चर उदाहरण:
- **इनपुट लेयर**: एक छवि या छवियों का बैच।
- **कन्व्होल्यूशनल लेयर 1**: कन्व्होल्यूशनल फ़िल्टर (कर्नेल) के एक सेट को लागू करना।
- **ReLU एक्टिवेशन**: गैर-रैखिकता लाने के लिए ReLU लागू करना।
- **पूलिंग लेयर 1**: मैक्स पूलिंग या एवरेज पूलिंग।
- **कन्व्होल्यूशनल लेयर 2**: अतिरिक्त कन्व्होल्यूशन लागू करना।
- **पूर्ण रूप से जुड़ी हुई लेयर**: आउटपुट को समतल करना और वर्गीकरण के लिए इसे पूर्ण रूप से जुड़ी हुई लेयर्स में फीड करना।
- **आउटपुट लेयर**: अंतिम वर्गीकरण परिणाम के लिए सॉफ्टमैक्स या सिग्मॉइड एक्टिवेशन।

---

### सीएनएन का स्क्रैच से कार्यान्वयन (TensorFlow/PyTorch जैसे फ्रेमवर्क का उपयोग किए बिना)

यहाँ **NumPy** का उपयोग करके एक सीएनएन का एक सरल कार्यान्वयन दिया गया है। यह आपको एक सीएनएन में ऑपरेशन (कन्व्होल्यूशन, ReLU, पूलिंग, आदि) के काम करने का एक बुनियादी विचार देगा।

हम एक बुनियादी सीएनएन को लागू करेंगे जिसमें शामिल है:
1. एक कन्व्होल्यूशन लेयर
2. एक ReLU एक्टिवेशन लेयर
3. एक पूलिंग लेयर
4. एक पूर्ण रूप से जुड़ी हुई लेयर

हम एक सीएनएन के बहुत सरलीकृत संस्करण पर ध्यान केंद्रित करेंगे, जहाँ हमारे पास बैच नॉर्मलाइजेशन, ड्रॉपआउट, आदि जैसी उन्नत सुविधाएँ नहीं हैं।

### चरण 1: कन्व्होल्यूशन लेयर

हम **कन्व्होल्यूशन** ऑपरेशन को लागू करेंगे, जिसमें इनपुट छवि पर एक फ़िल्टर (कर्नेल) को स्लाइड करना शामिल है।

```python
import numpy as np

def convolve2d(input_image, kernel):
    kernel_height, kernel_width = kernel.shape
    image_height, image_width = input_image.shape
    
    # कन्व्होल्यूशन के बाद आउटपुट आयाम
    output_height = image_height - kernel_height + 1
    output_width = image_width - kernel_width + 1
    
    output = np.zeros((output_height, output_width))
    
    # इनपुट छवि पर कर्नेल को स्लाइड करना
    for i in range(output_height):
        for j in range(output_width):
            region = input_image[i:i+kernel_height, j:j+kernel_width]
            output[i, j] = np.sum(region * kernel)  # एलिमेंट-वाइज़ गुणा और योग
    return output
```

### चरण 2: ReLU एक्टिवेशन

ReLU को कन्व्होल्यूशन आउटपुट पर एलिमेंट-वाइज़ लागू किया जाता है।

```python
def relu(input_image):
    return np.maximum(0, input_image)  # ReLU ऑपरेशन
```

### चरण 3: पूलिंग लेयर (मैक्स पूलिंग)

हम 2x2 विंडो और स्ट्राइड 2 के साथ एक साधारण **मैक्स पूलिंग** लेयर लागू करेंगे।

```python
def max_pooling(input_image, pool_size=2, stride=2):
    image_height, image_width = input_image.shape
    output_height = (image_height - pool_size) // stride + 1
    output_width = (image_width - pool_size) // stride + 1
    
    output = np.zeros((output_height, output_width))
    
    # मैक्स पूलिंग लागू करना
    for i in range(0, image_height - pool_size + 1, stride):
        for j in range(0, image_width - pool_size + 1, stride):
            region = input_image[i:i+pool_size, j:j+pool_size]
            output[i // stride, j // stride] = np.max(region)
    
    return output
```

### चरण 4: पूर्ण रूप से जुड़ी हुई लेयर

पूर्ण रूप से जुड़ी हुई लेयर केवल एक डेंस लेयर है जो पिछली लेयर्स के आउटपुट को लेती है और एक भारित योग की गणना करती है।

```python
def fully_connected(input_image, weights, bias):
    # इनपुट छवि को समतल करना (यदि यह बहु-आयामी है)
    flattened_input = input_image.flatten()
    
    # पूर्ण रूप से जुड़ी हुई लेयर के आउटपुट की गणना करना
    output = np.dot(flattened_input, weights) + bias
    return output
```

### चरण 5: सभी को एक साथ रखना

अब, आइए एक सरल उदाहरण को परिभाषित करें जहाँ हम एक सीएनएन बनाएंगे जो एक छवि लेता है, उस पर कन्व्होल्यूशन, ReLU, पूलिंग लागू करता है, और फिर एक पूर्ण रूप से जुड़ी हुई लेयर का उपयोग करके एक पूर्वानुमान लगाता है।

```python
# उदाहरण छवि (5x5)
image = np.array([[1, 2, 3, 0, 1],
                  [4, 5, 6, 1, 2],
                  [7, 8, 9, 2, 3],
                  [1, 2, 3, 4, 5],
                  [6, 7, 8, 5, 6]])

# एक साधारण कर्नेल (3x3) को परिभाषित करें
kernel = np.array([[1, 0, -1],
                   [1, 0, -1],
                   [1, 0, -1]])

# कन्व्होल्यूशन ऑपरेशन
conv_output = convolve2d(image, kernel)
print("Convolution Output:")
print(conv_output)

# ReLU एक्टिवेशन लागू करें
relu_output = relu(conv_output)
print("ReLU Output:")
print(relu_output)

# मैक्स पूलिंग लागू करें
pool_output = max_pooling(relu_output)
print("Max Pooling Output:")
print(pool_output)

# पूर्ण रूप से जुड़ी हुई लेयर (समतल आउटपुट, 1D वज़न और बायस)
weights = np.random.randn(pool_output.size)  # रैंडम वज़न
bias = np.random.randn()  # रैंडम बायस
fc_output = fully_connected(pool_output, weights, bias)
print("Fully Connected Output:")
print(fc_output)
```

### प्रक्रिया की व्याख्या:

1. **इनपुट छवि**: हम एक साधारण 5x5 छवि बनाते हैं।
2. **कन्व्होल्यूशन**: हम छवि पर एक 3x3 फ़िल्टर लागू करते हैं, जिसके परिणामस्वरूप एक छोटा आउटपुट प्राप्त होता है।
3. **ReLU**: हम गैर-रैखिकता लाने के लिए ReLU फंक्शन लागू करते हैं।
4. **पूलिंग**: हम 2x2 विंडो और स्ट्राइड 2 का उपयोग करके मैक्स पूलिंग के साथ छवि को डाउनसैंपल करते हैं।
5. **पूर्ण रूप से जुड़ा हुआ**: हम पूल्ड आउटपुट को समतल करते हैं और रैंडम वज़न और बायस के साथ एक पूर्ण रूप से जुड़ी हुई लेयर लागू करते हैं।

यह एक बहुत ही सरलीकृत सीएनएन है, लेकिन इससे आपको यह अंदाजा मिलना चाहिए कि कन्व्होल्यूशन, ReLU, पूलिंग और पूर्ण रूप से जुड़ी हुई लेयर्स कैसे काम करती हैं।

### निष्कर्ष

यह कार्यान्वयन यह समझने के लिए एक बुनियादी ढाँचा प्रदान करता है कि एक सीएनएन निम्न स्तर पर कैसे काम करता है। व्यवहार में, **TensorFlow**, **Keras**, और **PyTor** जैसे फ्रेमवर्क कई ऑप्टिमाइजेशन, दक्षता सुधार और अतिरिक्त सुविधाओं (जैसे बैच नॉर्मलाइजेशन, ड्रॉपआउट) को संभालते हैं जो सीएनएन को अधिक शक्तिशाली और बड़े डेटासेट पर प्रशिक्षित करने में आसान बनाते हैं।

यदि आप इसे विस्तारित करने के तरीके पर अधिक विवरण चाहते हैं या सीएनएन के किसी अन्य भाग में सहायता चाहते हैं तो मुझे बताएं!