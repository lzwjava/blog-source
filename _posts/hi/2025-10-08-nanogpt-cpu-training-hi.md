---
audio: false
generated: true
image: false
lang: hi
layout: post
model: x-ai/grok-4-fast
title: नैनोजीपीटी सीपीयू प्रशिक्षण विन्यास की व्याख्या
translated: true
type: note
---

### कमांड विवरण

यह पूर्ण कमांड Python 3.11 का उपयोग करके nanoGPT रिपॉजिटरी से `train.py` स्क्रिप्ट चलाती है, शेक्सपियर कैरेक्टर-लेवल डेटासेट कॉन्फ़िगरेशन से डिफ़ॉल्ट लोड करती है, और एक छोटे, CPU-आधारित ट्रेनिंग रन के लिए विशिष्ट हाइपरपैरामीटर्स को ओवरराइड करती है। नीचे, मैं प्रत्येक घटक और पैरामीटर को चरण दर चरण समझाऊंगा।

- **python3.11**: स्क्रिप्ट को निष्पादित करने के लिए Python इंटरप्रेटर वर्जन (3.11) निर्दिष्ट करता है। nanoGPT को Python 3.8+ की आवश्यकता होती है; यह हाल की सुविधाओं के साथ संगतता सुनिश्चित करता है।

- **train.py**: nanoGPT में मुख्य ट्रेनिंग स्क्रिप्ट। यह डेटा लोडिंग, मॉडल इनिशियलाइज़ेशन, ट्रेनिंग लूप (फॉरवर्ड/बैकवर्ड पास, ऑप्टिमाइज़ेशन), मूल्यांकन, लॉगिंग और चेकपॉइंटिंग को संभालती है।

- **config/train_shakespeare_char.py**: एक कॉन्फ़िगरेशन फ़ाइल जो डेटासेट-विशिष्ट डिफ़ॉल्ट सेट करती है (जैसे, `dataset = 'shakespeare_char'`, `vocab_size = 65`, प्रारंभिक लर्निंग रेट, आदि)। यह टास्क को परिभाषित करती है: शेक्सपियर के कार्यों से कैरेक्टर-लेवल टेक्स्ट पर प्रशिक्षण। इसके बाद के सभी `--` फ्लैग इस कॉन्फ़िग से मूल्यों को ओवरराइड करते हैं।

#### ओवरराइड पैरामीटर्स
ये कमांड-लाइन फ्लैग हैं जो argparse के माध्यम से `train.py` को पास किए जाते हैं, जो फाइलों को एडिट किए बिना कस्टमाइज़ेशन की अनुमति देते हैं। ये हार्डवेयर, ट्रेनिंग व्यवहार, मॉडल आर्किटेक्चर और रेगुलराइज़ेशन को नियंत्रित करते हैं।

| पैरामीटर | मान | स्पष्टीकरण |
|-----------|-------|-------------|
| `--device` | `cpu` | कंप्यूट डिवाइस निर्दिष्ट करता है: `'cpu'` सब कुछ होस्ट CPU पर चलाता है (धीमा लेकिन GPU की आवश्यकता नहीं)। यदि GPU उपलब्ध हो तो डिफ़ॉल्ट रूप से `'cuda'` पर सेट होता है। टेस्टिंग या लो-रिसोर्स सेटअप के लिए उपयोगी। |
| `--compile` | `False` | मॉडल पर PyTorch के `torch.compile()` ऑप्टिमाइज़ेशन को सक्षम/अक्षम करता है (तेज एक्सेक्यूशन के लिए ग्राफ़ कंपाइलेशन के माध्यम से PyTorch 2.0 में पेश किया गया)। संगतता समस्याओं (जैसे, पुराने हार्डवेयर या गैर-CUDA डिवाइस पर) से बचने के लिए `False` पर सेट करें। डिफ़ॉल्ट रूप से `True` पर सेट होता है। |
| `--eval_iters` | `20` | मूल्यांकन के दौरान वैलिडेशन लॉस का अनुमान लगाने के लिए चलने वाले फॉरवर्ड पास (पुनरावृत्तियों) की संख्या। अधिक मान अधिक सटीक अनुमान देते हैं लेकिन अधिक समय लेते हैं। डिफ़ॉल्ट 200 है; यहां इसे त्वरित जांच के लिए कम किया गया है। |
| `--log_interval` | `1` | वह आवृत्ति (पुनरावृत्तियों में) जिस पर ट्रेनिंग लॉस को कंसोल पर प्रिंट किया जाता है। हर चरण में वर्बोज़ आउटपुट के लिए 1 पर सेट करें; कम शोर के लिए डिफ़ॉल्ट 10 है। |
| `--block_size` | `64` | अधिकतम कॉन्टेक्स्ट लंबाई (अनुक्रम लंबाई) जिसे मॉडल एक बार में प्रोसेस कर सकता है। मेमोरी उपयोग और मॉडल द्वारा "याद" किए गए इतिहास की मात्रा को प्रभावित करता है। कॉन्फ़िग में डिफ़ॉल्ट 256 है; 64 सीमित हार्डवेयर पर तेजी से प्रशिक्षण के लिए छोटा है। |
| `--batch_size` | `12` | प्रति ट्रेनिंग चरण समानांतर में प्रोसेस किए गए अनुक्रमों की संख्या (बैच आकार)। बड़े बैच अधिक मेमोरी का उपयोग करते हैं लेकिन बेहतर GPU/CPU उपयोग के माध्यम से प्रशिक्षण को गति दे सकते हैं। डिफ़ॉल्ट 64 है; 12 CPU के लिए कम किया गया है। |
| `--n_layer` | `4` | ट्रांसफॉर्मर डिकोडर परतों की संख्या (नेटवर्क की गहराई)। अधिक परतें क्षमता बढ़ाती हैं लेकिन ओवरफिटिंग का जोखिम बढ़ाती हैं और अधिक कंप्यूट की मांग करती हैं। डिफ़ॉल्ट 6 है; 4 एक छोटा मॉडल बनाता है। |
| `--n_head` | `4` | प्रति परत मल्टी-हेड अटेंशन हेड्स की संख्या। अटेंशन कंप्यूटेशन में समानता को नियंत्रित करता है; `n_embd` में समान रूप से विभाजित होना चाहिए। डिफ़ॉल्ट 6 है; 4 जटिलता कम करता है। |
| `--n_embd` | `128` | मॉडल की एम्बेडिंग और हिडन स्टेट्स का आयाम (मॉडल की चौड़ाई)। बड़े मान अभिव्यक्ति बढ़ाते हैं लेकिन मेमोरी/कंप्यूट आवश्यकताएं भी बढ़ाते हैं। डिफ़ॉल्ट 384 है; 128 एक बहुत छोटा मॉडल बनाता है (~0.5M पैरामीटर्स)। |
| `--max_iters` | `2000` | प्रशिक्षण पुनरावृत्तियों (चरणों) की कुल संख्या। प्रत्येक पुनरावृत्ति एक बैच को प्रोसेस करती है। जल्दी रोके जाने तक प्रशिक्षण इतने चरणों के बाद रुक जाता है। डिफ़ॉल्ट 5000 है; 2000 प्रयोग के लिए रन को छोटा करता है। |
| `--lr_decay_iters` | `2000` | वह संख्या जिसके ऊपर लर्निंग रेट (LR) रैखिक रूप से शून्य तक कम हो जाती है (कोसाइन या रैखिक क्षय अनुसूची)। यहां अंत तक पूर्ण क्षय के लिए `max_iters` से मेल खाता है। डिफ़ॉल्ट 5000 है; क्षय को कुल चरणों से जोड़ता है। |
| `--dropout` | `0.0` | रेगुलराइज़ेशन के लिए ड्रॉपआउट संभावना (प्रशिक्षण के दौरान यादृच्छिक रूप से शून्य किए गए यूनिट्स का अंश)। 0.0 इसे पूरी तरह से अक्षम कर देता है (कोई रेगुलराइज़ेशन नहीं), छोटे डेटासेट/मॉडल के लिए उपयोगी। डिफ़ॉल्ट 0.2 है। |

यह सेटअप एक छोटे GPT-2 जैसे मॉडल (4 परतें, 128-आयाम) को शेक्सपियर टेक्स्ट पर 2000 चरणों के लिए CPU पर प्रशिक्षित करता है, जिसमें लगातार लॉगिंग और बिना ड्रॉपआउट के होता है—त्वरित प्रोटोटाइपिंग या डीबगिंग के लिए आदर्श।

[nanoGPT रिपॉजिटरी](https://github.com/karpathy/nanoGPT)