---
audio: false
generated: true
lang: hi
layout: post
title: सर्वर-साइड एक्सेक्यूशन के साथ एआई टूल कॉल का अनुकूलन
translated: true
type: note
---

हां, चैटबॉट्स में पारंपरिक टूल या फ़ंक्शन कॉल्स का यह आगे-पीछे होने वाला स्वभाव (जहां AI मॉडल एक कॉल जनरेट करता है, उसे क्लाइंट या किसी अन्य सेवा पर भेजता है, निष्पादन की प्रतीक्षा करता है, और फिर परिणाम प्राप्त करता है) वास्तव में विलंबता (लेटेंसी) पैदा कर सकता है, खासकर रीयल-टाइम एप्लिकेशन्स में। यह आमतौर पर नेटवर्क ओवरहेड, सीरियलाइज़ेशन/डी-सीरियलाइज़ेशन और मल्टीपल API राउंड-ट्रिप्स की आवश्यकता के कारण होता है। लेटेंसी को कम करने के लिए सर्वर-साइड निष्पादन के लिए लैम्ब्डा फ़ंक्शंस का उपयोग करने या Python कोड स्वीकार करने वाला API डिज़ाइन करने का आपका सुझाव एक चतुर तरीका है—मूल रूप से क्लाइंट-सर्वर इंटरैक्शन को कम करने के लिए कम्प्यूटेशन को सर्वर पर स्थानांतरित करना। मैं इसे वैचारिक रूप से समझाता हूं, व्यवहार्यता, संभावित कार्यान्वयन और ट्रेड-ऑफ़्स पर चर्चा करूंगा।

### पारंपरिक टूल कॉल्स धीमी क्यों हो सकती हैं
कई AI फ्रेमवर्क्स में (जैसे LangChain, OpenAI की फ़ंक्शन कॉलिंग, या Anthropic के टूल्स):
- मॉडल एक संरचित टूल कॉल आउटपुट करता है (जैसे, फ़ंक्शन नाम और आर्ग्युमेंट्स वाला JSON)।
- क्लाइंट (या एजेंट) फ़ंक्शन को स्थानीय रूप से या किसी अन्य API के माध्यम से निष्पादित करता है।
- परिणामों को कन्वर्सेशन हिस्ट्री में जोड़ा जाता है और अगले इनफ़रेंस स्टेप के लिए वापस मॉडल को भेजा जाता है।
यह लूप प्रति चक्र में सेकंडों की देरी जोड़ सकता है, जो डेटा विश्लेषण या मल्टी-स्टेप रीजनिंग जैसे जटिल कार्यों में और बढ़ जाती है।

### लैम्ब्डा फ़ंक्शंस या सर्वर-साइड कोड निष्पादन का उपयोग
आपका विचार "सर्वरलेस" या "सैंडबॉक्स्ड" निष्पादन मॉडल्स के साथ मेल खाता है, जहां AI कोड (या लैम्ब्डा-जैसा स्निपेट) जनरेट करता है जो सीधे मॉडल होस्ट करने वाले सर्वर पर चलाया जाता है। यह सब कुछ एक ही एनवायरनमेंट में रखता है, जिससे यूजर से संभावित API कॉल्स की संख्या केवल एक ही रह जाती है।

- **लैम्ब्डा फ़ंक्शंस अप्रोच**: AWS Lambda, Google Cloud Functions, या Azure Functions जैसी सेवाएं सर्वरों को मैनेज किए बिना ऑन-डिमांड छोटे, क्षणिक Python कोड स्निपेट्स को निष्पादित करने की अनुमति देती हैं। एक AI संदर्भ में:
  - चैटबॉट का बैकएंड AI मॉडल (जैसे, OpenAI API के माध्यम से) को रैप कर सकता है और लैम्ब्डा को एक टूल के रूप में इंटीग्रेट कर सकता है।
  - मॉडल एक लैम्ब्डा एक्सप्रेशन या छोटा फ़ंक्शन जनरेट करता है, जिसे सर्वर-साइड इनवोक किया जाता है।
  - फायदे: स्केलेबल, पे-पर-यूज, और फास्ट स्पिन-अप (अक्सर <100ms कोल्ड स्टार्ट)।
  - नुकसान: सीमित निष्पादन समय (जैसे, AWS पर 15 मिनट अधिकतम), और यदि कार्य मल्टीपल इनवोकेशन्स में फैला हो तो स्टेट मैनेजमेंट को हैंडल करने की आवश्यकता होगी।
  - उदाहरण: एक AI एजेंट डेटा प्रोसेस करने के लिए एक लैम्ब्डा जनरेट कर सकता है (जैसे, `lambda x: sum(x) if isinstance(x, list) else 0`), इसे एक लैम्ब्डा एंडपॉइंट पर भेज सकता है, और परिणाम इनलाइन प्राप्त कर सकता है।

- **Python कोड स्वीकार करने और निष्पादित करने के लिए API डिज़ाइन करना**:
  - हां, यह बिल्कुल संभव है और पहले से ही प्रोडक्शन सिस्टम्स में मौजूद है। मुख्य बात **सैंडबॉक्सिंग** है ताकि मनमाना कोड निष्पादन (जैसे, फ़ाइलों को हटाना या नेटवर्क कॉल करना) जैसे सुरक्षा जोखिमों को रोका जा सके।
  - यह कैसे काम करता है: API एंडपॉइंट एक कोड स्निपेट (एक स्ट्रिंग के रूप में) प्राप्त करता है, इसे एक अलग थलित एनवायरनमेंट में चलाता है, आउटपुट/एरर को कैप्चर करता है, और परिणाम लौटाता है। AI मॉडल सर्वर को छोड़े बिना इस कोड को पुनरावृत्त रूप से जनरेट और "कॉल" कर सकता है।
  - लाभ:
    - विलंबता कम करता है: निष्पादन मॉडल के समान डेटा सेंटर में होता है, अक्सर मिलीसेकंड में।
    - जटिल कार्यों को सक्षम बनाता है: जैसे बाहरी टूल्स के बिना डेटा प्रोसेसिंग, मैथ सिमुलेशन, या फ़ाइल हैंडलिंग।
    - स्टेटफुल सेशन: कुछ कार्यान्वयन कॉल्स के बीच एक REPL-जैसा एनवायरनमेंट बनाए रखते हैं।
  - सुरक्षा उपाय:
    - कंटेनर्स (Docker), माइक्रो-VMs (Firecracker), या प्रतिबंधित Python इंटरप्रेटर्स (जैसे, PyPy सैंडबॉक्सिंग या प्रतिबंधित ग्लोबल्स) का उपयोग करें।
    - संसाधन सीमित करें: CPU/समय कोटा, कोई नेटवर्क एक्सेस नहीं, व्हाइटलिस्टेड मॉड्यूल (जैसे, numpy, pandas, लेकिन os या subprocess नहीं)।
    - `restrictedpython` जैसी लाइब्रेरीज़ या E2B/Firecracker जैसे टूल्स रेडी-मेड सैंडबॉक्स प्रदान करते हैं।

### वास्तविक दुनिया के उदाहरण और कार्यान्वयन
कई AI प्लेटफ़ॉर्म पहले से ही इसे विभिन्न स्तरों पर सपोर्ट करते हैं:
- **OpenAI का Assistants API with Code Interpreter**: मॉडल को OpenAI के सर्वर पर एक सैंडबॉक्स्ड एनवायरनमेंट में Python कोड लिखने और चलाने की अनुमति देता है। मॉडल फ़ाइलें अपलोड कर सकता है, कोड निष्पादित कर सकता है, और परिणामों पर पुनरावृति कर सकता है—सब कुछ सर्वर-साइड। क्लाइंट-साइड निष्पादन की आवश्यकता नहीं है।
- **Google का Gemini API Code Execution**: एक बिल्ट-इन Python सैंडबॉक्स प्रदान करता है जहां मॉडल बाहरी कॉल्स के बिना आउटपुट से सीखते हुए, कोड को पुनरावृत्त रूप से जनरेट और रन करता है।
- **कस्टम सॉल्यूशन्स**:
  - **E2B Sandbox**: Jupyter kernels के साथ क्लाउड-आधारित सैंडबॉक्स बनाने के लिए एक SDK/API। AI एजेंट कोड को सुरक्षित रूप से चलाने के लिए भेज सकते हैं, जो डेटा विश्लेषण टूल्स के लिए आदर्श है।
  - **Modal Sandboxes**: AI-जनरेटेड कोड को अलग-थलग एनवायरनमेंट में चलाने के लिए एक प्लेटफ़ॉर्म, जिसका उपयोग अक्सर LLM एजेंट्स के लिए किया जाता है।
  - **SandboxAI (ओपन-सोर्स)**: AI-जनरेटेड Python को सैंडबॉक्स में निष्पादित करने के लिए विशेष रूप से एक रनटाइम।
  - DIY के लिए: एक FastAPI या Flask सर्वर बनाएं जो POST के माध्यम से कोड स्वीकार करता है, एक प्रतिबंधित नेमस्पेस में `exec()` का उपयोग करता है, या प्रति रिक्वेस्ट एक Docker कंटेनर स्पिन अप करता है।

कोड के संदर्भ में, एक सरल API एंडपॉइंट कुछ इस तरह दिख सकता है (उदाहरण के लिए स्यूडोकोड):

```python
from fastapi import FastAPI
import restrictedpython  # सुरक्षित निष्पादन के लिए

app = FastAPI()

@app.post("/execute")
def execute_code(code: str):
    safe_globals = {"__builtins__": restrictedpython.safe_builtins}  # प्रतिबंधित एनवायरनमेंट
    try:
        result = {}
        exec(code, safe_globals, result)  # कोड रन करें
        return {"output": result.get("output")}
    except Exception as e:
        return {"error": str(e)}
```

इसे अपने चैटबॉट के साथ इंटीग्रेट करें: AI यूजर इनपुट के आधार पर कोड जनरेट करता है, एंडपॉइंट को आंतरिक रूप से कॉल करता है, और परिणामों को शामिल करता है।

### संभावित कमियां और सर्वोत्तम अभ्यास
- **सुरक्षा जोखिम**: मनमाना कोड निष्पादन खतरनाक है—हमेशा सैंडबॉक्स का उपयोग करें। यदि अविश्वसनीय उपयोगकर्ताओं को हैंडल कर रहे हैं तो इससे बचें।
- **लागत और स्केलेबिलिटी**: सर्वर-साइड निष्पादन अधिक कम्प्यूट का उपयोग करता है; ऑप्टिमाइज़ेशन के लिए New Relic जैसे टूल्स से मॉनिटर करें।
- **डीबगिंग**: जनरेट किए गए कोड में एरर अच्छी तरह से हैंडल नहीं होने पर लूप कर सकती हैं।
- **ऑप्टिमाइज़ेशन के लिए विकल्प**:
  - बैच टूल कॉल्स: मल्टीपल फ़ंक्शंस को एक सर्वर राउंड-ट्रिप में ग्रुप करें।
  - स्ट्रीमिंग: समानांतर में टूल्स को निष्पादित करते समय प्रतिक्रिया स्ट्रीम करने वाले APIs का उपयोग करें।
  - यदि विलंबता सहनीय है तो क्लाइंट-साइड: गैर-संवेदनशील कार्यों के लिए, स्थानीय रूप से निष्पादित करें।
- यदि स्क्रैच से बना रहे हैं, तो LangChain या Vercel के AI SDK जैसे फ्रेमवर्क्स से शुरुआत करें, जो कस्टम टूल्स को सपोर्ट करते हैं और सर्वर-साइड रन के लिए कॉन्फ़िगर किए जा सकते हैं।

यह दृष्टिकोण चैटबॉट्स को बहुत तेज़ और अधिक सक्षम बना सकता है, खासकर कम्प्यूटेशनल कार्यों के लिए। यदि आप इसे लागू कर रहे हैं, तो त्वरित सफलताओं के लिए OpenAI या Gemini की बिल्ट-इन सुविधाओं से शुरुआत करने पर विचार करें।

[Building a Sandboxed Environment for AI generated Code Execution](https://anukriti-ranjan.medium.com/building-a-sandboxed-environment-for-ai-generated-code-execution-e1351301268a)  
[substratusai/sandboxai: Run AI generated code in isolated sandboxes](https://github.com/substratusai/sandboxai)  
[Build AI data analyst with sandboxed code execution using TS ... - E2B](https://e2b.dev/blog/build-ai-data-analyst-with-sandboxed-code-execution-using-typescript-and-gpt-4o)  
[Code execution | Gemini API](https://ai.google.dev/gemini-api/docs/code-execution)  
[Assistants Code Interpreter - OpenAI API](https://platform.openai.com/docs/assistants/tools/code-interpreter)  
[Modal Sandboxes](https://modal.com/use-cases/sandboxes)  
[Optimizing AI chatbot performance with New Relic AI monitoring](https://newrelic.com/blog/how-to-relic/optimizing-ai-chatbot-performance)