---
audio: false
generated: true
image: false
lang: hi
layout: post
model: x-ai/grok-4-fast
title: पायटॉर्च में कुशल फ़्लैश अटेंशन
translated: true
type: note
---

### PyTorch में Flash Attention CUDA Kernels के साथ Efficient Attention

आपने जो कोड लाइन साझा की है, वह PyTorch के `torch.nn.functional.scaled_dot_product_attention` फ़ंक्शन का उपयोग करके **स्केल्ड डॉट-प्रोडक्ट अटेंशन** (Transformer मॉडल्स में मुख्य तंत्र) को लागू करती है। यह फ़ंक्शन कुशल होने के लिए डिज़ाइन किया गया है और जब शर्तें पूरी होती हैं (जैसे, CUDA पर चलना, संगत टेंसर आकार और कोई कस्टम मास्क न होना) तो यह स्वचालित रूप से **Flash Attention** जैसे अनुकूलित इम्प्लीमेंटेशन का उपयोग करता है। मैं इसे चरण दर चरण समझाऊंगा, जिसमें शामिल है कि Flash Attention इसमें कैसे फिट बैठता है, आर्ग्युमेंट के आकार, और यह कुशल क्यों है।

#### स्केल्ड डॉट-प्रोडक्ट अटेंशन पर संक्षिप्त पृष्ठभूमि
Transformers में, अटेंशन गणना करता है कि अनुक्रम में प्रत्येक स्थिति को दूसरों पर कितना "ध्यान" (attend) देना चाहिए। सूत्र है:

\\[
\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right) V
\\]

- \\(Q\\): क्वेरी मैट्रिक्स (हम क्या पूछताछ कर रहे हैं)।
- \\(K\\): कुंजी मैट्रिक्स (जिसके विरुद्ध हम मिलान कर रहे हैं)।
- \\(V\\): वैल्यू मैट्रिक्स (जो हम प्राप्त करते हैं)।

इसे सीधे गणना करने के लिए एक बड़े \\(N \times N\\) अटेंशन मैट्रिक्स (जहां \\(N\\) अनुक्रम लंबाई है) को मटीरियलाइज़ करने की आवश्यकता होती है, जो \\(O(N^2)\\) मेमोरी का उपयोग करता है—लंबे अनुक्रमों (जैसे, \\(N > 10k\\)) के लिए खराब।

**Flash Attention** (2022 में Tri Dao et al. द्वारा प्रस्तुत) इसे CUDA का उपयोग करके एक **कर्नल फ्यूजन** तकनीक से ठीक करता है। यह अटेंशन की गणना टाइल्स (ब्लॉक्स) में **ऑन-द-फ्लाई** करता है, मेमोरी में पूर्ण मैट्रिक्स से बचते हुए। यह मेमोरी को \\(O(N)\\) तक कम कर देता है और GPU पर लंबे कॉन्टेक्स्ट के लिए विशेष रूप से गति को 2-4x बढ़ा देता है। PyTorch इसे इस फ़ंक्शन के माध्यम से सहजता से एकीकृत करता है—कस्टम कर्नल की कोई आवश्यकता नहीं है।

#### कोड Flash Attention का उपयोग कैसे करता है
```python
y = torch.nn.functional.scaled_dot_product_attention(
    q, k, v, 
    attn_mask=None, 
    dropout_p=self.dropout if self.training else 0, 
    is_causal=True
)
```
- यह कॉज़ल सेल्फ-अटेंशन (ऑटोरेग्रेसिव मॉडल जैसे GPT में आम, जहां भविष्य के टोकन अतीत के टोकन पर ध्यान नहीं दे सकते) की गणना करता है।
- **Flash Attention डिस्पैच**: PyTorch रनटाइम शर्तों की जांच करता है:
  - डिवाइस: CUDA (GPU)।
  - डीटाइप्स: float16/bfloat16 (या caveats के साथ float32)।
  - आकार: संगत (नीचे देखें)।
  - मास्क: `attn_mask=None` और `is_causal=True` इसे मटीरियलाइज़ किए बिना आंतरिक रूप से कॉज़ल मास्क को सक्षम करता है।
  - कोई अन्य बाधाएं नहीं (जैसे, कोई कस्टम `attn_mask` या कुछ हेड डायमेंशन जो टाइलिंग को तोड़ते हों)।

  यदि पूरी होती हैं, तो यह Flash Attention 2 (या नए PyTorch में 3) कर्नल का उपयोग करता है। अन्यथा, यह मानक (धीमा, मेमोरी-हंगरी) इम्प्लीमेंटेशन पर वापस आ जाता है। आप `torch.backends.cuda.sdp_kernel(enable_flash=True, enable_math=False)` का उपयोग करके इसे फोर्स/सक्षम कर सकते हैं।

- **ड्रॉपआउट**: प्रशिक्षण के दौरान (`dropout_p > 0`) नियमितिकरण के लिए अटेंशन वेट पर लागू होता है। ईवल मोड में, यह 0 होता है।
- आउटपुट `y`: `v` के समान आकार, जो अटेंडेड वैल्यूज का प्रतिनिधित्व करता है।

#### आर्ग्युमेंट आकार और आवश्यकताएं
सभी इनपुट (`q`, `k`, `v`) का आकार मेल खाना चाहिए और एक ही डिवाइस/डीटाइप पर होने चाहिए। PyTorch का फ़ंक्शन **बैच्ड** और **मल्टी-हेड** अटेंशन को लचीले ढंग से सपोर्ट करता है। यहां विवरण है:

| आर्ग्युमेंट | आकार (बैच-फर्स्ट, डिफॉल्ट) | विवरण | आवश्यकताएं |
|----------|------------------------------|-------------|--------------|
| **q** (क्वेरी) | `(B, S_q, H, D)` या `(B, S_q, E)` | - `B`: बैच आकार (जैसे, 32)।<br>- `S_q`: क्वेरी अनुक्रम लंबाई (जैसे, 512)।<br>- `H`: हेड्स की संख्या (जैसे, 8; वैकल्पिक यदि सिंगल-हेड)।<br>- `D`: हेड डायमेंशन (जैसे, 64; `E = H * D` फ्लैटन एम्बेड डायमेंशन के लिए)। | - सेल्फ-अटेंशन के लिए `S_q` का `S_k` से मेल होना चाहिए।<br>- Flash के लिए: `D` ≤ 256 (इष्टतम), लेकिन 512 तक काम करता है। |
| **k** (कुंजी) | `(B, S_k, H, D)` या `(B, S_k, E)` | `q` के समान, लेकिन `S_k` कुंजी अनुक्रम लंबाई है (अक्सर = `S_q`)। | - `q` के आकार में ब्रॉडकास्ट करने योग्य। |
| **v** (वैल्यू) | `(B, S_v, H, D)` या `(B, S_v, E)` | `k` के समान, `S_v` आमतौर पर = `S_k`। | - आउटपुट `y` का आकार `v` से मेल खाता है। |
| **attn_mask** | `(B, H, S_q, S_k)` या `(S_q, S_k)` (ब्रॉडकास्टेड) | वैकल्पिक एडिटिव मास्क (जैसे, मास्क की गई स्थितियों के लिए `-inf`)। यहां: `None`। | - Flash के लिए: यदि संभव हो तो इससे बचें; इसके बजाय `is_causal` का उपयोग करें। |
| **dropout_p** | स्केलर (फ्लोट) | ड्रॉपआउट दर (0.0-1.0)। | - Float32। |
| **is_causal** | बूल | लोअर-ट्राएंगुलर कॉज़ल मास्क को सक्षम करता है (भविष्य में झांकने से रोकता है)। यहां: `True`। | - Flash के लिए: मैनुअल मास्क पर वरीयता दी जाती है। |

- **बैच-फर्स्ट बनाम हेड-फर्स्ट**: डिफॉल्ट `batch_first=True` है (आकार ऊपर जैसा)। `batch_first=False` के लिए `(H, B, S, D)` सेट करें।
- **क्रॉस-अटेंशन**: यदि एनकोडर-डिकोडर, तो `S_q` (डिकोडर लेन) `S_k = S_v` (एनकोडर लेन) से भिन्न हो सकता है।
- **Flash के लिए एज केस**:
  - अनुक्रम लंबाई \\(S \leq 8192\\) (अधिक लंबा होने पर फॉलबैक हो सकता है)।
  - रैग्ड बैच या कुछ स्पार्स मास्क के लिए कोई सपोर्ट नहीं।
  - इसे सक्षम करें: `torch.backends.cuda.enable_flash_sdp(True)`।

#### उदाहरण उपयोग और सत्यापन
यहां एक न्यूनतम PyTorch उदाहरण है (CUDA मानते हुए):
```python
import torch
import torch.nn.functional as F

B, S, H, D = 2, 128, 8, 64  # Batch=2, Seq=128, Heads=8, Head_dim=64
q = torch.randn(B, S, H, D, device='cuda', dtype=torch.float16)
k = torch.randn(B, S, H, D, device='cuda', dtype=torch.float16)
v = torch.randn(B, S, H, D, device='cuda', dtype=torch.float16)

# Flash सक्षम करें
with torch.backends.cuda.sdp_kernel(enable_flash=True, enable_math=False):
    y = F.scaled_dot_product_attention(q, k, v, is_causal=True)

print(y.shape)  # torch.Size([2, 128, 8, 64])
```

यह बड़े \\(S\\) के लिए मैनुअल `q @ k.transpose(-2,-1)` + softmax + `@ v` की तुलना में ~3x तेज चलता है।

#### कुशल क्यों?
- **मेमोरी**: मानक: \\(O(B \cdot H \cdot S^2 \cdot 4\\) बाइट्स (float32 के लिए)। Flash: \\(O(B \cdot H \cdot S \cdot D)\\)—\\(S\\) के साथ रैखिक रूप से स्केल करता है।
- **कम्प्यूट**: फ्यूज़्ड कर्नल कर्नल लॉन्च को कम करते हैं (softmax + matmul एक साथ)।
- **ट्रेड-ऑफ**: float16 में संख्यात्मक स्थिरता थोड़ी कम (बेहतर के लिए bfloat16 का उपयोग करें), लेकिन नगण्य।

गहराई से जानने के लिए, [Flash Attention पेपर](https://arxiv.org/abs/2205.14135) या [PyTorch डॉक्स](https://pytorch.org/docs/stable/generated/torch.nn.functional.scaled_dot_product_attention.html) देखें। यदि आप और संदर्भ साझा करते हैं (जैसे, पूरा मॉडल), तो मैं परिष्कृत कर सकता हूं!