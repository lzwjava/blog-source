---
audio: false
generated: true
image: false
lang: hi
layout: post
model: x-ai/grok-4-fast
title: ग्रेडिएंट बूस्टिंग मशीनों का परिचय
translated: true
type: note
---

### अवलोकन
जेरोम एच. फ्रीडमैन का 2001 का पेपर *"Greedy Function Approximation: A Gradient Boosting Machine"* ग्रेडिएंट बूस्टिंग मशीनों (जीबीएम) का परिचय कराता है, जो रिग्रेशन और क्लासिफिकेशन जैसे सुपरवाइज्ड टास्क्स के लिए एक शक्तिशाली एन्सेम्बल लर्निंग विधि है। यह बूस्टिंग को फंक्शनल ग्रेडिएंट डिसेंट के एक रूप में प्रस्तुत करता है, जहां एक निर्दिष्ट लॉस फंक्शन को कम करने के लिए सरल "कमजोर" लर्नर्स (अक्सर डिसीजन ट्री) को एक एडिटिव मॉडल में क्रमिक रूप से जोड़ा जाता है। यह दृष्टिकोण पहले के बूस्टिंग एल्गोरिदम (जैसे, एडाबूस्ट) को सामान्य बनाता है और फंक्शन स्पेस में लालची ऑप्टिमाइजेशन पर जोर देता है, जिससे अत्यधिक सटीक, मजबूत और व्याख्यात्मक मॉडल बनते हैं।

### सारांश (संक्षिप्त)
जीबीएम एक अवकलनीय लॉस फंक्शन के मिनिमाइज़र का अनुमान लगाने के लिए कमजोर लर्नर्स को एक क्रमिक, योजक तरीके से जोड़कर लचीले प्रेडिक्टिव मॉडल बनाती हैं। बेस लर्नर के रूप में रिग्रेशन ट्री का उपयोग करने से रिग्रेशन और क्लासिफिकेशन के लिए प्रतिस्पर्धी, मजबूत प्रक्रियाएं मिलती हैं। यह विधि अन्य विकल्पों जैसे मल्टीवेरिएट एडाप्टिव रिग्रेशन स्प्लाइन्स (MARS) से अनुभवजन्य परीक्षणों में बेहतर प्रदर्शन करती है, जिसमें विभिन्न डेटासेट पर कम त्रुटि दरें देखी गई हैं।

### मुख्य विधियाँ
मुख्य विचार वर्तमान मॉडल की भविष्यवाणियों के संबंध में लॉस के *नकारात्मक ग्रेडिएंट* (स्यूडो-रेजिडुअल) के लिए नए लर्नर्स को पुनरावृत्त रूप से फिट करना है, जो फंक्शन स्पेस में ग्रेडिएंट डिसेंट की नकल करता है।

- **मॉडल संरचना**: अंतिम मॉडल \\( F_M(x) = \sum_{m=1}^M \beta_m h_m(x) \\) है, जहां प्रत्येक \\( h_m(x) \\) एक कमजोर लर्नर है (जैसे, एक छोटा रिग्रेशन ट्री)।
- **अपडेट नियम**: पुनरावृत्ति \\( m \\) पर, अवशेषों की गणना करें \\( r_{im} = -\left[ \frac{\partial L(y_i, F(x_i))}{\partial F(x_i)} \right]_{F=F_{m-1}} \\), फिर इन अवशेषों के लिए \\( h_m \\) को लीस्त स्क्वेयर्स के माध्यम से फिट करें। स्टेप साइज़ \\( \gamma_m \\) को लाइन सर्च द्वारा ऑप्टिमाइज़ किया जाता है: \\( \gamma_m = \arg\min_\gamma \sum_i L(y_i, F_{m-1}(x_i) + \gamma h_m(x_i)) \\)।
- **श्रिंकेज (सिकुड़न)**: ओवरफिटिंग को कम करने और अधिक पुनरावृत्तियों की अनुमति देने के लिए \\( \nu \in (0,1] \\) (जैसे, \\( \nu = 0.1 \\)) द्वारा जोड़ों को स्केल करें।
- **स्टोकेस्टिक वेरिएंट**: तेज ट्रेनिंग और बेहतर सामान्यीकरण के लिए प्रत्येक चरण पर डेटा (जैसे, 50%) का सबसैंपल लें।
- **ट्रीबूस्ट एल्गोरिदम** (स्यूडोकोड रूपरेखा):
  1. \\( F_0(x) \\) को लॉस को कम करने वाले स्थिरांक के रूप में इनिशियलाइज़ करें।
  2. \\( m = 1 \\) से \\( M \\) तक के लिए:
     - स्यूडो-रेजिडुअल \\( r_{im} \\) की गणना करें।
     - \\( \{ (x_i, r_{im}) \} \\) के लिए ट्री \\( h_m \\) को फिट करें।
     - लाइन सर्च के माध्यम से इष्टतम \\( \gamma_m \\) खोजें।
     - अपडेट करें \\( F_m(x) = F_{m-1}(x) + \nu \gamma_m h_m(x) \\)।
  3. पुनरावृत्तियों या लॉस सुधार के आधार पर रुकें।

समर्थित लॉस में शामिल हैं:
- लीस्त स्क्वेयर्स (रिग्रेशन): \\( L(y, F) = \frac{1}{2}(y - F)^2 \\), अवशेष = \\( y - F \\)।
- लीस्त एब्सोल्यूट डेविएशन (मजबूत रिग्रेशन): \\( L(y, F) = |y - F| \\)।
- लॉग-लाइकलीहुड (बाइनरी क्लासिफिकेशन): \\( L = -\sum [y \log p + (1-y) \log(1-p)] \\), \\( p = \frac{1}{1 + e^{-F}} \\) के साथ; अवशेष = \\( y - p \\)।
- ह्यूबर लॉस (आउटलायर-रोबस्ट)।

लॉजिटबूस्ट जैसे वेरिएंट विशिष्ट लॉस (जैसे, बाइनोमियल डेविएंस) के लिए इसे अपनाते हैं।

### योगदान
- **एकीकृत ढांचा**: ग्रेडिएंट के माध्यम से बूस्टिंग को किसी भी अवकलनीय लॉस तक विस्तारित करता है, एडाबूस्ट (एक्सपोनेंशियल लॉस) और लॉजिटबूस्ट को एकीकृत करता है।
- **व्यावहारिक उन्नयन**: ओवरफिटिंग और कम्प्यूटेशन पर बेहतर नियंत्रण के लिए श्रिंकेज और स्टोकेस्टिक सबसैंपलिंग का परिचय देता है।
- **लचीलापन**: विभिन्न बेस लर्नर्स और कार्यों (रिग्रेशन, क्लासिफिकेशन, रैंकिंग) के साथ काम करता है; ट्री उच्च-आयामी डेटा को कुशलतापूर्वक संभालने में सक्षम बनाती हैं।
- **व्याख्यात्मकता उपकरण**: वेरिएबल इम्पोर्टेंस (कुल अशुद्धि में कमी के माध्यम से) और आंशिक निर्भरता प्लॉट्स पर चर्चा करता है।

### मुख्य निष्कर्ष
- **प्रदर्शन**: जीबीएम अनुभवजन्य रूप से उत्कृष्ट प्रदर्शन करती हैं, वास्तव-विश्व के डेटासेट (जैसे, वित्त, भौतिकी सिमुलेशन) पर सिंगल ट्री, बैगिंग, न्यूरल नेट्स, और MARS जैसे बेसलाइन्स पर त्रुटि दरों में 10-50% की कमी लाती हैं।
- **मजबूती**: स्टोकेस्टिक जीबीएम तेजी से अभिसरण करती है; गहरे ट्री (6-8 स्तर) सटीकता बढ़ाते हैं लेकिन रेगुलराइजेशन की आवश्यकता होती है। LAD/ह्यूबर लॉस लीस्त स्क्वेयर्स की तुलना में आउटलायर्स को बेहतर ढंग से कम करते हैं।
- **दक्षता**: बड़े डेटा के लिए तेज; इष्टतम सेटअप \\( \nu = 0.1 \\), 1000+ पुनरावृत्तियों, और सबसैंपलिंग का उपयोग करते हैं।
- प्लॉट्स विकल्पों की तुलना में तीव्र लॉस अभिसरण और कम विचरण/पूर्वाग्रह दिखाते हैं।

इस पेपर ने XGBoost और LightGBM जैसे आधुनिक इम्प्लीमेंटेशन की नींव रखी।

### संदर्भ
- [Greedy Function Approximation: A Gradient Boosting Machine (PDF)](https://jerryfriedman.su.domains/ftp/trebst.pdf)