---
audio: false
generated: true
image: false
lang: hi
layout: post
model: x-ai/grok-4-fast
title: रोबस्ट पूर्वानुमानों के लिए एन्सेम्बल विधियाँ
translated: true
type: note
---

मशीन लर्निंग में, **एन्सेम्बल** एक ऐसी विधि को कहते हैं जो कई व्यक्तिगत मॉडलों (जिन्हें अक्सर "कमजोर लर्नर" कहा जाता है, जैसे साधारण डिसीजन ट्री) के पूर्वानुमानों या आउटपुट को संयोजित करके एक अधिक सटीक और मजबूत अंतिम पूर्वानुमान तैयार करती है, जो कोई भी एकल मॉडल अकेले हासिल नहीं कर सकता। इसका मुख्य विचार मॉडलों के बीच "जनसमूह की बुद्धिमत्ता" का लाभ उठाना है—उनकी ताकतों को औसतन या भारित करके और कमजोरियों को कम करके त्रुटियों, ओवरफिटिंग और वेरिएंस को कम करना।

**ग्रेडिएंट बूस्टिंग** (उदाहरण के लिए, XGBoost या LightGBM जैसे एल्गोरिदम) के विशिष्ट संदर्भ में:
- यह *पेड़ों का एक एन्सेम्बल* है: कमजोर डिसीजन ट्री क्रमिक रूप से बनाए जाते हैं, जहां हर नया पेड़ पिछले पेड़ों की अवशिष्ट त्रुटियों (गलतियों) को सुधारने पर केंद्रित होता है।
- यह बूस्टिंग प्रक्रिया समग्र मॉडल को "धीरे-धीरे" बेहतर बनाती है, जिसके परिणामस्वरूप एक मजबूत एन्सेम्बल बनता है जो रिग्रेशन या क्लासिफिकेशन जैसे कार्यों के लिए अत्यधिक प्रभावी होता है।

इसे एक ऐसी प्रशिक्षु टीम के रूप में समझें जो एक मास्टर कारीगर बनाने के लिए एक-दूसरे की गलतियों से सीखती है। यदि आप गहराई से जानना चाहते हैं, तो मैं इसके पीछे के गणित के लिए फ्रीडमैन (2001) के मूल ग्रेडिएंट बूस्टिंग शोध पत्र को देखने की सलाह देता हूं।