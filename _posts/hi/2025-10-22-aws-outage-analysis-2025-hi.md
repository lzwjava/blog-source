---
audio: false
generated: true
image: false
lang: hi
layout: post
model: x-ai/grok-4-fast
title: AWS आउटेज केस विश्लेषण अक्टूबर 2025
translated: true
type: note
---

### 20 अक्टूबर, 2025 को AWS आउटेज: विस्तृत केस विश्लेषण

20 अक्टूबर, 2025 को AWS आउटेज US East (N. Virginia) क्षेत्र (us-east-1) में शुरू हुई एक महत्वपूर्ण व्यवधान थी, जो AWS का सबसे बड़ा और सबसे महत्वपूर्ण डेटा सेंटर है। यह 19 अक्टूबर PDT को देर शाम शुरू हुई और लगभग 16 घंटे तक चली, जिसने 140 से अधिक सेवाओं को प्रभावित किया और इस क्षेत्र पर निर्भरताओं के कारण वैश्विक स्तर पर इसका प्रभाव पड़ा। इस घटना ने DNS रिज़ॉल्यूशन, सेवा अंतर्निर्भरताओं और पुनर्प्राप्ति प्रक्रियाओं में कमजोरियों को उजागर किया, जिससे ऐप्स, वेबसाइटों और सेवाओं में लाखों उपयोगकर्ता प्रभावित हुए। नीचे AWS की आधिकारिक पोस्ट-मॉर्टम और समकालीन रिपोर्टों के आधार पर विवरण दिया गया है।

#### समयरेखा
आउटेज चरणों में विकसित हुई, जो पहले पहचान के साथ शुरू हुई और फिर एक स्तरीय पुनर्प्राप्ति से पहले व्यापक विफलताओं तक बढ़ गई। प्रमुख मील के पत्थर (सभी समय PDT में):

| समय | घटना |
|------|-------|
| 11:49 PM (19 अक्टूबर) | us-east-1 में कई AWS सेवाओं में त्रुटि दर और विलंबता में वृद्धि दर्ज की गई। |
| 12:11 AM (20 अक्टूबर) | AWS ने सार्वजनिक रूप से उच्च त्रुटि दरों की सूचना दी; DownDetector जैसी निगरानी साइटों पर प्रारंभिक उपयोगकर्ता रिपोर्ट्स में उछाल। |
| 12:26 AM | समस्या us-east-1 में DynamoDB API एंडपॉइंट्स के लिए DNS रिज़ॉल्यूशन विफलताओं के रूप में पहचानी गई। |
| 1:26 AM | विशेष रूप से DynamoDB APIs, जिसमें Global Tables शामिल हैं, के लिए उच्च त्रुटि दरों की पुष्टि हुई। |
| 2:22 AM | प्रारंभिक शमन उपाय लागू किए गए; पुनर्प्राप्ति के शुरुआती संकेत दिखाई दिए। |
| 2:24 AM | DynamoDB DNS समस्या का समाधान हुआ, जिससे आंशिक सेवा पुनर्प्राप्ति शुरू हुई—लेकिन EC2 लॉन्च में बाधाएँ और Network Load Balancer (NLB) हेल्थ चेक विफलताएँ सामने आईं। |
| 3:35 AM | DNS पूरी तरह से ठीक हो गया; अधिकांश DynamoDB ऑपरेशन सफल रहे, लेकिन सभी Availability Zones (AZs) में EC2 लॉन्च में बाधा बनी रही। |
| 4:08 AM | EC2 त्रुटियों और SQS Event Source Mappings के लिए Lambda पोलिंग देरी पर काम जारी। |
| 5:48 AM | चुनिंदा AZs में आंशिक EC2 लॉन्च पुनर्प्राप्ति; SQS बैकलॉग साफ होना शुरू। |
| 6:42 AM | सभी AZs में शमन उपाय लागू किए गए; स्थिरता के लिए AWS ने नए EC2 इंस्टेंस लॉन्च पर Rate Limiting लागू की। |
| 7:14 AM | सेवाओं में API त्रुटियाँ और कनेक्टिविटी समस्याएँ बनी रहीं; उपयोगकर्ता-प्रभावित विफलताएँ (जैसे, ऐप आउटेज) चरम पर पहुँचीं। |
| 8:04 AM | जाँच EC2 आंतरिक नेटवर्क पर केंद्रित। |
| 8:43 AM | नेटवर्क समस्याओं का मूल कारण पहचाना गया: NLB हेल्थ मॉनिटरिंग के लिए EC2 के आंतरिक सबसिस्टम में बाधा। |
| 9:13 AM | NLB हेल्थ चेक के लिए अतिरिक्त शमन उपाय। |
| 9:38 AM | NLB हेल्थ चेक पूरी तरह से बहाल। |
| 10:03 AM – 12:15 PM | EC2 लॉन्च में क्रमिक सुधार; Lambda इनवोकेशन और कनेक्टिविटी चरणों में AZs में स्थिर हुई। |
| 1:03 PM – 2:48 PM | Redshift, Amazon Connect, और CloudTrail जैसी सेवाओं के लिए थ्रॉटल कम किए गए; बैकलॉग प्रोसेस किए गए। |
| 3:01 PM | सभी सेवाओं के लिए पूर्ण परिचालन सामान्य स्थिति बहाल; छोटे बैकलॉग (जैसे, AWS Config, Redshift) के कुछ घंटों में साफ होने की उम्मीद। |
| 3:53 PM | AWS ने आउटेज के समाधान की घोषणा की। |

DownDetector जैसे प्लेटफॉर्म पर उपयोगकर्ता रिपोर्ट्स लगभग 6 AM PDT पर चरम पर पहुँचीं, जिसमें 5,000 से अधिक घटनाएँ दर्ज की गईं, इसके बाद यह संख्या गिर गई।

#### मूल कारण
आउटेज की शुरुआत us-east-1 में DynamoDB सेवा एंडपॉइंट्स को प्रभावित करने वाली DNS रिज़ॉल्यूशन विफलता से हुई। DynamoDB, एक NoSQL डेटाबेस सेवा, कई AWS सुविधाओं के लिए एक महत्वपूर्ण "कंट्रोल प्लेन" के रूप में कार्य करती है—जो मेटाडेटा, सत्रों और रूटिंग को संभालती है। जब DNS इन एंडपॉइंट्स को रिज़ॉल्व करने में विफल रहा, तो DynamoDB APIs में उच्च विलंबता और त्रुटियाँ आईं।

यह प्रारंभिक समस्या जल्दी ही हल हो गई, लेकिन इसने एक श्रृंखला प्रतिक्रिया शुरू कर दी:
- मेटाडेटा संग्रहण के लिए DynamoDB पर उनकी निर्भरता के कारण EC2 इंस्टेंस लॉन्च विफल रहे।
- EC2 के आंतरिक सबसिस्टम (जो NLB हेल्थ की निगरानी के लिए जिम्मेदार है) में एक अंतर्निहित बग ने नेटवर्क कनेक्टिविटी समस्याओं को बढ़ा दिया, जिससे लोड बैलेंसिंग और API कॉल्स में व्यापक बाधाएँ उत्पन्न हुईं।
- ओवरलोड को रोकने के लिए पुनर्प्राप्ति प्रयासों में थ्रॉटलिंग (जैसे, EC2 लॉन्च और Lambda इनवोकेशन को सीमित करना) शामिल थी, लेकिन निर्भर सेवाओं से रिट्राइज़ ने दबाव को बढ़ा दिया।

AWS ने पुष्टि की कि यह एक साइबर हमला नहीं बल्कि एक इन्फ्रास्ट्रक्चर-संबंधी खराबी थी, संभवतः एक दोषपूर्ण DNS डेटाबेस अपडेट या बैकअप सिस्टम विफलता से जुड़ी हुई। वैश्विक प्रभाव इसलिए पड़ा क्योंकि us-east-1 IAM और Lambda जैसी सेवाओं के लिए महत्वपूर्ण कंट्रोल प्लेन होस्ट करता है, भले ही संसाधन अन्य क्षेत्रों में हों।

#### प्रभावित सेवाएँ
142 से अधिक AWS सेवाएँ प्रभावित हुईं, मुख्य रूप से वे जो DynamoDB, EC2, या us-east-1 एंडपॉइंट्स पर निर्भर हैं। मुख्य श्रेणियाँ:

- **डेटाबेस और स्टोरेज**: DynamoDB (प्राथमिक), RDS, Redshift, SQS (बैकलॉग)।
- **कम्प्यूट और ऑर्केस्ट्रेशन**: EC2 (लॉन्च), Lambda (इनवोकेशन, पोलिंग), ECS, EKS, Glue।
- **नेटवर्किंग और लोड बैलेंसिंग**: Network Load Balancer (हेल्थ चेक), API Gateway।
- **मॉनिटरिंग और प्रबंधन**: CloudWatch, CloudTrail, EventBridge, IAM (अपडेट), AWS Config।
- **अन्य**: Amazon Connect, Athena, और वैश्विक सुविधाएँ जैसे DynamoDB Global Tables।

सभी सेवाएँ पूरी तरह से डाउन नहीं थीं—कई में आंशिक त्रुटियाँ या देरी देखी गईं—लेकिन आपस में जुड़े होने के कारण छोटी समस्याएँ भी फैल गईं।

#### प्रभाव
आउटेज ने इंटरनेट-निर्भर अनुप्रयोगों के ~1/3 हिस्से को बाधित किया, जिससे दुनिया भर में अनुमानित 100+ मिलियन उपयोगकर्ता प्रभावित हुए। उच्च-प्रोफ़ाइल उदाहरण:
- **सोशल और मीडिया**: Snapchat (लॉगिन विफलताएँ), Reddit (आउटेज), Twitch (स्ट्रीमिंग समस्याएँ)।
- **गेमिंग**: Roblox (सर्वर क्रैश), Fortnite (मैचमेकिंग विफलताएँ)।
- **वित्त और भुगतान**: Venmo, Lloyds जैसे बैंक (लेन-देन में देरी), HMRC (UK कर सेवाएँ)।
- **रिटेल और ई-कॉमर्स**: Amazon की अपनी रिटेल साइट के हिस्से; एयरलाइन चेक-इन (जैसे, Delta, United में देरी)।
- **अन्य**: Alexa डिवाइस (वॉइस विफलताएँ), Twilio (संचार गड़बड़ियाँ)।

आर्थिक अनुमानों के अनुसार नुकसान $500 मिलियन+ आँका गया, साथ ही उपयोगकर्ताओं के घबराने के कारण साइबर सुरक्षा स्कैन में 300% की वृद्धि हुई। इसने इंटरनेट के केंद्रीकरण को रेखांकित किया: us-east-1 AWS ट्रैफ़िक का ~30% संभालता है, जिसके कारण मल्टी-AZ डिज़ाइन के बावजूद यह नाजुकता का एक बिंदु बन गया है।

#### समाधान और सीखे गए सबक
AWS ने लक्षित शमन उपायों के माध्यम से समस्या का समाधान किया: DNS ठीक करना, EC2/NLB के लिए सबसिस्टम पैच, और क्रमिक थ्रॉटल कमी। घटना के बाद, उन्होंने सलाह दी:
- विफल अनुरोधों को दोबारा कोशिश करना।
- DNS कैशे साफ करना।
- संसाधनों को कई AZs/क्षेत्रों में वितरित करना (जैसे, Auto Scaling Groups के माध्यम से)।
- रिट्राइज़ के खिलाफ बफर के रूप में सेवा कोटा और कैशिंग का उपयोग करना।

व्यापक टेकअवे में कंट्रोल प्लेन में बेहतर रिडंडेंसी, त्वरित ट्राइएज के लिए AI-चालित विसंगति पहचान, और विविध क्लाउड रणनीतियों की आवश्यकता शामिल है। AWS ने ग्राहकों के लिए एक पूर्ण मूल कारण विश्लेषण (RCA) करने की प्रतिबद्धता जताई, इस बात पर जोर देते हुए कि ऐसी घटनाएँ, हालांकि दुर्लभ, हाइपरस्केल पर स्केलिंग चुनौतियों को उजागर करती हैं।

यह 2021 के बाद से AWS की सबसे व्यापक us-east-1 आउटेज थी, लेकिन स्वचालित उपकरणों के कारण पुनर्प्राप्ति ऐतिहासिक मिसालों से तेज थी।

#### संदर्भ
- [AWS Health Dashboard: Service Health - Oct 20, 2025](https://health.aws.amazon.com/health/status?eventID=arn:aws:health:us-east-1::event/MULTIPLE_SERVICES/AWS_MULTIPLE_SERVICES_OPERATIONAL_ISSUE/AWS_MULTIPLE_SERVICES_OPERATIONAL_ISSUE_BA540_514A652BE1A)
- [AWS Outage Analysis: October 20, 2025](https://www.thousandeyes.com/blog/aws-outage-analysis-october-20-2025)
- [Amazon's massive AWS outage points to a key weakness in the modern internet](https://finance.yahoo.com/news/amazons-massive-aws-outage-points-to-a-key-weakness-in-the-modern-internet-145418180.html)
- [What caused Amazon's AWS outage, and why did so many major apps go offline?](https://www.aljazeera.com/news/2025/10/21/what-caused-amazons-aws-outage-and-why-did-so-many-major-apps-go-offline)
- [AWS Outage on October 20: How a DNS Glitch in DynamoDB Disrupted the Internet](https://medium.com/@ansababy/aws-outage-on-october-20-how-a-dns-glitch-in-dynamodb-disrupted-the-internet-40e1f0440a12)
- [Update - AWS services operating normally](https://www.aboutamazon.com/news/aws/aws-service-disruptions-outage-update)
- [AWS was down — live updates following massive outage](https://www.tomsguide.com/news/live/amazon-outage-october-2025)