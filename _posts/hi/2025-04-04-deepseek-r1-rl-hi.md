---
audio: false
generated: true
lang: hi
layout: post
title: डीपसीक आर 1
translated: true
type: note
---

DeepSeek R1 रीइन्फोर्समेंट लर्निंग (RL) के लिए एक नवीन दृष्टिकोण अपनाता है जो बड़े भाषा मॉडल (LLM) में **तर्क क्षमताओं** पर काफी जोर देता है। उनकी तकनीक पारंपरिक रीइन्फोर्समेंट लर्निंग फ्रॉम ह्यूमन फीडबैक (RLHF) से कई मुख्य पहलुओं में भिन्न है।

यहां वे तकनीकें दी गई हैं जिनका उपयोग DeepSeek ने R1 की रीइन्फोर्समेंट लर्निंग में किया:

**1. प्रारंभिक तर्क के लिए शुद्ध रीइन्फोर्समेंट लर्निंग (RL) (DeepSeek-R1-Zero):**

* **निर्देशों पर प्रारंभिक सुपरवाइज्ड फाइन-ट्यूनिंग (SFT) नहीं:** कई LLM के विपरीत जो पहले मानव-लिखित निर्देशों पर SFT से गुजरते हैं, DeepSeek-R1-Zero को बेस मॉडल (DeepSeek-V3-Base) पर सीधे **शुद्ध RL** के साथ प्रशिक्षित किया गया था।
* **ग्रुप रिलेटिव पॉलिसी ऑप्टिमाइजेशन (GRPO):** उन्होंने अपने मुख्य RL एल्गोरिदम के रूप में GRPO का उपयोग किया। GRPO को एक अलग क्रिटिक नेटवर्क की आवश्यकता को समाप्त करके प्रॉक्सिमल पॉलिसी ऑप्टिमाइजेशन (PPO) से अधिक कुशल बनाने के लिए डिजाइन किया गया है। यह उत्पन्न आउटपुट के एक समूह की तुलना करके बेसलाइन रिवार्ड का अनुमान लगाता है, उनकी गुणवत्ता के आधार पर सापेक्ष स्कोर निर्दिष्ट करता है। यह मॉडल को अपने स्वयं के पिछले प्रयासों की तुलना में बेहतर प्रतिक्रिया उत्पन्न करने के लिए प्रोत्साहित करता है।
* **नियम-आधारित रिवार्ड सिस्टम:** प्रारंभिक RL चरण के लिए केवल मानवीय प्राथमिकताओं पर निर्भर रहने के बजाय, DeepSeek-R1-Zero ने एक **नियम-आधारित रिवार्ड सिस्टम** का उपयोग किया। यह सिस्टम मुख्य रूप से इस पर केंद्रित था:
    * **सटीकता रिवार्ड:** मॉडल को सही उत्तर प्रदान करने के लिए पुरस्कृत करना, विशेष रूप से सत्यापन योग्य समाधान वाले कार्यों में जैसे गणित की समस्याएं (यह जांचना कि अंतिम उत्तर सही है या नहीं)।
    * **फॉर्मेट रिवार्ड:** मॉडल को एक विशिष्ट आउटपुट फॉर्मेट का पालन करने के लिए पुरस्कृत करना, विशेष रूप से अपनी तर्क प्रक्रिया को घेरने के लिए `` टैग का उपयोग करना। इसने चेन-ऑफ-थॉट तर्क के उद्भव को प्रोत्साहित किया।
* **उभरती हुई तर्क व्यवहार:** इस शुद्ध RL दृष्टिकोण ने DeepSeek-R1-Zero को स्वाभाविक रूप से प्रभावशाली तर्क कौशल विकसित करने की अनुमति दी, जिसमें स्व-सत्यापन, चिंतन और लंबी चेन-ऑफ-थॉट व्याख्याएं उत्पन्न करना शामिल है, इन व्यवहारों के लिए स्पष्ट मानवीय प्रदर्शनों के बिना।

**2. बेहतर पठनीयता और सामान्य क्षमताओं के लिए मल्टी-स्टेज ट्रेनिंग (DeepSeek-R1):**

DeepSeek-R1-Zero की सीमाओं (जैसे खराब पठनीयता और भाषा मिश्रण) को दूर करने के लिए, DeepSeek-R1 ने एक अधिक व्यापक मल्टी-स्टेज ट्रेनिंग पाइपलाइन का इस्तेमाल किया:

* **कोल्ड-स्टार्ट डेटा फाइन-ट्यूनिंग:** मुख्य RL चरण से पहले, बेस मॉडल को उच्च-गुणवत्ता, मानव-लिखित (या उत्पन्न और परिष्कृत) लंबी चेन-ऑफ-थॉट तर्क उदाहरणों के एक छोटे डेटासेट पर फाइन-ट्यून किया गया। इस "कोल्ड स्टार्ट" डेटा ने मॉडल को अधिक पठनीय और सुसंगत तर्क चरणों का उत्पादन करने की दिशा में मार्गदर्शन करने में मदद की।
* **तर्क-उन्मुख रीइन्फोर्समेंट लर्निंग (दूसरा RL स्टेज):** मॉडल ने तो बड़े पैमाने पर RL के दूसरे चरण (DeepSeek-R1-Zero के समान) से गुजरा, लेकिन एक अतिरिक्त **भाषा स्थिरता रिवार्ड** के साथ। इस रिवार्ड ने मॉडल को उसकी तर्क प्रक्रिया के भीतर भाषाओं को मिलाने के लिए दंडित किया।
* **सुपरवाइज्ड फाइन-ट्यूनिंग (SFT):** तर्क-उन्मुख RL के बाद, मॉडल को एक विविध डेटासेट पर आगे फाइन-ट्यून किया गया जिसमें तर्क डेटा (RL मॉडल से रिजेक्शन सैंपलिंग का उपयोग करके संश्लेषित, DeepSeek-V3 द्वारा निर्णयित) और सामान्य गैर-तर्क डेटा (चेन-ऑफ-थॉट के साथ संवर्धित) दोनों शामिल थे। इस SFT स्टेज का उद्देश्य मॉडल की सहायकता और हानिरहितता में सुधार करना था, जबकि उसके मजबूत तर्क क्षमताओं को संरक्षित रखा गया।
* **सभी परिदृश्यों के लिए RL (तीसरा RL स्टेज):** मॉडल की समग्र क्षमताओं और वांछित व्यवहारों के साथ संरेखण को और परिष्कृत करने के लिए व्यापक श्रेणी के परिदृश्यों से प्रॉम्प्ट का उपयोग करके एक अंतिम RL चरण आयोजित किया गया।

**पारंपरिक RLHF से मुख्य अंतर:**

* **व्यापक मानव प्राथमिकता डेटा पर कम निर्भरता:** हालांकि संश्लेषित डेटा की गुणवत्ता का निर्णय लेने में कुछ मानव मूल्यांकन शामिल हो सकता है, DeepSeek-R1 में मुख्य RL प्रशिक्षण ने भारी रूप से नियम-आधारित रिवार्ड का लाभ उठाया, विशेष रूप से प्रारंभिक चरणों में। यह सीधे मानव प्राथमिकता तुलना के बड़े मात्रा में डेटा एकत्र करने की लागत और जटिलता को कम करता है।
* **उभरते हुए तर्क पर जोर:** शुद्ध RL दृष्टिकोण का उद्देश्य मॉडल को प्रभावी तर्क रणनीतियों को *खोजने* के लिए प्रोत्साहित करना था, न कि केवल मानव-प्रदत्त तर्क के उदाहरणों से सीखना।
* **मल्टी-स्टेज दृष्टिकोण:** DeepSeek की पाइपलाइन में मजबूत तर्क और सामान्य भाषा क्षमताओं दोनों को प्राप्त करने के लिए प्री-ट्रेनिंग, लक्षित फाइन-ट्यूनिंग और विभिन्न रिवार्ड सिग्नल वाले कई RL चरणों के एक सावधानीपूर्वक समन्वित अनुक्रम शामिल हैं।

**रीइन्फोर्समेंट लर्निंग दिखाने के लिए कोड (वैचारिक और सरलीकृत):**

DeepSeek के RL प्रशिक्षण प्रक्रिया को पूरी तरह से दोहराने के लिए सीधा, चलने योग्य कोड उदाहरण प्रदान करना चुनौतीपूर्ण है क्योंकि यह जटिल और बड़े पैमाने पर है। हालांकि, निम्नलिखित वैचारिक PyTorch-जैसा स्निपेट GRPO और एक नियम-आधारित रिवार्ड के मूल विचार को दर्शाता है:

```python
import torch
import torch.optim as optim
from transformers import AutoModelForCausalLM, AutoTokenizer

# मान लें कि आपके पास एक प्री-ट्रेन्ड भाषा मॉडल और टोकनाइज़र है
model_name = "gpt2"  # एक अधिक उपयुक्त बेस मॉडल से बदलें
policy_model = AutoModelForCausalLM.from_pretrained(model_name)
tokenizer = AutoTokenizer.from_pretrained(model_name)
optimizer = optim.AdamW(policy_model.parameters(), lr=5e-6)
device = "cuda" if torch.cuda.is_available() else "cpu"
policy_model.to(device)

def generate_responses(prompt, num_responses=4, max_length=128):
    input_tokens = tokenizer(prompt, return_tensors="pt").to(device)
    outputs = policy_model.generate(
        input_tokens.input_ids,
        max_length=max_length,
        num_return_sequences=num_responses,
        do_sample=True,
        top_k=50,
        top_p=0.95,
        pad_token_id=tokenizer.eos_token_id
    )
    responses = [tokenizer.decode(output, skip_special_tokens=True) for output in outputs]
    return responses

def calculate_accuracy_reward(response):
    # एक गणित की समस्या के लिए सरलीकृत उदाहरण: "What is 2 + 2?"
    if "2 + 2" in response and "4" in response:
        return 1.0
    else:
        return 0.0

def calculate_format_reward(response):
    if "" in response:
        return 0.5
    else:
        return 0.0

def calculate_combined_reward(response):
    accuracy_reward = calculate_accuracy_reward(response)
    format_reward = calculate_format_reward(response)
    return accuracy_reward + format_reward

def train_step(prompt, num_samples=4):
    optimizer.zero_grad()
    responses = generate_responses(prompt, num_samples=num_samples)
    rewards = torch.tensor([calculate_combined_reward(resp) for resp in responses]).float().to(device)

    # सरलीकृत GRPO-जैसा अपडेट: उच्च रिवार्ड प्रतिक्रियाओं को प्रोत्साहित करें
    best_reward_index = torch.argmax(rewards)
    best_response = responses[best_reward_index]
    inputs = tokenizer(prompt + best_response, return_tensors="pt").to(device)
    outputs = policy_model(**inputs, labels=inputs.input_ids)
    loss = outputs.loss
    loss.backward()
    optimizer.step()
    return loss.item(), best_response, rewards.tolist()

# प्रशिक्षण लूप (बहुत सरलीकृत)
num_episodes = 10
training_prompts = ["Solve: 2 + 2 = ?", "Explain the concept of gravity