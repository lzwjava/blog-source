---
audio: false
generated: true
image: false
lang: hi
layout: post
model: x-ai/grok-4-fast
title: vLLM के साथ कुशल LLM अनुमान
translated: true
type: note
---

### vLLM: कुशल एलएलएम इन्फ़रेंस और सर्विंग

vLLM एक ओपन-सोर्स लाइब्रेरी है जिसे लार्ज लैंग्वेज मॉडल्स (एलएलएम) के तेज़, मेमोरी-कुशल इन्फ़रेंस और सर्विंग के लिए डिज़ाइन किया गया है। इसे मूल रूप से यूसी बर्कले के स्काई कंप्यूटिंग लैब में विकसित किया गया था, और अब यह एक कम्युनिटी-चालित प्रोजेक्ट है जिसका उपयोग ल्लामा या जीपीटी वेरिएंट जैसे एलएलएम को डिप्लॉय करने के लिए व्यापक रूप से प्रोडक्शन में किया जाता है। इसकी मुख्य नवीनता **PagedAttention** है, एक ऐसी तकनीक जो की-वैल्यू (केवी) कैश मेमोरी को वर्चुअल मेमोरी पेज की तरह मानती है, जिससे बर्बादी कम होती है और गतिशील रूप से गैर-सन्निकट ब्लॉक आवंटित करके उच्च थ्रूपुट सक्षम होता है।

#### यह कैसे काम करता है
- **निरंतर बैचिंग**: पारंपरिक सिस्टम की तरह जो पूरे बैच की प्रतीक्षा करते हैं, vLLM मध्य-निष्पादन में अनुरोधों को गतिशील रूप से जोड़ता/हटाता है, जिससे डिकोडिंग के दौरान आइडल GPU समय कम से कम होता है।
- **मेमोरी प्रबंधन**: PagedAttention केवी कैश (जो अनुक्रम लंबाई के साथ बढ़ता है) में फ़्रैग्मेंटेशन से बचाता है, जिससे OOM एरर के बिना लंबे कॉन्टेक्स्ट सपोर्ट होते हैं।
- **अनुकूलित निष्पादन**: तेज़ कर्नेल लॉन्च के लिए CUDA/HIP ग्राफ़ का उपयोग करता है, अटेंशन कम्प्यूटेशन के लिए FlashAttention/FlashInfer के साथ एकीकृत, और क्वांटिज़ेशन (जैसे, AWQ, GPTQ, FP8) को सपोर्ट करता है जिससे मेमोरी उपयोग 4x तक कम हो सकता है।
- **उन्नत सुविधाएँ**: इसमें स्पेक्युलेटिव डिकोडिंग (टोकन का अनुमान लगाने और सत्यापित करने के लिए), चंक्ड प्रीफिल (लंबे इनपुट के लिए), मल्टी-लोरा (मॉडल को ऑन-द-फ्लाई एडाप्ट करना), और डिस्ट्रिब्यूटेड पैरेललिज़्म (टेंसर, पाइपलाइन, एक्सपर्ट) शामिल हैं।

vLLM एक OpenAI-संगत API सर्वर एक्सपोज़ करता है, Hugging Face मॉडल्स के साथ सहजता से एकीकृत होता है, और विविध हार्डवेयर (NVIDIA/AMD/Intel GPU, TPU, CPU) पर चलता है। यह उच्च-थ्रूपुट परिदृश्यों के लिए आदर्श है, जो सर्विंग बेंचमार्क में Hugging Face Transformers जैसे बेसलाइन्स पर 2-10x स्पीडअप प्राप्त करता है।

#### मुख्य उपयोग के मामले
- स्ट्रीमिंग आउटपुट वाले चैटबॉट या API के लिए ऑनलाइन सर्विंग।
- सारांशीकरण जैसे कार्यों के लिए ऑफ़लाइन बैच इन्फ़रेंस।
- कस्टम प्लंबिंग के बिना मल्टी-GPU क्लस्टर तक स्केलिंग।

### Ray: एआई और पायथन ऐप्स को स्केल करने के लिए एकीकृत फ्रेमवर्क

Ray एक ओपन-सोर्स डिस्ट्रिब्यूटेड कंप्यूटिंग फ्रेमवर्क है जो पायथन कोड—विशेष रूप से AI/ML वर्कलोड—को एकल मशीन से विशाल क्लस्टर तक स्केल करना आसान बनाता है। एनीस्केल द्वारा बनाया गया (यूसी बर्कले की जड़ों के साथ), यह शेड्यूलिंग, फॉल्ट टॉलरेंस, और ऑर्केस्ट्रेशन जैसी डिस्ट्रिब्यूटेड सिस्टम की जटिलताओं को दूर करता है, जिससे डेवलपर्स तर्क पर ध्यान केंद्रित कर सकते हैं।

#### मुख्य घटक
- **Ray Core**: आधारशिला—कार्यों (समानांतर फ़ंक्शन), एक्टर्स (स्टेटफुल सेवाएं), और ऑब्जेक्ट्स (डिस्ट्रिब्यूटेड डेटा शेयरिंग) के लिए पायथनिक प्रिमिटिव्स। यह ऑटोस्केलिंग, रिट्राइज़, और रिसोर्स आवंटन को स्वचालित रूप से संभालता है।
- **Ray AI लाइब्रेरीज़**: कोर पर बने डोमेन-विशिष्ट टूल:
  - **Ray Data**: डेटासेट प्रीप्रोसेसिंग के लिए स्केलेबल ETL।
  - **Ray Train**: इंटीग्रेशन (PyTorch, TensorFlow, Hugging Face) के साथ डिस्ट्रिब्यूटेड ट्रेनिंग।
  - **Ray Tune**: स्केल पर हाइपरपैरामीटर ऑप्टिमाइज़ेशन।
  - **Ray Serve**: इन्फ़रेंस के लिए मॉडल डिप्लॉयमेंट, जिसमें रूटिंग, बैचिंग, और A/B टेस्टिंग शामिल है।
  - **RLlib**: रीइन्फोर्समेंट लर्निंग टूलकिट।
- **Ray क्लस्टर**: क्लाउड (AWS, GCP), कुबेरनेट्स, या ऑन-प्रिम पर डिप्लॉय करने के लिए मैनेज्ड इन्फ्रास्ट्रक्चर लेयर, जो मांग के आधार पर ऑटोस्केलिंग के साथ आती है।

#### यह कैसे काम करता है
Ray नोड्स पर एक डेमन के रूप में चलता है, जो समन्वय के लिए एक हेड नोड के साथ एक क्लस्टर बनाता है। आप समानांतरता के लिए फ़ंक्शन को `@ray.remote` से डेकोरेट करते हैं, और यह निष्पादन को CPU/GPU में वितरित करता है। ML के लिए, Serve जैसी लाइब्रेरी HTTP एंडपॉइंट और लोड बैलेंसिंग संभालती हैं, जबकि Core फॉल्ट-टॉलरेंट स्केलिंग सुनिश्चित करता है (जैसे, विफल कार्यों को पुनः आरंभ करना)।

#### ML/AI में मुख्य उपयोग के मामले
- एंड-टू-एंड पाइपलाइन: डेटा लोडिंग → ट्रेनिंग → ट्यूनिंग → सर्विंग।
- 100s GPU पर हाइपरपैरामीटर स्वीप।
- रियल-टाइम इन्फ़रेंस फ्लीट, जैसे रिकमेंडेशन सिस्टम में।
- कस्टम डिस्ट्रिब्यूटेड ऐप, जैसे सिमुलेशन।

Ray की ताकत इसका एकीकृत API है: एक बार लिखें, कहीं भी स्केल करें, स्पार्क या डास्क की तुलना में कम बॉयलरप्लेट के साथ।

### vLLM और Ray के बीच संबंध
SLOs-Serve जैसी प्रणालियों में (हमारी पिछली चर्चा से), vLLM लो-लेवल एलएलएम बैचिंग/इन्फ़रेंस संभालता है, जबकि Ray मल्टी-रिप्लिका डिप्लॉयमेंट को ऑर्केस्ट्रेट करता है, बर्स्ट हैंडलिंग और स्केलिंग के लिए नोड्स में अनुरोधों को रूट करता है।

[vLLM डॉक्युमेंटेशन](https://docs.vllm.ai/en/latest/)
[Ray डॉक्युमेंटेशन](https://docs.ray.io/en/latest/ray-overview/index.html)