---
audio: false
generated: true
image: false
lang: hi
layout: post
model: x-ai/grok-4-fast
title: आरएनएन पाठ जनरेशन तापमान को नियंत्रित करना
translated: true
type: note
---

### आरएनएन सैंपलिंग में तापमान को समझना

टेक्स्ट जनरेशन के लिए रिकरंट न्यूरल नेटवर्क्स (आरएनएन) में—जैसे कि आंद्रेज कार्पाथी अपने प्रसिद्ध ब्लॉग पोस्ट में करैक्टर-लेवल लैंग्वेज मॉडल्स पर एक्सप्लोर करते हैं—तापमान एक हाइपरपैरामीटर है जो यह ट्वीक करता है कि जनरेट किया गया टेक्स्ट कितना "रैंडम" या "प्रेडिक्टेबल" महसूस होता है। इसे सैंपलिंग स्टेप के दौरान लागू किया जाता है, जहां आरएनएन पिछले करैक्टर्स के आधार पर अगले करैक्टर (या टोकन) की प्रेडिक्ट करता है। तापमान नियंत्रण के बिना, जनरेशन बहुत ज्यादा रिजिड (हमेशा सबसे ज्यादा संभावित अगला करैक्टर चुनना, जिससे बोरिंग लूप बनते हैं) या बहुत ज्यादा वाइल्ड (प्योर रैंडमनेस) हो सकता है। तापमान संभावित अगले करैक्टर्स पर मॉडल की प्रोबेबिलिटी डिस्ट्रीब्यूशन को सॉफ्टन करके एक बैलेंस बनाता है।

#### इसके पीछे का क्विक मैथ
आरएनएन हर संभावित अगले करैक्टर के लिए *लॉजिट्स* (रॉ, अननॉर्मलाइज्ड स्कोर) आउटपुट करता है। इन्हें सॉफ्टमैक्स फंक्शन का उपयोग करके प्रोबेबिलिटीज में बदला जाता है:

\\[
p_i = \frac{\exp(\text{logit}_i / T)}{\sum_j \exp(\text{logit}_j / T)}
\\]

- \\(T\\) तापमान है (आमतौर पर 0.1 और 2.0 के बीच)।
- जब \\(T = 1\\), यह स्टैंडर्ड सॉफ्टमैक्स है: प्रोबेबिलिटीज मॉडल के "नेचुरल" कॉन्फिडेंस को रिफ्लेक्ट करती हैं।
- फिर आप इस डिस्ट्रीब्यूशन से अगला करैक्टर *सैंपल* करते हैं (उदाहरण के लिए, मल्टीनोमियल सैंपलिंग के जरिए) बजाय हमेशा सबसे हाई-प्रोबेबिलिटी वाले को चुनने (ग्रीडी डिकोडिंग) के।

यह सैंपलिंग इटरेटिवली होती है: चुने गए करैक्टर को इनपुट के रूप में वापस फीड करें, अगला प्रेडिक्ट करें, और एक सीक्वेंस जनरेट करने के लिए दोहराएं।

#### लो तापमान: रिपीटिटिव लेकिन सेफ
- **प्रभाव**: \\(T < 1\\) (उदाहरण के लिए, 0.5 या 0 के नजदीक) डिस्ट्रीब्यूशन को *शार्पन* करता है। हाई-कॉन्फिडेंस प्रेडिक्शन्स की प्रोबेबिलिटीज और भी ज्यादा हो जाती हैं, जबकि लो वाले जीरो की तरफ स्क्वैश हो जाते हैं।
- **आउटपुट**: टेक्स्ट "सेफ" और कोहिरेंट रहता है लेकिन जल्दी ही रिपीटिटिव हो जाता है। मॉडल सबसे प्रोबेबल पाथ्स पर चिपक जाता है, जैसे किसी लूप में फंस जाना।
- **कार्पाथी के पोस्ट से उदाहरण** (पॉल ग्राहम-स्टाइल निबंध जनरेट करना): बहुत लो तापमान पर, यह कुछ इस तरह का आउटपुट देता है:
  > “is that they were all the same thing that was a startup is that they were all the same thing that was a startup is that they were all the same thing that was a startup is that they were all the same”

  यह कॉन्फिडेंट और व्याकरण की दृष्टि से सही है लेकिन इसमें क्रिएटिविटी की कमी है—ट्रेनिंग डेटा के अनंत इकोज की तरह सोचें।

#### हाई तापमान: क्रिएटिव लेकिन एरैटिक
- **प्रभाव**: \\(T > 1\\) (उदाहरण के लिए, 1.5 या 2.0) डिस्ट्रीब्यूशन को *फ्लैटन* करता है। प्रोबेबिलिटीज ज्यादा यूनिफॉर्म हो जाती हैं, जिससे अंडरडॉग्स (कम संभावित करैक्टर्स) को बेहतर मौका मिलता है।
- **आउटपुट**: ज्यादा डाइवर्स और इन्वेंटिव टेक्स्ट, लेकिन यह नॉनसेंस, स्पेलिंग एरर्स, या वाइल्ड टेंजेंट्स में उतर सकता है। मॉडल दुर्लभ पाथ्स का पता लगाता है, जो ओरिजिनैलिटी पैदा कर सकता है लेकिन कोहिरेंस की कीमत पर।
- **कार्पाथी से उदाहरण**: हाई टेंप पर फ्रेश फ्रेजिंग के साथ वैराइड एसेज मिलते हैं, लेकिन "अधिक गलतियों (जैसे स्पेलिंग गलतियाँ, आदि) की कीमत पर।" यह ऐसा है जैसे आरएनएन फ्रीली ब्रेनस्टॉर्मिंग कर रहा हो—कविता या साइ-फाइ के लिए फन, फैक्चुअल राइटिंग के लिए रिस्की।

#### आरएनएन में यह महत्वपूर्ण क्यों है
आरएनएन (या कार्पाथी के सेटअप में एलएसटीएम) ऑटोरिग्रेसिव हैं: हर प्रेडिक्शन पिछले पर बनती है, इसलिए सैंपलिंग में छोटे बायसेज लंबी सीक्वेंस पर कंपाउंड हो जाते हैं। तापमान आपको एक्सप्लोरेशन बनाम एक्सप्लॉयटेशन को ट्यून करने देता है:
- विश्वसनीय, ऑन-ब्रांड टेक्स्ट (उदाहरण के लिए, प्रोडक्ट डिस्क्रिप्शन) के लिए लो \\(T\\) का उपयोग करें।
- ब्रेनस्टॉर्मिंग या आर्टिस्टिक जनरेशन (उदाहरण के लिए, सरियल स्टोरीज) के लिए इसे बढ़ाएं।
- कोड में एक्सपेरिमेंट करें: कार्पाथी के वैनिला आरएनएन रेपो में एक टेंपरेचर स्लाइडर शामिल है—ब्लैंड सॉनेट्स से लेकर कैओटिक वर्स तक के शिफ्ट को देखने के लिए शेक्सपियर सैंपल्स पर इसे आजमाएं।

संक्षेप में, तापमान कुछ नया "सीख" नहीं रहा है; यह सिर्फ इन्फरेंस के दौरान आरएनएन के क्रिएटिविटी नॉब को डायल कर रहा है। लो इसे चिल और प्रेडिक्टेबल रखता है; हाई कैओस को अनलीश करता है।

[The Unreasonable Effectiveness of Recurrent Neural Networks](http://karpathy.github.io/2015/05/21/rnn-effectiveness/)