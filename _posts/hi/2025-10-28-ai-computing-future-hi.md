---
audio: false
generated: true
image: false
lang: hi
layout: post
model: x-ai/grok-4-fast
title: कम्प्यूटिंग एआई के भविष्य को संचालित करती है
translated: true
type: note
---

स्टैनफोर्ड CS25 (ट्रांसफॉर्मर आर्किटेक्चर पर एक कोर्स) में, ओपनएआई के एक रिसर्च साइंटिस्ट, ह्युंग वॉन चुंग ने "एआई के भविष्य को आकार देना" (V4 श्रृंखला का हिस्सा) शीर्षक से एक अतिथि व्याख्यान दिया। वह किसी भी क्षेत्र में बड़े बदलावों के पीछे "प्रमुख प्रेरक शक्ति" की पहचान करने के महत्व पर चर्चा करते हैं ताकि उसके भविष्य के रास्ते का प्रभावी ढंग से अनुमान लगाया जा सके।

### उनकी चर्चा के मुख्य बिंदु
- **द पेन ड्रॉप एनालॉजी**: इसे स्पष्ट करने के लिए, चुंग एक साधारण भौतिकी का उदाहरण देते हैं: यदि आप एक पेन गिराते हैं, तो उसका रास्ता अनुमानित होता है क्योंकि हम **प्रमुख शक्ति**—गुरुत्वाकर्षण—को समझते हैं, जो वायु प्रतिरोध जैसे छोटे कारकों को ओवरराइड कर देती है। इस प्रमुख शक्ति पर ध्यान केंद्रित करके, हम जटिल प्रणालियों को सरल बनाते हैं और विश्वसनीय भविष्यवाणियाँ करते हैं। उनका तर्क है कि हमें एआई पर भी यही लेंस लगाना चाहिए।

- **एआई में प्रमुख शक्ति**: एआई रिसर्च के लिए, प्रमुख प्रेरक शक्ति **कम्प्यूट लागत में घातीय कमी** (यानी, सस्ती और अधिक प्रचुर कम्प्यूटेशनल शक्ति) है। यह तेजी से प्रगति का मुख्य साधक रही है, जिसने मॉडलों को अधिक डेटा और पैरामीटर्स के साथ स्केल करने की अनुमति दी है। चुंग इस बात पर जोर देते हैं कि इस शक्ति को समझना ओवर-इंजीनियर्ड, बायस-हैवी डिजाइनों के बजाय स्केलेबल, सामान्य तरीकों के निर्माण की ओर ध्यान खिसकाता है।

- **"बिटर लेसन" से कनेक्शन**: वह रिच सटन के "बिटर लेसन" निबंध का हवाला देते हैं, जो तर्क देता है कि एआई की उन्नति मानव-निर्मित इंडक्टिव बायसेज के बजाय सरल, लर्निंग-आधारित दृष्टिकोणों (जैसे ट्रांसफॉर्मर्स) के साथ अधिक कम्प्यूट और डेटा का लाभ उठाने से आती है। कम्प्यूट को प्रमुख शक्ति मानते हुए, आर्किटेक्चर को स्केलेबिलिटी को प्राथमिकता देनी चाहिए—उदाहरण के लिए, डिकोडर-ओनली ट्रांसफॉर्मर्स को अधिक जटिल एनकोडर-डिकोडर सेटअप पर तरजीह देना, क्योंकि अतिरिक्त संरचना प्रचुर संसाधनों के साथ अनावश्यक हो जाती है।

- **व्यापक निहितार्थ**: चुंग कम्प्यूट वृद्धि बनाम एआई प्रदर्शन के ऐतिहासिक प्लॉट्स दिखाते हैं, और नोट करते हैं कि अतीत के रुझान (जैसे मूर का लॉ) यह सुझाव देते हैं कि निरंतर स्केलिंग भविष्य की क्रांतियों को चलाती रहेगी। उनकी सलाह है: "जब हम भविष्य के बारे में बात करते हैं, तो सलाह लेने की सबसे अच्छी जगह इतिहास में झांकना है।"

यह ढांचा यह समझाने में मदद करता है कि हाल के एआई मॉडल (जैसे, ओपनएआई के) फाइन-ट्यून्ड ट्वीक्स पर बड़े पैमाने पर स्केलिंग पर जोर क्यों देते हैं।

[Stanford CS25: V4 - Shaping the Future of AI with Hyung Won Chung](https://www.linkedin.com/pulse/stanford-cs25-v4-shaping-future-ai-hyung-won-chung-tarazona-md-lf9pe)  
[Full Lecture Video on YouTube](https://www.youtube.com/watch?v=orDKvo8h71o)