---
audio: false
generated: true
lang: hi
layout: post
title: निर्णय वृक्ष
translated: true
type: note
---

एक **डिसीजन ट्री** (निर्णय वृक्ष) मशीन लर्निंग और डेटा विश्लेषण में उपयोग किया जाने वाला एक प्रेडिक्टिव मॉडलिंग टूल है जो इनपुट डेटा के आधार पर निर्णय लेने के लिए प्रयोग किया जाता है। यह निर्णयों और उनके संभावित परिणामों, जिसमें संयोग घटनाओं के परिणाम भी शामिल हैं, को एक वृक्ष जैसी संरचना में दर्शाता है। डिसीजन ट्री का व्यापक रूप से क्लासिफिकेशन (जैसे, यह भविष्यवाणी करना कि कोई ग्राहक उत्पाद खरीदेगा या नहीं) और रिग्रेशन (जैसे, घर की कीमतों का अनुमान लगाना) जैसे कार्यों के लिए उपयोग किया जाता है। ये सहज ज्ञान युक्त, व्याख्या करने में आसान और सरल तथा जटिल दोनों प्रकार के डेटासेट के लिए प्रभावी होते हैं।

यह व्यापक गाइड बताएगी कि डिसीजन ट्री क्या है, यह कैसे काम करता है, इसके घटक, निर्माण प्रक्रिया, फायदे, सीमाएँ और व्यावहारिक विचार, साथ ही उदाहरण भी।

---

### **डिसीजन ट्री क्या है?**

एक डिसीजन ट्री निर्णयों और उनके संभावित परिणामों का एक फ्लोचार्ट जैसा निरूपण है। इसमें नोड्स और शाखाएँ शामिल होती हैं:
- **नोड्स**: निर्णयों, शर्तों या परिणामों का प्रतिनिधित्व करते हैं।
- **शाखाएँ**: किसी निर्णय या शर्त के संभावित परिणामों का प्रतिनिधित्व करती हैं।
- **लीफ नोड्स**: अंतिम आउटपुट (जैसे, क्लासिफिकेशन के लिए एक वर्ग लेबल या रिग्रेशन के लिए एक संख्यात्मक मान) का प्रतिनिधित्व करते हैं।

डिसीजन ट्री का उपयोग सुपरवाइज्ड लर्निंग में किया जाता है, जहां मॉडल लेबल किए गए ट्रेनिंग डेटा से सीखता है ताकि नए, अदृश्य डेटा के लिए परिणामों का अनुमान लगाया जा सके। ये बहुमुखी हैं और सांख्यिक (कैटेगोरिकल) तथा संख्यात्मक (न्यूमेरिकल) दोनों प्रकार के डेटा को हैंडल कर सकते हैं।

---

### **डिसीजन ट्री के घटक**

1. **रूट नोड**:
   - वृक्ष का सबसे ऊपरी नोड।
   - पूरे डेटासेट और प्रारंभिक निर्णय बिंदु का प्रतिनिधित्व करता है।
   - यह उस फीचर के आधार पर विभाजित होता है जो सबसे अधिक जानकारी प्रदान करता है या अनिश्चितता को सबसे अधिक कम करता है।

2. **इंटरनल नोड्स**:
   - रूट और लीफ नोड्स के बीच के नोड्स।
   - विशिष्ट फीचर्स और शर्तों (जैसे, "क्या आयु > 30?") के आधार पर मध्यवर्ती निर्णय बिंदुओं का प्रतिनिधित्व करते हैं।

3. **शाखाएँ**:
   - नोड्स के बीच संबंध।
   - किसी निर्णय या शर्त के परिणाम का प्रतिनिधित्व करती हैं (जैसे, बाइनरी स्प्लिट के लिए "हाँ" या "नहीं")।

4. **लीफ नोड्स**:
   - टर्मिनल नोड्स जो अंतिम आउटपुट का प्रतिनिधित्व करते हैं।
   - क्लासिफिकेशन में, लीफ वर्ग लेबल्स (जैसे, "खरीदें" या "न खरीदें") का प्रतिनिधित्व करते हैं।
   - रिग्रेशन में, लीफ संख्यात्मक मान (जैसे, एक अनुमानित कीमत) का प्रतिनिधित्व करते हैं।

---

### **डिसीजन ट्री कैसे काम करता है?**

एक डिसीजन ट्री फीचर मानों के आधार पर इनपुट डेटा को क्षेत्रों में पुनरावर्ती रूप से विभाजित करके काम करता है, फिर उस क्षेत्र में बहुमत वर्ग या औसत मान के आधार पर निर्णय लेता है। यहां बताया गया है कि यह कैसे संचालित होता है:

1. **इनपुट डेटा**:
   - डेटासेट में फीचर्स (स्वतंत्र चर) और एक टारगेट वेरिएबल (आश्रित चर) होता है।
   - उदाहरण के लिए, एक डेटासेट जो भविष्यवाणी करता है कि कोई ग्राहक उत्पाद खरीदेगा या नहीं, उसमें फीचर्स में आयु, आय और ब्राउजिंग समय शामिल हो सकते हैं, जबकि टारगेट "खरीदें" या "न खरीदें" होगा।

2. **डेटा को विभाजित करना**:
   - एल्गोरिदम डेटा को सबसेट में विभाजित करने के लिए एक फीचर और एक थ्रेशोल्ड (जैसे, "आयु > 30") चुनता है।
   - लक्ष्य ऐसे विभाजन बनाना है जो वर्गों के पृथक्करण (क्लासिफिकेशन के लिए) को अधिकतम करें या विचरण (रिग्रेशन के लिए) को कम करें।
   - विभाजन मानदंडों में **जिनी इम्प्योरिटी**, **इनफॉर्मेशन गेन** या **वेरिएंस रिडक्शन** जैसे मेट्रिक्स शामिल हैं (नीचे समझाया गया है)।

3. **पुनरावर्ती विभाजन**:
   - एल्गोरिदम प्रत्येक सबसेट के लिए विभाजन प्रक्रिया को दोहराता है, नए नोड्स और शाखाएँ बनाता है।
   - यह तब तक जारी रहता है जब तक कोई स्टॉपिंग क्राइटेरिया पूरा नहीं हो जाता (जैसे, अधिकतम गहराई, प्रति नोड न्यूनतम नमूने, या कोई और सुधार न होना)।

4. **आउटपुट निर्दिष्ट करना**:
   - एक बार विभाजन रुक जाने के बाद, प्रत्येक लीफ नोड को एक अंतिम आउटपुट निर्दिष्ट किया जाता है।
   - क्लासिफिकेशन के लिए, लीफ उस क्षेत्र में बहुमत वर्ग का प्रतिनिधित्व करता है।
   - रिग्रेशन के लिए, लीफ उस क्षेत्र में टारगेट मानों के औसत (या माध्यिका) का प्रतिनिधित्व करता है।

5. **भविष्यवाणी**:
   - किसी नए डेटा पॉइंट के लिए परिणाम की भविष्यवाणी करने के लिए, वृक्ष रूट से लीफ तक डेटा पॉइंट के फीचर मानों के आधार पर निर्णय नियमों का पालन करते हुए चलता है।
   - लीफ नोड अंतिम भविष्यवाणी प्रदान करता है।

---

### **विभाजन मानदंड**

एक स्प्लिट की गुणवत्ता यह निर्धारित करती है कि वृक्ष डेटा को कितनी अच्छी तरह अलग करता है। सामान्य मानदंडों में शामिल हैं:

1. **जिनी इम्प्योरिटी (क्लासिफिकेशन)**:
   - एक नोड की अशुद्धता को मापता है (कितने मिश्रित वर्ग हैं)।
   - सूत्र: \( \text{जिनी} = 1 - \sum_{i=1}^n (p_i)^2 \), जहाँ \( p_i \) नोड में वर्ग \( i \) का अनुपात है।
   - कम जिनी इम्प्योरिटी एक बेहतर स्प्लिट (अधिक सजातीय नोड) का संकेत देती है।

2. **इनफॉर्मेशन गेन (क्लासिफिकेशन)**:
   - **एन्ट्रॉपी** पर आधारित, जो एक नोड में यादृच्छिकता या अनिश्चितता को मापती है।
   - एन्ट्रॉपी: \( \text{एन्ट्रॉपी} = - \sum_{i=1}^n p_i \log_2(p_i) \)।
   - इनफॉर्मेशन गेन = स्प्लिट से पहले की एन्ट्रॉपी - स्प्लिट के बाद की भारित औसत एन्ट्रॉपी।
   - उच्चतर इनफॉर्मेशन गेन एक बेहतर स्प्लिट का संकेत देता है।

3. **वेरिएंस रिडक्शन (रिग्रेशन)**:
   - स्प्लिट के बाद टारगेट वेरिएबल के विचरण में कमी को मापता है।
   - विचरण: \( \text{विचरण} = \frac{1}{n} \sum_{i=1}^n (y_i - \bar{y})^2 \), जहाँ \( y_i \) एक टारगेट मान है और \( \bar{y} \) माध्य है।
   - एल्गोरिदम उस स्प्लिट का चयन करता है जो विचरण में कमी को अधिकतम करता है।

4. **ची-स्क्वायर (क्लासिफिकेशन)**:
   - परीक्षण करता है कि क्या स्प्लिट वर्गों के वितरण में महत्वपूर्ण सुधार करता है।
   - कुछ एल्गोरिदम जैसे CHAID में उपयोग किया जाता है।

एल्गोरिदम प्रत्येक फीचर के लिए सभी संभावित स्प्लिट्स का मूल्यांकन करता है और सबसे अच्छे स्कोर (जैसे, सबसे कम जिनी इम्प्योरिटी या उच्चतम इनफॉर्मेशन गेन) वाले का चयन करता है।

---

### **डिसीजन ट्री कैसे बनाया जाता है?**

डिसीजन ट्री बनाने में निम्नलिखित चरण शामिल हैं:

1. **सर्वोत्तम फीचर का चयन करें**:
   - चुने गए मानदंड (जैसे, जिनी, इनफॉर्मेशन गेन) का उपयोग करके सभी फीचर्स और संभावित स्प्लिट पॉइंट्स का मूल्यांकन करें।
   - उस फीचर और थ्रेशोल्ड को चुनें जो डेटा को सबसे अच्छी तरह अलग करता है।

2. **डेटा को विभाजित करें**:
   - चयनित फीचर और थ्रेशोल्ड के आधार पर डेटासेट को सबसेट में विभाजित करें।
   - प्रत्येक सबसेट के लिए चाइल्ड नोड्स बनाएं।

3. **पुनरावर्ती रूप से दोहराएँ**:
   - प्रत्येक चाइल्ड नोड पर तब तक समान प्रक्रिया लागू करें जब तक कोई स्टॉपिंग कंडीशन पूरी न हो जाए:
     - अधिकतम वृक्ष गहराई तक पहुँच गया।
     - एक नोड में नमूनों की न्यूनतम संख्या।
     - विभाजन मानदंड में कोई महत्वपूर्ण सुधार नहीं।
     - एक नोड के सभी नमूने एक ही वर्ग के हैं (क्लासिफिकेशन के लिए) या समान मान रखते हैं (रिग्रेशन के लिए)।

4. **वृक्ष को प्रून करें (वैकल्पिक)**:
   - ओवरफिटिंग को रोकने के लिए, उन शाखाओं को हटाकर वृक्ष की जटिलता को कम करें जो भविष्यवाणी सटीकता में बहुत कम योगदान करती हैं।
   - प्रूनिंग **प्री-प्रूनिंग** (निर्माण के दौरान जल्दी रोकना) या **पोस्ट-प्रूनिंग** (निर्माण के बाद शाखाओं को हटाना) हो सकती है।

---

### **उदाहरण: क्लासिफिकेशन डिसीजन ट्री**

**डेटासेट**: आयु, आय और ब्राउजिंग समय के आधार पर भविष्यवाणी करना कि कोई ग्राहक उत्पाद खरीदेगा या नहीं।

| आयु | आय   | ब्राउजिंग समय | खरीदें? |
|-----|------|---------------|---------|
| 25  | कम   | कम            | नहीं    |
| 35  | उच्च | अधिक          | हाँ     |
| 45  | मध्यम | मध्यम        | हाँ     |
| 20  | कम   | कम            | नहीं    |
| 50  | उच्च | अधिक          | हाँ     |

**चरण 1: रूट नोड**:
- सर्वोत्तम स्प्लिट के लिए सभी फीचर्स (आयु, आय, ब्राउजिंग समय) का मूल्यांकन करें।
- मान लीजिए "आय = उच्च" सबसे अधिक इनफॉर्मेशन गेन देता है।
- डेटा को विभाजित करें:
  - आय = उच्च: सभी "हाँ" (शुद्ध नोड, यहीं रुकें)।
  - आय = कम या मध्यम: मिश्रित (विभाजन जारी रखें)।

**चरण 2: चाइल्ड नोड**:
- "कम या मध्यम आय" सबसेट के लिए, शेष फीचर्स का मूल्यांकन करें।
- मान लीजिए "आयु > 30" सबसे अच्छा स्प्लिट देता है:
  - आयु > 30: अधिकतर "हाँ।"
  - आयु ≤ 30: सभी "नहीं।"

**चरण 3: रुकें**:
- सभी नोड्स शुद्ध हैं (इसमें केवल एक वर्ग है) या स्टॉपिंग क्राइटेरिया को पूरा करते हैं।
- वृक्ष इस प्रकार दिखता है:
  - रूट: "क्या आय उच्च है?"
    - हाँ → लीफ: "खरीदें"
    - नहीं → "क्या आयु > 30 है?"
      - हाँ → लीफ: "खरीदें"
      - नहीं → लीफ: "न खरीदें"

**भविष्यवाणी**:
- नया ग्राहक: आयु = 40, आय = मध्यम, ब्राउजिंग समय = कम।
- पथ: आय ≠ उच्च → आयु = 40 > 30 → "खरीदें" की भविष्यवाणी करें।

---

### **उदाहरण: रिग्रेशन डिसीजन ट्री**

**डेटासेट**: आकार और स्थान के आधार पर घर की कीमतों का अनुमान लगाना।

| आकार (वर्ग फुट) | स्थान    | कीमत ($K) |
|------------------|----------|------------|
| 1000             | शहरी     | 300        |
| 1500             | उपनगरीय | 400        |
| 2000             | शहरी     | 600        |
| 800              | ग्रामीण  | 200        |

**चरण 1: रूट नोड**:
- स्प्लिट्स (जैसे, आकार > 1200, स्थान = शहरी) का मूल्यांकन करें।
- मान लीजिए "आकार > 1200" विचरण को कम करता है।
- विभाजन:
  - आकार > 1200: कीमतें = {400, 600} (माध्य = 500)।
  - आकार ≤ 1200: कीमतें = {200, 300} (माध्य = 250)।

**चरण 2: रुकें**:
- नोड्स पर्याप्त छोटे हैं या विचरण में कमी न्यूनतम है।
- वृक्ष:
  - रूट: "क्या आकार > 1200 है?"
    - हाँ → लीफ: $500K की भविष्यवाणी करें।
    - नहीं → लीफ: $250K की भविष्यवाणी करें।

**भविष्यवाणी**:
- नया घर: आकार = 1800, स्थान = शहरी → आकार > 1200 → $500K की भविष्यवाणी करें।

---

### **डिसीजन ट्री के फायदे**

1. **व्याख्यात्मकता**:
   - समझने और विज़ुअलाइज़ करने में आसान, जिससे गैर-तकनीकी हितधारकों को निर्णय समझाने के लिए आदर्श बनते हैं।
2. **मिश्रित डेटा को हैंडल करता है**:
   - व्यापक प्रीप्रोसेसिंग के बिना सांख्यिक और संख्यात्मक दोनों प्रकार के फीचर्स के साथ काम करता है।
3. **नॉन-पैरामेट्रिक**:
   - अंतर्निहित डेटा वितरण के बारे में कोई धारणा नहीं।
4. **फीचर महत्व**:
   - पहचानता है कि कौन से फीचर्स भविष्यवाणियों में सबसे अधिक योगदान करते हैं।
5. **तेज़ भविष्यवाणी**:
   - एक बार प्रशिक्षित होने के बाद, भविष्यवाणियाँ तेज़ होती हैं क्योंकि इसमें सरल तुलनाएँ शामिल होती हैं।

---

### **डिसीजन ट्री की सीमाएँ**

1. **ओवरफिटिंग**:
   - गहरे वृक्ष ट्रेनिंग डेटा को याद कर सकते हैं, जिससे सामान्यीकरण खराब होता है।
   - समाधान: प्रूनिंग का उपयोग करें, अधिकतम गहराई सीमित करें, या प्रति नोड न्यूनतम नमूने निर्धारित करें।
2. **अस्थिरता**:
   - डेटा में छोटे बदलाव से पूरी तरह से अलग वृक्ष बन सकते हैं।
   - समाधान: रैंडम फॉरेस्ट्स या ग्रेडिएंट बूस्टिंग जैसे एन्सेंबल मेथड्स का उपयोग करें।
3. **प्रभावी वर्गों के प्रति पक्षपाती**:
   - असंतुलित डेटासेट के साथ संघर्ष करता है जहां एक वर्ग हावी होता है।
   - समाधान: क्लास वेटिंग या ओवरसैंपलिंग जैसी तकनीकों का उपयोग करें।
4. **लालची दृष्टिकोण**:
   - स्प्लिट्स स्थानीय अनुकूलन के आधार पर चुने जाते हैं, जिससे वैश्विक रूप से इष्टतम वृक्ष नहीं बन सकता है।
5. **रैखिक संबंधों को हैंडल करने में कमज़ोर**:
   - उन डेटासेट के लिए कम प्रभावी जहां फीचर्स और टारगेट के बीच संबंध रैखिक या जटिल हैं।

---

### **व्यावहारिक विचार**

1. **हाइपरपैरामीटर्स**:
   - **अधिकतम गहराई**: ओवरफिटिंग को रोकने के लिए वृक्ष की गहराई को सीमित करती है।
   - **न्यूनतम नमूना विभाजन**: एक नोड को विभाजित करने के लिए आवश्यक नमूनों की न्यूनतम संख्या।
   - **न्यूनतम नमूना लीफ**: एक लीफ नोड में नमूनों की न्यूनतम संख्या।
   - **अधिकतम फीचर्स**: प्रत्येक स्प्लिट के लिए विचार करने योग्य फीचर्स की संख्या।

2. **प्रूनिंग**:
   - प्री-प्रूनिंग: वृक्ष निर्माण के दौरान बाधाएँ निर्धारित करना।
   - पोस्ट-प्रूनिंग: वैलिडेशन प्रदर्शन के आधार पर वृक्ष बनाने के बाद शाखाओं को हटाना।

3. **लापता मानों को हैंडल करना**:
   - कुछ एल्गोरिदम (जैसे, CART) लापता मानों को उस शाखा को निर्दिष्ट करते हैं जो त्रुटि को कम करती है।
   - वैकल्पिक रूप से, प्रशिक्षण से पहले लापता मानों को इम्प्यूट करें।

4. **स्केलेबिलिटी**:
   - डिसीजन ट्री छोटे से मध्यम डेटासेट के लिए कम्प्यूटेशनल रूप से कुशल हैं लेकिन बहुत बड़े डेटासेट और कई फीचर्स के लिए धीमे हो सकते हैं।

5. **एन्सेंबल मेथड्स**:
   - सीमाओं को दूर करने के लिए, डिसीजन ट्री का उपयोग अक्सर एन्सेंबल्स में किया जाता है:
     - **रैंडम फॉरेस्ट**: डेटा और फीचर्स के यादृच्छिक सबसेट पर प्रशिक्षित कई वृक्षों को जोड़ता है।
     - **ग्रेडिएंट बूस्टिंग**: क्रमिक रूप से वृक्ष बनाता है, प्रत्येक पिछले वाले की त्रुटियों को सही करता है।

---

### **डिसीजन ट्री के अनुप्रयोग**

1. **व्यवसाय**:
   - ग्राहक छूट भविष्यवाणी, क्रेडिट स्कोरिंग, मार्केटिंग सेगमेंटेशन।
2. **स्वास्थ्य सेवा**:
   - रोग निदान, जोखिम भविष्यवाणी (जैसे, हृदय रोग)।
3. **वित्त**:
   - धोखाधड़ी का पता लगाना, ऋण डिफॉल्ट भविष्यवाणी।
4. **नेचुरल लैंग्वेज प्रोसेसिंग**:
   - टेक्स्ट क्लासिफिकेशन (जैसे, सेंटीमेंट एनालिसिस)।
5. **रिग्रेशन कार्य**:
   - घर की कीमतों या बिक्री पूर्वानुमान जैसे सतत परिणामों की भविष्यवाणी करना।

---

### **विज़ुअलाइज़ेशन उदाहरण**

यह दर्शाने के लिए कि एक डिसीजन ट्री डेटा को कैसे विभाजित करता है, आइए दो फीचर्स (जैसे, आयु और आय) और दो वर्गों (खरीदें, न खरीदें) वाले एक सरल क्लासिफिकेशन डेटासेट पर विचार करें। नीचे एक संकल्पनात्मक चार्ट है जो दर्शाता है कि डिसीजन ट्री फीचर स्पेस को कैसे विभाजित करता है।

```
chartjs
{
  "type": "scatter",
  "data": {
    "datasets": [
      {
        "label": "खरीदें",
        "data": [
          {"x": 35, "y": 50000},
          {"x": 45, "y": 60000},
          {"x": 50, "y": 80000}
        ],
        "backgroundColor": "#4CAF50",
        "pointRadius": 6
      },
      {
        "label": "न खरीदें",
        "data": [
          {"x": 20, "y": 20000},
          {"x": 25, "y": 30000}
        ],
        "backgroundColor": "#F44336",
        "pointRadius": 6
      }
    ]
  },
  "options": {
    "scales": {
      "x": {
        "title": { "display": true, "text": "आयु" },
        "min": 15,
        "max": 60
      },
      "y": {
        "title": { "display": true, "text": "आय ($)" },
        "min": 10000,
        "max": 100000
      }
    },
    "plugins": {
      "title": { "display": true, "text": "डिसीजन ट्री फीचर स्पेस" },
      "legend": { "display": true }
    }
  }
}
```

यह चार्ट एक 2D फीचर स्पेस में डेटा पॉइंट्स दिखाता है। एक डिसीजन ट्री इस स्पेस को विभाजित कर सकता है (जैसे, आयु = 30 या आय = 40000 पर) ताकि "खरीदें" को "न खरीदें" से अलग किया जा सके।

---

### **व्यवहार में कार्यान्वयन**

डिसीजन ट्री को निम्नलिखित लाइब्रेरीज़ का उपयोग करके लागू किया जा सकता है:
- **Python**: Scikit-learn (`DecisionTreeClassifier`, `DecisionTreeRegressor`), XGBoost, LightGBM.
- **R**: `rpart`, `party`.
- **अन्य टूल्स**: Weka, MATLAB, or Spark MLlib.

Python में उदाहरण (Scikit-learn):
```python
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score

# नमूना डेटा
X = [[25, 20000], [35, 50000], [45, 60000], [20, 30000], [50, 80000]]  # फीचर्स: आयु, आय
y = [0, 1, 1, 0, 1]  # लेबल्स: 0 = न खरीदें, 1 = खरीदें

# डेटा को विभाजित करें
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# डिसीजन ट्री को प्रशिक्षित करें
clf = DecisionTreeClassifier(max_depth=3, random_state=42)
clf.fit(X_train, y_train)

# भविष्यवाणी करें
y_pred = clf.predict(X_test)

# मूल्यांकन करें
print("सटीकता:", accuracy_score(y_test, y_pred))
```

---

### **निष्कर्ष**

डिसीजन ट्री क्लासिफिकेशन और रिग्रेशन कार्यों के लिए शक्तिशाली, व्याख्यात्मक मॉडल हैं। वे फीचर शर्तों के आधार पर डेटा को पुनरावर्ती रूप से विभाजित करके काम करते हैं, जिनमें जिनी इम्प्योरिटी या इनफॉर्मेशन गेन जैसे मानदंडों का उपयोग किया जाता है। हालांकि वे उपयोग और विज़ुअलाइज़ करने में आसान हैं, वे ओवरफिट हो सकते हैं और अस्थिर हो सकते हैं, जिसे अक्सर प्रूनिंग या रैंडम फॉरेस्ट्स जैसे एन्सेंबल मेथड्स का उपयोग करके कम किया जाता है।

आगे की खोज के लिए, वास्तविक डेटासेट का उपयोग करके डिसीजन ट्री के साथ प्रयोग करें, हाइपरपैरामीटर्स को ट्यून करें, और बेहतर प्रदर्शन के लिए उन्हें एन्सेंबल मेथड्स के साथ जोड़ें। यदि आपके पास कोई विशिष्ट डेटासेट या उपयोग का मामला है, तो मैं एक अनुरूप उदाहरण या विज़ुअलाइज़ेशन प्रदान कर सकता हूँ!