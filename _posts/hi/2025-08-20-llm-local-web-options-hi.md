---
audio: false
generated: true
image: false
lang: hi
layout: post
title: हल्के स्थानीय एलएलएम वेब क्लाइंट विकल्प
translated: true
type: note
---

यदि आप हल्के, ओपन-सोर्स वेब क्लाइंट्स की तलाश कर रहे हैं जो LLMs को स्थानीय रूप से चला सकें—Open WebUI की तुलना में कम निर्भरताओं के साथ—तो यहाँ कुछ मजबूत विकल्प हैं:

**1. Lobe Chat**
- हल्का और एक्स्टेंसिबल, Lobe Chat स्थानीय और रिमोट दोनों तरह के LLMs को सपोर्ट करता है। इसमें प्लगइन सिस्टम और एक साफ UI है, और इसे Docker के माध्यम से या सीधे आपके मशीन पर चलाया जा सकता है। यह Ollama और अन्य बैकएंड्स के साथ आसान सेटअप और अच्छे प्रदर्शन के लिए जाना जाता है।

**2. LM Studio**
- LM Studio एक डेस्कटॉप ऐप है जो GGUF मॉडल्स (जैसे Mistral, Phi 3, और Gemma) को सपोर्ट करता है और एक सरल, यूजर-फ्रेंडली इंटरफेस प्रदान करता है। यह त्वरित स्थानीय इनफेरेंस के लिए आदर्श है और इसे सेटअप करने के लिए जटिल प्रक्रिया या अधिक निर्भरताओं की आवश्यकता नहीं होती।

**3. text-generation-webui (Oobabooga)**
- एक फीचर-रिच, Gradio-आधारित वेब UI जो कई बैकएंड्स (transformers, GPTQ, AWQ, EXL2, llama.cpp) को सपोर्ट करता है। यह अत्यधिक कस्टमाइज़ेबल और व्यापक रूप से उपयोग किया जाता है, हालाँकि LM Studio की तुलना में इसे सेटअप करना थोड़ा अधिक जटिल है।

**4. AnythingLLM**
- एक बहुमुखी, सेल्फ-होस्टेड चैट UI जो स्थानीय और क्लाउड-आधारित दोनों तरह के मॉडल्स के साथ अच्छी तरह काम करता है। इसे सामान्य उपयोग के लिए डिज़ाइन किया गया है और यह एक डेस्कटॉप ऐप के रूप में उपलब्ध है, जो इसे लचीला और अपेक्षाकृत हल्का बनाता है।

**5. Jan**
- Jan एक क्रॉस-प्लेटफॉर्म, ऑफलाइन-सक्षम चैट UI है जिसमें ड्रैग-एंड-ड्रॉप कस्टमाइज़ेशन और प्री-बिल्ट टेम्पलेट्स हैं। यह प्रदर्शन के लिए ऑप्टिमाइज़्ड है और कई LLM APIs को सपोर्ट करता है, जिससे यह प्राइवेसी-केंद्रित उपयोगकर्ताओं के लिए एक अच्छा विकल्प बन जाता है।

**6. Ollama UI**
- विशेष रूप से Ollama के लिए डिज़ाइन किया गया, यह UI मिनिमल और डिप्लॉय करने में आसान है, जो इसे एक अच्छा विकल्प बनाता है यदि आप पहले से ही मॉडल सर्विंग के लिए Ollama का उपयोग कर रहे हैं।

**7. Open-LLM-WebUI**
- कॉम्पैक्ट LLMs को स्थानीय रूप से चलाने के लिए एक सरल, ओपन-सोर्स वेब इंटरफेस। इसे उपयोग में आसानी और न्यूनतम सेटअप के लिए डिज़ाइन किया गया है, जो उन लोगों के लिए एक अच्छा विकल्प है जो कुछ सीधा-सादा चाहते हैं।

**सारांश तालिका:**

| क्लाइंट                | मुख्य विशेषताएं                           | सेटअप जटिलता    | सबसे उपयुक्त              |
|-----------------------|------------------------------------------|-----------------|---------------------------|
| Lobe Chat             | प्लगइन्स, Docker सपोर्ट, साफ UI        | कम             | सामान्य उपयोग, Ollama    |
| LM Studio             | डेस्कटॉप ऐप, GGUF सपोर्ट, सरल UI       | बहुत कम        | त्वरित स्थानीय इनफेरेंस  |
| text-generation-webui | मल्टी-बैकएंड, अत्यधिक कस्टमाइज़ेबल     | मध्यम          | उन्नत उपयोगकर्ता        |
| AnythingLLM           | डेस्कटॉप ऐप, लचीला, सामान्य उपयोग      | कम             | स्थानीय/क्लाउड हाइब्रिड  |
| Jan                   | ऑफलाइन, ड्रैग-एंड-ड्रॉप, टेम्पलेट्स    | कम             | प्राइवेसी, कस्टमाइज़ेशन |
| Ollama UI             | मिनिमल, Ollama-फोकस्ड                   | बहुत कम        | Ollama उपयोगकर्ता       |
| Open-LLM-WebUI        | सरल, हल्का                              | कम             | कॉम्पैक्ट LLMs           |

यदि आप सबसे हल्का विकल्प चाहते हैं, तो **LM Studio** या **Ollama UI** शुरुआत के लिए बेहतरीन हैं। अधिक फीचर्स और लचीलेपन के लिए, **Lobe Chat** या **text-generation-webui** मजबूत विकल्प हैं।

क्या आप इनमें से किसी भी विकल्प के लिए सेटअप या विशिष्ट उपयोग के मामलों पर अधिक जानकारी चाहेंगे?