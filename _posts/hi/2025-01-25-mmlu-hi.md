---
audio: true
lang: hi
layout: post
title: एमएमएलयूपी बेनचमार्क
translated: true
---

## भव्यवाक

यह पोस्ट MMLU (Massive Multitask Language Understanding) बेंचमार्क का उपयोग करके एक भाषा मॉडल का मूल्यांकन करता है।

MMLU बेंचमार्क एक मॉडल की विभिन्न कार्यों को एक विस्तृत विषयों की एक विस्तृत श्रेणी में करने की क्षमता का एक व्यापक परीक्षण है। इसमें विभिन्न विषयों जैसे गणित, इतिहास, कानून और चिकित्सा को कवर करने वाले बहुअवसरीय प्रश्न शामिल हैं।

**डेटासेट लिंक:**

*   [पेपर्सविथकोड](https://paperswithcode.com/dataset/mmlu)
*   [हगिंग फेस डेटासेट](https://huggingface.co/datasets/cais/mmlu)

## llama-server

llama-server चलाने के लिए:

```bash
build/bin/llama-server -m models/7B/mistral-7b-instruct-v0.2.Q4_K_M.gguf --port 8080
```

## MMLU बेंचमार्क

यह स्क्रिप्ट तीन अलग-अलग बैकएंड का उपयोग करके MMLU बेंचमार्क का मूल्यांकन करता है: `ollama`, `llama-server`, और `deepseek`.

MMLU बेंचमार्क कोड चलाने के लिए:

```python
import torch
from datasets import load_dataset
import requests
import json
from tqdm import tqdm
import argparse
import os
from openai import OpenAI
from dotenv import load_dotenv
import time

load_dotenv()

# तर्क संरचना सेटअप
parser = argparse.ArgumentParser(description="विभिन्न बैकएंड के साथ MMLU डेटासेट का मूल्यांकन करें।")
parser.add_argument("--type", type=str, default="ollama", choices=["ollama", "llama", "deepseek", "gemini", "deepseek-r1"], help="बैकएंड प्रकार: ollama, llama, deepseek, या gemini")
args = parser.parse_args()

# MMLU डेटासेट लोड करें
subject = "college_computer_science"  # अपने विषय का चयन करें
dataset = load_dataset("cais/mmlu", subject, split="test")

# कम-शॉट उदाहरणों के बिना प्रॉम्प प्रारूपित करें
def format_mmlu_prompt(example):
    prompt = "निम्नलिखित {} के बारे में बहुविकल्पीय प्रश्न हैं।".format(subject.replace("_", " "))
    prompt += "। कृपया केवल सही विकल्प का अक्षर (A, B, C, या D) के साथ जवाब दें।"
    prompt += " केवल अक्षर को ही उत्तर दें। व्याख्या की आवश्यकता नहीं है।"

    # वर्तमान प्रश्न जोड़ें
    prompt += f"प्रश्न: {example['question']}\n"
    prompt += "विकल्प:\nA. {}\nB. {}\nC. {}\nD. {}\n".format(*example['choices'])
    return prompt

# जरूरत पड़ने पर DeepSeek क्लाइंट को शुरू करें
def initialize_deepseek_client():
    api_key = os.environ.get("DEEPSEEK_API_KEY")
    if not api_key:
        print("त्रुटि: DEEPSEEK_API_KEY पर्यावरण चर नहीं सेट है।")
        exit()
    return OpenAI(api_key=api_key, base_url="https://api.deepseek.com")

def call_gemini_api(prompt, retries=3, backoff_factor=1):
    gemini_api_key = os.environ.get("GEMINI_API_KEY")
    if not gemini_api_key:
        print("त्रुटि: GEMINI_API_KEY पर्यावरण चर नहीं सेट है।")
        exit()
    url = f"https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash:generateContent"
    params = {"key": gemini_api_key}
    payload = {"contents": [{"parts": [{"text": prompt}]}]}

    for attempt in range(retries):
        response = requests.post(url, json=payload, params=params)
        if response.status_code != 429:
            return response.json()
        time.sleep(backoff_factor * (2 ** attempt))  # अपवर्तक समय

    return None

import re

def process_ollama_response(response):
    if response.status_code == 200:
        output_text = response.json()["choices"][0]["message"]["content"]
        match = re.search(r"Answer:\s*([A-D])", output_text, re.IGNORECASE)
        if match:
            predicted_answer = match.group(1).upper()
        else:
            predicted_answer = output_text.strip()[0] if len(output_text.strip()) > 0 else ""
        print(f"API से आउटपुट: {output_text}")
        return predicted_answer
    else:
        print(f"त्रुटि: {response.status_code} - {response.text}")
        return ""

def process_llama_response(response):
    if response.status_code == 200:
        output_text = response.json()["choices"][0]["message"]["content"]
        predicted_answer = output_text.strip()[0] if len(output_text.strip()) > 0 else ""
        print(f"API से आउटपुट: {output_text}")
        return predicted_answer
    else:
        print(f"त्रुटि: {response.status_code} - {response.text}")
        return ""

def process_deepseek_response(client, prompt):
    try:
        response = client.chat.completions.create(
            model="deepseek-chat",
            messages=[
                {"role": "user", "content": prompt}
            ],
            max_tokens=100
        )
        if response and response.choices:
            output_text = response.choices[0].message.content.strip()
            predicted_answer = output_text.strip()[0] if len(output_text.strip()) > 0 else ""
            print(f"API से आउटपुट: {output_text}")
            return predicted_answer
        else:
            print("त्रुटि: एपीआई से कोई जवाब नहीं।")
            return ""
    except Exception as e:
        print(f"एपीआई कॉल के दौरान त्रुटि: {e}")
        return ""

def process_deepseek_r1_response(client, prompt, retries=3, backoff_factor=1):
    for attempt in range(retries):
        try:
            response = client.chat.completions.create(
                model="deepseek-reasoner",
                messages=[
                    {"role": "user", "content": prompt}
                ],
                max_tokens=100
            )
            if response and response.choices:
                output_text = response.choices[0].message.content.strip()
                predicted_answer = output_text.strip()[0] if len(output_text.strip()) > 0 else ""
                print(f"API से आउटपुट: {output_text}")
                return predicted_answer
            else:
                print("त्रुटि: एपीआई से कोई जवाब नहीं।")
                return ""
        except Exception as e:
            if "502" in str(e):
                print(f"खराब गेटवे त्रुटि (502) एपीआई कॉल के दौरान, फिर से प्रयास करने में {backoff_factor * (2 ** attempt)} सेकंड..." )
                time.sleep(backoff_factor * (2 ** attempt))
            else:
                print(f"एपीआई कॉल के दौरान त्रुटि: {e}")
                return ""
    print("अधिकतम प्रयासों तक पहुंच गया, खाली जवाब वापस कर रहा है।")
    return ""

def process_gemini_response(prompt):
    json_response = call_gemini_api(prompt)
    if not json_response:
        print("पुनरावृत्ति के बाद Gemini API से कोई जवाब नहीं।")
        return ""
    if 'candidates' not in json_response or not json_response['candidates']:
        print("उत्तर में कोई उम्मीदवार नहीं मिला, पुनरावृत्ति...")
        json_response = call_gemini_api(prompt)
        if not json_response or 'candidates' not in json_response or not json_response['candidates']:
            print("पुनरावृत्ति के बाद उत्तर में कोई उम्मीदवार नहीं मिला।")
            return ""

    first_candidate = json_response['candidates'][0]
    if 'content' in first_candidate and 'parts' in first_candidate['content']:
        first_part = first_candidate['content']['parts'][0]
        if 'text' in first_part:
            output_text = first_part['text']
            predicted_answer = output_text.strip()[0] if len(output_text.strip()) > 0 else ""
            print(f"API से आउटपुट: {output_text}")
            return predicted_answer
        else:
            print("उत्तर में कोई पाठ नहीं मिला")
            return ""
    else:
        print("अपेक्षित उत्तर प्रारूप: सामग्री या हिस्सों में कमी")
        return ""

def evaluate_model(args, dataset):
    correct = 0
    total = 0
    client = None
    if args.type == "deepseek" or args.type == "deepseek-r1":
        client = initialize_deepseek_client()

    for i, example in tqdm(enumerate(dataset), total=len(dataset), desc="मूल्यांकन"):
        prompt = format_mmlu_prompt(example)
        predicted_answer = ""

        if args.type == "ollama":
            url = "http://localhost:11434/v1/chat/completions"
            data = {
                "messages": [{"role": "user", "content": prompt}],
                "model": "mistral:7b"
            }
            headers = {"Content-Type": "application/json"}
            print(f"API को इनपुट: {data}")
            response = requests.post(url, headers=headers, data=json.dumps(data))
            predicted_answer = process_ollama_response(response)

        elif args.type == "llama":
            url = "http://localhost:8080/v1/chat/completions"
            data = {
                "messages": [{"role": "user", "content": prompt}]
            }
            headers = {"Content-Type": "application/json"}
            print(f"API को इनपुट: {data}")
            response = requests.post(url, headers=headers, data=json.dumps(data))
            predicted_answer = process_llama_response(response)

        elif args.type == "deepseek":
            predicted_answer = process_deepseek_response(client, prompt)

        elif args.type == "deepseek-r1":
            predicted_answer = process_deepseek_r1_response(client, prompt)

        elif args.type == "gemini":
            predicted_answer = process_gemini_response(prompt)
        else:
            raise ValueError("अवैध बैकएंड प्रकार")

        answer_map = {0: "A", 1: "B", 2: "C", 3: "D"}
        ground_truth_answer = answer_map.get(example["answer"], "")
        is_correct = predicted_answer.upper() == ground_truth_answer
        if is_correct:
            correct += 1
        total += 1

        print(f"प्रश्न: {example['question']}")
        print(f"विकल्प: A. {example['choices'][0]}, B. {example['choices'][1]}, C. {example['choices'][2]}, D. {example['choices'][3]}")
        print(f"प्रत्याशित जवाब: {predicted_answer}, वास्तविक: {ground_truth_answer}, सही: {is_correct}")
        print("-" * 30)

        if (i+1) % 10 == 0:
            accuracy = correct / total
            print(f"प्रोसेस {i+1}/{len(dataset)}. वर्तमान Accuracy: {accuracy:.2%} ({correct}/{total})")

    return correct, total

# मूल्यांकन लूप
correct, total = evaluate_model(args, dataset)

# Accuracy को गणना करें
accuracy = correct / total
print(f"विषय: {subject}")
print(f"Accuracy: {accuracy:.2%} ({correct}/{total})")

## नतीजे

### Zero-Shot मूल्यांकन

| मॉडल                     | रास्ता                      | विषय                        | Accuracy   |
|---------------------------|--------------------------|--------------------------------|------------|
| mistral-7b-instruct-v0.2, Q4_K_M | macOS m2, 16GB, llama-server | MMLU college_computer_science | 40.00% (40/100) |
| Mistral-7B-Instruct-v0.3, Q4_0  | macOS m2, 16GB, ollama      | MMLU college_computer_science | 40.00% (40/100) |
| deepseek v3 (API)               | API, 2025.1.25           | MMLU college_computer_science | 78.00% (78/100) |
| gemini-1.5-flash (API)          | API, 2025.1.25           | MMLU college_computer_science | 72.00% (72/100) |
| deepseek r1 (API)               | API, 2025.1.26           | MMLU college_computer_science | 87.14% (61/70) |
| Mistral Small Latest (API) | API, 2025.01.31 | MMLU college_computer_science | 65.00% (65/100) |
| Mistral Large Latest (API) | API, 2025.01.31 | MMLU college_computer_science | 73.00% (73/100) |
| Mistral Small 2501 (API)   | API, 2025.01.31 | MMLU college_computer_science | 66.00% (66/100) |

### चित्र

चार्ट के ऊपर आधारित एक चित्र बनाने के लिए।

```python
import matplotlib.pyplot as plt
import os

# नमूना डेटा (अपने वास्तविक डेटा से बदलें)
models = ['mistral-7b-instruct-v0.2 (llama.cpp)', 'Mistral-7B-Instruct-v0.3 (ollama)', 'deepseek v3 (API)', 'gemini-1.5-flash (API)', 'deepseek r1 (API)']
accuracy = [40.00, 40.00, 78.00, 72.00, 87.14]
subject = "college_computer_science"

# बार चार्ट बनाएँ
plt.figure(figsize=(10, 6))
plt.bar(models, accuracy, color=['skyblue', 'lightcoral', 'lightgreen', 'gold', 'lightcoral'])
plt.xlabel('मॉडल')
plt.ylabel('Accuracy (%)')
plt.title(f'{subject} के लिए MMLU बेंचमार्क Accuracy')
plt.ylim(0, 100)  # येलिमिट 0-100 के लिए प्रतिशत
plt.xticks(rotation=45, ha="right")  # बेहतर पठनीयता के लिए x-अक्ष लेज़बेल घुमाएं
plt.tight_layout()

# बार के ऊपर Accuracy मूल्य जोड़ें
for i, val in enumerate(accuracy):
    plt.text(i, val + 1, f'{val:.2f}%', ha='center', va='bottom')

# चार्ट को वर्तमान डायरेक्टरी में एक JPG फ़ाइल के रूप में सेट करें
plt.savefig(os.path.join(os.path.dirname(__file__), f'mmlu_accuracy_chart.jpg'))
plt.show()

```

{: .centered }
![](assets/images/mmlu/mmlu_accuracy_chart.jpg)
*MMLU बेंचमार्क Accuracy*{: .caption }