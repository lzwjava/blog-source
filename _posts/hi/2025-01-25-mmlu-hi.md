---
audio: true
lang: hi
layout: post
title: '# MMLU बेंचमार्क'
translated: true
---

## प्रस्तावना

यह पोस्ट MMLU (मासिव मल्टीटास्क लैंग्वेज अंडरस्टैंडिंग) बेंचमार्क का उपयोग करके एक भाषा मॉडल का मूल्यांकन करती है।

MMLU बेंचमार्क एक व्यापक परीक्षण है जो मॉडल की विभिन्न विषयों पर कार्य करने की क्षमता का मूल्यांकन करता है। इसमें गणित, इतिहास, कानून, और चिकित्सा जैसे विविध क्षेत्रों को कवर करने वाले बहुविकल्पीय प्रश्न शामिल हैं।

**डेटासेट लिंक:**

*   [Papers with Code](https://paperswithcode.com/dataset/mmlu)
*   [Hugging Face Datasets](https://huggingface.co/datasets/cais/mmlu)

## llama-server

llama-server चलाने के लिए:

```bash
build/bin/llama-server -m models/7B/mistral-7b-instruct-v0.2.Q4_K_M.gguf --port 8080
```

## MMLU बेंचमार्क

यह स्क्रिप्ट MMLU बेंचमार्क का मूल्यांकन तीन अलग-अलग बैकएंड का उपयोग करके करती है: `ollama`, `llama-server`, और `deepseek`।

MMLU बेंचमार्क कोड चलाने के लिए:

```python
import torch
from datasets import load_dataset
import requests
import json
from tqdm import tqdm
import argparse
import os
from openai import OpenAI
from dotenv import load_dotenv

load_dotenv()

# तर्क पार्सिंग सेट करें
parser = argparse.ArgumentParser(description="Evaluate MMLU dataset with different backends.")
parser.add_argument("--type", type=str, default="ollama", choices=["ollama", "llama", "deepseek"], help="Backend type: ollama, llama, or deepseek")
args = parser.parse_args()

# MMLU डेटासेट लोड करें
subject = "college_computer_science"  # अपना विषय चुनें
dataset = load_dataset("cais/mmlu", subject, split="test", cache_dir="./.cache")

# फ्यू-शॉट उदाहरणों के बिना प्रॉम्प्ट फॉर्मेट करें
def format_mmlu_prompt(example):
    prompt = "The following are multiple-choice questions about {}".format(subject.replace("_", " "))
    prompt += ". Please answer with the letter of the correct choice (A, B, C, or D) only."
    prompt += " Answer the letter only. Do not need Explanation."
    
    # वर्तमान प्रश्न जोड़ें
    prompt += f"Question: {example['question']}\n"
    prompt += "Choices:\nA. {}\nB. {}\nC. {}\nD. {}\n".format(*example['choices'])
    return prompt

# मूल्यांकन लूप
correct = 0
total = 0

# DeepSeek क्लाइंट इनिशियलाइज़ करें यदि आवश्यक हो
if args.type == "deepseek":
    api_key = os.environ.get("DEEPSEEK_API_KEY")
    if not api_key:
        print("Error: DEEPSEEK_API_KEY environment variable not set.")
        exit()
    client = OpenAI(api_key=api_key, base_url="https://api.deepseek.com")


for i, example in tqdm(enumerate(dataset), total=len(dataset), desc="Evaluating"):
    prompt = format_mmlu_prompt(example)
    
    # बैकएंड को अनुरोध भेजें
    if args.type == "ollama":
        url = "http://localhost:11434/v1/chat/completions"
        data = {
            "messages": [{"role": "user", "content": prompt}],
            "model": "mistral:7b"
        }
        headers = {"Content-Type": "application/json"}
        print(f"Input to API: {data}")
        response = requests.post(url, headers=headers, data=json.dumps(data))
        if response.status_code == 200:
            output_text = response.json()["choices"][0]["message"]["content"]
            predicted_answer = output_text.strip()[0] if len(output_text.strip()) > 0 else ""
            print(f"Output from API: {output_text}")
        else:
            predicted_answer = ""
            print(f"Error: {response.status_code} - {response.text}")
    elif args.type == "llama":
        url = "http://localhost:8080/v1/chat/completions"
        data = {
            "messages": [{"role": "user", "content": prompt}]
        }
        headers = {"Content-Type": "application/json"}
        print(f"Input to API: {data}")
        response = requests.post(url, headers=headers, data=json.dumps(data))
        if response.status_code == 200:
            output_text = response.json()["choices"][0]["message"]["content"]
            predicted_answer = output_text.strip()[0] if len(output_text.strip()) > 0 else ""
            print(f"Output from API: {output_text}")
        else:
            predicted_answer = ""
            print(f"Error: {response.status_code} - {response.text}")
    elif args.type == "deepseek":
        try:
            response = client.chat.completions.create(
                model="deepseek-chat",
                messages=[
                    {"role": "user", "content": prompt}
                ],
                max_tokens=100
            )
            if response and response.choices:
                output_text = response.choices[0].message.content.strip()
                predicted_answer = output_text.strip()[0] if len(output_text.strip()) > 0 else ""
                print(f"Output from API: {output_text}")
            else:
                predicted_answer = ""
                print("Error: No response from the API.")
        except Exception as e:
            predicted_answer = ""
            print(f"Error during API call: {e}")
    else:
        raise ValueError("Invalid backend type")
    
    # ग्राउंड ट्रुथ के साथ तुलना करें
    
    answer_map = {0: "A", 1: "B", 2: "C", 3: "D"}
    ground_truth_answer = answer_map.get(example["answer"], "")
    is_correct = predicted_answer.upper() == ground_truth_answer
    if is_correct:
        correct += 1
    total += 1
    
    print(f"Question: {example['question']}")
    print(f"Choices: A. {example['choices'][0]}, B. {example['choices'][1]}, C. {example['choices'][2]}, D. {example['choices'][3]}")
    print(f"Predicted Answer: {predicted_answer}, Ground Truth: {ground_truth_answer}, Correct: {is_correct}")
    print("-" * 30)

    if (i+1) % 10 == 0:
        accuracy = correct / total
        print(f"Processed {i+1}/{len(dataset)}. Current Accuracy: {accuracy:.2%} ({correct}/{total})")


# सटीकता की गणना करें
accuracy = correct / total
print(f"Subject: {subject}")
print(f"Accuracy: {accuracy:.2%} ({correct}/{total})")
```

## परिणाम

### जीरो-शॉट मूल्यांकन

| मॉडल                     | तरीका                      | विषय                        | सटीकता   |
|---------------------------|--------------------------|--------------------------------|------------|
| mistral-7b-instruct-v0.2, Q4_K_M | macOS m2, 16GB, llama-server | MMLU college_computer_science | 40.00% (40/100) |
| Mistral-7B-Instruct-v0.3, Q4_0  | macOS m2, 16GB, ollama      | MMLU college_computer_science | 40.00% (40/100) |
| deepseek v3               | API, 2025.1.25           | MMLU college_computer_science | 78.00% (78/100) |
| gemini-1.5-flash          | API, 2025.1.25           | MMLU college_computer_science | 72.00% (72/100) |
| deepseek r1               | API, 2025.1.26           | MMLU college_computer_science | 87.14% (61/70) |

### चित्र

आइए उपरोक्त चार्ट के आधार पर एक चित्र बनाएं।

```python
import matplotlib.pyplot as plt
import os

# नमूना डेटा (अपने वास्तविक डेटा से बदलें)
models = ['mistral-7b-instruct-v0.2 (llama.cpp)', 'Mistral-7B-Instruct-v0.3 (ollama)', 'deepseek v3 (API)', 'gemini-1.5-flash (API)']
accuracy = [40.00, 40.00, 78.00, 72.00]

# बार चार्ट बनाएं
plt.figure(figsize=(10, 6))
plt.bar(models, accuracy, color=['skyblue', 'lightcoral', 'lightgreen', 'gold'])
plt.xlabel('मॉडल')
plt.ylabel('सटीकता (%)')
plt.title('MMLU बेंचमार्क सटीकता')
plt.ylim(0, 100)  # y-अक्ष सीमा 0-100 प्रतिशत के लिए सेट करें
plt.xticks(rotation=45, ha="right")  # x-अक्ष लेबल्स को बेहतर पठनीयता के लिए घुमाएं
plt.tight_layout()

# बार के ऊपर सटीकता मान जोड़ें
for i, val in enumerate(accuracy):
    plt.text(i, val + 1, f'{val:.2f}%', ha='center', va='bottom')

# चार्ट को वर्तमान डायरेक्टरी में JPG फ़ाइल के रूप में सहेजें
plt.savefig(os.path.join(os.path.dirname(__file__), 'mmlu_accuracy_chart.jpg'))
plt.show()
```

{: .centered }
![](assets/images/mmlu/mmlu_accuracy_chart.jpg)
*MMLU बेंचमार्क सटीकता*{: .caption }