---
audio: true
lang: hi
layout: post
title: MMLU बेंचमार्क
translated: true
---

यह पोस्ट MMLU (मासिव मल्टीटास्क लैंग्वेज अंडरस्टैंडिंग) बेंचमार्क पर एक भाषा मॉडल का मूल्यांकन करती है।

MMLU बेंचमार्क एक मॉडल की विभिन्न विषयों पर विभिन्न कार्यों को करने की क्षमता का एक व्यापक परीक्षण है। इसमें गणित, इतिहास, कानून और चिकित्सा जैसे विविध क्षेत्रों को कवर करने वाले बहुविकल्पीय प्रश्न शामिल हैं।

**डेटासेट लिंक:**

*   [Papers with Code](https://paperswithcode.com/dataset/mmlu)
*   [Hugging Face Datasets](https://huggingface.co/datasets/cais/mmlu)

```python
import torch
from datasets import load_dataset
import requests
import json
from tqdm import tqdm

# MMLU डेटासेट लोड करें
subject = "college_computer_science"  # अपना विषय चुनें
dataset = load_dataset("cais/mmlu", subject, split="test")

# फ्यू-शॉट उदाहरणों के बिना प्रॉम्प्ट को फॉर्मेट करें
def format_mmlu_prompt(example):
    prompt = "निम्नलिखित {} के बारे में बहुविकल्पीय प्रश्न हैं।".format(subject.replace("_", " "))
    prompt += " कृपया सही विकल्प (A, B, C, या D) के अक्षर के साथ उत्तर दें।"
    prompt += " केवल अक्षर का उत्तर दें। व्याख्या की आवश्यकता नहीं है।"
    
    # वर्तमान प्रश्न जोड़ें
    prompt += f"प्रश्न: {example['question']}\n"
    prompt += "विकल्प:\nA. {}\nB. {}\nC. {}\nD. {}\n".format(*example['choices'])
    return prompt

# मूल्यांकन लूप
correct = 0
total = 0

for i, example in tqdm(enumerate(dataset), total=len(dataset), desc="मूल्यांकन"):
    prompt = format_mmlu_prompt(example)
    
    # llama-server को अनुरोध भेजें
    url = "http://localhost:8080/v1/chat/completions"
    headers = {"Content-Type": "application/json"}
    data = {
        "messages": [{"role": "user", "content": prompt}]
    }
    
    print(f"API को इनपुट: {data}")
    response = requests.post(url, headers=headers, data=json.dumps(data))
    
    if response.status_code == 200:
        output_text = response.json()["choices"][0]["message"]["content"]
        predicted_answer = output_text.strip()[0] if len(output_text.strip()) > 0 else ""
        print(f"API से आउटपुट: {output_text}")
    else:
        predicted_answer = ""
        print(f"त्रुटि: {response.status_code} - {response.text}")
    
    # ग्राउंड ट्रुथ के साथ तुलना करें
    
    answer_map = {0: "A", 1: "B", 2: "C", 3: "D"}
    ground_truth_answer = answer_map.get(example["answer"], "")
    is_correct = predicted_answer.upper() == ground_truth_answer
    if is_correct:
        correct += 1
    total += 1
    
    print(f"प्रश्न: {example['question']}")
    print(f"विकल्प: A. {example['choices'][0]}, B. {example['choices'][1]}, C. {example['choices'][2]}, D. {example['choices'][3]}")
    print(f"पूर्वानुमानित उत्तर: {predicted_answer}, ग्राउंड ट्रुथ: {ground_truth_answer}, सही: {is_correct}")
    print("-" * 30)

    if (i+1) % 10 == 0:
        accuracy = correct / total
        print(f"प्रोसेस्ड {i+1}/{len(dataset)}. वर्तमान सटीकता: {accuracy:.2%} ({correct}/{total})")


# सटीकता की गणना करें
accuracy = correct / total
print(f"विषय: {subject}")
print(f"सटीकता: {accuracy:.2%} ({correct}/{total})")
```

लॉग:

```bash
% python scripts/mmlu.py

मूल्यांकन:   9%| 9/100 [01:31<15:19, 10.10s/it]प्रोसेस्ड 10/100. वर्तमान सटीकता: 0.00% (0/10)
मूल्यांकन:  19%| 19/100 [03:14<12:47,  9.47s/it]प्रोसेस्ड 20/100. वर्तमान सटीकता: 0.00% (0/20)
मूल्यांकन:  26%| 26/100 [04:30<13:44, 11.14s/it]

...

प्रोसेस्ड 100/100. वर्तमान सटीकता: 40.00% (40/100)
मूल्यांकन: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [15:15<00:00,  9.16s/it]
विषय: college_computer_science
सटीकता: 40.00% (40/100)
```