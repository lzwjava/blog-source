---
audio: false
generated: true
image: false
lang: hi
layout: post
model: x-ai/grok-4-fast
title: आवर्ती तंत्रिका नेटवर्क की आश्चर्यजनक शक्ति
translated: true
type: note
---

यह 2015 की आंद्रेज कार्पैथी की ब्लॉग पोस्ट रिकरंट न्यूरल नेटवर्क्स (आरएनएन) का एक क्लासिक परिचय है, जो टेक्स्ट जनरेशन जैसे सीक्वेंस को हैंडल करने में उनकी आश्चर्यजनक शक्ति को उजागर करती है। कार्पैथी इमेज कैप्शनिंग जैसे कार्यों के लिए आरएनएन को प्रशिक्षित करने के बाद अपना उत्साह साझा करते हैं, जिसने न्यूनतम ट्यूनिंग के साथ सुसंगत आउटपुट तैयार किए। वह एलएसटीएम (आरएनएन का एक प्रकार) का उपयोग करके कैरेक्टर-लेवल लैंग्वेज मॉडल के लिए ओपन-सोर्स कोड जारी करते हैं और विभिन्न टेक्स्ट-जनरेशन प्रयोगों के माध्यम से उनके "जादू" को प्रदर्शित करते हैं। नीचे मुख्य खंडों का एक संरचित सारांश दिया गया है।

## परिचय
कार्पैथी आरएनएन को अनुक्रमिक डेटा के लिए "अतितार्किक रूप से प्रभावी" बताते हैं, और उनकी तुलना पारंपरिक फीडफॉरवर्ड नेटवर्क्स से करते हैं जो निश्चित आकार के इनपुट/आउटपुट को संभालते हैं। वह कैरेक्टर्स की भविष्यवाणी करने और उत्पन्न करने के लिए टेक्स्ट कॉर्पोरा पर साधारण आरएनएन को प्रशिक्षित करते हैं, और सवाल करते हैं कि वे भाषा पैटर्न को इतनी अच्छी तरह से कैसे पकड़ पाते हैं। पोस्ट में डेमो को दोहराने के लिए GitHub पर कोड शामिल है।

## मुख्य अवधारणाएँ: आरएनएन कैसे काम करते हैं
आरएनएन एक आंतरिक "स्टेट" (हिडन वेक्टर) को बनाए रखकर अनुक्रमों (जैसे वाक्य, वीडियो) में माहिर होते हैं, जो समय के चरणों में जानकारी ले जाता है। स्टैटिक नेटवर्क्स के विपरीत, वे एक ही परिवर्तन को बार-बार लागू करते हैं:

-   **इनपुट/आउटपुट प्रकार**: निश्चित इनपुट से अनुक्रम आउटपुट (जैसे, इमेज से कैप्शन); अनुक्रम से निश्चित आउटपुट (जैसे, वाक्य से भावना); अनुक्रम-से-अनुक्रम (जैसे, अनुवाद)।
-   **मूल तंत्र**: प्रत्येक चरण पर, नई स्टेट \\( h_t = \tanh(W_{hh} h_{t-1} + W_{xh} x_t) \\), जहां \\( x_t \\) इनपुट है, और आउटपुट \\( y_t \\) स्टेट से प्राप्त होता है। बैकप्रोपागेशन थ्रू टाइम (BPTT) के माध्यम से प्रशिक्षित।
-   **गहराई और प्रकार**: गहराई के लिए लेयर्स को स्टैक करें; वैनिला आरएनएन की तुलना में लंबी अवधि की निर्भरताओं को बेहतर ढंग से संभालने के लिए एलएसटीएम का उपयोग करें।
-   **दार्शनिक टिप्पणी**: आरएनएन ट्यूरिंग-पूर्ण हैं, अनिवार्य रूप से "सीखने वाले प्रोग्राम" हैं न कि केवल फ़ंक्शन।

एक साधारण Python स्निपेट स्टेप फ़ंक्शन को दर्शाता है:
```python
def step(self, x):
    self.h = np.tanh(np.dot(self.W_hh, self.h) + np.dot(self.W_xh, x))
    y = np.dot(self.W_hy, self.h)
    return y
```

## कैरेक्टर-लेवल लैंग्वेज मॉडलिंग
मुख्य उदाहरण: अगले कैरेक्टर की भविष्यवाणी करने के लिए टेक्स्ट पर प्रशिक्षण (वन-हॉट एन्कोडेड), एक शब्दावली (जैसे, अंग्रेजी के लिए 65 कैरेक्टर्स) पर संभाव्यता वितरण का निर्माण करना। जनरेशन भविष्यवाणियों से सैंपलिंग करके और वापस फीड करके काम करता है। यह रिकरंट कनेक्शन के माध्यम से संदर्भ सीखता है—जैसे, "hel" के बाद 'l' की भविष्यवाणी करना बनाम "he" के बाद। मिनी-बैच SGD और RMSProp जैसे ऑप्टिमाइज़र के साथ प्रशिक्षित।

## प्रदर्शन: आरएनएन-जनरेटेड टेक्स्ट
सभी लेखक के char-rnn कोड का उपयोग सिंगल टेक्स्ट फाइलों पर करते हैं, जो बकवास से लेकर सुसंगत आउटपुट तक की प्रगति दिखाते हैं।

-   **पॉल ग्राहम निबंध** (~1MB): स्टार्टअप सलाह शैली की नकल करता है। नमूना: "The surprised in investors weren’t going to raise money... Don’t work at first member to see the way kids will seem in advance of a bad successful startup."
-   **शेक्सपियर** (4.4MB): नाटक जैसे संवाद तैयार करता है। नमूना: "PANDARUS: Alas, I think he shall be come approached and the day When little srain would be attain'd into being never fed..."
-   **विकिपीडिया** (96MB): मार्कडाउन, लिंक और सूचियों के साथ लेख जैसा टेक्स्ट उत्पन्न करता है। नमूना: "Naturalism and decision for the majority of Arab countries' capitalide was grounded by the Irish language by [[John Clair]]..."
-   **अलजेब्रिक ज्योमेट्री LaTeX** (16MB): लगभग कंपाइल होने वाले गणितीय प्रमाण आउटपुट करता है। नमूना: "\begin{proof} We may assume that $\mathcal{I}$ is an abelian sheaf on $\mathcal{C}$..."
-   **लिनक्स कर्नल C कोड** (474MB): टिप्पणियों और सिंटैक्स के साथ यथार्थवादी फ़ंक्शन। नमूना: "static int indicate_policy(void) { ... if (ss->segment < mem_total) unblock_graph_and_set_blocked(); ... }"
-   **बेबी नेम्स** (8K नाम): "Rudi Levette" या "Hany Mareanne" जैसे नए आविष्कार।

"टेम्परेचर" सैंपलिंग को नियंत्रित करता है: दोहराए जाने वाले लेकिन सुरक्षित टेक्स्ट के लिए कम; रचनात्मक लेकिन अनियमित के लिए उच्च।

## चुनौतियाँ और सीमाएँ
-   लंबी अवधि की निर्भरताएं: शुरुआती संदर्भ भूल जाता है (जैसे, बेमेल LaTeX टैग)।
-   सामान्यीकरण पर याद रखना: सिंटैक्स/संरचना में अच्छा लेकिन सामग्री का हलूसिनेशन करता है।
-   कंप्यूट-गहन: जीपीयू की आवश्यकता; वैनिला आरएनएन एलएसटीएम के बिना अस्थिर।
-   भविष्य के सुधार: एक्सटर्नल मेमोरी (Neural Turing Machines), अटेंशन मैकेनिज्म।

## निष्कर्ष
आरएनएन एनएलपी (अनुवाद, स्पीच), विजन (कैप्शनिंग) और इसके अलावा भी कई क्षेत्रों में चमकते हैं, बकवास से संरचित जनरेशन तक विकसित होते हैं। कार्पैथी एक्टिवेशन को विज़ुअलाइज़ करते हैं (जैसे, न्यूरॉन्स जो उद्धरणों का पता लगाते हैं) और पोस्ट पर ही प्रशिक्षित एक आरएनएन का मजाक उड़ाते हैं, जो "I've the RNN with and works..." जैसा मेटा-टेक्स्ट आउटपुट करता है। वह आगे पढ़ने और कोड संसाधनों की सलाह देते हैं, और अधिक बुद्धिमान एआई में आरएनएन की भूमिका की भविष्यवाणी करते हैं।

[The Unreasonable Effectiveness of Recurrent Neural Networks](http://karpathy.github.io/2015/05/21/rnn-effectiveness/)