---
audio: false
generated: true
image: false
lang: hi
layout: post
model: x-ai/grok-4-fast
title: फ़्लोटिंग-पॉइंट परिशुद्धता प्रारूपों की तुलना
translated: true
type: note
---

### float32, float16, और bfloat16 के बीच अंतर

ये कंप्यूटिंग में उपयोग किए जाने वाले फ्लोटिंग-पॉइंट डेटा टाइप हैं, विशेष रूप से मशीन लर्निंग फ्रेमवर्क जैसे PyTorch में। ये बिट चौड़ाई में भिन्न होते हैं, जो सटीकता (संख्या प्रतिनिधित्व की शुद्धता), डायनामिक रेंज (मानों की सीमा), मेमोरी उपयोग और कम्प्यूटेशनल गति को प्रभावित करते हैं। यहां एक विवरण दिया गया है:

- **float32 (सिंगल प्रिसिजन)**:
  - कुल 32 बिट: 1 साइन बिट, 8 एक्सपोनेंट बिट, 23 मैन्टिसा (फ्रैक्शन) बिट।
  - उच्च सटीकता और व्यापक डायनेमिक रेंज (एक्सपोनेंट लगभग 1e-38 से 1e+38 तक के मानों की अनुमति देता है)।
  - फायदे: अधिकांश गणनाओं के लिए सटीक, कई परिदृश्यों में ओवरफ्लो/अंडरफ्लो से बचाता है।
  - नुकसान: अधिक मेमोरी उपयोग (प्रति संख्या 4 बाइट्स) और GPU पर धीमी गणना, क्योंकि इसके लिए अधिक बैंडविड्थ और प्रोसेसिंग पावर की आवश्यकता होती है।
  - पारंपरिक CPU-आधारित कंप्यूटिंग में या पूर्ण सटीकता की आवश्यकता होने पर आम।

- **float16 (हाफ प्रिसिजन)**:
  - कुल 16 बिट: 1 साइन बिट, 5 एक्सपोनेंट बिट, 10 मैन्टिसा बिट।
  - कम सटीकता और संकीर्ण डायनेमिक रेंज (एक्सपोनेंट मानों को लगभग 1e-7 से 65504 तक सीमित करता है)।
  - फायदे: मेमोरी उपयोग आधा कर देता है (प्रति संख्या 2 बाइट्स) और उस हार्डवेयर पर गणना को गति देता है जो इसका समर्थन करता है (जैसे, आधुनिक GPU), जिससे यह बड़े मॉडल जैसे LLM के लिए बेहतरीन बन जाता है जहां मेमोरी एक बाधा है।
  - नुकसान: ओवरफ्लो (बड़ी संख्याएं) या अंडरफ्लो (छोटी संख्याएं/ग्रेडिएंट्स) की संभावना, जो प्रशिक्षण के दौरान NaNs (नॉट ए नंबर) जैसी समस्याएं पैदा कर सकता है। यह प्रतिनिधित्व में अधिक विवरण भी खो देता है।

- **bfloat16 (ब्रेन फ्लोटिंग पॉइंट)**:
  - कुल 16 बिट: 1 साइन बिट, 8 एक्सपोनेंट बिट, 7 मैन्टिसा बिट।
  - float32 की डायनेमिक रेंज से मेल खाता है (समान एक्सपोनेंट बिट, इसलिए समान मान सीमा) लेकिन कम सटीकता के साथ (कम मैन्टिसा बिट)।
  - फायदे: float16 के समान मेमोरी बचत (2 बाइट्स), लेकिन व्यापक रेंज के कारण डीप लर्निंग में बेहतर स्थिरता—ओवरफ्लो/अंडरफ्लो की संभावना कम। यह न्यूरल नेटवर्क के लिए डिज़ाइन किया गया है और अधिक स्केलिंग या सामान्यीकरण की आवश्यकता के बिना प्रशिक्षण में अच्छा प्रदर्शन करता है।
  - नुकसान: float16 से भी कम सटीकता, जो सन्निकटन त्रुटियों का कारण बन सकती है, लेकिन व्यवहार में, यह अक्सर LLM के लिए नगण्य होती है।

आपके दिखाए गए कोड में (`dtype = 'bfloat16' if torch.cuda.is_available() and torch.cuda.is_bf16_supported() else 'float16'`), यह bfloat16 चुन रहा है यदि GPU इसका समर्थन करता है (नए NVIDIA/AMD हार्डवेयर पर आम), अन्यथा float16 पर वापस आ जाता है। यह मिश्रित-सटीकता सेटअप के लिए है, जहां गणना गति के लिए कम सटीकता का उपयोग करती है जबकि सटीकता बनाए रखने के लिए कुछ हिस्सों (जैसे एक्यूमुलेटर) को उच्च सटीकता में रखती है। bfloat16 कई आधुनिक सेटअप (जैसे, Google द्वारा TPU के लिए) में पसंद किया जाता है क्योंकि यह रेंज के मामले में float32 की तरह व्यवहार करता है, जिससे प्रशिक्षण अस्थिरता कम होती है।

### क्वांटिज़ेशन विधियाँ और उनका संबंध

क्वांटिज़ेशन मॉडल वजन, एक्टिवेशन और कभी-कभी ग्रेडिएंट्स की बिट चौड़ाई को कम करने के लिए एक तकनीक है, जो केवल float16/bfloat16 का उपयोग करने से परे मॉडल को और संपीड़ित करती है। यह आपके कोड में dtypes बदलने जैसा नहीं है (जो रनटाइम के दौरान फ्लोटिंग-पॉइंट सटीकता के बारे में अधिक है), लेकिन यह संबंधित है क्योंकि दोनों का लक्ष्य LLM में दक्षता के लिए अनुकूलन करना है।

- **क्वांटिज़ेशन क्या है?**
  - यह उच्च-सटीकता मानों (जैसे float32) को निम्न-बिट प्रतिनिधित्व (जैसे int8, int4, या यहां तक कि कस्टम फ्लोट्स) पर मैप करता है। यह मेमोरी फुटप्रिंट और इनफेरेंस समय को काटता है, जो एज डिवाइस पर या बड़े पैमाने पर LLM तैनात करने के लिए महत्वपूर्ण है।
  - उदाहरण: एक float32 वजन (32 बिट) को int8 (8 बिट) में परिवर्तित किया जा सकता है, जिससे आकार 4x कम हो जाता है।

- **सामान्य क्वांटिज़ेशन विधियाँ**:
  - **पोस्ट-ट्रेनिंग क्वांटिज़ेशन (PTQ)**: प्रशिक्षण के बाद लागू करें। सरल लेकिन अंशांकन न होने पर सटीकता कम कर सकता है (जैसे, स्केल समायोजित करने के लिए एक छोटे डेटासेट का उपयोग)। min-max स्केलिंग या हिस्टोग्राम-आधारित (जैसे, TensorRT या ONNX में) जैसी विधियाँ।
  - **क्वांटिज़ेशन-अवेयर ट्रेनिंग (QAT)**: प्रशिक्षण के दौरान क्वांटिज़ेशन का अनुकरण करें (जैसे, PyTorch में फेक क्वांट ऑप्स), ताकि मॉडल कम सटीकता को संभालना सीख जाए। अधिक सटीक लेकिन पुनः प्रशिक्षण की आवश्यकता होती है।
  - **उन्नत वेरिएंट**:
    - **वेट-ओनली क्वांटिज़ेशन**: केवल वजन को क्वांटाइज़ करें (जैसे, int4 में), एक्टिवेशन को float16/bfloat16 में रखें।
    - **ग्रुप क्वांटिज़ेशन**: समूहों में क्वांटाइज़ करें (जैसे, GPTQ विधि बेहतर सटीकता के लिए वजन को समूहित करती है)।
    - **AWQ (एक्टिवेशन-अवेयर वेट क्वांटिज़ेशन)**: बेहतर क्लिपिंग के लिए एक्टिवेशन वितरण को ध्यान में रखता है।
    - **डीक्वांटिज़ेशन के साथ INT4/INT8**: इनफेरेंस के दौरान, गणना के लिए float16 में वापस डीक्वांटाइज़ करें।

- **float16/bfloat16/float32 से संबंध**:
  - आपकी dtype पसंद *मिश्रित सटीकता* का एक रूप है (जैसे, PyTorch में AMP), जो अधिकांश ऑप्स के लिए float16/bfloat16 का उपयोग करती है लेकिन अंडरफ्लो को रोकने के लिए float32 में स्केल करती है। क्वांटिज़ेशन पूर्णांक या यहां तक कि निम्न-बिट फ्लोट्स का उपयोग करके और आगे बढ़ जाती है।
  - वे अनुकूलन पाइपलाइन में संबंधित हैं: float32 प्रशिक्षण से शुरू करें, तेज प्रशिक्षण के लिए bfloat16 पर स्विच करें, फिर तैनाती के लिए int8 में क्वांटाइज़ करें। उदाहरण के लिए, Hugging Face Transformers जैसे लाइब्रेरी लोडिंग के दौरान `torch_dtype=bfloat16` का उपयोग करती हैं, फिर क्वांटिज़ेशन (जैसे, BitsAndBytes के माध्यम से) लागू करके 4-बिट तक कम करती हैं।
  - ट्रेड-ऑफ: कम सटीकता/क्वांटिज़ेशन चीजों को गति देती है लेकिन सटीकता हानि का जोखिम (जैसे, LLM में पेरप्लेक्सिटी वृद्धि) होता है। bfloat16 अक्सर पूर्ण क्वांटिज़ेशन से पहले एक स्वीट स्पॉट होता है।

### फ्लैश अटेंशन से संबंध

फ्लैश अटेंशन ट्रांसफॉर्मर (LLM जैसे GPT का मुख्य हिस्सा) में अटेंशन की गणना के लिए एक अनुकूलित एल्गोरिदम है। यह इंटरमीडिएट्स को स्टोर करने के बजाय उन्हें ऑन-द-फ्लाई पुनर्गणना करके मेमोरी उपयोग को कम करता है और गति बढ़ाता है, विशेष रूप से लंबे अनुक्रमों के लिए उपयोगी।

- **सटीकता कैसे संबंधित है**:
  - फ्लैश अटेंशन (जैसे, `torch.nn.functional.scaled_dot_product_attention` या flash-attn लाइब्रेरी के माध्यम से) float16/bfloat16 जैसी कम सटीकता का मूल रूप से समर्थन करता है। वास्तव में, यह अक्सर इन dtypes में तेज होता है क्योंकि GPU (जैसे, NVIDIA Ampere+) के पास उनके लिए हार्डवेयर त्वरण (जैसे, Tensor Cores) होता है।
  - आपके कोड की dtype पसंद सीधे इस पर प्रभाव डालती है: bfloat16 या float16 का उपयोग फ्लैश अटेंशन के हाई-परफॉर्मेंस मोड को सक्षम करता है, क्योंकि यह ऑपरेशन्स को फ्यूज कर सकता है और मेमोरी बाधाओं से बच सकता है। float32 में, यह धीमी इम्प्लीमेंटेशन पर वापस आ सकता है।
  - क्वांटिज़ेशन भी इसमें शामिल है—क्वांटाइज़्ड मॉडल गणना के दौरान float16 में डीक्वांटाइज़ होने पर फ्लैश अटेंशन का उपयोग कर सकते हैं। vLLM या ExLlama जैसी लाइब्रेरी अल्ट्रा-फास्ट इनफेरेंस के लिए क्वांटिज़ेशन के साथ फ्लैश अटेंशन को एकीकृत करती हैं।

PyTorch में, यदि आप `torch.backends.cuda.enable_flash_sdp(True)` सेट करते हैं, तो यह फ्लैश अटेंशन को प्राथमिकता देता है जब dtype float16/bfloat16 हो और हार्डवेयर इसका समर्थन करता हो।

### LLM मॉडल में फ्लोट प्रिसिजन का सामान्य उपयोग

बड़े भाषा मॉडल (LLM) जैसे GPT, Llama, या Grok में:

- **प्रशिक्षण**: अक्सर स्थिरता के लिए float32 में शुरू होता है, लेकिन विशाल डेटासेट को तेजी से संभालने के लिए bfloat16 (जैसे, Google के मॉडल में) या मिश्रित सटीकता (float32 स्केलिंग के साथ float16) में स्थानांतरित हो जाता है। bfloat16 लोकप्रियता प्राप्त कर रहा है (जैसे, PyTorch 2.0+ में) क्योंकि इसके लिए float16 की तुलना में कम हाइपरपैरामीटर ट्यूनिंग की आवश्यकता होती है।
- **इनफेरेंस/तैनाती**: GPU पर गति के लिए float16 या bfloat16 आम है। और अधिक दक्षता के लिए (जैसे, उपभोक्ता हार्डवेयर पर), मॉडल को 8-बिट या 4-बिट (जैसे, llama.cpp में GGUF फॉर्मेट) में क्वांटाइज़ किया जाता है। यह <24GB VRAM पर 70B-पैरामीटर मॉडल चलाने की अनुमति देता है।
- **ट्रेड-ऑफ**:
  - उच्च सटीकता (float32): बेहतर सटीकता, लेकिन धीमी और अधिक मेमोरी-भूखी।
  - कम सटीकता: तेज, हल्की, लेकिन अवनत आउटपुट की संभावना (जैसे, चरम क्वांटिज़ेशन में हलुसिनेशन्स में मामूली वृद्धि)।
- **ट्रेंड्स**: NVIDIA H100 जैसे हार्डवेयर के साथ (मजबूत bfloat16 समर्थन), bfloat16 मानक बन रहा है। PEFT (पैरामीटर-एफिशिएंट फाइन-ट्यूनिंग) जैसे टूल फाइन-ट्यूनिंग के लिए इसे क्वांटिज़ेशन के साथ जोड़ते हैं। ओपन-सोर्स में, Transformers या Optimum जैसी लाइब्रेरी स्वचालित dtype रूपांतरण और क्वांटिज़ेशन को संभालती हैं।

यदि आप इसे कोड में लागू कर रहे हैं, तो अपने हार्डवेयर पर परीक्षण करें—bfloat16 नए GPU पर चमकता है, जबकि float16 अधिक सार्वभौमिक है लेकिन ग्रेडिएंट्स के साथ मुश्किल हो सकता है।

संदर्भ:  
[Understanding Floating Point Precision in Deep Learning](https://towardsdatascience.com/understanding-fp32-fp16-bf16-in-neural-networks-27075c96b4d3)  
[Quantization Techniques for LLMs](https://huggingface.co/docs/transformers/quantization)  
[Flash Attention Explained](https://arxiv.org/abs/2205.14135)  
[Mixed Precision Training in PyTorch](https://pytorch.org/docs/stable/amp.html)