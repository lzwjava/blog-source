---
audio: false
generated: true
image: false
lang: hi
layout: post
model: x-ai/grok-4-fast
title: आरएमएसप्रॉप और इसी तरह के ऑप्टिमाइज़र समझाया गया
translated: true
type: note
---

### आरएमएसप्रॉप क्या है?

आरएमएसप्रॉप (रूट मीन स्क्वायर प्रोपेगेशन) मशीन लर्निंग में ग्रेडिएंट डिसेंट के माध्यम से न्यूरल नेटवर्क को प्रशिक्षित करने के लिए इस्तेमाल होने वाला एक एडाप्टिव ऑप्टिमाइज़ेशन एल्गोरिदम है। यह गायब होने या फैलने वाले ग्रेडिएंट्स की चुनौतियों का समाधान करता है, जिसके लिए यह प्रत्येक पैरामीटर के लिए लर्निंग रेट को सामान्य करने के लिए वर्गीकृत ग्रेडिएंट्स की एक चलती औसत को बनाए रखता है। यह इसे गैर-स्थिर उद्देश्यों, जैसे कि रिकरंट न्यूरल नेटवर्क (आरएनएन) में होते हैं, के लिए विशेष रूप से प्रभावी बनाता है। जेफ्री हिंटन द्वारा प्रस्तुत, यह एडाग्रैड का एक प्रकार है जो सभी पिछले ग्रेडिएंट्स को जमा करने के बजाय एक घातीय रूप से क्षय होने वाली औसत का उपयोग करता है, जिससे समय के साथ लर्निंग रेट के बहुत अधिक सिकुड़ने से रोका जा सके।

### आरएमएसप्रॉप के समान ऑप्टिमाइज़र्स

आरएमएसप्रॉप के "समान" ऑप्टिमाइज़र्स आमतौर पर एडाप्टिव तरीके हैं जो ग्रेडिएंट इतिहास के आधार पर लर्निंग रेट्स को गतिशील रूप से समायोजित करते हैं। वे मोमेंटम के साथ ग्रेडिएंट डिसेंट के विचारों पर निर्मित होते हैं, लेकिन विरल या शोर वाले डेटा को संभालने के लिए प्रति-पैरामीटर अनुकूलन पर ध्यान केंद्रित करते हैं। नीचे प्रमुख समान ऑप्टिमाइज़र्स की एक तुलना दी गई है:

| ऑप्टिमाइज़र | मुख्य विशेषताएं | आरएमएसप्रॉप से समानताएं | आरएमएसप्रॉप से अंतर |
|-----------|--------------|--------------------------|---------------------------|
| **एडाग्रैड** | लर्निंग रेट्स को अनुकूलित करने के लिए वर्गीकृत ग्रेडिएंट्स के योग को जमा करता है; विरल डेटा के लिए आदर्श। | दोनों ग्रेडिएंट परिमाणों का उपयोग करके प्रति पैरामीटर लर्निंग रेट्स को अनुकूलित करते हैं। | एडाग्रैड *सभी* पिछले ग्रेडिएंट्स को जोड़ता है, जिससे लर्निंग रेट एकदिशीय रूप से घटते हैं (अक्सर बहुत जल्दी); आरएमएसप्रॉप अधिक स्थिर अनुकूलन के लिए एक चलती औसत का उपयोग करता है। |
| **एडाडेल्टा** | एडाग्रैड का विस्तार जो ग्रेडिएंट अपडेट्स की एक चलती विंडो का उपयोग करता है; मैन्युअल लर्निंग रेट ट्यूनिंग की आवश्यकता नहीं होती। | अनुकूली दरों के लिए ग्रेडिएंट्स के रूट मीन स्क्वायर (आरएमएस) सामान्यीकरण को साझा करता है। | पैरामीटर अपडेट्स (सिर्फ ग्रेडिएंट्स नहीं) के लिए एक अलग चलती औसत पेश करता है, जिससे यह आरंभीकरण के प्रति अधिक मजबूत बनता है और हाइपरपैरामीटर संवेदनशीलता को कम करता है। |
| **एडम** (एडाप्टिव मोमेंट एस्टिमेशन) | मोमेंटम (ग्रेडिएंट्स का प्रथम क्षण) को आरएमएसप्रॉप-जैसे अनुकूलन (द्वितीय क्षण) के साथ जोड़ता है; बेहतर प्रारंभिक प्रशिक्षण के लिए पूर्वाग्रह-सुधारित। | प्रति-पैरामीटर स्केलिंग के लिए, आरएमएसप्रॉप की तरह ही, वर्गीकृत ग्रेडिएंट्स की एक घातीय रूप से क्षय होने वाली औसत का उपयोग करता है। | तेज अभिसरण के लिए मोमेंटम शब्द जोड़ता है; पूर्वाग्रह सुधार शामिल करता है और अक्सर बड़े डेटासेट पर आरएमएसप्रॉप से बेहतर प्रदर्शन करता है, हालांकि कुछ मामलों में यह सामान्यीकरण में थोड़ा बदतर हो सकता है। |
| **एडमडब्ल्यू** | नियमितीकरण के लिए डिकपल्ड वेट डिके के साथ एडम का एक प्रकार। | द्वितीय-क्षण अनुमानों के माध्यम से आरएमएसप्रॉप के अनुकूली स्केलिंग को विरासत में प्राप्त करता है। | अनुकूली लर्निंग रेट से वेट डिके को अलग करता है, जिससे ट्रांसफॉर्मर्स को फाइन-ट्यून करने जैसे कार्यों में मानक एडम (और इस प्रकार आरएमएसप्रॉप) पर सामान्यीकरण में सुधार होता है। |
| **नेस्टेरोव मोमेंटम के साथ एसजीडी** | ग्रेडिएंट्स का अनुमान लगाने के लिए "लुक-अहेड" मोमेंटम के साथ बुनियादी स्टोकैस्टिक ग्रेडिएंट डिसेंट। | व्यवहार में चिकने अपडेट्स के लिए अनुकूली व्यवहार की नकल कर सकता है। | वास्तव में अनुकूली नहीं (निश्चित लर्निंग रेट); त्वरण के लिए प्रति-पैरामीटर स्केलिंग के बजाय मोमेंटम पर निर्भर करता है, लेकिन अक्सर आरएमएसप्रॉप जैसे शुद्ध अनुकूली तरीकों से बेहतर सामान्यीकरण करता है। |

ये आमतौर पर PyTorch या TensorFlow जैसे फ्रेमवर्क में लागू किए जाते हैं (उदाहरण के लिए, `torch.optim.RMSprop`)। अधिकांश डीप लर्निंग कार्यों के लिए आरएमएसप्रॉप या एडम से शुरुआत करें, क्योंकि वे समस्याओं की एक विस्तृत श्रृंखला को अच्छी तरह से संभालते हैं। विरल डेटा के लिए, एडाग्रैड आज़माएं; आरएनएन/एलएसटीएम के लिए, आरएमएसप्रॉप या एडम बेहतर होते हैं।

### संदर्भ
- [Understanding Deep Learning Optimizers: Momentum, AdaGrad, RMSProp & Adam](https://towardsdatascience.com/understanding-deep-learning-optimizers-momentum-adagrad-rmsprop-adam-e311e377e9c2)
- [Which Optimizer should I use for my ML Project?](https://www.lightly.ai/blog/which-optimizer-should-i-use-for-my-machine-learning-project)
- [A Comprehensive Guide on Deep Learning Optimizers](https://www.analyticsvidhya.com/blog/2021/10/a-comprehensive-guide-on-deep-learning-optimizers/)