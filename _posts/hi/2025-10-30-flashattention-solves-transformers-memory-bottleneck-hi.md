---
audio: false
generated: true
image: false
lang: hi
layout: post
model: x-ai/grok-4-fast
title: फ्लैशअटेंशन ट्रांसफॉर्मर्स की मेमोरी बाधा को हल करता है
translated: true
type: note
---

### पृष्ठभूमि: ट्रांसफॉर्मर्स में मानक अटेंशन की चुनौतियाँ

ट्रांसफॉर्मर्स, जो GPT और BERT जैसे मॉडल्स की रीढ़ हैं, अनुक्रमों को प्रोसेस करने के लिए स्व-अटेंशन (self-attention) मैकेनिज्म पर भारी निर्भर करते हैं। स्व-अटेंशन में, इनपुट अनुक्रमों को क्वेरी मैट्रिक्स \\(Q\\), की मैट्रिक्स \\(K\\), और वैल्यू मैट्रिक्स \\(V\\) (प्रत्येक का आकार \\(N \times d\\), जहां \\(N\\) अनुक्रम लंबाई है और \\(d\\) हेड डायमेंशन है, आमतौर पर 64-128) के रूप में दर्शाया जाता है, अटेंशन आउटपुट \\(O\\) की गणना इस प्रकार की जाती है:

\\[
S = QK^T \in \mathbb{R}^{N \times N}, \quad P = \softmax(S) \in \mathbb{R}^{N \times N}, \quad O = PV \in \mathbb{R}^{N \times d},
\\]

जहां \\(\softmax\\) को पंक्ति-वार (row-wise) लागू किया जाता है, और \\(S\\) को अक्सर स्थिरता के लिए \\(\tau = 1 / \sqrt{d}\\) द्वारा स्केल किया जाता है। कॉजल मास्किंग (ऑटोरेग्रेसिव मॉडल्स के लिए) और ड्रॉपआउट जैसे अतिरिक्त ऑपरेशन आम हैं।

यह सूत्रीकरण सुरुचिपूर्ण है लेकिन कम्प्यूटेशनली महंगा है। इंटरमीडिएट मैट्रिक्स \\(S\\) और \\(P\\) \\(N \times N\\) के होते हैं, जिसके परिणामस्वरूप अनुक्रम लंबाई \\(N\\) में **द्विघातीय समय और मेमोरी जटिलता (quadratic time and memory complexity)** \\(O(N^2)\\) होती है। लंबे कॉन्टेक्स्ट (जैसे, GPT-2 में \\(N = 4096\\) या आधुनिक LLM में 128k तक) के लिए, यह एक गंभीर बाधा बन जाता है:

- **मेमोरी हंगर (Memory Hunger)**: GPU पर, हाई-बैंडविड्थ मेमोरी (HBM) प्राथमिक स्टोरेज है, लेकिन \\(S\\) और \\(P\\) को मटीरियलाइज़ करने से उपलब्ध HBM (जैसे, A100/H100 पर 40-80 GB) से अधिक खपत हो सकती है। \\(N=4096\\), \\(d=64\\) पर, यह अकेले इंटरमीडिएट्स के लिए ~1-2 GB की खपत करता है, साथ ही इनपुट/आउटपुट/एक्टिवेशन्स, जिसके कारण अक्सर आउट-ऑफ-मेमोरी (OOM) एरर होते हैं।
- **स्पीड लिमिटेशन (Speed Limitations)**: अटेंशन मेमोरी-बाउंड होता है, कम्प्यूट-बाउंड नहीं। आधुनिक GPU (जैसे, NVIDIA A100) में ~1.5 TB/s HBM बैंडविड्थ होती है लेकिन ~19 TFLOPS कम्प्यूट—फिर भी softmax जैसे ऑपरेशन को पूर्ण \\(N^2\\) मैट्रिक्स को कई बार (जैसे, फॉरवर्ड/बैकवर्ड पास में प्रति एलिमेंट 4-6 HBM एक्सेस) पढ़ने/लिखने की आवश्यकता होती है। इसके परिणामस्वरूप वॉल-क्लॉक समय द्विघातीय रूप से स्केल होता है: जैसे, PyTorch में \\(N=4096\\) पर फॉरवर्ड पास ~36 ms, बैकवर्ड ~88 ms।
- **ट्रेनिंग/जनरेशन बैरियर्स (Training/Generation Barriers)**: ट्रेनिंग के दौरान, ग्रेडिएंट के लिए बैकवर्ड पास में \\(P\\) को स्टोर करने की आवश्यकता होती है, जिससे मेमोरी दोगुनी हो जाती है। इनफरेंस के लिए, लंबे कॉन्टेक्स्ट (जैसे, 64k टोकन) स्पार्स अटेंशन या लो-रैंक मेथड (जैसे, Reformer, Linformer) जैसे अनुमानों के बिना असंभव हैं, जो सटीकता के बदले दक्षता प्रदान करते हैं लेकिन I/O लागतों को नजरअंदाज करने के कारण अक्सर अंडरपरफॉर्म करते हैं।

FlashAttention (2022 में Tri Dao et al. द्वारा पेश किया गया) इन समस्याओं का समाधान एल्गोरिदम को **I/O-अवेयर** बनाकर करता है, GPU मेमोरी हायरार्की (फास्ट SRAM ~20 MB बनाम स्लो HBM) का लाभ उठाते हुए बिना किसी अनुमान के।

### मुख्य विचार: टाइलिंग, कर्नल फ्यूजन और ऑनलाइन सॉफ्टमैक्स

FlashAttention **सटीक (exact)** अटेंशन की गणना करता है (कोई अनुमान नहीं) निम्नलिखित तरीके से:

1.  **टाइलिंग (Tiling)**: पूर्ण \\(N \times N\\) मैट्रिक्स को मटीरियलाइज़ करने के बजाय, यह \\(Q, K, V\\) को छोटे ब्लॉक्स में विभाजित करता है जो SRAM में फिट होते हैं। \\(Q\\) को \\(T_r = \lceil N / B_r \rceil\\) रो-ब्लॉक्स (आकार \\(B_r \times d\\), जैसे \\(B_r \approx 64-256\\)) में विभाजित किया जाता है, और \\(K, V\\) को \\(T_c = \lceil N / B_c \rceil\\) कॉलम-ब्लॉक्स (आकार \\(B_c \times d\\), जैसे \\(B_c \approx 128-1024\\)) में विभाजित किया जाता है। ब्लॉक साइज को SRAM क्षमता \\(M\\) (जैसे, \\(B_c \approx M / (4d)\\)) के आधार पर डायनामिक रूप से चुना जाता है ताकि पुन: उपयोग (reuse) अधिकतम हो।

2.  **कर्नल फ्यूजन (Kernel Fusion)**: सभी ऑपरेशन (\\(S\\) के लिए matmul, मास्किंग, softmax, dropout, \\(O\\) के लिए matmul) एक ही CUDA कर्नल में फ्यूज़ कर दिए जाते हैं। यह इंटरमीडिएट्स को HBM में लिखने से बचाता है, जिससे I/O ~50-70% कम हो जाता है। कर्नल HBM से ब्लॉक्स को SRAM में लोड करता है, ऑन-चिप कम्प्यूटेशन करता है, और केवल आंशिक योग (partial sums) वापस लिखता है—जैसे, प्रति एलिमेंट के बजाय प्रति ब्लॉक एक HBM रीड/राइट।

3.  **सांख्यिकी के साथ ऑनलाइन सॉफ्टमैक्स (Online Softmax with Statistics)**: Softmax की गणना पूरी पंक्ति के बिना आंशिक रूप से नहीं की जा सकती, इसलिए FlashAddress वृद्धिशील गणना (incremental computation) के लिए एक **सहयोगी अपघटन (associative decomposition)** का उपयोग करता है। एक पंक्ति जो ब्लॉक्स \\(x = [x^{(1)}; x^{(2)}]\\) में विभाजित है, के लिए, रनिंग सांख्यिकी ट्रैक करें:
    -   रो-मैक्स \\(m_i = \max_j S_{ij}\\),
    -   एक्सपोनेंशियल्स का रो-सम \\(\ell_i = \sum_j \exp(S_{ij} - m_i)\\).

    स्थानीय स्टैट्स \\(\tilde{m}_t, \tilde{\ell}_t\\) के साथ एक नए ब्लॉक \\(x^{(t)}\\) के लिए अपडेट करना:
    \\[
    m_i^{\new} = \max(m_i, \tilde{m}_t), \quad \ell_i^{\new} = e^{m_i - m_i^{\new}} \ell_i + e^{\tilde{m}_t - m_i^{\new}} \tilde{\ell}_t.
    \\]
    आंशिक softmax तब \\(\tilde{P}_{ij} = \exp(S_{ij} - m_i^{\new})\\) होता है, और आउटपुट इस प्रकार जमा होता है \\(O_i \leftarrow \frac{\ell_i}{\ell_i^{\new}} e^{m_i - m_i^{\new}} O_i + \frac{\tilde{\ell}_t}{\ell_i^{\new}} e^{\tilde{m}_t - m_i^{\new}} \tilde{P}_{ij} V_j\\).

    यह संख्यात्मक रूप से स्थिर (फ्यूज़्ड softmax से मेल खाता है) और सटीक है, जैसा कि आगमनात्मक रूप से सिद्ध होता है: सभी ब्लॉक्स के बाद, \\(O = \softmax(S) V\\)।

ये विचार **मेमोरी को \\(O(N)\\)** (इनपुट + आउटपुट + \\(O(N)\\) स्टैट्स जैसे \\(m, \ell\\)) तक कम कर देते हैं और **HBM एक्सेस को \\(O(N^2 d / M)\\)** तक कम कर देते हैं—उप-द्विघात (sub-quadratic), क्योंकि प्रत्येक \\(K/V\\) एलिमेंट को एक बार पढ़ा जाता है, और \\(Q/O\\) को \\(T_c \approx N d / M\\) बार पढ़ा जाता है।

### फॉरवर्ड पास: ब्लॉक-बाय-ब्लॉक कम्प्यूटेशन

फॉरवर्ड पास (पेपर के एल्गोरिदम 2 में स्यूडोकोड) \\(K, V\\) के कॉलम-ब्लॉक्स पर पुनरावृत्ति करता है:

-   HBM में \\(O = 0^{N \times d}\\), \\(m = -\infty^N\\), \\(\ell = 0^N\\) इनिशियलाइज़ करें।
-   प्रत्येक कॉलम-ब्लॉक \\(j = 1\\) से \\(T_c\\) तक के लिए:
    -   \\(K_j, V_j\\) को SRAM में लोड करें (पंक्तियों में पुन: उपयोग)।
    -   प्रत्येक पंक्ति-ब्लॉक \\(i = 1\\) से \\(T_r\\) तक के लिए:
        -   \\(Q_i, O_i, m_i, \ell_i\\) को SRAM में लोड करें।
        -   स्थानीय \\(S_{ij} = \tau Q_i K_j^T\\) (\\(B_r \times B_c\\)) की गणना करें।
        -   मास्क: \\(S_{ij}^{\masked} = \mask(S_{ij})\\) (जैसे, कॉजल: लोअर ट्राएंगल को \\(-\infty\\) बनाना)।
        -   स्थानीय softmax स्टैट्स: \\(\tilde{m}_{ij} = \rowmax(S_{ij}^{\masked})\\), \\(\tilde{P}_{ij} = \exp(S_{ij}^{\masked} - \tilde{m}_{ij})\\), \\(\tilde{\ell}_{ij} = \rowsum(\tilde{P}_{ij})\\)।
        -   उपरोक्त सूत्रों का उपयोग करके वैश्विक स्टैट्स और आउटपुट को अपडेट करें, \\(\tilde{P}_{ij}\\) पर dropout लागू करते हुए।
        -   अपडेट किए गए \\(O_i, m_i, \ell_i\\) को HBM में लिखें।

यह सब कुछ फ्यूज़ कर देता है: कुल FLOPs \\(O(N^2 d)\\) बने रहते हैं, लेकिन I/O नाटकीय रूप से गिर जाता है (जैसे, मानक की तुलना में 9x कम एक्सेस)। कॉजल अटेंशन के लिए, मास्किंग सस्ती है (वेक्टराइज्ड)। Dropout एक साझा RNG स्टेट \\(R\\) का उपयोग करता है जिसे बैकवर्ड के लिए सेव किया जाता है।

### बैकवर्ड पास: रिकम्प्यूटेशन के माध्यम से ग्रेडिएंट कम्प्यूटेशन

बैकवर्ड पास (एल्गोरिदम 4) पेचीदा है, क्योंकि ग्रेडिएंट \\(P\\) पर निर्भर करते हैं:

\\[
dP = dO \cdot V^T, \quad dS = P \odot (dP - \rowsum(dO \odot O)), \quad dQ = dS \cdot K, \quad dK = Q^T \cdot dS, \quad dV = P^T \cdot dO.
\\]

\\(P\\) को स्टोर करना \\(O(N^2)\\) होगा, इसलिए FlashAttention **ब्लॉक्स को ऑन-द-फ्लाई रिकम्प्यूट करता है** (चयनात्मक रिकम्प्यूटेशन, चेकपॉइंटिंग की तरह लेकिन टाइल्ड):

-   इसी तरह पुनरावृत्ति करें: प्रत्येक \\(j\\) के लिए, \\(K_j, V_j\\) लोड करें; स्थानीय \\(dK_j, dV_j = 0\\) इनिशियलाइज़ करें।
-   प्रत्येक \\(i\\) के लिए: सेव किए गए \\(m_i, \ell_i\\) का उपयोग करके \\(S_{ij}, P_{ij}\\) को रिकम्प्यूट करें; \\(R\\) से dropout मास्क को पुनः उत्पन्न करें।
-   स्थानीय ग्रेडिएंट की गणना करें: \\(dV_j += P_{ij}^{dropped^T} dO_i\\), \\(dP_{ij} = dO_i V_j^T \odot Z_{ij}\\) (dropout मास्क), \\(dS_{ij} = P_{ij} \odot (dP_{ij} - D_i)\\) जहां \\(D_i = \rowsum(dO_i \odot O_i)\\)।
-   \\(dQ_i += \tau dS_{ij} K_j\\), \\(dK_j += \tau Q_i^T dS_{ij}\\) जमा करें।

यह दूसरे \\(O(N^2 d)\\) FLOPs का उपयोग करता है लेकिन केवल \\(O(N)\\) अतिरिक्त मेमोरी (कोई \\(P\\) स्टोरेज नहीं)। कुल फॉरवर्ड + बैकवर्ड: मानक के FLOPs का ~2-3x लेकिन I/O बचत के कारण 2-4x तेज।

### I/O-अवेयरनेस और GPU ऑप्टिमाइजेशन

GPU में एक हायरार्की होती है: रजिस्टर्स/SRAM (फास्ट, छोटा) >> HBM (स्लो, बड़ा)। मानक अटेंशन प्रति पास \\(\Theta(N^2)\\) एक्सेस के साथ HBM को थ्रैश करता है। FlashAttention की टाइलिंग सुनिश्चित करती है:
-   \\(K, V\\) एक बार लोड होते हैं (\\(O(N d)\\))।
-   \\(Q, O\\) \\(T_c \approx N / B_c \approx N d / M\\) बार लोड होते हैं (\\(O(N^2 d / M)\\))।
-   निचली सीमा: मध्यम श्रेणी के \\(M\\) के लिए कोई भी सटीक एल्गोरिदम \\(\Omega(N^2 d^2 / M)\\) को हरा नहीं सकता।

अनुभवजन्य: A100 पर, HBM स्टॉल रनटाइम पर हावी होते हैं; FlashAttention उन्हें 50-80% तक कम कर देता है, कम्प्यूट-बाउंड शासन (compute-bound regime) तक पहुंच जाता है। यह और भी अधिक लाभ (घने की तुलना में 2-4x) के लिए ब्लॉक-स्पार्सिटी (शून्य-मास्क ब्लॉक्स को छोड़ें) का समर्थन करता है।

### लाभ: गति, मेमोरी और डाउनस्ट्रीम प्रभाव

-   **मेमोरी**: रैखिक \\(O(N d)\\), सिंगल GPU पर 64k+ अनुक्रम सक्षम करता है (बनाम 2k-4k मानक)। जैसे, \\(N=65k\\) पर 13 GB बनाम 200+ GB मानक।
-   **गति**: GPT/BERT ट्रेनिंग पर एंड-टू-एंड 2-4x तेज; रॉ अटेंशन पर 7x तक। जैसे, संयुक्त fwd/bwd: \\(N=128\\) पर 0.43 ms से \\(N=65k\\) पर 9s (बनाम PyTorch OOM)।
-   **गुणवत्ता**: सटीक, इसलिए कोई पर्प्लेक्सिटी ड्रॉप नहीं। लंबे कॉन्टेक्स्ट सक्षम करता है: 4x लंबाई पर GPT-2 पर 0.7-पॉइंट पर्प्लेक्सिटी लाभ; लंबे-दस्तावेज़ कार्यों पर स्टेट-ऑफ-द-आर्ट (जैसे, 64k seqs पर Path-256 पर 63%)।
-   **एक्सटेंशन**: FlashAttention-2 (2023) बेटर पैरेललिज्म जोड़ता है (2x तक तेज); FlashAttention-3 (2024) H100 के लिए एसिंक्रोनसी/लो-प्रिसिजन का उपयोग करता है (50% तेज)।

### PyTorch इंटीग्रेशन: सहज उपयोग

PyTorch 2.0+ `torch.nn.functional.scaled_dot_product_attention(q, k, v)` के माध्यम से FlashAttention को मूल रूप से एकीकृत करता है, जो शर्तें पूरी होने पर स्वचालित रूप से फ्यूज़्ड कर्नल पर डिस्पैच करता है (जैसे, निरंतर टेंसर, समर्थित डीटाइप)। कस्टम CUDA की आवश्यकता नहीं:

```python
import torch
from torch.nn.functional import scaled_dot_product_attention

q, k, v = torch.randn(1, 8, 1024, 64, device='cuda')  # batch, heads, seq, dim
with torch.backends.cuda.sdp_kernel(enable_flash=True, enable_math=False):
    out = scaled_dot_product_attention(q, k, v, attn_mask=mask, dropout_p=0.1)
```

यह मास्किंग/ड्रॉपआउट को हैंडल करता है; असमर्थित होने पर मैथ कर्नल पर फॉलबैक करता है। कस्टम मॉडल्स के लिए, यह मैनुअल अटेंशन लूप्स के लिए ड्रॉप-इन रिप्लेसमेंट है, जो आउट-ऑफ-बॉक्स 2-3x स्पीडअप प्रदान करता है।

FlashAttention ने लंबे-कॉन्टेक्स्ट ट्रेनिंग में क्रांति ला दी, जिससे Llama-2 (4k→70B) और उससे आगे के मॉडल्स को शक्ति मिली।

[FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness](https://arxiv.org/abs/2205.14135)  
[PyTorch 2.2: FlashAttention-v2 Integration](https://pytorch.org/blog/pytorch2-2/)  
[GitHub: Dao-AILab/flash-attention](https://github.com/Dao-AILab/flash-attention)