---
audio: false
lang: hi
layout: post
title: लीनियर अलजेब्रा प्लस - वार्तालाप
translated: true
type: note
---

ए: हे, मैं हाल ही में द्विघात रूपों को फिर से देख रहा हूं, खासकर उन्हें विहित रूप में बदलने की प्रक्रिया। क्या तुम उस उदाहरण के साथ समझा सकते हो कि तुम इसे कैसे करते हो, Q(x, y) = 2x² + 4xy + 3y²?

बी: ज़रूर! चलिए मूल बातों से शुरू करते हैं। उस द्विघात रूप को एक मैट्रिक्स समीकरण के रूप में लिखा जा सकता है, है ना? तुम गुणांक लेते हो और एक सममित मैट्रिक्स A बनाते हो। इसके लिए, यह [2, 2; 2, 3] है, क्योंकि 4xy पद समान रूप से 2xy + 2yx के रूप में विभाजित होता है। क्या यह तुम्हारे देखने के तरीके से मेल खाता है?

ए: बिल्कुल, मैं मैट्रिक्स सेटअप से सहमत हूं। ऑफ-डायगोनल पर 2, 4 को आधा करने से आता है, जो सममिति के लिए समझ में आता है। तो, अगला कदम eigenvalues है, है ना? तुम यहां इसे कैसे हल करते हो?

बी: हां, eigenvalues महत्वपूर्ण हैं। हम det(A - λI) = 0 हल करते हैं। तो, [2-λ, 2; 2, 3-λ] के लिए, सारणिक है (2-λ)(3-λ) - 4। इसे विस्तारित करने पर, तुम्हें मिलता है λ² - 5λ + 2 = 0। इसे हल करने पर मिलता है λ = (5 ± √17)/2। तुम इन मूल्यों के बारे में क्या सोचते हो?

ए: मुझे जांचने दो... हां, विविक्तकर है 25 - 8 = 17, तो (5 ± √17)/2 सही लगता है। दोनों धनात्मक हैं, जो सुझाव देता है कि यह रूप धनात्मक निश्चित हो सकता है। लेकिन आगे न बढ़ें—तुम आगे eigenvectors को कैसे संभालते हो?

बी: धनात्मकता पर अच्छी नज़र! Eigenvectors के लिए, पहले λ₁ = (5 + √17)/2 लो। इसे A - λI में डालो, तो [2 - λ₁, 2; 2, 3 - λ₁]। इस प्रणाली को पंक्ति-न्यून करने पर, तुम्हें एक eigenvector मिलता है जैसे [2, λ₁ - 2]। फिर λ₂ = (5 - √17)/2 के लिए दोहराओ। यह थोड़ा थकाऊ है—क्या तुम उन्हें तुरंत सामान्य करते हो या इंतज़ार करते हो?

ए: मैं आमतौर पर तब तक इंतज़ार करता हूं जब तक मैं P मैट्रिक्स नहीं बना लेता, ताकि शुरुआत में बीजगणित साफ रहे। तो, P के कॉलम वे eigenvectors होंगे, और फिर D विकर्ण है λ₁ और λ₂ के साथ। यह Q को विहित रूप में कैसे बदलता है?

बी: ठीक, P, A को विकर्णीकृत करता है, इसलिए P^T A P = D। तुम नए चर परिभाषित करते हो, मान लो [x; y] = P [u; v], और वापस प्रतिस्थापित करते हो। द्विघात रूप बन जाता है Q(u, v) = λ₁u² + λ₂v²। चूंकि यहां दोनों eigenvalues धनात्मक हैं, यह वर्गों का योग है—कोई क्रॉस टर्म्स नहीं। क्या यह सरलता कभी तुम्हें आश्चर्यचकित करती है?

ए: कभी-कभी, हां! यह सुरुचिपूर्ण है कि कैसे क्रॉस टर्म्स गायब हो जाते हैं। लेकिन मैं उत्सुक हूं—अगर एक eigenvalue ऋणात्मक होता तो क्या होता? उदाहरण के लिए, अनुकूलन के संदर्भों में व्याख्या कैसे बदलती?

बी: बढ़िया सवाल! अगर λ₂ ऋणात्मक होता, तो तुम्हें मिलता Q = λ₁u² - |λ₂|v², जो इसे अनिश्चित बना देता। अनुकूलन में, यह एक काठी बिंदु है—एक दिशा में अधिकतम करना, दूसरी में न्यूनतम। f(x, y) = 2x² + 4xy - 3y² जैसे फ़ंक्शन के बारे में सोचो। चरम सीमा को वर्गीकृत करना मुश्किल होता है। क्या तुम्हारा वास्तविक अनुप्रयोगों में कभी सामना हुआ है?

ए: अरे, निश्चित रूप से। मशीन लर्निंग में, अनिश्चित रूप Hessian matrices के साथ तब आते हैं जब तुम द्वितीय-कोटि शर्तों की जांच कर रहे होते हो। धनात्मक निश्चित का मतलब स्थानीय न्यूनतम, लेकिन अनिश्चित एक काठी बिंदु का संकेत देता है। क्या तुम्हें लगता है कि यह विकर्णीकरण दृष्टिकोण उच्च आयामों के लिए अच्छी तरह से स्केल करता है?

बी: करता है, लेकिन गणना जटिल हो जाती है। n चरों के लिए, तुम eigenvalues के लिए एक n-घात बहुपद हल कर रहे होते हो, और संख्यात्मक स्थिरता एक मुद्दा बन जाती है। NumPy या LAPACK जैसी लाइब्रेरी इसे संभालती हैं, लेकिन विश्लेषणात्मक रूप से? कठिन। बड़ी प्रणालियों के लिए तुम्हारा पसंदीदा तरीका क्या है?

ए: मैं भी संख्यात्मक टूल्स का सहारा लेता हूं—वहां eigenvalue decomposition जीवनरक्षक है। लेकिन मैं सोचता हूं, क्या विकर्णीकरण के विकल्प हैं? जैसे, वर्ग को पूरा करना?

बी: अरे, बिल्कुल! 2x² + 4xy + 3y² के लिए, तुम वर्ग पूरा करने की कोशिश कर सकते हो: 2(x² + 2xy) + 3y² = 2(x + y)² - 2y² + 3y² = 2(x + y)² + y²। यह अभी तक पूरी तरह से विहित नहीं है, लेकिन u = x + y, v = y जैसा प्रतिस्थापन इसे साफ कर सकता है। यह विकर्णीकरण से कम व्यवस्थित है, हालांकि—व्यापार-नापसंद पर विचार?

ए: मुझे यह पसंद है—यह छोटे मामलों के लिए अधिक सहज है, लेकिन मैं सामान्यता की कमी देखता हूं। विकर्णीकरण कठोर है और n आयामों तक फैलता है, जबकि वर्ग पूरा करना तीन चरों के बाद तदर्थ लगता है। कभी संकर दृष्टिकोण आज़माए?

बी: वास्तव में नहीं, लेकिन यह एक विचार है! शायद एक अहसास पाने के लिए वर्ग पूरा करने से शुरू करो, फिर विकर्णीकरण के साथ औपचारिक रूप दो। उभरते रुझान वैसे भी कम्प्यूटेशनल दक्षता की ओर झुकते हैं—विरल matrices के लिए पुनरावृत्त विधियों के बारे में सोचो। तुम्हें क्या लगता है यह कहां जा रहा है?

ए: मैं संकर संख्यात्मक-प्रतीकात्मक विधियों पर दांव लगाऊंगा, खासकर AI के साथ मैट्रिक्स ऑप्स को अनुकूलित करते हुए। विहित रूप शाश्वत हैं, लेकिन वहां पहुंचने के टूल? वे तेजी से विकसित हो रहे हैं। यह मजेदार था—अगली बार एक 3D उदाहरण करना चाहते हो?

बी: पूरी तरह! चलो Q(x, y, z) = x² + 2xy + 2yz + z² या कुछ और करते हैं। फिर मिलते हैं!

---

ए: हे, मैं हाल ही में matrices की बुनियादी बातों—नोटेशन, ऑपरेशन्स, वह सब—दोहरा रहा हूं। क्या तुम मुझे समझा सकते हो कि तुम किसी को मूल बातें कैसे समझाओगे, शायद पहले वाले 2x² + 4xy + 3y² द्विघात रूप मैट्रिक्स से शुरू करके?

बी: ज़रूर, चलिए इसमें गोता लगाते हैं! एक मैट्रिक्स बस एक आयताकार सरणी होती है, है ना? उस द्विघात रूप के लिए, हमने इसे एक सममित मैट्रिक्स में बदल दिया: [2, 2; 2, 3]। ऑफ-डायगोनल पर 2, 4xy पद को विभाजित करने से आते हैं। तुम आमतौर पर मैट्रिक्स नोटेशन कैसे पेश करते हो?

ए: मैं सामान्य रूप से जाऊंगा: A = [a_ij], जहां i पंक्ति है, j स्तंभ है। तो, उस उदाहरण के लिए, a_11 = 2, a_12 = 2, और इसी तरह। यह एक 2×2 वर्ग मैट्रिक्स है। तुम्हारा अगला कदम क्या है—matrices के प्रकार या ऑपरेशन्स?

बी: पहले प्रकार देखते हैं। वह [2, 2; 2, 3] वर्गाकार है, m = n = 2। फिर identity matrix है, जैसे [1, 0; 0, 1], जो गुणन में '1' की तरह कार्य करती है। क्या तुम्हें कभी अजीब लगता है कि यह कितना सरल लेकिन शक्तिशाली है?

ए: हां, यह लगभग बहुत साफ है—AI = IA = A बस क्लिक करता है। zero matrix के बारे में क्या? मैं [0, 0; 0, 0] डालूंगा—इससे गुणा करने पर सब कुछ खत्म हो जाता है। क्या यह तुम्हारे लिए ऑपरेशन्स से जुड़ता है?

बी: पूरी तरह! ऑपरेशन्स तब मजेदार हो जाते हैं। जोड़ सीधा है—समान आकार, तत्वों को जोड़ो। मान लो [1, 2; 3, 4] + [2, 0; 1, 3] = [3, 2; 4, 7]। घटाव भी वैसा ही है। scalar multiplication के बारे में क्या—तुम उसे कैसे दिखाते हो?

ए: आसान—हर प्रविष्टि को एक संख्या से गुणा करो। जैसे 3 × [1, -2; 4, 0] = [3, -6; 12, 0]। यह सहज है, लेकिन matrix multiplication? वहीं मैं पंक्ति-स्तंभ के नृत्य को समझाने में फंस जाता हूं। तुम इसे कैसे तोड़ते हो?

बी: मैं एक उदाहरण देता हूं। लो [1, 2; 3, 4] गुणा [2, 0; 1, 3]। (1,1) प्रविष्टि है 1×2 + 2×1 = 4, (1,2) है 1×0 + 2×3 = 6, और इसी तरह। तुम्हें अंत में मिलता है [4, 6; 10, 12]। यह सब डॉट उत्पाद हैं। क्या यह क्लिक करता है, या शर्त वाला हिस्सा मुश्किल है?

ए: डॉट उत्पाद वाला हिस्सा स्पष्ट है, लेकिन मैं हमेशा शर्त पर जोर देता हूं: पहले के स्तंभ दूसरे की पंक्तियों से मेल खाने चाहिए। यहां, 2×2 गुणा 2×2 काम करता है। अगर वे मेल नहीं खाते तो क्या—कोई वास्तविक दुनिया के मामले जहां यह चीजों को खराब कर देता है?

बी: अरे, बहुत सारे! डेटा साइंस में, बेमेल आयाम तुम्हारे कोड को क्रैश कर देते हैं—जैसे गलत आकार वाले वजन वेक्टर द्वारा एक फीचर मैट्रिक्स को गुणा करना। अगला, transpose—पंक्तियों और स्तंभों को बदलो। [1, 2; 3, 4] के लिए, यह [1, 3; 2, 4] है। कोई पसंदीदा transpose गुण?

ए: मुझे (AB)^T = B^T A^T पसंद है—यह पहली बार में कितना प्रतिकूल है! पंक्तियाँ स्तंभ बन जाती हैं, और क्रम उलट जाता है। यह हमारे द्विघात रूप मैट्रिक्स में कैसे काम करता है?

बी: अच्छा सवाल! [2, 2; 2, 3] के लिए, यह सममित है, इसलिए A^T = A। इसीलिए Q(x, y) = x^T A x काम करता है—सममिति इसे साफ रखती है। अब, inverses—केवल गैर-शून्य सारणिक वाली वर्ग matrices। क्या [4, 7; 2, 6] के लिए A^-1 ढूंढना चाहते हो?

ए: ज़रूर! Det = 4×6 - 7×2 = 24 - 14 = 10। फिर A^-1 = (1/10) × [6, -7; -2, 4] = [0.6, -0.7; -0.2, 0.4]। क्या मैंने सही किया?

बी: एकदम सही! A A^-1 गुणा करो, तुम्हें identity मिलती है। सिस्टम हल करने या अनुकूलन के लिए inverses महत्वपूर्ण हैं। कभी बड़े संदर्भों में उनका उपयोग किया है, जैसे 3×3 या उससे आगे?

ए: हां, ग्राफिक्स में—रोटेशन matrices को transformations पूर्ववत करने के लिए inverses की आवश्यकता होती है। लेकिन 2×2 के बाद, मैं सॉफ्टवेयर का सहारा लेता हूं। हाथ से 3×3 inverse की गणना करना एक कठिन कार्य है। तुम?

बी: वही—संख्यात्मक लाइब्रेरी हर जगह। हालांकि, पढ़ाने के लिए, मैं पैटर्न दिखाने के लिए एक 2×2 करूंगा। उभरते टूल्स पर तुम्हारा क्या विचार है—जैसे AI मैट्रिक्स ऑप्स को तेज करना?

ए: मैं इसके लिए पूरी तरह से तैयार हूं। AI वास्तविक समय में विरल मैट्रिक्स गुणन या inverses को अनुकूलित कर सकता है। इन ऑपरेशन्स जैसी क्लासिक्स नहीं बदलतीं, लेकिन टेक? यह गेम-चेंजर है। अगली बार एक 3×3 आज़माना चाहते हो?

बी: चलो करते हैं! [1, 2, 0; 0, 3, 1; 2, -1, ]
4] के बारे में क्या? हम inverse या गुणन करेंगे—तुम्हारी पसंद!

---

ए: हे, मैं एक लीनियर अलजेब्रा परीक्षा की तैयारी कर रहा हूं और मुख्य बिंदुओं को याद करने की कोशिश कर रहा हूं। क्या साथ में कुछ देखना चाहते हो? शायद यह शुरू करें कि लीनियर अलजेब्रा है भी क्या?

बी: ज़रूर, चलो करते हैं! लीनियर अलजेब्रा वेक्टर स्पेस और लीनियर मैपिंग के बारे में है—जैसे समीकरणों की प्रणाली हल करना। यह इतने सारे गणित की रीढ़ है। तुम्हारी पहली बड़ी अवधारणा क्या है?

ए: वेक्टर्स, मुझे लगता है। उनमें परिमाण और दिशा होती है, है ना? और तुम उन्हें n-आयामी स्पेस में रख सकते हो। तुम उन्हें कैसे देखते हो—पंक्तियों या स्तंभों के रूप में?

बी: संदर्भ पर निर्भर करता है! मैं उन्हें आमतौर पर स्तंभों के रूप में देखता हूं, जैसे [x; y], लेकिन पंक्ति वेक्टर्स भी आते हैं। अगला—matrices? वे बस संख्याओं की सरणियाँ हैं, लेकिन वे इस सामान में हर जगह हैं।

ए: हां, पंक्तियों और स्तंभों के साथ आयताकार सरणियाँ। वर्गाकार वालों के m = n होते हैं, जैसे [2, -1; 4, 3]। identity matrix के बारे में क्या खास है?

बी: अरे, identity मजेदार है—इसमें विकर्ण पर 1 होते हैं, बाकी जगह 0, जैसे [1, 0; 0, 1]। इसे किसी भी मैट्रिक्स से गुणा करो, और कुछ नहीं बदलता। कभी zero matrix के साथ खेला?

ए: सभी शून्य वाला? जैसे [0, 0; 0, 0]? यह कुछ भी गुणा करने पर मिटा देता है। ऑपरेशन्स की बात करें, तो matrix addition कैसे काम करती है?

बी: सरल—समान आकार, तत्व-दर-तत्व जोड़ो। [1, 2] + [3, 4] = [4, 6]। लेकिन गुणन trickier है—पहले के स्तंभ दूसरे की पंक्तियों से मेल खाने चाहिए। क्या तुमने कभी गौर किया कि यह क्रम-विनिमेय नहीं है?

ए: हां, AB ≠ BA मुझे उलझाता है! determinants के बारे में क्या? मुझे पता है कि वे invertibility से जुड़े हैं।

बी: बिल्कुल! एक मैट्रिक्स तभी invertible होती है जब उसका सारणिक शून्य न हो। 2×2 के लिए, यह ad - bc है। तुम्हारे लिए inverses का क्या महत्व है?

ए: A^-1 गुणा A identity देता है, लेकिन केवल वर्ग, गैर-एकवचन matrices के लिए। eigenvalues इसमें कैसे फिट होते हैं?

बी: eigenvalues वे अदिश हैं जहां Av = λv कुछ वेक्टर v के लिए होता है। तुम det(A - λI) = 0 हल करते हो। eigenvectors दिशा नहीं बदलते, बस स्केल करते हैं। विकर्णीकरण में बड़े—क्या उस पर गहराई से बात करना चाहते हो?

ए: हां, विकर्णीकरण बहुत बड़ा है। एक मैट्रिक्स विकर्णीकरण योग्य होती है अगर उसमें पर्याप्त स्वतंत्र eigenvectors हों, है ना? इसे एक विकर्ण मैट्रिक्स में बदल देता है। यह हमारे लिए क्या करता है?

बी: सब कुछ सरल कर देता है—समीकरणों की प्रणाली, matrices की घातें। द्विघात रूपों से भी जुड़ता है, जैसे xᵀAx। कभी सममित matrices के साथ खेला?

ए: सममित वाले जहां A = Aᵀ? वे द्विघात रूपों के लिए बड़े हैं। तुम समीकरणों की प्रणाली कैसे संभालते हो—गाउसियन निष्कासन?

बी: हां, गाउसियन निष्कासन तुम्हें पंक्ति सोपानक रूप में ले जाता है, या घटा हुआ पंक्ति सोपानक समाधानों के लिए। सजातीय प्रणालियों में हमेशा शून्य समाधान होता है। तुम्हारा consistent बनाम inconsistent systems पर क्या विचार है?

ए: consistent का मतलब कम से कम एक समाधान, inconsistent का मतलब कोई नहीं। आश्रित प्रणालियों में अनंत समाधान होते हैं, स्वतंत्र में बस एक। यह rank से कैसे जुड़ता है?

बी: Rank स्वतंत्र पंक्तियों या स्तंभों की संख्या है। पूर्ण रैंक का मतलब अधिकतम स्वतंत्रता। null space सभी वेक्टर्स है जहां Ax = 0—rank-nullity प्रमेय उन्हें जोड़ता है। कभी उसका उपयोग किया?

ए: अभी तक नहीं, लेकिन मैं समझता हूं rank + nullity = स्तंभों की संख्या। vector spaces और bases के बारे में क्या?

बी: Vector space वेक्टर्स है जिन्हें तुम जोड़ और स्केल कर सकते हो। एक आधार रैखिक रूप से स्वतंत्र होता है और इसे फैलाता है—आयाम आधार का आकार है। उपस्पेस उसके अंदर छोटे वेक्टर स्पेस हैं। कूल, है ना?

ए: बहुत कूल! रैखिक स्वतंत्रता का मतलब कोई वेक्टर दूसरों का संयोजन नहीं है। span उनके सभी संयोजन हैं। transformations इसमें कैसे फिट होते हैं?

बी: रैखिक transformations जोड़ और स्केलिंग संरक्षित करते हैं। kernel वह है जो शून्य पर मैप करता है, image आउटपुट रेंज है। घूर्णन या प्रक्षेपण के बारे में सोचो। अगला लंबकोणिता?

ए: हां, लंबकोणीय वेक्टर्स—डॉट उत्पाद शून्य। ऑर्थोनॉर्मल वह प्लस इकाई लंबाई। ऑर्थोगोनल matrices अद्भुत हैं—उनका inverse उनका transpose होता है। यह कैसे उपयोगी है?

बी: लंबाई और कोणों को संरक्षित करता है—ग्राफिक्स में बहुत बड़ा। Gram-Schmidt वेक्टर्स को लंबकोणीय बनाता है। बड़ी matrices में determinants के बारे में क्या?

ए: 3×3 के लिए, सहखंड विस्तार, है ना? त्रिकोणीय वाले बस विकर्ण उत्पाद हैं। एकवचन अगर det = 0। यह सिस्टम में कैसे मदद करता है?

बी: बताता है कि क्या कोई अद्वितीय समाधान है—det ≠ 0 का मतलब invertible। पंक्ति ऑप्स इसे सरल करते हैं। कभी SVD या LU अपघटन आज़माया?

ए: उनके बारे में सुना है—SVD एक मैट्रिक्स को तीन में तोड़ता है, LU सिस्टम हल करने के लिए है। वास्तविक दुनिया का सामान जैसे ग्राफिक्स या डेटा साइंस यह सब उपयोग करता है, हं?

बी: अरे हां—अनुकूलन, इंजीनियरिंग, मशीन लर्निंग। अतिनिर्धारित प्रणालियों के लिए least-squares भी। तुम्हारा पसंदीदा अनुप्रयोग क्या है?

ए: कंप्यूटर ग्राफिक्स—घूर्णन और प्रक्षेपण सभी matrices हैं। यह बहुत कुछ है—क्या एक पेचीदा करना चाहते हो, जैसे 3×3 inverse?

बी: चलो करते हैं! एक चुनो—शायद [1, 2, 0; 0, 3, 1; 2, -1, 4]? हम इसे साथ में हल करेंगे!

---

ए: ठीक है, चलो उस 3×3 inverse को हल करते हैं [1, 2, 0; 0, 3, 1; 2, -1, 4] के लिए। पहला कदम सारणिक है, है ना? तुम आमतौर पर वह कैसे शुरू करते हो?

बी: हां, पहले सारणिक! 3×3 के लिए, मैं पहली पंक्ति के साथ सहखंड विस्तार करता हूं। तो, यह है 1 गुणा det([3, 1; -1, 4]) घटा 2 गुणा det([0, 1; 2, 4]) जमा 0 गुणा कुछ। क्या मेरे साथ उन 2×2s की गणना करना चाहते हो?

ए: ज़रूर! पहला है [3, 1; -1, 4], तो 3×4 - 1×(-1) = 12 + 1 = 13। दूसरा है [0, 1; 2, 4], तो 0×4 - 1×2 = -2। अंतिम पद 0 है, तो det = 1×13 - 2×(-2) = 13 + 4 = 17। सही लगता है?

बी: एकदम सही! Det = 17, तो यह invertible है। अगला, हमें adjugate चाहिए—transposed cofactors। cofactor matrix से शुरू करो—एक तत्व चुनो, जैसे (1,1)। इसका minor और cofactor क्या है?

ए: (1,1) के लिए, पंक्ति 1, स्तंभ 1 कवर करो, तो minor है [3, 1; -1, 4], det = 13। Cofactor है (-1)^(1+1) × 13 = 13। अगला, (1,2)—minor है [0, 1; 2, 4], det = -2, cofactor है (-1)^(1+2) × (-2) = 2। जारी रखें?

बी: हां, चलो एक और करते हैं—(1,3)। Minor है [0, 3; 2, -1], det = 0×(-1) - 3×2 = -6, cofactor है (-1)^(1+3) × (-6) = -6। तुम शानदार कर रहे हो! क्या cofactor matrix पूरा करना चाहते हो या adjugate पर कूदना चाहते हो?

ए: चलो इसे पूरा करते हैं। पंक्ति 2: (2,1) minor [2, 0; -1, 4], det = 8, cofactor = -8; (2,2) minor [1, 0; 2, 4], det = 4, cofactor = 4; (2,3) minor [1, 2; 2, -1], det = -5, cofactor = 5। पंक्ति 3?

बी: पंक्ति 3: (3,1) minor [2, 0; 3, 1], det = 2, cofactor = -2; (3,2) minor [1, 0; 0, 1], det = 1, cofactor = -1; (3,3) minor [1, 2; 0, 3], det = 3, cofactor = 3। तो cofactor matrix है [13, 2, -6; -8, 4, 5; -2, -1, 3]। इसे transpose करो!

ए: Adjugate है [13, -8, -2; 2, 4, -1; -6, 5, 3]। Inverse है (1/17) गुणा यह,