---
audio: false
generated: true
image: false
lang: hi
layout: post
model: x-ai/grok-4-fast
title: पाइटॉर्च ड्रॉपआउट लेयर को समझना
translated: true
type: note
---

### `drop = nn.Dropout(config.dropout)` को समझना

PyTorch (एक लोकप्रिय डीप लर्निंग लाइब्रेरी) में, `nn.Dropout` एक बिल्ट-इन मॉड्यूल है जिसका उपयोग मॉडल ट्रेनिंग के दौरान **ड्रॉपआउट रेगुलराइजेशन** लागू करने के लिए किया जाता है। यह तकनीक प्रत्येक फॉरवर्ड पास में टेंसर के इनपुट एलिमेंट्स के एक अंश को यादृच्छिक रूप से "ड्रॉप" (यानी, शून्य पर सेट) करके ओवरफिटिंग को रोकने में मदद करती है। यह केवल ट्रेनिंग के दौरान सक्रिय रहता है; मूल्यांकन/इनफेरेंस के दौरान, यह कुछ भी ड्रॉप नहीं करता है और अपेक्षित आउटपुट परिमाण बनाए रखने के लिए शेष मानों को स्केल करता है।

आपकी कोड लाइन को तोड़ते हुए:
```python
drop = nn.Dropout(config.dropout)
```
- `nn.Dropout`: यह ड्रॉपआउट लेयर के लिए PyTorch क्लास है।
- `config.dropout`: यह आमतौर पर एक कॉन्फ़िगरेशन ऑब्जेक्ट/डिक्शनरी से एक फ्लोट वैल्यू (जैसे, 0.1 या 0.5) होती है, जो **ड्रॉपआउट प्रोबेबिलिटी** `p` को दर्शाती है। इसका मतलब है "तत्वों के इस प्रतिशत को ड्रॉप करें।"
  - उदाहरण के लिए, यदि `config.dropout = 0.2`, तो इनपुट में 20% तत्व यादृच्छिक रूप से शून्य पर सेट हो जाएंगे।
- `drop = ...`: यह ड्रॉपआउट मॉड्यूल का एक इंस्टेंस बनाता है और इसे एक वेरिएबल `drop` को असाइन करता है। आप इसे अपने न्यूरल नेटवर्क में किसी भी अन्य लेयर की तरह उपयोग कर सकते हैं (जैसे, `nn.Sequential` या फॉरवर्ड मेथड में)।

#### जब आप `drop(x)` को कॉल करते हैं तो ड्रॉपआउट कैसे काम करता है
नहीं, `drop(x)` का मतलब **"सभी को 0 बना दो"** नहीं है। बल्कि:
- यह एक इनपुट टेंसर `x` (जैसे, पिछली लेयर से एक्टिवेशन्स) लेता है।
- प्रोबेबिलिटी `p` (जो `config.dropout` से आती है) के आधार पर तत्वों को **यादृच्छिक रूप से** ड्रॉप करने के लिए चुनता है।
  - ड्रॉप किए गए तत्व 0 पर सेट हो जाते हैं।
  - गैर-ड्रॉप किए गए तत्वों को `1 / (1 - p)` से स्केल किया जाता है ताकि अपेक्षित योग समान बना रहे (यह ट्रेनिंग के दौरान अंडरफ्लो से बचाता है)।
- यह **केवल ट्रेनिंग** (`model.train()` मोड) के दौरान होता है। ईवल मोड (`model.eval()`) में, यह `x` को बिना बदले पास कर देता है।
- उदाहरण: यदि `x` एक टेंसर है जैसे `[1.0, 2.0, 3.0, 4.0]` और `p=0.5`, तो एक संभावित आउटपुट `[0.0, 4.0, 0.0, 8.0]` हो सकता है (50% ड्रॉप, बचे हुए 2x से स्केल)। लेकिन यह रैंडम है, इसलिए परिणाम हर बार अलग-अलग होते हैं।

#### सरल कोड उदाहरण
यहां समझाने के लिए एक मिनिमल PyTorch स्निपेट है:
```python
import torch
import torch.nn as nn

# मान लें config.dropout = 0.5 (50% dropout)
config = type('Config', (), {'dropout': 0.5})()
drop = nn.Dropout(config.dropout)

# सैंपल इनपुट टेंसर
x = torch.tensor([1.0, 2.0, 3.0, 4.0])

# ट्रेनिंग के दौरान
torch.manual_seed(42)  # पुनरुत्पादनशीलता के लिए
print(drop(x))  # संभावित आउटपुट: tensor([0., 0., 0., 8.])  # रैंडम ड्रॉप + स्केलिंग

# ईवल के दौरान
drop.eval()
print(drop(x))  # आउटपुट: tensor([1., 2., 3., 4.])  # कोई बदलाव नहीं
```

#### ड्रॉपआउट का उपयोग क्यों करें?
- **ओवरफिटिंग को रोकता है**: मॉडल को किसी एक न्यूरॉन पर बहुत अधिक निर्भर नहीं रहने के लिए मजबूर करता है।
- फीडफॉरवर्ड/MLP लेयर्स, RNNs, या एम्बेडिंग्स के बाद (जैसे, ट्रांसफॉर्मर्स में) आम है।
- `p` को हाइपरपैरामीटर सर्च के माध्यम से ट्यून करें; बहुत अधिक (जैसे, >0.5) अंडरफिटिंग का कारण बन सकता है।

पूर्ण डॉक्स के लिए, [PyTorch nn.Dropout रेफरेंस](https://pytorch.org/docs/stable/generated/torch.nn.Dropout.html) देखें। यदि यह किसी विशिष्ट मॉडल (जैसे, BERT) से है, तो अधिक संदर्भ के लिए मुझे बताएं!