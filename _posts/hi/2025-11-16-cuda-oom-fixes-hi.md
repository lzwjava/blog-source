---
audio: false
generated: true
image: false
lang: hi
layout: post
model: x-ai/grok-4-fast
title: CUDA मेमोरी त्रुटि के समाधान
translated: true
type: note
---

### समस्या विश्लेषण
nanoGPT से प्रशिक्षण स्क्रिप्ट प्रारंभिक हानि अनुमान (`estimate_loss()`) के दौरान, फॉरवर्ड पास में CUDA आउट ऑफ मेमोरी (OOM) त्रुटि के साथ विफल हो रही है। यह मॉडल आरंभीकरण और संकलन के तुरंत बाद होता है। मुख्य समस्या मॉडल के आउटपुट लॉजिट्स के लिए एक बड़े बफर को आवंटित करते समय मेमोरी समाप्त होना है:

- **बफर विवरण**: `(65536, 50304)` आकार का एक टेंसर `bfloat16` में (प्रति तत्व 2 बाइट्स), जो लगभग ~6.14 GiB है। यह इसके अनुरूप है:
  - बैच आकार: 64
  - अनुक्रम लंबाई (block_size): 1024
  - कुल टोकन: 64 × 1024 = 65,536
  - शब्दावली आकार: 50,304 (GPT-2 डिफॉल्ट)
- **GPU स्थिति**: आपके GPU (संभवतः एक RTX 3060 या इसी तरह का 12 GB VRAM वाला) की कुल क्षमता 11.69 GiB है, लेकिन आवंटन के समय केवल 2.68 GiB खाली है। प्रक्रिया पहले से ही ~7.04 GiB (PyTorch द्वारा 6.78 GiB) का उपयोग कर रही है, जिससे मॉडल (~124M पैरामीटर × bfloat16 में 2 बाइट्स ≈ 248 MB), ऑप्टिमाइज़र स्टेट्स (AdamW के लिए ~1-2 GB), संकलन कैश, सक्रियताएं और ओवरहेड को ध्यान में रखने के बाद अपर्याप्त हेडरूम बचता है।

यह उपभोक्ता ग्रेड GPU पर बड़े बैच आकार या अनुक्रम लंबाई का उपयोग करते समय GPT-2 आकार के मॉडल (124M पैरामीटर) के लिए आम है, खासकर जब `torch.compile` सक्षम हो, जो ग्राफ कैप्चर और अनुकूलन के दौरान अस्थायी रूप से मेमोरी उपयोग बढ़ा सकता है।

### मूल कारण
1. **उच्च बैच आकार (64)**: `block_size=1024` के साथ संयुक्त, यह विशाल इंटरमीडिएट टेंसर (जैसे लॉजिट्स, अटेंशन आउटपुट) बनाता है। प्रति पुनरावृत्ति प्रभावी टोकन (65,536) VRAM सीमा को धक्का देते हैं।
2. **मॉडल संकलन**: `torch.compile` (डिफॉल्ट रूप से सक्षम) Torch Inductor का उपयोग करता है, जो अस्थायी CUDA कर्नेल और बफर जनरेट करता है। चेतावनी `[0/0] Not enough SMs to use max_autotune_gemm mode` बताती है कि आपके GPU के स्ट्रीमिंग मल्टीप्रोसेसर (SMs) आक्रामक ऑटोट्यूनिंग के लिए सीमित हैं, संभावित रूप से विखंडन बढ़ा रहे हैं।
3. **डेटा प्रकार और सटीकता**: `bfloat16` ( `torch.cuda.amp` के माध्यम से) का उपयोग, लेकिन अप्रचलित `GradScaler` चेतावनी संभावित अक्षमताओं को इंगित करती है। अन्य प्रक्रियाएं या पिछले रन ने VRAM को खंडित किया हो सकता है।
4. **मूल्यांकन ओवरहेड**: `estimate_loss()` मूल्यांकन डेटा पर फॉरवर्ड पास चलाता है (`eval_iters=200`, लेकिन बैच किया हुआ), जिससे प्रशिक्षण शुरू होने से पहले ही समस्या बढ़ जाती है।
5. **पूर्व-मौजूदा मेमोरी उपयोग**: ~7 GB पहले से ही आवंटित होना बताता है कि मॉडल, ऑप्टिमाइज़र और डेटासेट लोडर ने जगह ले ली है। गैर-PyTorch मेमोरी (प्रक्रिया द्वारा 224.90 MiB) में CUDA संदर्भ या लाइब्रेरी शामिल हो सकती हैं।

### अनुशंसित समाधान
`config/train_openwebtext.py` में सरलतम परिवर्तनों से शुरू करें (या कमांड लाइन के माध्यम से ओवरराइड करें)। प्रत्येक ट्वीक के बाद क्या काम करता है यह अलग करने के लिए पुनः चलाएँ। लक्ष्य: प्रशिक्षण गुणवत्ता को संरक्षित करते हुए पीक VRAM को ~8-9 GB तक कम करना।

#### 1. **बैच आकार कम करें (प्राथमिक समाधान)**
   - `batch_size = 4` सेट करें (या शुरुआत में 1-2 भी) ताकि लॉजिट्स बफर ~0.38 GiB (batch=4 के लिए) तक गिर जाए।
   - `gradient_accumulation_steps = 16` के साथ क्षतिपूर्ति करें (प्रभावी बैच=64, लेकिन कम पीक मेमोरी)।
   - **क्यों?** बैच आकार अधिकांश टेंसर के लिए मेमोरी के साथ रैखिक रूप से बढ़ता है। यह प्रशिक्षण को बहुत अधिक धीमे किए बिना OOM के लिए सबसे प्रभावी है।
   - अद्यतन कॉन्फ़िग स्निपेट:
     ```
     batch_size = 4
     gradient_accumulation_steps = 16  # मूल प्रभावी बैच से मेल खाने के लिए समायोजित करें
     ```
   - अपेक्षित VRAM: ~4-6 GB कुल।

#### 2. **संकलन अक्षम करें या अनुकूलित करें**
   - Torch Inductor ओवरहेड (~1-2 GB अस्थायी स्पाइक) से बचने के लिए `compile = False` जोड़कर `torch.compile` को छोड़ दें।
   - यदि संकलन रखते हैं, तो तेज़ लेकिन कम अनुकूलित कर्नेल के लिए `mode='reduce-overhead'` जोड़ें।
   - अद्यतन कॉन्फ़िग:
     ```
     compile = False
     ```
   - **विकल्प**: डिबगिंग के लिए स्क्रिप्ट में `torch._dynamo.config.suppress_errors = True` के साथ चलाएं, लेकिन पहले OOM को ठीक करें।

#### 3. **अनुक्रम लंबाई कम करें**
   - `block_size = 512` (संदर्भ को आधा) सेट करें ताकि टोकन/पुनरावृत्ति ~32,768 तक कट जाए, लॉजिट्स मेमोरी (~3.07 GiB) आधी हो जाए।
   - ट्रेड-ऑफ: छोटा संदर्भ मॉडल गुणवत्ता को थोड़ा नुकसान पहुंचा सकता है, लेकिन यह अधिक प्रशिक्षण के साथ ठीक हो सकता है।
   - कॉन्फ़िग:
     ```
     block_size = 512
     ```

#### 4. **मेमोरी प्रबंधन ट्वीक्स**
   - **विखंडन के लिए पर्यावरण चर**: त्रुटि में सुझाए अनुसार, चलाने से पहले `export PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True` सेट करें। यह PyTorch को CUDA 12+ पर विस्तार योग्य मेमोरी सेगमेंट का उपयोग करने की अनुमति देता है (आरक्षित लेकिन अप्रयुक्त ब्लॉकों से अपव्यय कम करता है)।
   - **मैन्युअल रूप से कैश साफ़ करें**: `train.py` में मॉडल आरंभ के बाद (लगभग लाइन 100 के आसपास) `torch.cuda.empty_cache()` जोड़ें, लेकिन यह एक अस्थायी समाधान है।
   - **CPU ऑफलोडिंग का उपयोग**: केवल मूल्यांकन के लिए, `estimate_loss()` को छोटे बैच या गैर-महत्वपूर्ण भागों के लिए CPU का उपयोग करने के लिए संशोधित करें, लेकिन यह चीजों को धीमा कर देता है।
   - **VRAM की निगरानी**: रीयल-टाइम में उपयोग देखने के लिए दूसरे टर्मिनल में `nvidia-smi -l 1` चलाएं।

#### 5. **अन्य कॉन्फ़िग समायोजन**
   - `eval_interval = 1000` बढ़ाएं और `eval_iters = 50` घटाएं ताकि मूल्यांकन लोड हल्का हो (शुरुआत में कम मेमोरी उथल-पुथल)।
   - यदि मल्टी-GPU सेटअप पर है, तो DDP सक्षम करें, लेकिन यह सिंगल-GPU लगता है।
   - bfloat16 समस्याओं के लिए: सुनिश्चित करें कि आपका GPU इसका समर्थन करता है (Ampere+ जैसे RTX 30-श्रृंखला)। यदि नहीं, तो स्क्रिप्ट में `dtype = 'float16'` फोर्स करें।
   - अप्रचलित चेतावनी सुधार: `train.py` लाइन 196 में, `scaler = torch.amp.GradScaler('cuda', enabled=(dtype == 'float16'))` में बदलें।

#### 6. **हार्डवेयर/सेटअप जांच**
   - **VRAM खाली करें**: अन्य GPU प्रक्रियाओं को समाप्त करें (`nvidia-smi` से PID सूचीबद्ध करें, फिर `kill -9 <PID>`)। अपना टर्मिनल/सत्र पुनरारंभ करें।
   - **डेटासेट**: सुनिश्चित करें कि `openwebtext_local` कुशलतापूर्वक लोड हो रहा है; बड़े डेटासेट DataLoader के माध्यम से RAM/VRAM में प्रीलोड हो सकते हैं।
   - **PyTorch/CUDA संस्करण**: अनुकूलता सत्यापित करें (जैसे, CUDA 12 के साथ PyTorch 2.4+)। यदि आवश्यक हो तो पुनः स्थापित करें: `pip install torch --index-url https://download.pytorch.org/whl/cu121`।
   - **CPU पर फॉलबैक**: परीक्षण के लिए, `device = 'cpu'` और `compile = False` जोड़ें, लेकिन बहुत धीमी प्रशिक्षण (GPU पर सेकंड बनाम प्रति पुनरावृत्ति घंटे) की अपेक्षा करें।

### अपेक्षित परिणाम
`batch_size=4`, `gradient_accumulation_steps=16`, और `compile=False` के साथ, आपको OOM साफ़ हो जाना चाहिए और प्रशिक्षण शुरू हो जाना चाहिए। `max_iters=20,000` के लिए कुल रनटाइम संचय के कारण लंबा होगा, लेकिन `log_interval=100` के माध्यम से हानि की निगरानी करें। यदि यह अभी भी विफल होता है, तो बैच_साइज को और कम करें और बाद में बढ़ाएं।

यदि ये समस्या का समाधान नहीं करते हैं, तो अधिक लक्षित सलाह के लिए अपना GPU मॉडल (`nvidia-smi`), PyTorch संस्करण (`torch.__version__`), और अद्यतन कॉन्फ़िग/लॉग साझा करें।