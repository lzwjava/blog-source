---
audio: false
generated: true
lang: hi
layout: post
title: डीप लर्निंग में सॉफ्टमैक्स फ़ंक्शन
translated: true
type: note
---

### सॉफ्टमैक्स क्या है?

**सॉफ्टमैक्स फ़ंक्शन** एक गणितीय फ़ंक्शन है जिसका उपयोग मशीन लर्निंग, विशेष रूप से डीप लर्निंग में, कच्चे स्कोर (जिन्हें अक्सर लॉजिट्स कहा जाता है) के वेक्टर को प्रायिकता वितरण में बदलने के लिए किया जाता है। यह सुनिश्चित करता है कि आउटपुट मान धनात्मक हों और उनका योग 1 हो, जिससे उन्हें प्रायिकताओं के रूप में व्याख्या करना संभव होता है।

सॉफ्टमैक्स फ़ंक्शन को एक वेक्टर \\( z = [z_1, z_2, \dots, z_n] \\) के लिए इस प्रकार परिभाषित किया गया है:

\\[
\text{Softmax}(z_i) = \frac{e^{z_i}}{\sum_{j=1}^n e^{z_j}}
\\]

जहाँ:
- \\( z_i \\): \\( i \\)-वें वर्ग के लिए इनपुट स्कोर (लॉजिट)।
- \\( e^{z_i} \\): इनपुट स्कोर का घातांक, जो धनात्मकता सुनिश्चित करता है।
- \\( \sum_{j=1}^n e^{z_j} \\): सभी इनपुट स्कोर के घातांकों का योग, जिसका उपयोग सामान्यीकरण के लिए किया जाता है।
- आउटपुट \\( \text{Softmax}(z_i) \\) \\( i \\)-वें वर्ग की प्रायिकता को दर्शाता है।

मुख्य गुण:
- **आउटपुट सीमा**: प्रत्येक आउटपुट मान 0 और 1 के बीच होता है।
- **योग 1**: सभी आउटपुट मानों का योग 1 के बराबर होता है, जो इसे एक वैध प्रायिकता वितरण बनाता है।
- **अंतरों को बढ़ाता है**: सॉफ्टमैक्स में घातांक फ़ंक्शन बड़े इनपुट मानों पर जोर देता है, जिससे बड़े लॉजिट्स के लिए आउटपुट प्रायिकताएँ अधिक निर्णायक हो जाती हैं।

### डीप लर्निंग में सॉफ्टमैक्स कैसे लागू होता है

सॉफ्टमैक्स फ़ंक्शन का उपयोग आमतौर पर **मल्टी-क्लास क्लासिफिकेशन** कार्यों के लिए न्यूरल नेटवर्क की **आउटपुट लेयर** में किया जाता है। यहाँ बताया गया है कि इसे कैसे लागू किया जाता है:

1. **न्यूरल नेटवर्क में संदर्भ**:
   - एक न्यूरल नेटवर्क में, अंतिम लेयर अक्सर प्रत्येक वर्ग के लिए कच्चे स्कोर (लॉजिट्स) उत्पन्न करती है। उदाहरण के लिए, 3 वर्गों (जैसे, बिल्ली, कुत्ता, चिड़िया) वाली क्लासिफिकेशन समस्या में, नेटवर्क \\([2.0, 1.0, 0.5]\\) जैसे लॉजिट्स आउटपुट कर सकता है।
   - ये लॉजिट्स सीधे तौर पर प्रायिकताओं के रूप में व्याख्या योग्य नहीं होते क्योंकि वे ऋणात्मक, असीमित हो सकते हैं और उनका योग 1 नहीं होता।

2. **सॉफ्टमैक्स की भूमिका**:
   - सॉफ्टमैक्स फ़ंक्शन इन लॉजिट्स को प्रायिकताओं में बदल देता है। ऊपर दिए गए उदाहरण के लिए:
     \\[
     \text{Softmax}([2.0, 1.0, 0.5]) = \left[ \frac{e^{2.0}}{e^{2.0} + e^{1.0} + e^{0.5}}, \frac{e^{1.0}}{e^{2.0} + e^{1.0} + e^{0.5}}, \frac{e^{0.5}}{e^{2.0} + e^{1.0} + e^{0.5}} \right]
     \\]
     इसके परिणामस्वरूप \\([0.665, 0.245, 0.090]\\) जैसी प्रायिकताएँ प्राप्त हो सकती हैं, जो वर्ग 1 (बिल्ली) के लिए 66.5%, वर्ग 2 (कुत्ता) के लिए 24.5% और वर्ग 3 (चिड़िया) के लिए 9.0% की संभावना दर्शाती हैं।

3. **अनुप्रयोग**:
   - **मल्टी-क्लास क्लासिफिकेशन**: सॉफ्टमैक्स का उपयोग इमेज क्लासिफिकेशन (जैसे, छवियों में वस्तुओं की पहचान करना), नेचुरल लैंग्वेज प्रोसेसिंग (जैसे, कई श्रेणियों वाली सेंटीमेंट एनालिसिस) या किसी भी ऐसी समस्या में किया जाता है जहां किसी इनपुट को कई वर्गों में से एक को निर्दिष्ट करना होता है।
   - **लॉस कैलकुलेशन**: सॉफ्टमैक्स को आमतौर पर **क्रॉस-एंट्रॉपी लॉस** फ़ंक्शन के साथ जोड़ा जाता है, जो भविष्यित प्रायिकता वितरण और वास्तविक वितरण (वन-हॉट एन्कोडेड लेबल) के बीच के अंतर को मापता है। यह लॉस न्यूरल नेटवर्क के प्रशिक्षण को मार्गदर्शन प्रदान करता है।
   - **निर्णय-निर्माण**: आउटपुट प्रायिकताओं का उपयोग सबसे संभावित वर्ग का चयन करने के लिए किया जा सकता है (जैसे, सबसे अधिक प्रायिकता वाले वर्ग को लेकर)।

4. **डीप लर्निंग में उदाहरण**:
   - **इमेज क्लासिफिकेशन**: ResNet जैसे कन्व्होल्यूशनल न्यूरल नेटवर्क (CNN) में, अंतिम फुली कनेक्टेड लेयर प्रत्येक वर्ग (जैसे, ImageNet में 1000 वर्ग) के लिए लॉजिट्स उत्पन्न करती है। सॉफ्टमैक्स इन्हें प्रायिकताओं में बदलकर किसी छवि में वस्तु की भविष्यवाणी करता है।
   - **नेचुरल लैंग्वेज प्रोसेसिंग**: ट्रांसफॉर्मर (जैसे, BERT) जैसे मॉडलों में, सॉफ्टमैक्स का उपयोग टेक्स्ट क्लासिफिकेशन या नेक्स्ट-वर्ड प्रेडिक्शन जैसे कार्यों के लिए आउटपुट लेयर में किया जाता है, जहां शब्दावली या वर्गों के सेट पर प्रायिकताओं की आवश्यकता होती है।
   - **रिइन्फोर्समेंट लर्निंग**: सॉफ्टमैक्स का उपयोग पॉलिसी-आधारित विधि में कार्यों का चयन करने के लिए एक्शन स्कोर को प्रायिकताओं में बदलने के लिए किया जा सकता है।

5. **फ्रेमवर्क्स में इम्प्लीमेंटेशन**:
   - **PyTorch** या **TensorFlow** जैसे फ्रेमवर्क्स में, सॉफ्टमैक्स को अक्सर एक बिल्ट-इन फ़ंक्शन के रूप में लागू किया जाता है:
     - PyTorch: `torch.nn.Softmax(dim=1)` या `torch.nn.functional.softmax()`
     - TensorFlow: `tf.nn.softmax()`
   - कई फ्रेमवर्क्स संख्यात्मक स्थिरता के लिए सॉफ्टमैक्स और क्रॉस-एंट्रॉपी लॉस को एक सिंगल ऑपरेशन में जोड़ते हैं (जैसे, PyTorch में `torch.nn.CrossEntropyLoss`), क्योंकि सॉफ्टमैक्स को अलग से कंप्यूट करने से बड़े लॉजिट्स के साथ ओवरफ्लो जैसी समस्याएं हो सकती हैं।

### प्रैक्टिकल विचार
- **संख्यात्मक स्थिरता**: सॉफ्टमैक्स की सीधी गणना घातांक फ़ंक्शन के कारण ओवरफ्लो का कारण बन सकती है। एक आम तरकीब सॉफ्टमैक्स लागू करने से पहले सभी लॉजिट्स से अधिकतम लॉजिट मान घटाना (\\( z_i - \max(z) \\)) है, जो आउटपुट को नहीं बदलता लेकिन बड़े घातांकों को रोकता है।
- **सॉफ्टमैक्स बनाम सिग्मॉइड**: **बाइनरी क्लासिफिकेशन** के लिए, सिग्मॉइड फ़ंक्शन का उपयोग अक्सर सॉफ्टमैक्स के बजाय किया जाता है, क्योंकि यह दो वर्गों को अधिक कुशलता से संभालता है। सॉफ्टमैक्स सिग्मॉइड को कई वर्गों तक सामान्यीकृत करता है।
- **सीमाएँ**:
  - सॉफ्टमैक्स परस्पर अनन्यता मानता है (एक सही वर्ग)। मल्टी-लेबल क्लासिफिकेशन (जहां कई वर्ग सही हो सकते हैं) के लिए, सिग्मॉइड को प्राथमिकता दी जाती है।
  - सॉफ्टमैक्स घातांक फ़ंक्शन के कारण भविष्यवाणियों में अत्यधिक आत्मविश्वासी हो सकता है, जो लॉजिट्स में छोटे अंतरों को बढ़ा-चढ़ाकर पेश कर सकता है।

### उदाहरण गणना
मान लीजिए एक न्यूरल नेटवर्क 3-वर्ग की समस्या के लिए लॉजिट्स \\([1.5, 0.8, -0.2]\\) आउटपुट करता है:
1. घातांकों की गणना करें: \\( e^{1.5} \approx 4.482, e^{0.8} \approx 2.225, e^{-0.2} \approx 0.819 \\).
2. घातांकों का योग करें: \\( 4.482 + 2.225 + 0.819 = 7.526 \\).
3. प्रायिकताओं की गणना करें:
   - वर्ग 1: \\( \frac{4.482}{7.526} \approx 0.596 \\)
   - वर्ग 2: \\( \frac{2.225}{7.526} \approx 0.296 \\)
   - वर्ग 3: \\( \frac{0.819}{7.526} \approx 0.109 \\)
4. आउटपुट: \\([0.596, 0.296, 0.109]\\), एक वैध प्रायिकता वितरण।

### सॉफ्टमैक्स का विज़ुअलाइज़ेशन
यह दर्शाने के लिए कि सॉफ्टमैक्स लॉजिट्स को प्रायिकताओं में कैसे बदलता है, लॉजिट्स और उनकी संबंधित सॉफ्टमैक्स प्रायिकताओं को दिखाता हुआ निम्नलिखित चार्ट देखें:

```chartjs
{
  "type": "bar",
  "data": {
    "labels": ["Class 1", "Class 2", "Class 3"],
    "datasets": [
      {
        "label": "Logits",
        "data": [1.5, 0.8, -0.2],
        "backgroundColor": "rgba(75, 192, 192, 0.6)",
        "borderColor": "rgba(75, 192, 192, 1)",
        "borderWidth": 1
      },
      {
        "label": "Softmax Probabilities",
        "data": [0.596, 0.296, 0.109],
        "backgroundColor": "rgba(255, 99, 132, 0.6)",
        "borderColor": "rgba(255, 99, 132, 1)",
        "borderWidth": 1
      }
    ]
  },
  "options": {
    "scales": {
      "y": {
        "beginAtZero": true,
        "title": {
          "display": true,
          "text": "Value"
        }
      },
      "x": {
        "title": {
          "display": true,
          "text": "Classes"
        }
      }
    },
    "plugins": {
      "legend": {
        "display": true
      },
      "title": {
        "display": true,
        "text": "Logits vs. Softmax Probabilities"
      }
    }
  }
}
```

यह चार्ट कच्चे लॉजिट्स की तुलना सॉफ्टमैक्स लागू करने के बाद की प्रायिकताओं से करता है, और हाइलाइट करता है कि कैसे सॉफ्टमैक्स मानों को एक प्रायिकता वितरण में सामान्यीकृत करता है।

### सारांश
सॉफ्टमैक्स फ़ंक्शन डीप लर्निंग में मल्टी-क्लास क्लासिफिकेशन का एक आधारशिला है, जो कच्चे स्कोर को प्रायिकता वितरण में बदलता है। इसका व्यापक रूप से इमेज और टेक्स्ट क्लासिफिकेशन जैसे कार्यों के लिए न्यूरल नेटवर्क की आउटपुट लेयर में उपयोग किया जाता है, जो मॉडलों को कई वर्गों को प्रायिकताएँ निर्दिष्ट करने और क्रॉस-एंट्रॉपी जैसे लॉस फ़ंक्शन के साथ प्रशिक्षण को सुगम बनाने में सक्षम बनाता है। इसकी घातांकीय प्रकृति इसे लॉजिट्स में अंतरों के प्रति संवेदनशील बनाती है, और सावधानीपूर्वक इम्प्लीमेंटेशन संख्यात्मक स्थिरता सुनिश्चित करता है।