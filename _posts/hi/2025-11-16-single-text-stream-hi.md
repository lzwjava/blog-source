---
audio: false
generated: true
image: false
lang: hi
layout: post
model: x-ai/grok-4-fast
title: एकल संयुक्त पाठ स्ट्रीम पर प्रशिक्षण
translated: true
type: note
---

nanoGPT में (और अनिवार्य रूप से किसी भी GPT-शैली के प्रशिक्षण में, जिसमें मूल GPT-2/GPT-3 प्रशिक्षण भी शामिल है), **आपको एक ही विशाल संयुक्त पाठ स्ट्रीम पर प्रशिक्षण देना चाहिए**, न कि अलग-अलग अनुक्रमों के रूप में एक-एक करके दिए गए व्यक्तिगत लेखों/दस्तावेजों पर।

यहाँ कारण बताया गया है और व्यवहार में यह FineWeb (और Andrej के nanoGPT कोड में) के साथ कैसे किया जाता है:

### आपको क्या करना चाहिए (सही तरीका)
1. **सभी** व्यक्तिगत लेखों को **एक बहुत बड़ी .txt फ़ाइल** (या एकाधिक बड़ी शार्ड फ़ाइलों) में जोड़ें, जो केवल एक विशेष end-of-text टोकन `<|endoftext|>` (या आपके द्वारा चुने गए किसी भी विभाजक) द्वारा अलग किए गए हों।
2. प्रशिक्षण के दौरान, nanoGPT यह करेगा:
   - पूरे संयुक्त पाठ को एक विशाल टोकन सरणी में लोड करेगा।
   - इसे निश्चित-लंबाई वाले ब्लॉकों (जैसे, block_size = 1024 टोकन) में विभाजित करेगा।
   - ये ब्लॉक अक्सर दस्तावेज़ सीमाओं को पार करते हैं — यह न केवल ठीक है, बल्कि वांछनीय है।

यह ठीक वैसा ही है जैसे OpenAI ने GPT-2 और GPT-3 का प्रशिक्षण दिया था, और यही Andrej शेक्सपियर, OpenWebText, FineWeb-Edu, आदि के साथ करते हैं।

### extract_fineweb.py स्क्रिप्ट क्या कर रही है
आपके द्वारा चलाई गई वह स्क्रिप्ट पहले से ही सही काम कर रही है:

```bash
$ python ./scripts/train/extract_fineweb.py
```

यह सभी parquet फ़ाइलों को पढ़ती है, प्रत्येक पंक्ति (यानी, प्रत्येक वेबपेज/लेख) के केवल `text` कॉलम को निकालती है, कच्चे पाठ को एक के बाद एक `fineweb_extracted.txt` में लिखती है, और (नवीनतम संस्करणों में) दस्तावेजों के बीच स्वचालित रूप से `<|endoftext|>` विभाजक डालती है।

तो परिणामी फ़ाइल इस तरह दिखती है:

```
<लेख 1 का पाठ> <|endoftext|> <लेख 2 का पाठ> <|endoftext|> <लेख 3 का पाठ> <|endoftext|> ...
```

यह एकदम सही है। आपको उसी एक विशाल फ़ाइल (या शार्डेड bin फ़ाइलों जिन्हें `train.py` इससे बनाएगा) पर प्रशिक्षण देना चाहिए।

### आपको क्या नहीं करना चाहिए
- मॉडल को प्रत्येक लेख को एक अलग उदाहरण/अनुक्रम के रूप में न दें और हर लेख के लिए attention को पुनः आरंभ न करें।
- प्रत्येक लेख को बिल्कुल block_size पर छोटा न करें या उसके अनुरूप न बनाएँ और केवल दस्तावेज़-भीतर प्रशिक्षण न दें।
- दस्तावेज़-स्तरीय बैचिंग का उपयोग न करें जहाँ बैच में प्रत्येक अनुक्रम एक पूर्ण दस्तावेज़ हो।

ऐसा करने से मॉडल की बहु-दस्तावेजों तक फैली लंबी-सीमा वाली निर्भरताएँ सीखने की क्षमता नष्ट हो जाएगी और प्रदर्शन बुरी तरह प्रभावित होगा।

### सारांश
हाँ → **एक ही विशाल संयुक्त पाठ फ़ाइल** पर प्रशिक्षण दें (ठीक वैसा ही जैसा `extract_fineweb.py` तैयार करता है)।  
नहीं → लेख-दर-लेख को अलग-अलग अनुक्रमों के रूप में प्रशिक्षण न दें।

GPTs का प्रशिक्षण हमेशा से इसी तरह दिया जाता रहा है, और यही कारण है कि nanoGPT का `train.py` डेटा को एक लंबी टोकन स्ट्रीम के रूप में मानता है।

तो बस सामान्य nanoGPT वर्कफ़्लो जारी रखें:

```bash
python scripts/train/extract_fineweb.py   # आप यह पहले ही कर चुके हैं
python train.py --dataset=your_extracted_file_or_fineweb10B आदि।
```

आप पूरी तरह से तैयार हैं!