---
audio: false
generated: true
image: false
lang: hi
layout: post
model: x-ai/grok-4-fast
title: कार्पाथी का नैनोजीपीटी और नैनोचैट
translated: true
type: note
---

### अवलोकन
nanoGPT और nanochat दोनों ही आंद्रेज कार्पैथी द्वारा बनाए गए ओपन-सोर्स प्रोजेक्ट हैं, जिन्हें बड़े भाषा मॉडल (LLM) प्रशिक्षण को सुलभ और शैक्षिक बनाने के लिए डिज़ाइन किया गया है। nanoGPT (2023 में जारी) रॉ टेक्स्ट डेटा से GPT-शैली के मॉडल को स्क्रैच से प्रशिक्षित करने की मूल बातों पर केंद्रित है, जबकि nanochat (अक्टूबर 2025 में जारी) इस पर एक और अधिक व्यापक "फुल-स्टैक" पाइपलाइन के रूप में निर्मित होता है जो ChatGPT जैसा चैटबॉट बनाने के लिए होता है। मुख्य अंतर दायरे, प्रशिक्षण के चरणों, कोडबेस की जटिलता और एंड-टू-एंड उपयोगिता में निहित हैं—nanochat अनिवार्य रूप से वार्तालाप AI के लिए nanoGPT को एक संपूर्ण प्रोडक्शन-जैसी प्रणाली में विकसित करता है।

### प्रशिक्षण कोड में मुख्य अंतर
nanochat में प्रशिक्षण कोड, nanoGPT के दृष्टिकोण का एक विस्तार और परिष्करण है, लेकिन इसमें चैट एप्लिकेशन के लिए तैयार किए गए अतिरिक्त चरण, ऑप्टिमाइज़ेशन और एकीकरण शामिल हैं। यहां एक विवरण दिया गया है:

| पहलू                     | nanoGPT                                                                 | nanochat                                                                 |
|-------------------------|-------------------------------------------------------------------------|--------------------------------------------------------------------------|
| **प्राथमिक फोकस**      | रॉ टेक्स्ट डेटा (जैसे, OpenWebText या शेक्सपियर) पर ट्रांसफॉर्मर-आधारित GPT मॉडल का प्री-ट्रेनिंग। टोकननाइजेशन, मॉडल आर्किटेक्चर, और बुनियादी प्रशिक्षण लूप जैसी मूल अवधारणाएं सिखाता है। | फुल पाइपलाइन: प्री-ट्रेनिंग + मिड-ट्रेनिंग (वार्तालाप/बहुविकल्पीय) + सुपरवाइज्ड फाइन-ट्यूनिंग (SFT) + वैकल्पिक रीइन्फोर्समेंट लर्निंग (RLHF via GRPO) + मूल्यांकन + अनुमान। एक डिप्लॉय करने योग्य चैटबॉट बनाता है। |
| **प्रशिक्षण चरण**    | - सिंगल-स्टेज प्री-ट्रेनिंग.<br>- बुनियादी मूल्यांकन (जैसे, perplexity). | - **प्री-ट्रेन**: nanoGPT के समान लेकिन FineWeb डेटासेट पर।<br>- **मिड-ट्रेन**: SmolTalk (उपयोगकर्ता-सहायक संवाद), बहुविकल्पीय प्रश्नोत्तरी, और टूल-यूज़ डेटा पर।<br>- **SFT**: चैट अलाइनमेंट के लिए फाइन-ट्यून, MMLU, ARC-E/C, GSM8K (गणित), HumanEval (कोड) जैसे बेंचमार्क पर मूल्यांकन।<br>- **RL**: प्राथमिकता अलाइनमेंट के लिए GSM8K पर वैकल्पिक RLHF।<br>- मेट्रिक्स (जैसे, CORE स्कोर) के साथ स्वचालित रिपोर्ट कार्ड जनरेशन। |
| **कोडबेस आकार और संरचना** | कुल ~600 लाइनें (जैसे, `train.py` ~300 लाइनें, `model.py` ~300 लाइनें)। न्यूनतम, हैक करने योग्य PyTorch; पूर्णता की तुलना में सरलता को प्राथमिकता देता है। nanochat के पक्ष में अप्रचलित। | साफ, मॉड्यूलर PyTorch कोड की ~8,000 लाइनें। इसमें Rust-आधारित टोकननाइज़र, कुशल इंफरेंस इंजन (KV कैश, प्रीफिल/डिकोड), टूल इंटीग्रेशन (जैसे, Python सैंडबॉक्स), और वेब UI शामिल है। अधिक सुसंगत लेकिन फिर भी फोर्क करने योग्य। |
| **ऑप्टिमाइज़र और हाइपरपैरामीटर्स** | स्टैंडर्ड AdamW; मध्यम आकार के मॉडल (जैसे, GPT-2 124M पैरामीटर्स) के लिए ट्यून किए गए लर्निंग रेट। | Muon + AdamW हाइब्रिड (modded-nanoGPT से प्रेरित); अनुकूली लर्निंग रेट (जैसे, ओवरफिटिंग से बचने के लिए छोटे डेटासेट के लिए कम)। मॉडल आकार के लिए `--depth` फ्लैग के माध्यम से स्केल करता है। |
| **डेटा हैंडलिंग**      | रॉ टेक्स्ट कॉर्पोरा; बेसिक BPE टोकननाइज़र प्रशिक्षण। | एन्हांस्ड: कस्टम टोकननाइज़र प्रशिक्षित करें (वोकैब साइज ~65K); Hugging Face डेटासेट का उपयोग करता है (प्री-ट्रेन के लिए FineWeb, वार्तालाप के लिए SmolTalk)। व्यक्तित्व इन्फ्यूजन के लिए सिंथेटिक डेटा को सपोर्ट करता है। |
| **प्रशिक्षण समय और लागत** | GPT-2 समकक्ष के लिए 8xA100 पर ~4 दिन (~$500+)। शैक्षिक रन पर केंद्रित। | एक बेसिक 560M-पैरामीटर मॉडल के लिए 8xH100 पर ~4 घंटे (~$100); GPT-2 को पार करने में ~12 घंटे; मजबूत मॉडल (जैसे, 24 घंटे के बाद 40% MMLU) के लिए ~$1,000 तक स्केल करता है। |
| **इंफरेंस और डिप्लॉयमेंट** | बेसिक टेक्स्ट जनरेशन; कोई बिल्ट-इन चैट या UI नहीं। | KV कैश के साथ ऑप्टिमाइज़्ड इंजन; CLI चैट; ChatGPT-शैली वेब UI; टूल यूज़ (जैसे, कोड एक्सेक्यूशन)। मॉडल छोटे स्केल के लिए "नाइव/मूर्ख" लेकिन मनोरंजक होते हैं। |
| **शैक्षिक लक्ष्य**   | एक भाषा मॉडल की "आंतरिक संरचना" (दिमाग) बनाना। आर्किटेक्चर के साथ प्रयोग करने वाले शुरुआती लोगों के लिए बढ़िया। | एक पूर्ण ChatGPT क्लोन (दिमाग से इंटरफेस तक) बनाना। LLM101n जैसे उन्नत पाठ्यक्रमों के लिए कैपस्टोन; संभावित शोध बेंचमार्क। |

### कब किसका उपयोग करें
- **nanoGPT चुनें** यदि आप LLM में नए हैं और प्री-ट्रेनिंग का एक हल्का-फुल्का परिचय चाहते हैं (जैसे, आर्किटेक्चर या डेटासेट में बदलाव करना)। यह सरल है लेकिन गैर-वार्तालाप मॉडल तक सीमित है।
- **एंड-टू-एंड चैटबॉट डेवलपमेंट के लिए nanochat चुनें।** यह चैट कार्यों पर nanoGPT से बेहतर प्रदर्शन करता है (जैसे, कुछ RLHF परीक्षणों में 58% सुधार) और डेटा से लेकर काम करने वाले UI तक जाने के लिए आवश्यक हर चीज एक ही रेपो में शामिल करता है।

दोनों ही हैक करने की क्षमता पर जोर देते हैं—उदाहरण के लिए, nanochat आसान स्केलिंग (मॉडल की गहराई बदलें) या कस्टमाइजेशन (व्यक्तित्व के लिए सिंथेटिक डेटा जोड़ें) की अनुमति देता है। nanochat मॉडल को आत्म-जागरूकता भी देता है (उदाहरण के लिए, यह अपने आकार और सीमाओं को जानता है)।

हाथों-हाथ अन्वेषण के लिए:
- [nanoGitHub](https://github.com/karpathy/nanoGPT)
- [nanochat GitHub](https://github.com/karpathy/nanochat)

यदि आप इन्हें चला रहे हैं, तो पूर्ण अनुभव के लिए Lambda Labs जैसे क्लाउड GPU पर nanochat के `speedrun.sh` स्क्रिप्ट से शुरुआत करें।