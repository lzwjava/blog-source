---
audio: false
generated: true
lang: hi
layout: post
title: मानव प्रतिपुष्टि सुदृढ़ीकरण शिक्षण (आरएलएचएफ) समझाया गया
translated: true
type: note
---

**रिइन्फोर्समेंट लर्निंग फ्रॉम ह्यूमन फीडबैक (RLHF)** एक मशीन लर्निंग तकनीक है जिसका उपयोग AI मॉडल्स, विशेष रूप से बड़े लैंग्वेज मॉडल्स (LLMs), को मानवीय प्राथमिकताओं और निर्देशों के साथ बेहतर तालमेल बिठाने के लिए फाइन-ट्यून करने में किया जाता है। केवल पूर्वनिर्धारित रिवार्ड फंक्शन पर निर्भर रहने के बजाय, RLHF सीखने की प्रक्रिया को निर्देशित करने के लिए मनुष्यों से सीधे फीडबैक को शामिल करता है।

**RLHF क्यों महत्वपूर्ण है?**

*   **व्यक्तिपरक कार्य:** RLHF उन कार्यों में उत्कृष्ट प्रदर्शन करता है जहां वांछित परिणाम को स्पष्ट नियमों या संख्यात्मक पुरस्कारों के साथ परिभाषित करना मुश्किल होता है, जैसे रचनात्मक पाठ उत्पन्न करना, प्राकृतिक बातचीत में शामिल होना, या सहायक और हानिरहित सामग्री का उत्पादन करना।
*   **सूक्ष्मता और संरेखण:** यह AI मॉडल्स को मानवीय प्राथमिकताओं, नैतिक विचारों और वांछित इंटरैक्शन शैलियों को समझने और उनका पालन करने में मदद करता है।
*   **बेहतर प्रदर्शन:** RLHF के साथ प्रशिक्षित मॉडल अक्सर पारंपरिक रिइन्फोर्समेंट लर्निंग या सुपरवाइज्ड लर्निंग से केवल प्रशिक्षित मॉडलों की तुलना में काफी बेहतर प्रदर्शन और उपयोगकर्ता संतुष्टि प्रदर्शित करते हैं।

**RLHF कैसे काम करता है (आमतौर पर तीन चरणों में):**

1.  **प्री-ट्रेनिंग और सुपरवाइज्ड फाइन-ट्यूनिंग (SFT):**
    *   एक बेस लैंग्वेज मॉडल को पहले टेक्स्ट और कोड के एक विशाल डेटासेट पर प्री-ट्रेन किया जाता है ताकि वह सामान्य भाषा की समझ और जनरेशन सीख सके।
    *   इस प्री-ट्रेंड मॉडल को फिर अक्सर सुपरवाइज्ड लर्निंग का उपयोग करके वांछित व्यवहार के उच्च-गुणवत्ता वाले डेमोंस्ट्रेशन (जैसे, प्रॉम्प्ट के लिए आदर्श प्रतिक्रियाएं लिखते हुए मनुष्य) के एक छोटे डेटासेट पर फाइन-ट्यून किया जाता है। यह कदम मॉडल को अपेक्षित आउटपुट के प्रारूप और शैली को समझने में मदद करता है।

2.  **रिवार्ड मॉडल ट्रेनिंग:**
    *   यह RLHF में एक महत्वपूर्ण कदम है। एक अलग **रिवार्ड मॉडल** को मानवीय प्राथमिकताओं की भविष्यवाणी करने के लिए प्रशिक्षित किया जाता है।
    *   मानव एनोटेटर्स को एक ही इनपुट प्रॉम्प्ट के लिए SFT मॉडल (या बाद के संस्करण) के विभिन्न आउटपुट प्रस्तुत किए जाते हैं। वे विभिन्न मानदंडों (जैसे, सहायता, सुसंगतता, सुरक्षा) के आधार पर इन आउटपुट को रैंक या रेट करते हैं।
    *   इस प्राथमिकता डेटा (जैसे, "आउटपुट A, आउटपुट B से बेहतर है") का उपयोग रिवार्ड मॉडल को प्रशिक्षित करने के लिए किया जाता है। रिवार्ड मॉडल सीखता है कि किसी दिए गए मॉडल आउटपुट को एक स्केलर रिवार्ड स्कोर कैसे देना है, जो दर्शाता है कि एक मनुष्य उसे कितना पसंद करेगा।

3.  **रिइन्फोर्समेंट लर्निंग फाइन-ट्यूनिंग:**
    *   मूल लैंग्वेज मॉडल (SFT मॉडल से आरंभ किया गया) को रिइन्फोर्समेंट लर्निंग का उपयोग करके आगे फाइन-ट्यून किया जाता है।
    *   पिछले चरण में प्रशिक्षित रिवार्ड मॉडल पर्यावरण के रिवार्ड फंक्शन के रूप में कार्य करता है।
    *   RL एजेंट (लैंग्वेज मॉडल) प्रॉम्प्ट के लिए प्रतिक्रियाएं उत्पन्न करता है, और रिवार्ड मॉडल इन प्रतिक्रियाओं को स्कोर करता है।
    *   RL एल्गोरिदम (अक्सर Proximal Policy Optimization - PPO) लैंग्वेज मॉडल की पॉलिसी (यह कैसे टेक्स्ट जनरेट करता है) को रिवार्ड मॉडल द्वारा भविष्यवाणी किए गए पुरस्कारों को अधिकतम करने के लिए अपडेट करता है। यह लैंग्वेज मॉडल को उन आउटपुट को उत्पन्न करने के लिए प्रोत्साहित करता है जो मानवीय प्राथमिकताओं के साथ अधिक संरेखित होते हैं।
    *   RL फाइन-ट्यूनिंग को SFT मॉडल के व्यवहार से बहुत दूर भटकने से रोकने के लिए (जिसके अवांछित परिणाम हो सकते हैं), RL उद्देश्य में अक्सर एक नियमितीकरण शब्द (जैसे, KL divergence penalty) शामिल किया जाता है।

**RLHF कैसे करें (सरलीकृत चरण):**

1.  **मानव प्राथमिकता डेटा एकत्र करें:**
    *   अपने वांछित AI व्यवहार से संबंधित प्रॉम्प्ट या कार्य डिजाइन करें।
    *   अपने मौजूदा मॉडल का उपयोग करके इन प्रॉम्प्ट के लिए कई प्रतिक्रियाएं उत्पन्न करें।
    *   मानव एनोटेटर्स को इन प्रतिक्रियाओं की तुलना करने और उनकी प्राथमिकताएं (जैसे, उन्हें रैंक करना, सर्वश्रेष्ठ चुनना, या उन्हें रेट करना) बताने के लिए कहें।
    *   इस डेटा को (प्रॉम्प्ट, पसंदीदा प्रतिक्रिया, कम पसंदीदा प्रतिक्रिया) के जोड़े या इसी तरह के फॉर्मेट में स्टोर करें।

2.  **एक रिवार्ड मॉडल को प्रशिक्षित करें:**
    *   अपने रिवार्ड मॉडल के लिए एक उपयुक्त मॉडल आर्किटेक्चर चुनें (अक्सर लैंग्वेज मॉडल के समान एक ट्रांसफॉर्मर-आधारित मॉडल)।
    *   एकत्र किए गए मानव प्राथमिकता डेटा पर रिवार्ड मॉडल को प्रशिक्षित करें। लक्ष्य यह है कि रिवार्ड मॉडल उन प्रतिक्रियाओं को उच्च स्कोर दे जिन्हें मनुष्यों ने पसंद किया। उपयोग किया जाने वाला एक सामान्य लॉस फंक्शन पसंदीदा और कम पसंदीदा प्रतिक्रियाओं के स्कोर के बीच के अंतर को अधिकतम करने पर आधारित होता है।

3.  **रिइन्फोर्समेंट लर्निंग के साथ लैंग्वेज मॉडल को फाइन-ट्यून करें:**
    *   अपने लैंग्वेज मॉडल को SFT चरण (यदि आपने एक किया है) से वेट के साथ इनिशियलाइज़ करें।
    *   एक रिइन्फोर्समेंट लर्निंग एल्गोरिदम (जैसे PPO) का उपयोग करें।
    *   प्रत्येक ट्रेनिंग स्टेप के लिए:
        *   एक प्रॉम्प्ट का नमूना लें।
        *   लैंग्वेज मॉडल को एक प्रतिक्रिया उत्पन्न करने दें।
        *   उत्पन्न प्रतिक्रिया के लिए रिवार्ड स्कोर प्राप्त करने के लिए प्रशिक्षित रिवार्ड मॉडल का उपयोग करें।
        *   उच्च पुरस्कारों की ओर ले जाने वाली कार्रवाइयों (टोकन जनरेशन) को प्रोत्साहित करने के लिए रिवार्ड सिग्नल के आधार पर लैंग्वेज मॉडल के पैरामीटर्स को अपडेट करें।
        *   अपडेट की गई पॉलिसी को SFT पॉलिसी के करीब रखने के लिए एक नियमितीकरण शब्द (जैसे, KL divergence) शामिल करें।

**कोड उदाहरण (कॉन्सेप्चुअल और सरलीकृत PyTorch का उपयोग करके):**

यह कोर आइडिया को समझाने के लिए एक अत्यधिक सरलीकृत कॉन्सेप्चुअल उदाहरण है। एक पूर्ण RLHF इम्प्लीमेंटेशन काफी अधिक जटिल है और इसमें Hugging Face Transformers, Accelerate, और RL लाइब्रेरीज़ जैसी लाइब्रेरीज़ शामिल होती हैं।

```python
import torch
import torch.nn as nn
import torch.optim as optim
from transformers import AutoModelForCausalLM, AutoTokenizer

# मान लें कि आपने मानव प्राथमिकता डेटा एकत्र कर लिया है:
# टुपल्स की सूची: (prompt, preferred_response, less_preferred_response)
preference_data = [
    ("Write a short story about a cat.", "Whiskers the cat lived in a cozy cottage...", "A cat story. The end."),
    ("Summarize this article:", "The article discusses...", "Article summary."),
    # ... अधिक डेटा
]

# 1. प्री-ट्रेंड लैंग्वेज मॉडल और टोकनाइज़र लोड करें
model_name = "gpt2"  # या कोई अन्य उपयुक्त प्री-ट्रेंड मॉडल
policy_model = AutoModelForCausalLM.from_pretrained(model_name)
tokenizer = AutoTokenizer.from_pretrained(model_name)
device = "cuda" if torch.cuda.is_available() else "cpu"
policy_model.to(device)

# 2. एक सरल रिवार्ड मॉडल को परिभाषित करें
class RewardModel(nn.Module):
    def __init__(self, base_model):
        super().__init__()
        self.base_model = base_model.transformer  # ट्रांसफॉर्मर लेयर्स का उपयोग करें
        self.v_head = nn.Linear(base_model.config.n_embd, 1)

    def forward(self, input_ids, attention_mask):
        outputs = self.base_model(input_ids=input_ids, attention_mask=attention_mask)
        last_hidden_states = outputs.last_hidden_state
        reward = self.v_head(last_hidden_states[:, -1])  # अंतिम टोकन से रिवार्ड प्राप्त करें
        return reward

reward_model = RewardModel(policy_model)
reward_model.to(device)
reward_optimizer = optim.AdamW(reward_model.parameters(), lr=1e-5)
reward_criterion = nn.MarginRankingLoss(margin=1.0) # पसंदीदा के लिए उच्च रिवार्ड को प्रोत्साहित करें

# रिवार्ड मॉडल को प्रशिक्षित करें
num_reward_epochs = 3
for epoch in range(num_reward_epochs):
    for prompt, preferred, less_preferred in preference_data:
        preferred_tokens = tokenizer(prompt + preferred, return_tensors="pt", truncation=True, max_length=128).to(device)
        less_preferred_tokens = tokenizer(prompt + less_preferred, return_tensors="pt", truncation=True, max_length=128).to(device)

        preferred_reward = reward_model(**preferred_tokens)
        less_preferred_reward = reward_model(**less_preferred_tokens)

        labels = torch.ones(preferred_reward.size(0)).to(device) # हम चाहते हैं कि पसंदीदा > कम पसंदीदा
        loss = reward_criterion(preferred_reward, less_preferred_reward, labels)

        reward_optimizer.zero_grad()
        loss.backward()
        reward_optimizer.step()
    print(f"Reward Epoch {epoch+1}, Loss: {loss.item()}")

# 3. रिइन्फोर्समेंट लर्निंग फाइन-ट्यूनिंग (कॉन्सेप्चुअल - PPO जटिल है)
policy_optimizer = optim.AdamW(policy_model.parameters(), lr=5e-6)

num_rl_episodes = 5
for episode in range(num_rl_episodes):
    for prompt in [data[0] for data in preference_data]: # प्रॉम्प्ट का नमूना लें
        input_tokens = tokenizer(prompt, return_tensors="pt").to(device)
        output_sequences = policy_model.generate(
            input_tokens.input_ids,
            max_length=60,
            num_return_sequences=1,
            do_sample=True,
            top_k=50,
            top_p=0.95,
        )
        generated_response = tokenizer.decode(output_sequences[0], skip_special_tokens=True)

        response_tokens = tokenizer(prompt + generated_response, return_tensors="pt", truncation=True, max_length=128).to(device)
        reward = reward_model(**response_tokens)

        # (सरलीकृत) पॉलिसी अपडेट - वास्तव में, PPO अधिक परिष्कृत तरीकों का उपयोग करता है
        loss = -reward.mean() # रिवार्ड को अधिकतम करने का प्रयास करें
        policy_optimizer.zero_grad()
        loss.backward()
        policy_optimizer.step()

    print(f"RL Episode {episode+1}, Reward: {reward.item()}, Response: {generated_response}")

print("RLHF Training Done!")
```

**RLHF में प्रमुख चुनौतियाँ:**

*   **डेटा संग्रह:** उच्च-गुणवत्ता और विविध मानव प्राथमिकता डेटा प्राप्त करना महंगा और समय लेने वाला हो सकता है।
*   **व्यक्तिपरकता और पूर्वाग्रह:** मानवीय प्राथमिकताएं व्यक्तिपरक हो सकती हैं और इनमें पूर्वाग्रह हो सकते हैं, जिन्हें रिवार्ड मॉडल द्वारा सीखा जा सकता है।
*   **रिवार्ड हैकिंग:** RL एजेंट को रिवार्ड मॉडल का फायदा उठाने के तरीके मिल सकते हैं जो वास्तविक मानवीय प्राथमिकताओं के साथ संरेखित नहीं होते हैं।
*   **स्केलेबिलिटी:** बहुत बड़े मॉडल और डेटासेट के लिए RLHF को स्केल करने के लिए महत्वपूर्ण कम्प्यूटेशनल संसाधनों और सावधानीपूर्वक इंजीनियरिंग की आवश्यकता होती है।
*   **अस्थिरता:** RLHF मॉडलों का प्रशिक्षण अस्थिर हो सकता है और इसके लिए सावधानीपूर्वक हाइपरपैरामीटर ट्यूनिंग की आवश्यकता होती है।

इन चुनौतियों के बावजूद, RLHF AI मॉडलों को मानवीय मूल्यों के साथ संरेखित करने के लिए एक शक्तिशाली तकनीक साबित हुई है और यह ChatGPT और अन्य जैसे अत्याधुनिक लैंग्वेज मॉडल्स के विकास में एक प्रमुख घटक है।