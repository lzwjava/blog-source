---
audio: false
generated: true
image: false
lang: hi
layout: post
model: anthropic/claude-sonnet-4
title: टिकटोकन टोकनकरण गाइड और उदाहरण
translated: true
type: note
---

यह कोड **TikToken** लाइब्रेरी के उपयोग का प्रदर्शन करता है, जो OpenAI का टोकनाइज़र है और GPT-3.5, GPT-4, आदि जैसे भाषा मॉडल को भेजे जाने वाले टेक्स्ट में टोकनों को गिनने और प्रोसेस करने के लिए उपयोग किया जाता है।

## TikToken क्या करता है

TikToken टेक्स्ट को **टोकन** में परिवर्तित करता है - ये वे मूल इकाइयाँ हैं जिन्हें भाषा मॉडल प्रोसेस करते हैं। प्रत्येक टोकन मोटे तौर पर एक शब्द या शब्द के एक हिस्से के अनुरूप होता है। टोकनाइज़ेशन को समझना महत्वपूर्ण है क्योंकि:

- API लागत टोकन संख्या पर आधारित होती है
- मॉडलों के इनपुट/आउटपुट के लिए टोकन सीमा होती है
- टोकन संख्या प्रोसेसिंग गति को प्रभावित करती है

## कोड विवरण

### 1. बेसिक एन्कोडिंग (`basic_encoding()`)
```python
enc = tiktoken.encoding_for_model("gpt-3.5-turbo")
tokens = enc.encode("Hello, how are you doing today?")
```
- GPT-3.5-turbo के लिए एक टोकनाइज़र बनाता है
- टेक्स्ट को टोकन ID की सूची में परिवर्तित करता है: `[9906, 11, 1268, 527, 499, 3815, 3432, 30]`
- दर्शाता है कि "Hello, how are you doing today?" = 8 टोकन
- टोकन को वापस मूल टेक्स्ट में डीकोड कर सकता है

### 2. मॉडल तुलना (`different_models()`)
एक ही टेक्स्ट को अलग-अलग मॉडल कैसे टोकनाइज़ करते हैं, इसकी तुलना करता है:
- **GPT-4**: "The quick brown fox jumps over the lazy dog." के लिए 10 टोकन
- **GPT-3.5-turbo**: 10 टोकन (समान एन्कोडिंग)
- **text-davinci-003**: 10 टोकन (समान एन्कोडिंग)

अलग-अलग मॉडल अलग-अलग टोकनाइज़र का उपयोग कर सकते हैं, इसलिए टोकन संख्या भिन्न हो सकती है।

### 3. बैच प्रोसेसिंग (`batch_processing()`)
कई टेक्स्ट को कुशलतापूर्वक कैसे प्रोसेस करें, यह दर्शाता है:
- अलग-अलग लंबाई के 3 संदेशों को प्रोसेस करता है
- सभी संदेशों में कुल टोकन की गणना करता है (कुल 15)
- कई संदेश भेजते समय लागत का अनुमान लगाने के लिए उपयोगी

### 4. विशेष टोकन (`special_tokens()`)
`<|endoftext|>` जैसे विशेष नियंत्रण टोकन को हैंडल करता है:
- मॉडल के लिए उपलब्ध विशेष टोकन दिखाता है
- विशेष टोकन वाले टेक्स्ट को एन्कोड करने का प्रदर्शन करता है
- विशेष टोकन को प्रोसेस करने के लिए `allowed_special="all"` का उपयोग करता है

### 5. कुशल गिनती (`count_tokens_efficiently()`)
टोकन गिनने के दो तरीके:
- **तरीका 1**: टोकन स्टोर करें फिर गिनें (`len(enc.encode(text))`)
- **तरीका 2**: सीधी गिनती (बड़े टेक्स्ट के लिए अधिक मेमोरी-कुशल)
- टोकन-प्रति-शब्द अनुपात की गणना करता है (इस उदाहरण में 1.29)

## व्यावहारिक अनुप्रयोग

1. **लागत अनुमान**: अनुरोध करने से पहले API लागत की गणना करें
2. **इनपुट सत्यापन**: सुनिश्चित करें कि टेक्स्ट मॉडल टोकन सीमा के भीतर फिट बैठता है
3. **बैच ऑप्टिमाइज़ेशन**: API कॉल के लिए संदेशों को कुशलतापूर्वक समूहित करें
4. **प्रदर्शन निगरानी**: एप्लिकेशन में टोकन उपयोग पर नज़र रखें

## आउटपुट से मुख्य अंतर्दृष्टि

- सरल वाक्यांश आपकी अपेक्षा से कम टोकन का उपयोग करते हैं
- टोकन संख्या हमेशा शब्द संख्या के बराबर नहीं होती
- अलग-अलग मॉडल अलग तरह से टोकनाइज़ कर सकते हैं
- विशेष टोकन के लिए सावधानीपूर्वक हैंडलिंग की आवश्यकता होती है

यह लाइब्रेरी OpenAI की API के साथ एप्लिकेशन बनाने वाले किसी भी व्यक्ति के लिए आवश्यक है, क्योंकि यह सटीक टोकन गिनती प्रदान करती है जो इस बात से मेल खाती है कि API वास्तव में आपसे क्या शुल्क लेगा।