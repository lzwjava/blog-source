---
audio: false
generated: true
image: false
lang: hi
layout: post
model: x-ai/grok-4-fast
title: विकिपीडिया डाउनलोड गाइड
translated: true
type: note
---

यह आधिकारिक विकिमीडिया डाउनलोड मिरर (आमतौर पर https://dumps.wikimedia.org/enwiki/latest/) से एक डायरेक्टरी लिस्टिंग है।  
इसमें वर्तमान अंग्रेजी विकिपीडिया लेखों (नवंबर 2025 की शुरुआत तक) को डाउनलोड करने के सभी अलग-अलग तरीके शामिल हैं।

### मुख्य फॉर्मेट और उनमें वास्तव में क्या होता है

| फ़ाइल नाम पैटर्न | यह क्या है | अनुमानित आकार (अनकम्प्रेस्ड) | क्या यह LLM ट्रेनिंग के लिए सबसे अच्छा है? | नोट्स |
| --- | --- | --- | --- | --- |
| `enwiki-latest-pages-articles.xml.bz2` | **सभी** लेख + वार्ता पृष्ठ, टेम्पलेट्स, रीडायरेक्ट्स, आदि वाली एक विशाल फ़ाइल | ~85–90 GB अनकम्प्रेस्ड | हाँ, बहुत आमतौर पर प्रयुक्त | सबसे आसान यदि आपके पास स्थान और बैंडविड्थ है |
| `enwiki-latest-pages-articles1.xml-p1p41242.bz2`  … up to … `enwiki-latest-pages-articles27.xml-…` | वही डेटा, लेकिन 27 छोटे हिस्सों (मल्टीस्ट्रीम) में विभाजित | प्रत्येक ~200–600 MB कम्प्रेस्ड → कुल अभी भी ~85–90 GB अनकम्प्रेस्ड | हाँ, सबसे लोकप्रिय विकल्प | समानांतर में डाउनलोड करने और आसानी से फिर से शुरू करने देता है |
| `enwiki-latest-pages-articles-multistreamX.xml.bz2` (जैसे multistream27) | वास्तविक विशाल कम्प्रेस्ड डेटा फ़ाइलें जो ऊपर वाले विभाजित संस्करण से संबंधित हैं | 300–600 MB प्रत्येक कम्प्रेस्ड | ये वास्तविक डेटा फ़ाइलें हैं जो आप चाहते हैं | आपको इनकी + इंडेक्स फ़ाइलों की आवश्यकता होगी |
| `enwiki-latest-pages-articles-multistreamX.xml.bz2.md5` / `.meta` | चेकसम और छोटी मेटाडेटा फ़ाइलें | < 1 KB | टेक्स्ट के लिए आवश्यक नहीं | केवल डाउनलोड सत्यापित करने के लिए |
| `enwiki-latest-pages-articles-multistream-indexX.xml.bz2` | इंडेक्स फ़ाइलें जो बताती हैं कि कौन सा लेख बड़ी मल्टीस्ट्रीम फ़ाइलों में किस बाइट ऑफ़सेट पर है | ~30–60 MB प्रत्येक कम्प्रेस्ड | आवश्यक यदि आप मल्टीस्ट्रीम का उपयोग करते हैं | तेज़ रैंडम एक्सेस के लिए आवश्यक; अधिकांश प्रोसेसिंग स्क्रिप्ट्स इनकी अपेक्षा करती हैं |

### सिफारिश: 2025 में मॉडल ट्रेन करने के लिए लगभग हर कोई क्या उपयोग करता है

डी-फैक्टो स्टैंडर्ड तरीका (जिसका LLaMA, Mistral, Qwen, DeepSeek, Gemma, Phi, आदि द्वारा उपयोग किया जाता है) है:

**27 मल्टीस्ट्रीम फ़ाइलें + उनकी 27 इंडेक्स फ़ाइलें** डाउनलोड करें  
(यानी वह सब कुछ जो `enwiki-latest-pages-articlesXX.xml-p…p….bz2` से शुरू होता है और मेल खाती `…-indexXX…bz2` फ़ाइलें)

या और भी सरल: बस **एकल विशाल फ़ाइल** डाउनलोड करें यदि आपका कनेक्शन और डिस्क इसे हैंडल कर सकती है:

```
enwiki-latest-pages-articles.xml.bz2    ← 24.6 GB कम्प्रेस्ड, ~85–90 GB अनकम्प्रेस्ड
```

दोनों में बिल्कुल वही कच्चा XML डेटा होता है (सभी मुख्य-नामस्थान लेख + बहुत सारे अतिरिक्त पृष्ठ)।

### छोटी फ़ाइलें (multistream3.xml-p…, multistream4.xml-p… आदि) क्यों नहीं?
वे 2010–2015 की बहुत पुरानी स्प्लिट्स हैं जिनमें विकिपीडिया का एक बहुत छोटा हिस्सा ही शामिल है (वे लेख जो उस समय उस पेज-आईडी रेंज में आते थे)। उन्हें केवल ऐतिहासिक कारणों से रखा गया है और आज के समय में मूल रूप से बेकार हैं।

### क्विक स्टार्ट कमांड्स (2025 डंप)

सिफारिश की गई 27+27 फ़ाइलों का समानांतर डाउनलोड (सबसे तेज़):

```bash
# फ़ाइलों की सूची प्राप्त करें
wget -q https://dumps.wikimedia.org/enwiki/latest/ -O - | \
  grep -E 'enwiki-latest-pages-articles[0-9]+[02468]\.xml-p[0-9]+p[0-9]+\.bz2"' | \
  sed -e 's/.*href="//' -e 's/".*//' | \
  xargs -n1 -P10 wget https://dumps.wikimedia.org/enwiki/latest/

# मेल खाती इंडेक्स फ़ाइलें भी डाउनलोड करें
wget -q https://dumps.wikimedia.org/enwiki/latest/ -O - | \
  grep -E 'enwiki-latest-pages-articles-multistream-index' | \
  sed -e 's/.*href="//' -e 's/".*//' | \
  xargs -n1 -P10 wget https://dumps.wikimedia.org/enwiki/latest/
```

या बस एकल फ़ाइल (सबसे सरल):

```bash
wget https://dumps.wikimedia.org/enwiki/latest/enwiki-latest-pages-articles.xml.bz2
```

उसके बाद आप आमतौर पर इसे जैसे टूल्स से प्रोसेस करते हैं:
- wikiextractor (Python) → साफ़ टेक्स्ट
- Hugging Face `datasets` विकिपीडिया लोडर (आपके लिए पहले से ही सब कुछ कर देता है)
- DeepSeek या EleutherAI की प्रोसेसिंग स्क्रिप्ट्स, आदि।

संक्षेप में:  
या तो एकल 24 GB वाली `enwiki-latest-pages-articles.xml.bz2` डाउनलोड करें या 27 विभाजित फ़ाइलें + इंडेक्स। उस सूची में बाकी सब कुछ या तो चेकसम, पुराना कचरा, या मेटाडेटा है जिसकी आपको ट्रेनिंग के लिए आवश्यकता नहीं है।