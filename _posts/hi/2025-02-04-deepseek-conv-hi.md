---
audio: false
generated: false
lang: hi
layout: post
title: डीपसीक - वार्तालाप
translated: true
type: note
---

ए: मैं DeepSeek-V3 की तकनीकी रिपोर्ट पढ़ रहा हूं, और इस मॉडल के पैमाने से मैं वाकई प्रभावित हूं। 671 बिलियन पैरामीटर, लेकिन प्रति टोकन केवल 37 बिलियन सक्रिय? यह एक बहुत बड़ी MoE आर्किटेक्चर है। यह आखिर काम कैसे करता है?

बी: हां, यह काफी उपलब्धि है! DeepSeek-V3 Mixture-of-Experts (MoE) फ्रेमवर्क पर बनाया गया है, जो प्रत्येक टोकन के लिए पैरामीटर्स के केवल एक सबसेट को सक्रिय करने की अनुमति देता है। विशेष रूप से, यह 256 रूटेड एक्सपर्ट्स का उपयोग करता है, लेकिन प्रति टोकन केवल 8 सक्रिय होते हैं। यह इसे घने मॉडल्स (dense models) की तुलना में अविश्वसनीय रूप से कुशल बनाता है, जहां प्रत्येक टोकन के लिए सभी पैरामीटर सक्रिय होते हैं।

ए: यह समझ आता है। लेकिन यह कैसे तय करता है कि किन एक्सपर्ट्स को सक्रिय करना है? क्या यह सिर्फ यादृच्छिक है, या कोई रूटिंग मैकेनिज्म है?

बी: बढ़िया सवाल! रूटिंग टोकन-टू-एक्सपर्ट एफिनिटी स्कोर पर आधारित है। प्रत्येक टोकन को प्रत्येक विशेषज्ञ के लिए एक स्कोर दिया जाता है, और सबसे अधिक स्कोर वाले शीर्ष-K विशेषज्ञ सक्रिय होते हैं। DeepSeek-V3 इन स्कोरों की गणना करने के लिए एक सिग्मॉइड फ़ंक्शन का उपयोग करता है, जो विशेषज्ञों में लोड को संतुलित करने में मदद करता है।

ए: आह, तो यह यादृच्छिक नहीं है—यह प्रशिक्षण के दौरान सीखा जाता है। लेकिन क्या इससे असंतुलित एक्सपर्ट उपयोग नहीं होता? मैंने सुना है कि यह MoE मॉडल्स में एक आम समस्या है।

बी: बिल्कुल! असंतुलित एक्सपर्ट उपयोग एक समस्या हो सकती है, लेकिन DeepSeek-V3 इसे संभालने के लिए एक ऑक्जिलियरी-लॉस-फ्री रणनीति पेश करता है। लोड बैलेंसिंग को प्रोत्साहित करने के लिए एक अलग लॉस टर्म जोड़ने के बजाय, यह प्रत्येक विशेषज्ञ के लिए एक बायस टर्म को डायनामिक रूप से एडजस्ट करता है। यदि कोई विशेषज्ञ ओवरलोडेड है, तो उसका बायस कम हो जाता है, और यदि वह अंडरलोडेड है, तो बायस बढ़ जाता है। यह मॉडल परफॉर्मेंस को खराब किए बिना लोड को संतुलित रखता है।

ए: यह चतुराई भरा है। तो, कोई ऑक्जिलियरी लॉस नहीं होने का मतलब है मुख्य प्रशिक्षण उद्देश्य के साथ कम हस्तक्षेप। लेकिन यह पारंपरिक MoE मॉडल्स से कैसे तुलना करता है जो ऑक्जिलियरी लॉस का उपयोग करते हैं?

बी: सही। पारंपरिक MoE मॉडल अक्सर लोड बैलेंसिंग को प्रोत्साहित करने के लिए ऑक्जिलियरी लॉस का उपयोग करते हैं, लेकिन ये लॉस कभी-कभी परफॉर्मेंस को नुकसान पहुंचा सकते हैं। DeepSeek-V3 की ऑक्जिलियरी-लॉस-फ्री approach इस ट्रेड-ऑफ से बचती है। वास्तव में, ablation studies से पता चलता है कि यह लगातार उन मॉडल्स से बेहतर प्रदर्शन करता है जो ऑक्जिलियरी लॉस पर निर्भर करते हैं, खासकर कोडिंग और गणित जैसे कार्यों पर।

ए: दिलचस्प। कोडिंग और गणित की बात करें तो, मैंने देखा कि DeepSeek-V3 HumanEval और MATH जैसे बेंचमार्क पर असाधारण रूप से अच्छा प्रदर्शन करता है। वहां क्या खास है?

बी: इसका एक बड़ा हिस्सा मल्टी-टोकन प्रिडिक्शन (MTP) ऑब्जेक्टिव है। केवल अगले टोकन की भविष्यवाणी करने के बजाय, DeepSeek-V3 प्रत्येक पोजीशन पर कई भविष्य के टोकन की भविष्यवाणी करता है। यह प्रशिक्षण सिग्नल को सघन करता है और मॉडल को आगे की योजना बनाने में मदद करता है, जो कोडिंग और गणित जैसे कार्यों के लिए विशेष रूप से उपयोगी है जिनके लिए अनुक्रमिक तर्क (sequential reasoning) की आवश्यकता होती है।

ए: रुको, तो यह एक साथ कई टोकन की भविष्यवाणी कर रहा है? इनफरेंस के दौरान यह कैसे काम करता है? क्या यह अभी भी MTP का उपयोग करता है, या यह सिर्फ प्रशिक्षण के लिए है?

बी: इनफरेंस के दौरान, MTP मॉड्यूल को छोड़ा जा सकता है, और मॉडल एक मानक ऑटोरेग्रेसिव मॉडल की तरह व्यवहार करता है। लेकिन यहां मजेदार बात यह है: MTP मॉड्यूल को स्पेक्युलेटिव डिकोडिंग के लिए पुनः उपयोग में लाया जा सकता है, जो कई टोकन को समानांतर रूप से भविष्यवाणी करके और फिर उन्हें सत्यापित करके जनरेशन की गति बढ़ाता है।

ए: यह एक अच्छी तरकीब है। तो, यह प्रशिक्षण के दौरान MTP के लाभ प्राप्त करने और फिर इनफरेंस को तेज करने के लिए इसका उपयोग करने जैसा है। लेकिन अटेंशन मैकेनिज्म के बारे में क्या? मैंने Multi-head Latent Attention (MLA) के बारे में कुछ देखा। यह इसमें कैसे फिट बैठता है?

बी: MLA एक और महत्वपूर्ण नवाचार है। यह Key-Value (KV) कैश को कंप्रेस करके मेमोरी फुटप्रिंट को कम करता है। पूर्ण अटेंशन कीज़ और वैल्यूज़ को स्टोर करने के बजाय, यह उन्हें प्रस्तुत करने के लिए लो-रैंक जॉइंट कंप्रेशन का उपयोग करता है। यह स्टैंडर्ड Multi-Head Attention के बराबर परफॉर्मेंस बनाए रखते हुए इनफरेंस के दौरान KV कैश के आकार को काफी कम कर देता है।

ए: यह दक्षता के लिए एक बहुत बड़ी जीत है। लेकिन क्या कंप्रेशन से सूचना का कुछ नुकसान नहीं होता? यह प्रदर्शन कैसे बनाए रखता है?

बी: अच्छा बिंदु है। कंप्रेशन को सबसे महत्वपूर्ण जानकारी को संरक्षित करने के लिए डिज़ाइन किया गया है, जो कीज़ और वैल्यूज़ की आवश्यक विशेषताओं को कैप्चर करने वाले latent वैक्टर पर ध्यान केंद्रित करता है। मॉडल पोजिशनल जानकारी को बनाए रखने के लिए Rotary Positional Embedding (RoPE) का भी उपयोग करता है, जो कंप्रेशन से होने वाले किसी भी नुकसान को कम करने में मदद करता है।

ए: समझ गया। तो, MLA एक win-win स्थिति है—यह बहुत अधिक प्रदर्शन का त्याग किए बिना मेमोरी उपयोग को कम करती है। लेकिन प्रशिक्षण के बारे में क्या? इस आकार के मॉडल को प्रशिक्षित करना अविश्वसनीय रूप से महंगा होना चाहिए। DeepSeek-V3 लागत को कम रखने का प्रबंधन कैसे करता है?

बी: प्रशिक्षण दक्षता एक प्रमुख फोकस है। DeepSeek-V3 एक FP8 मिक्स्ड प्रिसिजन फ्रेमवर्क का उपयोग करता है, जो मेमोरी उपयोग को कम करता है और कम्प्यूटेशन की गति बढ़ाता है। यह पाइपलाइन समानांतरता (pipeline parallelism) के लिए DualPipe एल्गोरिदम का भी उपयोग करता है, जो पाइपलाइन बबल्स को कम करता है और कम्प्यूटेशन को कम्युनिकेशन के साथ ओवरलैप करता है। ये ऑप्टिमाइजेशन मॉडल को केवल 2.788 मिलियन H800 GPU घंटों में 14.8 ट्रिलियन टोकन पर प्रशिक्षित होने की अनुमति देते हैं।

ए: यह प्रभावशाली है। लेकिन FP8 प्रशिक्षण मुश्किल हो सकता है—वे प्रिसिजन समस्याओं को कैसे संभालते हैं? मैंने सुना है कि कम-सटीकता वाला प्रशिक्षण अस्थिरता का कारण बन सकता है।

बी: आप सही कह रहे हैं। सीमित डायनामिक रेंज के कारण FP8 प्रशिक्षण चुनौतीपूर्ण है। DeepSeek-V3 इसे फाइन-ग्रेन्ड क्वांटिज़ेशन के साथ संबोधित करता है, जहां एक्टिवेशन और वेट को छोटे टाइल्स या ब्लॉक्स में समूहीकृत किया जाता है और स्वतंत्र रूप से स्केल किया जाता है। यह आउटलायर्स के प्रभाव को कम करता है और प्रशिक्षण को स्थिर रखता है। वे महत्वपूर्ण ऑपरेशन्स के लिए सटीकता बनाए रखने के लिए हाई-प्रिसिजन एक्यूमुलेशन का भी उपयोग करते हैं।

ए: यह समझ आता है। तो, यह दक्षता और सटीकता के बीच संतुलन है। लेकिन डेटा के बारे में क्या? 14.8 ट्रिलियन टोकन एक बहुत बड़ा डेटासेट है। यह किस प्रकार के डेटा पर प्रशिक्षित है?

बी: डेटासेट विविध और उच्च-गुणवत्ता वाला है, जिसमें अंग्रेजी और चीनी पाठ पर ध्यान केंद्रित किया गया है। इसमें गणितीय और प्रोग्रामिंग डेटा की एक महत्वपूर्ण मात्रा भी शामिल है, जो मॉडल को उन डोमेन में उत्कृष्ट बनाने में मदद करती है। डेटा पाइपलाइन को विविधता बनाए रखते हुए अतिरेक (redundancy) को कम करने के लिए अनुकूलित किया गया है, और वे डेटा अखंडता सुनिश्चित करने के लिए document packing जैसी तकनीकों का उपयोग करते हैं।

ए: यह कोडिंग और गणित कार्यों पर मजबूत प्रदर्शन की व्याख्या करता है। लेकिन बहुभाषी प्रदर्शन के बारे में क्या? क्या यह अन्य भाषाओं को अच्छी तरह संभालता है?

बी: हां, DeepSeek-V3 एक बहुभाषी कॉर्पस पर प्रशिक्षित है, और यह MMMLU जैसे बेंचमार्क पर अच्छा प्रदर्शन करता है, जिसमें गैर-अंग्रेजी कार्य शामिल हैं। यह विशेष रूप से चीनी में मजबूत है, जो C-Eval और CMMLU जैसे चीनी बेंचमार्क पर Qwen2.5 जैसे मॉडल्स से बेहतर प्रदर्शन करता है।

ए: यह प्रभावशाली है। लेकिन लंबे-संदर्भ (long-context) कार्यों के बारे में क्या? मैंने देखा कि यह 128K टोकन तक का समर्थन करता है। यह इतने लंबे इनपुट को कैसे संभालता है?

बी: DeepSeek-V3 अपनी संदर्भ लंबाई (context length) को दो चरणों में बढ़ाता है: पहले 32K टोकन तक और फिर YaRN तकनीक का उपयोग करके 128K टोकन तक। यह इसे दस्तावेज़ सारांशण और रिट्रीवल जैसे लंबे-संदर्भ कार्यों को प्रभावी ढंग से संभालने की अनुमति देता है। यह 'Needle In A Haystack' टेस्ट पर भी अच्छा प्रदर्शन करता है, जो लंबे-संदर्भ की समझ का मूल्यांकन करता है।

ए: यह पिछले मॉडल्स की तुलना में एक बहुत बड़ा सुधार है। लेकिन डिप्लॉयमेंट के बारे में क्या? वे इतने बड़े मॉडल के लिए इनफरेंस कैसे संभालते हैं?

बी: इनफरेंस एक H800 क्लस्टर पर संभाला जाता है, जिसमें GPU NVLink और InfiniBand का उपयोग करके आपस में जुड़े होते हैं। डिप्लॉयमेंट रणनीति उच्च थ्रूपुट और कम विलंबता (low latency) दोनों सुनिश्चित करने के लिए प्री-फिलिंग और डिकोडिंग चरणों को अलग करती है। वे इनफरेंस के दौरान लोड को संतुलित करने के लिए रिडंडेंट एक्सपर्ट्स का भी उपयोग करते हैं, जो दक्षता बनाए रखने में मदद करता है।

ए: यह बहुत सारे ऑप्टिमाइजेशन हैं। लेकिन सीमाएं क्या हैं? निश्चित रूप से, इस आकार के मॉडल में कुछ ट्रेड-ऑफ हैं।

बी: एक सीमा डिप्लॉयमेंट यूनिट का आकार है। कुशल इनफरेंस के लिए DeepSeek-V3 को अपेक्षाकृत बड़े क्लस्टर की आवश्यकता होती है, जो छोटी टीमों के लिए एक चुनौती हो सकती है। जनरेशन स्पीड में सुधार की गुंजाइश भी है, हालांकि MTP के साथ स्पेक्युलेटिव डिकोडिंग मदद करती है।

ए: यह उचित है। लेकिन कुल मिलाकर, यह एक बहुत बड़ा कदम आगे लगता है। DeepSeek-V3 के लिए आगे क्या है? क्या कोई भविष्य की दिशाएं हैं जिनकी वे खोज कर रहे हैं?

बी: वे कई क्षेत्रों पर विचार कर रहे हैं, जैसे अनंत संदर्भ लंबाई (infinite context length) का समर्थन करने के लिए आर्किटेक्चर को परिष्कृत करना, अतिरिक्त प्रशिक्षण सिग्नल स्रोतों की खोज करना और मॉडल की तर्क क्षमताओं को बढ़ाना। वे मॉडल प्रदर्शन का बेहतर आकलन करने के लिए अधिक व्यापक मूल्यांकन विधियों पर भी काम कर रहे हैं।

ए: लगता है कि वे जल्द ही धीमे होने वाले नहीं हैं। मुझे यह सब समझाने के लिए धन्यवाद—DeepSeek-V3 निश्चित रूप से ओपन-सोर्स LLM स्पेस में एक गेम-चेंजर है।

बी: बिल्कुल! यह देखकर रोमांच होता है कि ओपन-सोर्स मॉडल्स कितनी दूर आ गए हैं। DeepSeek-V3 सीमाओं को आगे बढ़ा रहा है, और मैं उत्सुक हूं कि वे आगे क्या करते हैं।

ए: आपने उल्लेख किया कि DeepSeek-V3 FP8 मिक्स्ड प्रिसिजन ट्रेनिंग का उपयोग करता है। मैं उत्सुक हूं—यह BF16 या FP16 की तुलना कैसे करता है? क्या FP8 वास्तव में इतने बड़े मॉडल को प्रशिक्षित करने के लिए पर्याप्त स्थिर है?

बी: यह एक बढ़िया सवाल है। FP8 वास्तव में अपनी सीमित डायनामिक रेंज के कारण अधिक चुनौतीपूर्ण है, लेकिन इसे कम करने के लिए DeepSeek-V3 एक फाइन-ग्रेन्ड क्वांटिज़ेशन रणनीति का उपयोग करता है। उदाहरण के लिए, एक्टिवेशन को 1x128 टाइल्स में समूहीकृत किया जाता है, और वेट को 128x128 ब्लॉक्स में समूहीकृत किया जाता है। प्रत्येक समूह को स्वतंत्र रूप से स्केल किया जाता है, जो आउटलायर्स को संभालने और प्रशिक्षण को स्थिर रखने में मदद करता है।

ए: दिलचस्प। तो, यह सिर्फ एक समान FP8 क्वांटिज़ेशन नहीं है—यह अधिक सूक्ष्म है। लेकिन क्या इन सभी समूहों और स्केलिंग कारकों को प्रबंधित करने के लिए अतिरिक्त ओवरहेड नहीं आता है?

बी: आता है, लेकिन लाभों की तुलना में ओवरहेड न्यूनतम है। मुख्य बात यह है कि FP8 मेमोरी उपयोग को कम करता है और कम्प्यूटेशन की गति बढ़ाता है, जो इतने बड़े मॉडल को प्रशिक्षित करने के लिए महत्वपूर्ण है। वे संख्यात्मक स्थिरता सुनिश्चित करने के लिए महत्वपूर्ण ऑपरेशन्स, जैसे मैट्रिक्स गुणन, के लिए हाई-प्रिसिजन एक्यूमुलेशन का भी उपयोग करते हैं।

ए: समझ गया। तो, यह सटीकता और दक्षता के बीच एक ट्रेड-ऑफ है, लेकिन वे एक अच्छा संतुलन बनाने में कामयाब रहे हैं। DualPipe एल्गोरिदम के बारे में क्या? यह कैसे काम करता है?

बी: DualPipe को पाइपलाइन समानांतरता (pipeline parallelism) में पाइपलाइन बबल्स को कम करने के लिए डिज़ाइन किया गया है। यह काम के प्रत्येक हिस्से को चार घटकों में विभाजित करके कम्प्यूटेशन और कम्युनिकेशन को ओवरलैप करता है: अटेंशन, all-to-all dispatch, MLP, और all-to-all combine। बैकवर्ड पास के दौरान, यह कम्प्यूटेशन को आगे 'बैकवर्ड फॉर इनपुट' और 'बैकवर्ड फॉर वेट' में विभाजित करता है, जो अधिक कुशल ओवरलैप की अनुमति देता है।

ए: यह जटिल लगता है, लेकिन यह समझ में आता है। तो, यह अनिवार्य रूप से कम्युनिकेशन ओवरहेड को कम्प्यूटेशन के साथ ओवरलैप करके छिपा रहा है। यह 1F1B या Zero Bubble जैसी अन्य पाइपलाइन समानांतरता विधियों से कैसे तुलना करता है?

बी: DualPipe में 1F1B और Zero Bubble की तुलना में कम पाइपलाइन बबल्स होते हैं। यह द्वि-दिशात्मक शेड्यूलिंग (bidirectional scheduling) की भी अनुमति देता है, जहां माइक्रो-बैच पाइपलाइन के दोनों सिरों से फीड किए जाते हैं। यह निष्क्रिय समय (idle time) को और कम करता है और समग्र दक्षता में सुधार करता है। वास्तव में, DualPipe near-zero all-to-all कम्युनिकेशन ओवरहेड प्राप्त करता है, जो MoE मॉडल्स को स्केल अप करने के लिए महत्वपूर्ण है।

ए: यह प्रभावशाली है। लेकिन मेमोरी उपयोग के बारे में क्या? क्या DualPipe को अन्य तरीकों की तुलना में अधिक मेमोरी की आवश्यकता होती है?

बी: इसे थोड़ी अधिक मेमोरी की आवश्यकता होती है क्योंकि यह मॉडल पैरामीटर्स की दो प्रतियां रखता है, लेकिन वृद्धि प्रबंधनीय है। मेमोरी फुटप्रिंट को RMSNorm और MLA up-projections के रीकम्प्यूटेशन जैसी तकनीकों के माध्यम से अनुकूलित किया जाता है, जो इंटरमीडिएट एक्टिवेशन को स्टोर करने की आवश्यकता को समाप्त करता है।

ए: आह, तो वे बेहतर दक्षता के लिए थोड़ी सी मेमोरी का व्यापार कर रहे हैं। यह एक उचित ट्रेड-ऑफ लगता है। मेमोरी की बात करें तो, वे इतने बड़े संदर्भ लंबाई के लिए KV कैश को कैसे संभालते हैं? 128K टोकन के लिए एक बहुत बड़े कैश की आवश्यकता होनी चाहिए।

बी: यही वह जगह है जहां MLA वास्तव में चमकती है। KV कैश को कंप्रेस करके, वे इसके आकार को काफी कम कर देते हैं। पूर्ण अटेंशन कीज़ और वैल्यूज़ को स्टोर करने के बजाय, वे कंप्रेस्ड latent वैक्टर को स्टोर करते हैं, जो बहुत छोटे होते हैं। यह DeepSeek-V3 को मेमोरी बॉटलनेक में फंसे बिना लंबे संदर्भों को संभालने की अनुमति देता है।

ए: यह एक चतुर समाधान है। लेकिन अटेंशन की गुणवत्ता के बारे में क्या? क्या कंप्रेशन मॉडल की सही टोकन पर ध्यान देने की क्षमता को प्रभावित करता है?

बी: कंप्रेशन को सबसे महत्वपूर्ण जानकारी को संरक्षित करने के लिए डिज़ाइन किया गया है, इसलिए अटेंशन की गुणवत्ता पर प्रभाव न्यूनतम है। वे पोजिशनल जानकारी बनाए रखने के लिए RoPE (Rotary Positional Embedding) का भी उपयोग करते हैं, जो कंप्रेस्ड कीज़ और वैल्यूज़ के साथ भी टोकन की सापेक्ष स्थिति को समझने में मॉडल की मदद करता है।

ए: समझ आता है। तो, MLA एक win-win स्थिति है—यह बहुत अधिक प्रदर्शन का त्याग किए बिना मेमोरी उपयोग को कम करती है। लेकिन प्रशिक्षण डेटा के बारे में क्या? आपने उल्लेख किया कि यह 14.8 ट्रिलियन टोकन है। वे इतने बड़े डेटासेट की गुणवत्ता और विविधता कैसे सुनिश्चित करते हैं?

बी: डेटासेट को उच्च-गुणवत्ता और विविध टोकन शामिल करने के लिए सावधानीपूर्वक क्यूरेट किया गया है। वे अतिरेक को कम करते हुए विविधता बनाए रखने के लिए डेटा पाइपलाइन को अनुकूलित करते हैं, और वे डेटा अखंडता सुनिश्चित करने के लिए document packing जैसी तकनीकों का उपयोग करते हैं। कॉर्पस में अंग्रेजी और चीनी पाठ का मिश्रण शामिल है, जिसमें गणितीय और प्रोग्रामिंग नमूनों पर जोर दिया गया है।

ए: यह कोडिंग और गणित कार्यों पर मजबूत प्रदर्शन की व्याख्या करता है। लेकिन बहुभाषी कार्यों के बारे में क्या? क्या यह अन्य भाषाओं को अच्छी तरह संभालता है?

बी: हां, DeepSeek-V3 एक बहुभाषी कॉर्पस पर प्रशिक्षित है, और यह MMMLU जैसे बेंचमार्क पर अच्छा प्रदर्शन करता है, जिसमें गैर-अंग्रेजी कार्य शामिल हैं। यह विशेष रूप से चीनी में मजबूत है, जो C-Eval और CMMLU जैसे चीनी बेंचमार्क पर Qwen2.5 जैसे मॉडल्स से बेहतर प्रदर्शन करता है।

ए: यह प्रभावशाली है। लेकिन लंबे-संदर्भ (long-context) कार्यों के बारे में क्या? मैंने देखा कि यह 128K टोकन तक का समर्थन करता है। यह इतने लंबे इनपुट को कैसे संभालता है?

बी: DeepSeek-V3 अपनी संदर्भ लंबाई (context length) को दो चरणों में बढ़ाता है: पहले 32K टोकन तक और फिर YaRN तकनीक का उपयोग करके 128K टोकन तक। यह इसे दस्तावेज़ सारांशण और रिट्रीवल जैसे लंबे-संदर्भ कार्यों को प्रभावी ढंग से संभालने की अनुमति देता है। यह 'Needle In A Haystack' टेस्ट पर भी अच्छा प्रदर्शन करता है, जो लंबे-संदर्भ की समझ का मूल्यांकन करता है।

ए: यह पिछले मॉडल्स की तुलना में एक बहुत बड़ा सुधार है। लेकिन डिप्लॉयमेंट के बारे में क्या? वे इतने बड़े मॉडल के लिए इनफरेंस कैसे संभालते हैं?

बी: इनफरेंस एक H800 क्लस्टर पर संभाला जाता है, जिसमें GPU NVLink और InfiniBand का उपयोग करके आपस में जुड़े होते हैं। डिप्लॉयमेंट रणनीति उच्च थ्रूपुट और कम विलंबता (low latency) दोनों सुनिश्चित करने के लिए प्री-फिलिंग और डिकोडिंग चरणों को अलग करती है। वे इनफरेंस के दौरान लोड को संतुलित करने के लिए रिडंडेंट एक्सपर्ट्स का भी उपयोग करते हैं, जो दक्षता बन