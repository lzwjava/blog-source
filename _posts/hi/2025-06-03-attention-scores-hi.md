---
audio: false
generated: true
lang: hi
layout: post
title: ट्रांसफॉर्मर्स में ध्यान स्कोर को स्थिर करना
translated: true
type: note
---

स्केल्ड डॉट-प्रोडक्ट अटेंशन मैकेनिज्म (जैसा कि फॉर्मूला \\(\text{स्केल्ड स्कोर} = \frac{Q K^T}{\sqrt{d_k}}\\) में है) में की डायमेंशन के वर्गमूल, \\(\sqrt{d_k}\\), से भाग देना एक सोचा-समझा विकल्प है, जिसका उद्देश्य ग्रेडिएंट्स को स्थिर करना और बड़े मानों को रोकना है, खासकर ट्रांसफॉर्मर्स के संदर्भ में। यहाँ बताया गया है कि केवल \\(d_k\\) के बजाय \\(\sqrt{d_k}\\) का उपयोग क्यों किया जाता है:

1.  **डॉट प्रोडक्ट का वेरिएंस (प्रसरण)**:
    *   डॉट प्रोडक्ट \\( Q K^T \\) क्वेरी (\\( Q \\)) और की (\\( K \\)) वेक्टर्स के बीच की समानता की गणना करता है, जहाँ प्रत्येक वेक्टर का डायमेंशन \\( d_k \\) होता है। यदि मान लें कि \\( Q \\) और \\( K \\) के एलिमेंट्स स्वतंत्र हैं और उनका माध्य 0 तथा वेरिएंस 1 है (इनिशियलाइजेशन या नॉर्मलाइजेशन के बाद आम बात), तो डॉट प्रोडक्ट \\( Q_i \cdot K_j \\) (क्वेरी और की वेक्टर्स की एक जोड़ी के लिए) का वेरिएंस \\( d_k \\) होता है। ऐसा इसलिए है क्योंकि \\( d_k \\) स्वतंत्र प्रोडक्ट्स के योग का वेरिएंस, \\( d_k \\) के साथ रैखिक रूप से बढ़ता है।
    *   स्केलिंग के बिना, \\( Q K^T \\) का परिमाण \\( d_k \\) के साथ बढ़ता है, जो बड़े \\( d_k \\) (ट्रांसफॉर्मर्स में आम, जहाँ \\( d_k \\) 64, 128, या उससे बड़ा हो सकता है) के लिए बहुत बड़े मान पैदा करता है। अटेंशन स्कोर में बड़े मान सॉफ्टमैक्स फंक्शन से गुजरने पर समस्याएँ पैदा कर सकते हैं।

2.  **सॉफ्टमैक्स स्टेबिलिटी**:
    *   अटेंशन स्कोर \\( \frac{Q K^T}{\sqrt{d_k}} \\) को अटेंशन वेट की गणना करने के लिए एक सॉफ्टमैक्स में फीड किया जाता है। यदि स्कोर बहुत बड़े हैं (जैसा कि बिना स्केलिंग या अपर्याप्त स्केलिंग के होगा), तो सॉफ्टमैक्स फंक्शन बहुत तीक्ष्ण डिस्ट्रीब्यूशन पैदा कर सकता है, जहाँ एक एलिमेंट हावी हो जाता है (1 के करीब पहुँचना) और बाकी 0 के करीब होते हैं। इससे अधिकांश एलिमेंट्स के लिए ग्रेडिएंट लगभग शून्य हो जाते हैं, जिससे मॉडल के लिए प्रभावी ढंग से सीखना मुश्किल हो जाता है।
    *   \\(\sqrt{d_k}\\) से भाग देने से यह सुनिश्चित होता है कि स्केल किए गए स्कोर का वेरिएंस लगभग 1 रहता है, जिससे स्कोर एक ऐसी रेंज में रहते हैं जहाँ सॉफ्टमैक्स फंक्शन अच्छे से काम करता है, जिससे अधिक संतुलित अटेंशन वेट और स्थिर ग्रेडिएंट मिलते हैं।

3.  **केवल \\( d_k \\) क्यों नहीं?**:
    *   \\(\sqrt{d_k}\\) के बजाय \\( d_k \\) से भाग देने से डॉट प्रोडक्ट का अति-स्केलिंग हो जाएगा, जिससे स्कोर का वेरिएंस \\( \frac{1}{d_k} \\) तक कम हो जाएगा। बड़े \\( d_k \\) के लिए, यह स्कोर को बहुत छोटा बना देगा, जिससे सॉफ्टमैक्स लगभग एकसमान डिस्ट्रीब्यूशन पैदा करेगा (क्योंकि सॉफ्टमैक्स को छोटे इनपुट \\( \frac{1}{n} \\) के करीब आउटपुट देते हैं)। इससे अटेंशन मैकेनिज्म की संबंधित कीज़ पर फोकस करने की क्षमता कमजोर हो जाएगी, क्योंकि स्कोर के बीच का अंतर कम हो जाएगा।
    *   \\( d_k \\) के साथ अति-स्केलिंग से कुछ मामलों में न्यूमेरिकल इंस्टेबिलिटी भी हो सकती है, क्योंकि फ्लोटिंग-पॉइंट अंकगणित में बहुत छोटे मानों को सटीक रूप से हैंडल करना मुश्किल हो सकता है।

4.  **\\(\sqrt{d_k}\\) क्यों?**:
    *   \\(\sqrt{d_k}\\) से भाग देने से डॉट प्रोडक्ट के वेरिएंस को लगभग 1 पर नॉर्मलाइज़ कर देता है, क्योंकि \\( \text{Var}\left(\frac{Q K^T}{\sqrt{d_k}}\right) = \frac{\text{Var}(Q K^T)}{d_k} = \frac{d_k}{d_k} = 1 \\) होता है। यह स्कोर को एक उचित रेंज में रखता है, यह सुनिश्चित करते हुए कि सॉफ्टमैक्स अर्थपूर्ण अटेंशन वेट पैदा करता है, न कि अत्यधिक तीक्ष्ण या अत्यधिक एकसमान।
    *   वर्गमूल डॉट प्रोडक्ट के वेरिएंस के गणितीय गुण से स्वाभाविक रूप से उत्पन्न होता है। यह बिना स्केलिंग के होने वाले विस्फोटक मानों को रोकने और स्कोर के अति-दमन (जो \\( d_k \\) के साथ होगा) से बचने के बीच संतुलन बनाता है।

5.  **अनुभवजन्य प्रभावशीलता**:
    *   \\(\sqrt{d_k}\\) का चुनाव मूल ट्रांसफॉर्मर पेपर ("Attention is All You Need" by Vaswani et al., 2017) में पेश किया गया था और कई ट्रांसफॉर्मर-आधारित मॉडल्स में अनुभवजन्य रूप से मान्य किया जा चुका है। यह \\( d_k \\) मानों की एक विस्तृत श्रृंखला के लिए व्यवहार में अच्छा काम करता है, स्थिर ट्रेनिंग और अच्छे परफॉर्मेंस को सुनिश्चित करता है।

संक्षेप में, \\(\sqrt{d_k}\\) से भाग देना एक गणितीय रूप से प्रेरित विकल्प है जो अटेंशन स्कोर के वेरिएंस को स्थिर करता है, यह सुनिश्चित करता है कि सॉफ्टमैक्स एक सुव्यवस्थित शासन में काम करता है, और ग्रेडिएंट्स के विस्फोट या लुप्त होने की समस्याओं से बचाता है। \\( d_k \\) से भाग देने से स्कोर का अत्यधिक कमजोर होना होगा, जिससे कम भेदभाव करने वाले अटेंशन वेट मिलेंगे और संभावित रूप से मॉडल के प्रदर्शन को नुकसान पहुँचेगा।