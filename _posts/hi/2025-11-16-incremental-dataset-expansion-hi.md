---
audio: false
generated: true
image: false
lang: hi
layout: post
model: x-ai/grok-4-fast
title: वृद्धिशील डेटासेट विस्तार रणनीति
translated: true
type: note
---

हाँ, डेटासेट आकार को चरणबद्ध तरीके से बढ़ाना (उदाहरण के लिए, पहले articles1 से शुरू करके, फिर articles2 और articles3 जोड़कर, और इसी तरह आगे बढ़ते हुए) एक स्मार्ट दृष्टिकोण हो सकता है, खासकर यदि आप सीमित हार्डवेयर (जैसे RTX 4070) पर NanoGPT के साथ प्रयोग कर रहे हैं। हालाँकि, इसी पर बने रहना है या सीधे articles1 से articles10 तक डाउनलोड करना है, यह आपके लक्ष्यों, समय और संसाधनों पर निर्भर करता है। मैं आपको निर्णय लेने में मदद के लिए इसे नीचे विभाजित करता हूँ।

### चरणबद्ध वृद्धि के फायदे (जिस पर आप विचार कर रहे हैं)
- **नियंत्रित प्रयोग**: आप निगरानी कर सकते हैं कि अधिक डेटा जोड़ने से मॉडल के प्रदर्शन (जैसे, लॉस कर्व्स, पेरप्लेक्सिटी, या जनरेट किए गए टेक्स्ट की गुणवत्ता) पर क्या प्रभाव पड़ता है। आपके लॉग्स से, सिर्फ articles1 (विकिपीडिया का एक छोटा सा हिस्सा) पर प्रशिक्षण के बाद 20k पुनरावृत्तियों के बाद ~3.9 ट्रेन/वैल लॉस प्राप्त हुआ, जो एक छोटे मॉडल के लिए एक अच्छी शुरुआत है। articles2 और articles3 जोड़ने (जो आपको ~3 भागों तक ले जाता है) से आप यह देख सकते हैं कि क्या मॉडल बेहतर सामान्यीकरण करता है या कम ओवरफिटिंग करता है, बिना किसी बड़े रन के प्रतिबद्ध हुए।
- **संसाधन प्रबंधन**:
  - डिस्क: आपकी 391GB उपलब्ध जगह अभी के लिए पर्याप्त से अधिक है। दो नई bz2 फाइलें कुल ~5GB संपीड़ित हैं। wikiextractor (जैसा कि इको में सुझाया गया है) का उपयोग करके, निकाले गए साफ टेक्स्ट की मात्रा इन दोनों के लिए ~10-15GB अनकंप्रेस्ड हो सकती है (विकिपीडिया XML अच्छी तरह से संपीड़ित होता है, लेकिन साफ टेक्स्ट अधिक सघन होता है)। articles1 के निकाले गए डेटा (~5GB?) के साथ मिलाकर, आप कुल ~15-20GB पर पहुँच जाएंगे—पर्याप्त हेडरूम है।
  - RAM/GPU: 62GB सिस्टम RAM टोकननाइजेशन और डेटा लोडिंग को आसानी से संभाल सकती है। RTX 4070 (12GB VRAM) NanoGPT के डिफ़ॉल्ट tiny/shakespeare कॉन्फ़िग या छोटे GPT-2 जैसे मॉडल (जैसे, 124M पैरामीटर्स) के लिए मजबूत है। यदि आप bf16 या मिश्रित परिशुद्धता का उपयोग कर रहे हैं, तो आप बड़े बैच का उपयोग कर सकते हैं। चरणबद्ध तरीका विशाल डेटासेट से VRAM को एक साथ अभिभूत होने से बचाता है।
  - समय: आपके सेटअप पर `--processes 8` के साथ एक्सट्रैक्शन प्रति फाइल 1-2 घंटे ले सकता है। प्रशिक्षण वृद्धि (जैसे, आपके articles1 चेकपॉइंट से जारी रखते हुए) प्रति चरण दिनों में किया जा सकता है, जिससे आप तेजी से पुनरावृति कर सकते हैं।
- **पाठ्यक्रम सीखने का पहलू**: विकिपीडिया लेख कुछ हद तक ID द्वारा क्रमबद्ध होते हैं, इसलिए क्रमिक रूप से जोड़ना एक ढीले पाठ्यक्रम की तरह कार्य कर सकता है (शुरुआती लेख अधिक "मौलिक" हो सकते हैं)। लेकिन पूर्वाग्रहों से बचने के लिए NanoGPT की प्रीप स्क्रिप्ट में अपने डेटासेट को अच्छी तरह से फेंटें।
- **यह कब करें**: यदि आप प्रोटोटाइप बना रहे हैं, हाइपरपैरामीटर (जैसे, lr, बैच आकार) का परीक्षण कर रहे हैं, या सिर्फ सीख रहे हैं, तो यह कुशल है। आप अपने मौजूदा चेकपॉइंट को नए डेटा पर फाइन-ट्यून कर सकते हैं (articles2/3 का निकाला गया टेक्स्ट अपने मौजूदा डेटासेट में जोड़ें, पुनः टोकननाइज करें, और NanoGPT में `--init_from resume` के साथ प्रशिक्षण फिर से शुरू करें)।

### चरणबद्ध तरीके के नुकसान और अधिक (जैसे, Articles1-10) पर सीधे कूदने का समय
- **दक्षता समस्याएं**: यदि आपका अंतिम लक्ष्य विकिपीडिया के एक बड़े हिस्से पर एक मॉडल बनाना है, तो बढ़ते हुए सबसेट पर कई बार पुनः प्रशिक्षण या फाइन-ट्यूनिंग करना कंप्यूट संसाधनों की बर्बादी है। भाषा मॉडल शुरू से ही विविध, फेंटे हुए डेटा से लाभान्वित होते हैं—क्रमिक जोड़ से सावधानीपूर्वक न संभालने पर भयानक विस्मरण हो सकता है (हालाँकि NanoGPT का सरल सेटअप इसे कम करता है)।
- **बेहतर परिणामों के लिए डेटा स्केल**: Articles1-3 अभी भी अंग्रेजी विकिपीडिया का एक छोटा सा अंश है (पूर्ण डंप के लिए कुल ~20GB साफ टेक्स्ट)। आपका लॉस ~3.9-4.0 के आसपास पठार पर पहुँच गया, जो छोटे डेटा के लिए ठीक है लेकिन सुसंगत जनरेशन नहीं देगा। वास्तविक सुधार देखने के लिए (जैसे, 3.0 से कम लॉस), आपको 10+ भागों (~50-100GB निकाला गया टेक्स्ट) की आवश्यकता होगी। पूर्ण enwiki में हाल के डंप में ~27 भाग होते हैं, लेकिन articles1-10 कॉर्पस का एक ठोस ~30-40% कवर करेगा—सब कुछ डाउनलोड किए बिना एक अच्छे खिलौना मॉडल के लिए पर्याप्त।
- **व्यावहारिक कमियां**:
  - डाउनलोड समय: Articles1-10 की bz2 फाइलें कुल ~20-25GB संपीड़ित होती हैं (विशिष्ट डंप आकार के आधार पर)। अच्छे कनेक्शन पर, यह 1-2 घंटे का है, लेकिन ftp.acc.umu.se जैसे मिरर धीमे हो सकते हैं।
  - एक्सट्रैक्शन ओवरहेड: 10 फाइलों पर wikiextractor चलाने में कुल 10-20 घंटे लग सकते हैं, भले ही यह समानांतर हो। आउटपुट डिरेक्टरी ~50-100GB तक फूल सकती है, फिर भी आपकी 391GB डिस्क पर ठीक है।
  - प्रशिक्षण समय: RTX 4070 पर, articles1-10 पर पूर्ण रन मॉडल आकार के आधार पर 20k+ पुनरावृत्तियों पर हफ्तों ले सकता है। लेकिन आप गति बढ़ाने के लिए डाउनसैंपल कर सकते हैं या छोटे context_length का उपयोग कर सकते हैं।
- **कब कूदें**: यदि आपका लक्ष्य जल्दी से एक मजबूत मॉडल बनाना है (जैसे, Q&A या जनरेशन जैसे डाउनस्ट्रीम टास्क के लिए), अभी articles1-10 डाउनलोड करें। सभी निकाले गए टेक्स्ट को एक बड़ी फाइल में मिलाएं, एक बार टोकननाइज करें, और स्क्रैच से प्रशिक्षण शुरू करें। यह दोहराए गए सेटअप से बचाता है और बेहतर सामान्यीकरण देता है। विकिपीडिया डेटा उच्च-गुणवत्ता और सुसंगत है, इसलिए आपके हार्डवेयर की सीमा तक अधिक डेटा लगभग हमेशा बेहतर होता है।

### सिफारिश
अभी articles1+2+3 के साथ चरणबद्ध तरीके से आगे बढ़ें—यह कम जोखिम भरा है और आपके पास जो कुछ है उस पर निर्माण करता है। नई फाइलें निकालें, साफ टेक्स्ट को जोड़ें (जैसे, `cat extracted/*/* > all_text.txt`), पुनः टोकननाइज करें, और अपने अंतिम चेकपॉइंट से प्रशिक्षण फिर से शुरू करें। देखें कि क्या वैल लॉस 3.5-4.0 से नीचे गिरता है; यदि हां और जनरेशन में सुधार होता है, तो जोड़ते रहें। यदि नहीं, या यदि आप अधीर हैं, तो अगली बार articles4-10 ले लें (उसी मिरर से समान wget का उपयोग करें, part numbers को बढ़ाते हुए जैसे enwiki-20251101-pages-articles4.xml-p311330pXYZ.bz2—सटीक रेंज के लिए डंप इंडेक्स जांचें)।

अगले चरणों के लिए सुझाव:
- **डेटा तैयार करें**: एक्सट्रैक्शन के बाद, यदि आवश्यक हो तो और साफ करें (जैसे, `awk` या Python के साथ छोटे लेख हटाएं)। अपने टोकननाइज़र (जैसे, GPT-2 का) के साथ NanoGPT के `prepare.py` का उपयोग करें।
- **प्रशिक्षण समायोजन**: प्रशिक्षण फिर से शुरू करते समय लर्निंग रेट कम करें (जैसे, 3e-4)। MFU पर नजर रखें (आपके लॉग ~9-10% दिखाते हैं, जो ठीक है; अनुकूलन के साथ 15%+ का लक्ष्य रखें)। यदि VRAM पूरी हो जाती है, तो batch_size या gradient_accumulation_steps कम करें।
- **स्पेस जांच**: अधिक डाउनलोड करने से पहले, 2+3 निकालने के बाद `df -h` चलाकर उपयोग की पुष्टि करें।
- **विकल्प**: यदि विकिपीडिया बहुत बड़ा लगे, तो विविधता के लिए BookCorpus या C4 सबसेट जैसे बेहतर डेटासेट पर विचार करें।

यदि आप अधिक विवरण साझा करते हैं (जैसे, आपकी NanoGPT कॉन्फ़िग, मॉडल आकार, या विशिष्ट लक्ष्य), तो मैं इस सलाह को परिष्कृत कर सकता हूं।