---
audio: false
generated: true
image: false
lang: ar
layout: post
model: x-ai/grok-4-fast
title: نظرة عامة على الشبكات العصبية والتعلم العميق
translated: true
type: note
---

### نظرة عامة
"الشبكات العصبية والتعلم العميق" لمايكل نيلسن هو كتاب إلكتروني مجاني على الإنترنت يقدم أساسيات الشبكات العصبية والتعلم العميق. يستخدم مجموعة بيانات التعرف على الأرقام المكتوبة بخط اليد MNIST كمثال مستمر لبناء الحدس، متقدمًا من المفاهيم الأساسية إلى التقنيات المتقدمة. يركز الكتاب على التطبيق العملي (مع أمثلة كود بايثون)، والاشتقاقات الرياضية، والسياق التاريخي، بينما يستكشف سبب قوة الشبكات العصبية في مهام مثل التعرف على الصور ومعالجة الكلام وفهم اللغة الطبيعية. يغطي الخوارزميات الأساسية مثل الانتشار العكسي والنزول العشوائي للانزياح، ويتناول التحديات في تدريب الشبكات العميقة، ويعرض الإنجازات في الشبكات العصبية التلافيفية (convnets). النبرة سهلة الوصول مع الدقة، مع تمارين وتصورات لتعزيز الأفكار.

### الفصل 1: استخدام الشبكات العصبية للتعرف على الأرقام المكتوبة بخط اليد
يحفز هذا الفصل التمهيدي الشبكات العصبية بمقارنة سهولة الرؤية البشرية مع صعوبات الحواسيب في التعرف على الأنماط. يقدم البيرسيترونات (خلايا عصبية قرار ثنائية) والخلايا العصبية السينية (مخرجات سلسة واحتمالية) كوحدات بناء، موضحًا كيف تعالج الشبكات الأمامية ذات طبقات الإدخال والمخفية والمخرجات البيانات بشكل هرمي. باستخدام MNIST (60,000 صورة تدريبية بمقاس 28x28 بكسل)، يوضح تدريب شبكة ثلاثية الطبقات ([784 مدخل، 30-100 مخفية، 10 مخرجات]) عبر النزول العشوائي للانزياح (SGD) لتقليل التكلفة التربيعية، محققًا دقة تبلغ حوالي 95-97%. الأفكار الرئيسية: النزول العشوائي للانزياح يحسن الأوزان والانحيازات باتباع سطح التكلفة نحو الأسفل؛ الدُفعات الصغيرة تسرع التدريب؛ الدالة السينية تمكن التعلم القابل للاشتقاق. الاستنتاجات: تتعلم الشبكات العصبية القواعد من البيانات تلقائيًا، متفوقة على الخطوط الأساسية مثل التخمين العشوائي (10%) أو آلات ناقلات الدعم (~98% مضبوطة)، لكنها تتطلب ضبط المعاملات الفائقة (مثل معدل التعلم η).

### الفصل 2: كيف تعمل خوارزمية الانتشار العكسي
يشتق الانتشار العكسي كطريقة فعالة لحساب المتجهات الانحدارية لـ SGD، باستخدام قاعدة السلسلة لنشر الأخطاء للخلف عبر الطبقات. يتضمن الترميز مصفوفات الأوزان \\(w^l\\)، والانحيازات \\(b^l\\)، والتفعيلات \\(a^l = \sigma(z^l)\\) مع \\(z^l = w^l a^{l-1} + b^l\\). أربع معادلات تعرفه: خطأ المخرجات \\(\delta^L = \nabla_a C \odot \sigma'(z^L)\\)، الانتشار الخلفي \\(\delta^l = (w^{l+1})^T \delta^{l+1} \odot \sigma'(z^l)\\)، والمتجهات الانحدارية \\(\partial C / \partial b^l = \delta^l\\)، \\(\partial C / \partial w^l = a^{l-1} (\delta^l)^T\\). بالنسبة للدُفعات الصغيرة، يتم حساب المتوسط على الأمثلة. تظهر الأمثلة تسريعًا هائلاً مقارنة بالطرق الساذجة للفروق المحدودة (مثلاً، مرورتين مقابل الملايين). الرؤى: التشبع يسبب تلاشي المتجهات الانحدارية (\\(\sigma' \approx 0\\))؛ أشكال المصفوفات تمكن حسابًا سريعًا. الاستنتاجات: الانتشار العكسي (1986 روميلهارت وآخرون) هو حصان العمل للتعلم العصبي، عام لتكاليف/تفعيلات قابلة للاشتقاق، لكنه يكشف عن ديناميكيات مثل تدفق الخطأ.

### الفصل 3: تحسين طريقة تعلم الشبكات العصبية
معالجة مشاكل تشبع التكلفة التربيعية، تكلفة الانتروبيا المتقاطعة \\(C = -\frac{1}{n} \sum [y \ln a + (1-y) \ln(1-a)]\\) تلغي \\(\sigma'\\)، مما ينتج مشتقات أسرع \\(\partial C / \partial w = \sigma(z) - y\\). مخرجات softmax تمكن التصنيف الاحتمالي. يتم تشخيص التجهيز الزائد (دقة تدريب عالية/دقة اختبار منخفضة) عبر مجموعات التحقق والتخفيف منه عن طريق التنظيم L2 (\\(C += \lambda/2n \sum w^2\\)، تقليص الأوزان) والإسقاط (تحييد الخلايا العصبية عشوائيًا). توسيع البيانات (مثل الدوران) يحاكي الاختلافات. التهيئة الأفضل (أوزان ~ Gaussian std \\(1/\sqrt{n_{in}}\\)) تتجنب التشبع المبكر. يستخدم ضبط المعاملات الفائقة التحقق: ابدأ على نطاق واسع (مثل تجارب η)، وحسن مع التوقف المبكر. أفكار أخرى: الزخم يسرع SGD؛ دوال التفعيل ReLU/tanh. أمثلة MNIST تظهر مكاسب من 95% إلى 98%+. الاستنتاجات: اجمع التقنيات (الانتروبيا المتقاطعة + L2 + الإسقاط) لتعميم قوي؛ غالبًا ما تفوق البيانات الإضافية تعديلات الخوارزميات.

### الفصل 4: برهان بصري أن الشبكات العصبية يمكنها حساب أي دالة
يظهر برهان بناء أن الشبكات السينية ذات الطبقة المخفية الواحدة تقارب أي دالة مستمرة \\(f(x)\\) بدقة \\(\epsilon > 0\\) بعدد كافٍ من الخلايا العصبية، عبر دوال "نتوء" (أزواج خطوة تشكل مستطيلات) و"أبراج" (نظائرها في الأبعاد الأعلى). تقارب الخطوات قفزات Heaviside بأوزان كبيرة؛ تصلح التداخلات العيوب. للمدخلات/المخرجات المتعددة، ابني جداول بحثية ثابتة قطعة قطعة. محاذير: تقريب فقط (ليس بالضبط)؛ دوال مستمرة. دوال التفعيل الخطية تفشل في العالمية. الاستنتاجات: الشبكات العصبية كاملة حسب تورينغ مثل بوابات NAND، محولة التركيز من "هل يمكنها؟" إلى "كيف ندربها بكفاءة؟". الشبكات العميقة تتفوق عمليًا للتسلسلات الهرمية، رغم كفاية الضحلة نظريًا.

### الفصل 5: لماذا يصعب تدريب الشبكات العصبية العميقة؟
رغم المزايا النظرية (مثل حساب التكافؤ الفعال)، فإن الشبكات العميقة تؤدي أداءً أقل من الضحلة على MNIST (~96.5% مقابل 96.9% لطبقتين، تنخفض إلى 96.5% لـ 4). تشبيهات الدوائر تسلط الضوء على قوة التجريد للعمق، لكن المتجهات الانحدارية المتلاشية تفسر الإخفاقات: تضمر منتجات قاعدة السلسلة \\(\partial C / \partial b^1 = \prod (w_j \sigma'(z_j)) \partial C / \partial a^L\\) بشكل أسي (\\(\sigma' \leq 0.25\\)، |w| <1). تحدث المتجهات الانحدارية المتفجرة إذا كان |w σ'| >1. عدم الاستقرار متأصل؛ الطبقات المبكرة تتعلم أبطأ بحوالي ~100 مرة. قضايا أخرى: التشبع، التهيئة الضعيفة. الاستنتاجات: مشاكل المتجه الانحداري خوارزمية، وليست معمارية—قابلة للحل عبر تفعيل/تهيئة أفضل، ممهدة الطريق لنجاح العمق.

### الفصل 6: التعلم العميق
بتطبيق الإصلاحات، تستغل الشبكات التلافيفية بنية الصورة: الحقول المستقبلة المحلية (مثل نواة 5x5)، والأوزان المشتركة (عدم التأثر بالانزياح)، والتجميع (مثل أقصى 2x2) يقللون المعاملات. تتطور MNIST: خط أساسي متصل بالكامل (97.8%) → تلافيفي-تجميع (99.1%) → ReLU + توسيع (99.4%) → إسقاط/مجموعات (99.7%، قريب من البشر). فائز KSH في ImageNet 2012 (شبكة تلافيفية 7 طبقات، دقة 84.7% أعلى 5) أشعل ثورات عبر ReLUs، الإسقاط، وحدات معالجة الرسومات. البشر يتطابقون/يتفوقون في المعايير لكن بشكل أبطأ. يستعرض الشبكات العصبية المتكررة (متواليات، مثل الكلام) والشبكات العميقة ذات الاعتقاد (توليدية). تكشف الأمثلة الخصمية عن فجوات. الاستنتاجات: العمق + التلافيفية + التقنيات تمكن الإنجازات؛ المستقبل: واجهات النية، علم البيانات، لكن الذكاء العام يحتاج رؤى متكاملة.

### الملحق: هل توجد خوارزمية بسيطة للذكاء؟
بتأمل يتجاوز الشبكات، يجادل أن الذكاء قد ينبع من مبادئ بسيطة (مثل الجاذبية التي توحد الحركات)، وليس من 100 تريليون اتصال في الدماغ (70 كوادريليون بت) بل من فرق ~125 مليون بت في الجينوم عن الشمبانزي. إعادة توصيل ابن مقرض تشير إلى قواعد حسية مشتركة؛ علم النفس التطوري يشير إلى التخصص. "مجتمع العقل" لمينسكي يعارض بعوامل متنوعة. الاستنتاجات: التفاؤل يدفع الاكتشاف—اسعَ وراء خوارزميات بسيطة، حتى لو كانت معقدة؛ قد يتطلب الذكاء الاصطناعي "100 جائزة نوبل" من الأفكار.

[Neural Networks and Deep Learning](http://neuralnetworksanddeeplearning.com/)