---
audio: false
generated: true
image: false
lang: ar
layout: post
model: x-ai/grok-4-fast
title: التحكم في درجة حرارة توليد النصوص بالشبكات العصبية المتكررة
translated: true
type: note
---

### فهم درجة الحرارة في أخذ العينات للشبكات العصبية المتكررة

في الشبكات العصبية المتكررة (RNNs) المستخدمة في توليد النص—مثل تلك التي يستكشفها أندريه كارباثي في منشور مدونته الشهير حول نماذج اللغة على مستوى الحرف—فإن `temperature` هي معلمة فائقة تضبط مدى "العشوائية" أو "القدرة على التنبؤ" للنص المُنتَج. يتم تطبيقها خلال خطوة أخذ العينات، حيث تتنبأ الـ RNN بالحرف (أو الرمز) التالي بناءً على الأحرف السابقة. بدون التحكم في درجة الحرارة، يمكن أن يكون التوليد جامدًا جدًا (اختيار الحرف التالي الأكثر احتمالية دائمًا، مما يؤدي إلى حلقات مملة) أو جامحًا جدًا (عشوائية محضة). تهدف درجة الحرارة إلى تحقيق توازن من خلال تليين التوزيع الاحتمالي للنموذج على الأحرف التالية المحتملة.

#### الرياضيات السريعة وراءها
تُخرج الـ RNN *قيم logits* (درجات أولية غير معيارية) لكل حرف تالي محتمل. يتم تحويل هذه القيم إلى احتمالات باستخدام دالة softmax:

\\[
p_i = \frac{\exp(\text{logit}_i / T)}{\sum_j \exp(\text{logit}_j / T)}
\\]

- \\(T\\) هي درجة الحرارة (عادة بين 0.1 و 2.0).
- عندما تكون \\(T = 1\\)، تكون هذه هي دالة softmax القياسية: تعكس الاحتمالات "الثقة" الطبيعية للنموذج.
- بعد ذلك، تقوم *بأخذ عينة* للحرف التالي من هذا التوزيع (على سبيل المثال، عبر أخذ عينات متعددة الحدود) بدلاً من اختيار الحرف ذي الاحتمالية الأعلى دائمًا (الاستدلال الجشع).

يحدث أخذ العينات هذا بشكل متكرر: قم بتغذية الحرف المختار مرة أخرى كمدخل، وتنبأ بالحرف التالي، وكرر العملية لتوليد تسلسل.

#### درجة الحرارة المنخفضة: متكررة ولكن آمنة
- **التأثير**: \\(T < 1\\) (على سبيل المثال، 0.5 أو قريبة من الصفر) *تُحدث تضييقًا* في التوزيع. تزداد احتمالات التنبؤات عالية الثقة بشكل أكبر، بينما تُسحق التنبؤات منخفضة الثقة نحو الصفر.
- **المخرجات**: يبقى النص "آمنًا" ومترابطًا ولكنه سرعان ما يصبح متكررًا. يلتزم النموذج بالمسارات الأكثر احتمالية، وكأنه محاصر في حلقة.
- **مثال من منشور كارباثي** (توليد نصوص على طراز مقالات Paul Graham): في درجات الحرارة المنخفضة جدًا، ينتج النموذج شيئًا مثل:
  > "is that they were all the same thing that was a startup is that they were all the same thing that was a startup is that they were all the same thing that was a startup is that they were all the same"

  النص واثق نحويًا ولكنه يفتقر إلى الإبداع—يشبه أصداء لا نهائية من بيانات التدريب.

#### درجة الحرارة المرتفعة: مبدعة ولكن غير منتظمة
- **التأثير**: \\(T > 1\\) (على سبيل المثال، 1.5 أو 2.0) *تعمل على تسطيح* التوزيع. تصبح الاحتمالات أكثر تجانسًا، مما يعطي الأحرف الأقل احتمالية فرصة أفضل.
- **المخرجات**: نص أكثر تنوعًا وإبداعًا، لكنه قد ينحرف نحو الهراء، أو أخطاء الهجاء، أو الاستطرادات البرية. يستكشف النموذج المسارات النادرة، مما يمكن أن يشعل الإبداع ولكن على حساب الترابط المنطقي.
- **مثال من كارباثي**: درجات الحرارة المرتفعة تنتج مقالات متنوعة بعبارات جديدة، ولكن "على حساب المزيد من الأخطاء (مثل أخطاء الهجاء، إلخ)". يشبه الأمر أن الـ RNN تقوم بعصف ذهني بحرية—ممتع لكتابة الشعر أو الخيال العلمي، لكنه محفوف بالمخاطر للكتابة الواقعية.

#### لماذا هي مهمة في الـ RNNs
الشبكات العصبية المتكررة (أو الـ LSTMs في إعداد كارباثي) هي autoregressive: كل تنبؤ يبني على سابقه، لذا فإن التحيزات الصغيرة في أخذ العينات تتراكم على التسلسلات الطويلة. تتيح لك درجة الحرارة ضبط التوازن بين الاستكشاف والاستغلال:
- استخدم \\(T\\) منخفضة للنص الموثوق والمتناسق مع الهوية (على سبيل المثال، أوصاف المنتجات).
- ارفعها للعصف الذهني أو التوليد الفني (على سبيل المثال، القصص السريالية).
- جرب ذلك في الكود: يحتوي مستودع كارباثي للـ RNN الأساسي على منزلق لدرجة الحرارة—جربه على عينات من شكسبير لترى التحول من السوناتات الباهتة إلى الأبيات الفوضوية.

في الجوهر، درجة الحرارة لا "تتعلم" أي شيء جديد؛ إنها مجرد ضبط لعنصر الإبداع في الـ RNN أثناء الاستدلال. المنخفضة تبقيها هادئة وقابلة للتنبؤ؛ المرتفعة تطلق العنان للفوضى.

[The Unreasonable Effectiveness of Recurrent Neural Networks](http://karpathy.github.io/2015/05/21/rnn-effectiveness/)