---
audio: false
generated: true
image: false
lang: ar
layout: post
model: x-ai/grok-4-fast
title: مقارنة صيغ دقة الفاصلة العائمة
translated: true
type: note
---

### الفروقات بين float32 و float16 و bfloat16

هذه أنواع بيانات للنقطة العائمة تُستخدم في الحوسبة، خاصة في أطر عمل التعلم الآلي مثل PyTorch (من مقتطف الكود الذي شاركته). تختلف في عرض البت، مما يؤثر على الدقة (مدى دقة تمثيل الأرقام)، المدى الديناميكي (مدى القيم التي يمكنها التعامل معها)، استخدام الذاكرة، وسرعة الحساب. إليك تحليل مفصل:

- **float32 (الدقة المفردة)**:
  - 32 بت إجمالاً: 1 بت للإشارة، 8 بت للأُس، 23 بت للجزء الكسري (المانتيسا).
  - دقة عالية ومدى ديناميكي واسع (يسمح الأُس بقيم تتراوح تقريبًا من 1e-38 إلى 1e+38).
  - الإيجابيات: دقيق لمعظم العمليات الحسابية، يتجنب الفائض/التفريغ في العديد من السيناريوهات.
  - السلبيات: استخدام أعلى للذاكرة (4 بايت لكل رقم) وحساب أبطأ على وحدات معالجة الرسومات، حيث يتطلب نطاقًا تردديًا وقوة معالجة أكبر.
  - شائع في الحوسبة التقليدية القائمة على وحدة المعالجة المركزية أو عندما تكون الدقة الكاملة مطلوبة.

- **float16 (نصف الدقة)**:
  - 16 بت إجمالاً: 1 بت للإشارة، 5 بت للأُس، 10 بت للجزء الكسري.
  - دقة أقل ومدى ديناميكي أضيق (يحد الأُس من القيم إلى حوالي 1e-7 إلى 65504).
  - الإيجابيات: يخفض استخدام الذاكرة إلى النصف (2 بايت لكل رقم) ويسرع العمليات الحسابية على الأجهزة التي تدعمه (مثل وحدات معالجة الرسومات الحديثة)، مما يجعله رائعًا للنماذج الكبيرة مثل النماذج اللغوية الكبيرة حيث تكون الذاكرة عائقًا.
  - السلبيات: عرضة للفائض (أرقام كبيرة) أو التفريغ (أرقام صغيرة/تدرجات)، مما قد يسبب مشاكل مثل NaNs (ليس رقمًا) أثناء التدريب. كما أنه يفقد المزيد من التفاصيل في التمثيلات.

- **bfloat16 (نقطة عائمة دماغية)**:
  - 16 بت إجمالاً: 1 بت للإشارة، 8 بت للأُس، 7 بت للجزء الكسري.
  - يطابق المدى الديناميكي لـ float32 (نفس بتات الأُس، لذا مدى القيم مشابه) ولكن بدقة أقل (بتات جزئ كسري أقل).
  - الإيجابيات: نفس توفير الذاكرة مثل float16 (2 بايت)، ولكن استقرار أفضل في التعلم العميق بسبب المدى الأوسع – أقل عرضة للفائض/التفريغ. مصمم للشبكات العصبية ويؤدي أداءً جيدًا في التدريب دون الحاجة إلى الكثير من التحجيم أو التطبيع.
  - السلبيات: دقة أقل حتى من float16، مما قد يؤدي إلى أخطاء تقريبية أكثر قليلاً، ولكن عمليًا، غالبًا ما تكون ضئيلة للنماذج اللغوية الكبيرة.

في الكود الذي عرضته (`dtype = 'bfloat16' if torch.cuda.is_available() and torch.cuda.is_bf16_supported() else 'float16'`)، يتم اختيار bfloat16 إذا كانت وحدة معالجة الرسومات تدعمه (شائع على أجهزة NVIDIA/AMD الأحدث)، والتراجع إلى float16 otherwise. هذا من أجل إعدادات الدقة المختلطة، حيث تستخدم العمليات الحسابية دقة منخفضة للسرعة مع الاحتفاظ ببعض الأجزاء (مثل المجمعات) بدقة أعلى للحفاظ على الدقة. يُفضل bfloat16 في العديد من الإعدادات الحديثة (على سبيل المثال، من قبل Google لوحدات معالجة Tensor) لأنه يتصرف بشكل أشبه بـ float32 من حيث المدى، مما يقلل من عدم استقرار التدريب.

### طرق التكميم وكيفية ارتباطها

التكميم هو تقنية لتقليل عرض البت لأوزان النموذج، والتفعيلات، وأحيانًا التدرجات، مما يضغط النماذج بشكل يتجاوز مجرد استخدام float16/bfloat16. إنه ليس هو نفسه تبديل أنواع البيانات كما في الكود الخاص بك (والذي يتعلق أكثر بدقة النقطة العائمة أثناء وقت التشغيل)، لكنه مرتبط لأن كلاهما يهدف إلى التحسين للكفاءة في النماذج اللغوية الكبيرة.

- **ما هو التكميم؟**
  - يعين قيم عالية الدقة (مثل float32) إلى تمثيلات منخفضة البت (مثل int8, int4، أو حتى نقطة عائمة مخصصة). هذا يقلص البصمة التخزينية للذاكرة ووقت الاستدلال، وهو أمر حاسم لنشر النماذج اللغوية الكبيرة على الأجهزة الطرفية أو على نطاق واسع.
  - مثال: وزن float32 (32 بت) قد يتم تقليده إلى int8 (8 بت)، مما يقلل الحجم بمقدار 4x.

- **طرق التكميم الشائعة**:
  - **التكميم بعد التدريب (PTQ)**: يُطبق بعد التدريب. بسيط ولكن يمكن أن يقلل الدقة إذا لم يتم معايرته (مثل استخدام مجموعة بيانات صغيرة لضبط المقاييس). طرق مثل تحجيم الحد الأدنى-الأقصى أو القائمة على المدرج التكراري (مثل في TensorRT أو ONNX).
  - **التدريب الواعي بالتكميم (QAT)**: محاكاة التكميم أثناء التدريب (مثل عمليات التكميم الزائفة في PyTorch)، بحيث يتعلم النموذج التعامل مع الدقة المخفضة. أكثر دقة ولكنه يتطلب إعادة تدريب.
  - **المتغيرات المتقدمة**:
    - **تكميم الأوزان فقط**: يتم تقليص الأوزان فقط (مثل إلى int4)، مع الاحتفاظ بالتفعيلات في float16/bfloat16.
    - **التكميم المجمع**: يتم التقليص في مجموعات (مثل طريقة GPTQ التي تجمع الأوزان لدقة أفضل).
    - **AWQ (التكميم الوزني الواعي بالتفعيل)**: يأخذ في الاعتبار توزيعات التفعيل لقص أفضل.
    - **INT4/INT8 مع إزالة التكميم**: أثناء الاستدلال، يتم إزالة التكميم back إلى float16 للحساب.

- **العلاقة مع float16/bfloat16/float32**:
  - اختيار نوع البيانات الخاص بك هو شكل من أشكال *الدقة المختلطة* (مثل AMP في PyTorch)، والذي يستخدم float16/bfloat16 لمعظم العمليات ولكن يتوسع إلى float32 لمنع التفريغ. التكميم يذهب إلى أبعد من ذلك باستخدام الأعداد الصحيحة أو حتى نقطة عائمة منخفضة البت.
  - يرتبطان في خطوط الأنابيب التحسينية: ابدأ بالتدريب بـ float32، انتقل إلى bfloat16 للتدريب الأسرع، ثم كمم إلى int8 للنشر. على سبيل المثال، تستخدم مكتبات مثل Hugging Face Transformers `torch_dtype=bfloat16` أثناء التحميل، ثم تطبق التكميم (عبر BitsAndBytes مثلًا) للتقليل إلى 4-bit.
  - المفاضلة: الدقة الأقل/التكميم تسرع الأمور ولكنها تخاطر بفقدان الدقة (مثل زيادة الالتباس في النماذج اللغوية الكبيرة). غالبًا ما تكون bfloat16 نقطة مثالية قبل التكميم الكامل.

### العلاقة مع الانتباه السريع (Flash Attention)

الانتباه السريع هو خوارزمية مُحسنة لحساب الانتباه في المحولات (جزء رئيسي من النماذج اللغوية الكبيرة مثل GPT). يقلل استخدام الذاكرة ويسرع من خلال إعادة حساب المتوسطات على الطاير بدلاً من تخزينها، وهو مفيد بشكل خاص للتسلسلات الطويلة.

- **كيف ترتبط الدقة**:
  - يدعم الانتباه السريع (عبر `torch.nn.functional.scaled_dot_product_attention` أو مكتبة flash-attn) الدقة المنخفضة مثل float16/bfloat16 بشكل أصلي. في الواقع، غالبًا ما يكون أسرع في هذه الأنواع لأن وحدات معالجة الرسومات (مثل NVIDIA Ampere+) لديها تسريع بالأجهزة لها (مثل Tensor Cores).
  - يؤثر اختيار نوع البيانات في الكود الخاص بك مباشرة على ذلك: استخدام bfloat16 أو float16 يمكن وضع الأداء العالي للانتباه السريع، حيث يمكنه دمج العمليات وتجنب اختناقات الذاكرة. في float32، قد يعود إلى تطبيقات أبطأ.
  - يرتبط التكميم أيضًا – يمكن للنماذج المكممة استخدام الانتباه السريع إذا تمت إزالة التكميم إلى float16 أثناء الحساب. تقوم مكتبات مثل vLLM أو ExLlama بدمج الانتباه السريع مع التكميم لاستدلال فائق السرعة.

في PyTorch، إذا قمت بتعيين `torch.backends.cuda.enable_flash_sdp(True)`، فإنه يفضل الانتباه السريع عندما يكون نوع البيانات هو float16/bfloat16 ويدعمه الجهاز.

### الاستخدام العام لدقة الفاصلة العائمة في نماذج LLM

في النماذج اللغوية الكبيرة مثل GPT، Llama، أو Grok:

- **التدريب**: غالبًا ما يبدأ بـ float32 للاستقرار، لكنه ينتقل إلى bfloat16 (مثل في نماذج Google) أو الدقة المختلطة (float16 مع تحجيم float32) للتعامل مع مجموعات البيانات الضخمة بشكل أسرع. bfloat16 تكتسب شعبية (مثل في PyTorch 2.0+) لأنها تتطلب ضبطًا أقل للمعاملات الفائقة مقارنة بـ float16.
- **الاستدلال/النشر**: float16 أو bfloat16 شائع للسرعة على وحدات معالجة الرسومات. لمزيد من الكفاءة (مثل على أجهزة المستهلك)، يتم تقليص النماذج إلى 8-bit أو 4-bit (مثل تنسيق GGUF في llama.cpp). هذا يسمح بتشغيل نماذج ذات 70B معامل على <24GB من ذاكرة الوصول العشوائي للرسومات.
- **المفاضلات**:
  - دقة أعلى (float32): دقة أفضل، ولكن أبطأ وأكثر جوعًا للذاكرة.
  - دقة أقل: أسرع، أخف، ولكن إمكانية تدهور المخرجات (مثل زيادة الهلوسات قليلاً في التكميم المتطرف).
- **الاتجاهات**: مع الأجهزة مثل NVIDIA H100 (دعم قوي لـ bfloat16)، أصبح bfloat16 معياريًا. تجمع أدوات مثل PEFT (الضبط الدقيق الفعال للمعاملات) بينه وبين التكميم للضبط الدقيق. في المصادر المفتوحة، تتولى مكتبات مثل Transformers أو Optimum التعامل مع تحويل نوع البيانات التلقائي والتكميم.

إذا كنت تنفذ هذا في الكود، اختبر على جهازك – يبرز bfloat16 على وحدات معالجة الرسومات الأحدث، بينما float16 أكثر عالمية ولكنه أصعب مع التدرجات.

المراجع:  
[Understanding Floating Point Precision in Deep Learning](https://towardsdatascience.com/understanding-fp32-fp16-bf16-in-neural-networks-27075c96b4d3)  
[Quantization Techniques for LLMs](https://huggingface.co/docs/transformers/quantization)  
[Flash Attention Explained](https://arxiv.org/abs/2205.14135)  
[Mixed Precision Training in PyTorch](https://pytorch.org/docs/stable/amp.html)