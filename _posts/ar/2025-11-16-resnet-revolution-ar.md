---
audio: false
generated: true
image: false
lang: ar
layout: post
model: x-ai/grok-4-fast
title: ثورة الريزنيت في التعلم العميق
translated: true
type: note
---

ResNet (الشبكة المتبقية) هي واحدة من أهم الإنجازات في تاريخ التعلم العميق. تم تقديمها في عام 2015 من قبل كايمينغ هو وزملاؤه من Microsoft Research وفازت بمسابقة ImageNet ذلك العام بمعدل خطأ مذهل في أفضل 5 نتائج بلغ 3.57% فقط (أفضل من الأداء البشري في ذلك الوقت).

إليك الأسباب التي جعلت ResNet تكتسب هذه الأهمية الكبيرة:

1.  **حلت "مشكلة التدهور" (اختفاء/انفجار التدرجات في الشبكات العميقة جدًا)**  
    قبل ResNet، كان الاعتقاد السائد "إذا كانت 20 طبقة تعمل بشكل جيد، فإن 56 طبقة يجب أن تعمل بشكل أفضل". في الممارسة العملية، جعلت إضافة المزيد من الطبقات الدقة أسوأ — ليس بسبب الإفراط في التخصيص، ولكن لأن عملية التحسين نفسها أصبحت أصعب. أثبتت ResNet أنه يمكنك تدريب شبكات ذات 100+ طبقة (عرضوا 152 طبقة، ولاحقًا وصل الآخرون إلى 1000+ طبقة) ولا تزال تحصل على أداء أفضل.

2.  **الكتل المتبقية + وصلات التخطي (الفكرة الرئيسية)**  
    بدلاً من مطالبة الطبقة بتعلم H(x) مباشرة، تتعلم ResNet المتبقي F(x) = H(x) − x، لذا يحسب الكتلة x + F(x).  
    رياضياً:  
    y = F(x, {W_i}) + x  
    يسمح "الاختصار المتطابق" هذا للتدرجات بالتدفق مباشرة عبر الشبكة عبر وصلات التخطي، مما يجعل التدريب العميق جدًا ممكنًا.

3.  **تأثير عملي فوري وضخم**  
    - أصبحت العمود الفقري لجميع نماذج الرؤية المتطورة تقريبًا من 2016 إلى ~2020 (ولا تزال مستخدمة على نطاق واسع اليوم).  
    - استخدمت في الكشف عن الأشياء (Faster R-CNN + ResNet)، والتجزئة (DeepLab, Mask R-CNN)، والنماذج التوليدية (StyleGAN، إلخ)، والفيديو، والتصوير الطبي، إلخ.  
    - المتغيرات مثل ResNet-50 و ResNet-101 و ResNeXt و Wide ResNet و ResNet-RS لا تزال معايير أساسية قياسية في 2025.

4.  **مكّنت عصر "التعلم العميق يصبح عميقًا حقًا"**  
    قبل ResNet: أفضل نماذج ImageNet كانت تحتوي على ~20–30 طبقة (VGG, GoogLeNet).  
    بعد ResNet: أصبحت 100–1000+ طبقة أمرًا طبيعيًا. كان هذا العمق حاسمًا لتعلم ميزات أكثر ثراءً.

5.  **فكرة بسيطة للغاية، لكنها تحويلية**  
    الورقة البحثية مشهورة بوضوحها وتضيف حيلة ذكية واحدة فقط (وصلات التخطي + الجمع العنصري). لا تحتوي على دوال تنشيط جديدة فاخرة، ولا دوال خسارة جديدة — مجرد بصيرة معمارية.

باختصار: أثبتت ResNet أن "الأعمق أفضل" يمكن أن يعمل حقًا إذا قمت بتصميم البنية بشكل صحيح، وأصبحت نقطة البداية الافتراضية لكل نموذج رؤية حاسوبية تقريبًا في العقد التالي.

حتى اليوم في عام 2025، عندما تهيمن نماذج Transformer و ConvNeXt/ViT على قوائم المتصدرين، لا تزال ResNet-50 هي العمود الفقري القياسي الأكثر شيوعًا للمقارنة لأنها سريعة، مفهومة جيدًا، وموثوقة للغاية.

الورقة البحثية (لا تزال تستحق القراءة):  
[Deep Residual Learning for Image Recognition](https://arxiv.org/abs/1512.03385)