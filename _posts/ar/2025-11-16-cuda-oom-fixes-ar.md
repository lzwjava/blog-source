---
audio: false
generated: true
image: false
lang: ar
layout: post
model: x-ai/grok-4-fast
title: إصلاحات خطأ نفاد ذاكرة CUDA
translated: true
type: note
---

### تحليل المشكلة
يفشل سكريبت التدريب من nanoGPT بخطأ نفاد ذاكرة CUDA (OOM) أثناء تقدير الخسارة الأولية (`estimate_loss()`) في التمرير الأمامي. يحدث هذا مباشرة بعد تهيئة النموذج وتحويله. المشكلة الأساسية هي استنفاد الذاكرة عند تخصيص مخزن مؤقت كبير لمخرجات النموذج (logits):

- **تفاصيل المخزن المؤقت**: موتر ذو شكل `(65536, 50304)` بنوع `bfloat16` (2 بايت لكل عنصر)، أي ما يعادل ~6.14 جيجابايت. وهذا يتوافق مع:
  - حجم الدفعة: 64
  - طول التسلسل (block_size): 1024
  - إجمالي الرموز (tokens): 64 × 1024 = 65,536
  - حجم المفردات: 50,304 (الإعداد الافتراضي لـ GPT-2)
- **حالة وحدة معالجة الرسومات**: السعة الإجمالية لوحدة معالجة الرسومات الخاصة بك (على الأرجح RTX 3060 أو مشابهة بسعة 12 جيجابايت من VRAM) هي 11.69 جيجابايت، ولكن لم يتبق سوى 2.68 جيجابايت مجانية في وقت التخصيص. تستخدم العملية بالفعل ~7.04 جيجابايت (6.78 جيجابايت بواسطة PyTorch)، مما لا يترك هامشًا كافيًا بعد احتساب النموذج (~124 مليون معامل × 2 بايت في bfloat16 ≈ 248 ميجابايت)، وحالات المُحسّن (~1-2 جيجابايت لـ AdamW)، وذاكرة التخزين المؤقت للتحويل، والتفعيلات، والنفقات العامة.

هذا شائع بالنسبة للنماذج بحجم GPT-2 (124 مليون معامل) على وحدات معالجة الرسومات الاستهلاكية عند استخدام أحجام دفعات كبيرة أو أطوال تسلسل طويلة، خاصة عند تمكين `torch.compile`، والذي يمكن أن يزيد مؤقتًا من استخدام الذاكرة أثناء التقاط الرسم البياني وتحسينه.

### الأسباب الجذرية
1. **حجم الدفعة كبير (64)**: بالاقتران مع `block_size=1024`، يخلق هذا موترات وسيطة ضخمة (مثل logits، ومخرجات الاهتمام). تدفع الرموز الفعالة لكل تكرار (65,536) حدود VRAM إلى أقصى حد.
2. **تحويل النموذج**: يستخدم `torch.compile` (المفعل افتراضيًا) Torch Inductor، الذي يولد نواة ومخازن مؤقتة مؤقتة لـ CUDA. يشير التحذير `[0/0] Not enough SMs to use max_autotune_gemm mode` إلى أن وحدات المعالجة الدقيقة (SMs) في وحدة معالجة الرسومات الخاصة بك محدودة للضبط التلقائي العدواني، مما قد يزيد من التجزئة.
3. **نوع البيانات والدقة**: استخدام `bfloat16` (عبر `torch.cuda.amp`)، لكن تحذير `GradScaler` القديم يشير إلى احتمالية وجود عدم كفاءة. العمليات الأخرى أو عمليات التشغيل السابقة قد تكون قامت بتجزئة VRAM.
4. **نفقات التقييم**: `estimate_loss()` يقوم بتشغيل تمريرات أمامية على بيانات التقييم (eval_iters=200، ولكن مجمعة)، مما يفاقم المشكلة قبل أن يبدأ التدريب حتى.
5. **استخدام الذاكرة الموجود مسبقًا**: يشير تخصيص ~7 جيجابايت مسبقًا إلى أن النموذج والمحسن ومحمل البيانات استهلكوا المساحة مقدمًا. قد تتضمن الذاكرة غير التابعة لـ PyTorch (224.90 ميجابايت بواسطة العملية) سياق CUDA أو المكتبات.

### الإصلاحات الموصى بها
ابدأ بأبسط التغييرات في `config/train_openwebtext.py` (أو تجاوزها عبر سطر الأوامر). أعد التشغيل بعد كل تعديل لعزل ما ينجح. الهدف: تقليل ذروة VRAM إلى ~8-9 جيجابايت مع الحفاظ على جودة التدريب.

#### 1. **تقليل حجم الدفعة (الإصلاح الأساسي)**
   - عيّن `batch_size = 4` (أو حتى 1-2 في البداية) لخفض المخزن المؤقت لـ logits إلى ~0.38 جيجابايت (لـ batch=4).
   - عوّض ذلك بـ `gradient_accumulation_steps = 16` (دفعة فعالة = 64، ولكن ذروة ذاكرة أقل).
   - **لماذا؟** يتغير حجم الدفعة خطيًا مع الذاكرة لمعظم الموترات. هذا هو الأكثر فعالية لـ OOM دون إبطاء التدريب كثيرًا.
   - مقتطف التكوين المحدث:
     ```
     batch_size = 4
     gradient_accumulation_steps = 16  # اضبط لتطابق الدفعة الفعالة الأصلية
     ```
   - VRAM المتوقع: ~4-6 جيجابايت إجمالاً.

#### 2. **تعطيل أو تحسين التحويل**
   - أضف `compile = False` لتخطي `torch.compile`، وتجنب نفقات Inductor (~1-2 جيجابايت زيادة مؤقتة).
   - إذا كنت تحتفظ بالتحويل، أضف `mode='reduce-overhead'` للحصول على نواة أسرع ولكن أقل تحسينًا.
   - التكوين المحدث:
     ```
     compile = False
     ```
   - **بديل**: شغّل باستخدام `torch._dynamo.config.suppress_errors = True` في السكريبت لتصحيح الأخطاء، ولكن أصلح OOM أولاً.

#### 3. **تقليل طول التسلسل**
   - عيّن `block_size = 512` (نصف السياق) لخفض الرموز/التكرار إلى ~32,768، مما يخفض ذاكرة logits إلى النصف (~3.07 جيجابايت).
   - المقايضة: قد يؤدي السياق الأقصر إلى الإضرار بجودة النموذج قليلاً، ولكن يمكن تعويضه بمزيد من التدريب.
   - التكوين:
     ```
     block_size = 512
     ```

#### 4. **تعديلات إدارة الذاكرة**
   - **متغير البيئة للتجزئة**: كما هو مقترح في الخطأ، عيّن `export PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True` قبل التشغيل. يسمح هذا لـ PyTorch باستخدام مقاطع ذاكرة قابلة للتوسيع على CUDA 12+ (يقلل الهدر من الكتل المحجوزة غير المستخدمة).
   - **مسح ذاكرة التخزين المؤقت يدويًا**: أضف `torch.cuda.empty_cache()` بعد تهيئة النموذج في `train.py` (حوالي السطر 100)، ولكن هذا حل مؤقت.
   - **استخدام التفريغ إلى وحدة المعالجة المركزية**: للتقييم فقط، عدّل `estimate_loss()` لاستخدام دفعات أصغر أو وحدة المعالجة المركزية للأجزاء غير الحرجة، ولكن هذا يبطئ الأمور.
   - **مراقبة VRAM**: شغّل `nvidia-smi -l 1` في طرفية أخرى لمراقبة الاستخدام في الوقت الفعلي.

#### 5. **تعديلات أخرى على التكوين**
   - زِد `eval_interval = 1000` وقلل `eval_iters = 50` لتخفيف حمل التقييم (ذاكرة أقل اضطرابًا في البداية).
   - إذا كنت تستخدم إعدادًا متعدد وحدات معالجة الرسومات، فعّل DDP، ولكن يبدو أن هذا الإعداد يستخدم وحدة معالجة رسومات واحدة.
   - لمشاكل bfloat16: تأكد من دعم وحدة معالجة الرسومات الخاصة بك له (Ampere+ مثل سلسلة RTX 30). إذا لم يكن كذلك، فرض `dtype = 'float16'` في السكريبت.
   - إصلاح تحذير القديم: في `train.py` السطر 196، غيّر إلى `scaler = torch.amp.GradScaler('cuda', enabled=(dtype == 'float16'))`.

#### 6. **فحوصات الأجهزة/الإعداد**
   - **تحرير VRAM**: أوقف عمليات وحدة معالجة الرسومات الأخرى (استخدم `nvidia-smi` لسرد PIDs، ثم `kill -9 <PID>`). أعد تشغيل جلسة الطرفية الخاصة بك.
   - **مجموعة البيانات**: تأكد من تحميل `openwebtext_local` بكفاءة؛ يمكن لمجموعات البيانات الكبيرة التحميل المسبق في RAM/VRAM عبر DataLoader.
   - **إصدار PyTorch/CUDA**: تحقق من التوافق (مثل PyTorch 2.4+ مع CUDA 12). أعد التثبيت إذا لزم الأمر: `pip install torch --index-url https://download.pytorch.org/whl/cu121`.
   - **التراجع إلى وحدة المعالجة المركزية**: للاختبار، أضف `device = 'cpu'` و `compile = False`، ولكن توقع تدريبًا بطيئًا جدًا (ساعات لكل iter مقابل ثوانٍ على وحدة معالجة الرسومات).

### النتيجة المتوقعة
مع `batch_size=4`، و`gradient_accumulation_steps=16`، و`compile=False`، يجب أن تتجاوز OOM وتبدأ التدريب. سيكون إجمالي وقت التشغيل لـ max_iters=20,000 أطول بسبب التراكم، لكن راقب الخسارة عبر `log_interval=100`. إذا استمر الفشل، اخفض batch_size أكثر وقم بزيادته لاحقًا.

إذا لم تحل هذه الإجراءات المشكلة، شارك طراز وحدة معالجة الرسومات الخاصة بك (`nvidia-smi`)، وإصدار PyTorch (`torch.__version__`)، والتكوين/السجل المحدث للحصول على نصائح أكثر استهدافًا.