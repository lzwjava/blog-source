---
audio: false
generated: true
image: false
lang: ar
layout: post
model: x-ai/grok-code-fast-1
title: لورا لضبط الكفاءة في النماذج اللغوية الكبيرة
translated: true
type: note
---

### ملخص منشور المدونة حول LoRA من Thinking Machines

يوفر منشور المدونة من Thinking Machines (بعنوان "LoRA" على موقعهم) شرحًا متعمقًا لتقنية التكيف ذي الرتبة المنخفضة (LoRA)، وهي تقنية لضبط نماذج اللغة الكبيرة (LLMs) بكفاءة باستخدام الحد الأدنى من الموارد الحسابية. يقوم المنشور بتفكيك الفكرة الأساسية لـ LoRA وتنفيذها ومزاياها وتطبيقاتها العملية، بهدف جعل المفهوم في متناول القراء المطلعين على أساسيات التعلم الآلي.

#### المفهوم الأساسي لـ LoRA

تتناول تقنية LoRA تحدي تكيف نماذج اللغة الكبيرة المدربة مسبقًا، والتي يمكن أن تحتوي على مليارات المعاملات، لمهام جديدة دون إعادة تدريب النموذج بالكامل. بدلاً من تحديث جميع الأوزان، تقدم التقنية "تكيفات ذات رتبة منخفضة" من خلال تجميد النموذج الأصلي وإضافة مصفوفات ذات رتبة منخفضة قابلة للتدريب إلى طبقات محددة. هذا يقلل عدد المعاملات القابلة للتدريب بشكل كبير، أحيانًا بمقدار 10,000 مرة، مع تحقيق أداء مماثل للضبط الدقيق الكامل.

تشمل الآليات الرئيسية:
- **التحليل**: يتم تقريب تحديث الوزن \\(\Delta W\\) كـ \\(A \times B\\)، حيث \\(A\\) هي \\(d \times r\\) و \\(B\\) هي \\(r \times k\\)، مع كون \\(r\\) (الرتبة) أصغر بكثير من \\(d\\) أو \\(k\\).
- **نقاط الحقن**: تتم عادةً إضافة طبقات LoRA إلى وحدات الانتباه (مصفوفات الاستعلام، والمفتاح، والقيمة، والإسقاط) في المحولات (transformers)، لأن هذه هي الأكثر تحديدًا بالمهمة.
- **التخزين والاستدلال**: يخزن النموذج المكيف فقط مصفوفات \\(A\\) و \\(B\\) الصغيرة، وأثناء الاستدلال، يتم دمج أوزان LoRA مرة أخرى في الأوزان الأصلية للكفاءة.

#### المزايا والمفاضلات

يسلط المنشور الضوء على كفاءة LoRA للتدريب على وحدات معالجة الرسومات (GPUs) أصغر ببيانات أقل، مما يمكن من التكيف السريع لمهام مثل ضبط التعليمات أو الضبط الدقيق الخاص بمجال معين. يمكنها تحقيق أداء قريب من الضبط الدقيق الكامل مع تدريب 0.5-1% من المعاملات فقط. ومع ذلك، قد تؤدي أداءً أقل في المهام شديدة الاختلاف التي تتطلب تغييرات معمارية كبيرة، وهناك زيادة طفيفة في زمن الاستدلال بسبب عمليات ضرب المصفوفات.

#### التنفيذ والأمثلة

تتضمن المدونة مقاطع كود (في PyTorch) توضح كيفية إضافة طبقات LoRA، مثل تغليف الوحدات الخطية (linear modules) بفئة LoRA مخصصة. وهي توضح حالات الاستخدام مثل ضبط نماذج LLaMA أو GPT لمجموعات بيانات محددة، مع نصائح مثل البدء بالرتبة \\(r=8\\) وضبط معاملات LoRA فقط. كما تناقش ضبط المعاملات الفائقة (hyperparameter tuning)، مثل موازنة الرتبة والتقييد لتجنب الإفراط في التطبيع (overfitting).

#### السياق المقارن والإمكانات المستقبلية

يتم وضع LoRA كبديل للضبط الدقيق الكامل وطرق PEFT (الضبط الدقيق الفعال للمعاملات) الأخرى مثل ضبط المطالبة (prompt tuning) أو المحولات (adapters)، مع التأكيد على بساطتها وقابليتها للتطوير. يلاحظ المنشور اعتمادها المتزايد في المجتمعات مفتوحة المصدر وإمكاناتها في إعدادات التعلم المستمر. تم تضمين مراجع للأوراق العلمية الأصلية (مثل Hu et al.) للقراءة المتعمقة.

بشكل عام، يخدم المنشور كدليل عملي، يشجع على التجربة مع الإشارة إلى دور LoRA في ديمقراطية تخصيص نماذج اللغة الكبيرة.