---
audio: false
generated: true
image: false
lang: ar
layout: post
model: x-ai/grok-4-fast
title: تدريب على دفق نصي واحد مدمج
translated: true
type: note
---

في nanoGPT (وبشكل أساسي في أي تدريب على نمط GPT، بما في ذلك التدريب الأصلي لـ GPT-2/GPT-3)، **يجب أن تتدرب على تيار نصي واحد ضخم مدمج**، وليس على مقالات/مستندات فردية تُغذى واحدة تلو الأخرى كمتواليات منفصلة.

إليك السبب وكيف يتم ذلك عمليًا مع FineWeb (وفي كود Andrej الخاص بـ nanoGPT):

### ما يجب عليك فعله (الطريقة الصحيحة)
1. ادمج **جميع** المقالات الفردية في **ملف .txt واحد ضخم** (أو ملفات أجزاء كبيرة متعددة)، مفصولة فقط بواسطة رمز خاص لنهاية النص `<|endoftext|>` (أو أي فاصل تختاره).
2. أثناء التدريب، سيقوم nanoGPT بما يلي:
   - تحميل النص المدمج بالكامل في مصفوفة رموز عملاقة واحدة.
   - تقسيمه إلى كتل ذات طول ثابت (على سبيل المثال، block_size = 1024 رمز).
   - غالبًا ما تعبر هذه الكتل حدود المستندات — هذا ليس مقبولًا فحسب، بل إنه مرغوب فيه.

هذا بالضبط كيف دربت OpenAI نماذج GPT-2 و GPT-3، وهو ما يفعله Andrej مع Shakespeare و OpenWebText و FineWeb-Edu، إلخ.

### ما يفعله النص البرمجي extract_fineweb.py
هذا النص البرمجي الذي قمت بتشغيله يفعل بالفعل الشيء الصحيح:

```bash
$ python ./scripts/train/extract_fineweb.py
```

يقرأ جميع ملفات parquet، ويستخرج فقط عمود `text` من كل صف (أي كل صفحة ويب/مقال)، ويكتب النص الخام الواحد تلو الآخر في `fineweb_extracted.txt`، و(في الإصدارات الأحدث) يقوم تلقائيًا بإدخال الفاصل `<|endoftext|>` بين المستندات.

لذا تبدو الملف الناتج كالتالي:

```
<نص المقال 1> <|endoftext|> <نص المقال 2> <|endoftext|> <نص المقال 3> <|endoftext|> ...
```

هذا مثالي. يجب أن تتدرب على ذلك الملف الضخم المفرد (أو ملفات bin المجزأة التي سينشئها `train.py` منه).

### ما يجب عليك ألا تفعله
- لا **تغذي** كل مقال كمثال/تسلسل منفصل للنموذج وتبدأ الانتباه من جديد لكل مقال.
- لا **تقصّر** أو تملأ كل مقال ليكون بطول block_size بالضبط وتتدرب فقط داخل المستند الواحد.
- لا تستخدم **التجميع على مستوى المستند** حيث يكون كل تسلسل في الدفعة عبارة عن مستند كامل واحد.

هذا من شأنه أن يدمر قدرة النموذج على تعلم التبعيات طويلة المدى التي تمتد عبر مستندات متعددة وسيؤذي الأداء بشكل كبير.

### الملخص
نعم → تدرب على **ملف نصي واحد ضخم مدمج** (بالضبط ما ينتجه `extract_fineweb.py`).  
لا → لا **تتدرب** مقالًا تلو مقال كمتواليات منفصلة.

هكذا تم دائمًا تدريب نماذج GPT، وهو السبب في أن `train.py` الخاص بـ nanoGPT يعامل البيانات ببساطة كتيار طويل واحد من الرموز.

لذا فقط تابع سير العمل المعتاد لـ nanoGPT:

```bash
python scripts/train/extract_fineweb.py   # لقد فعلت هذا بالفعل
python train.py --dataset=your_extracted_file_or_fineweb10B etc.
```

أنت على المسار الصحيح!