---
audio: false
generated: false
image: false
lang: ar
layout: post
title: محادثة منشأة بالذكاء الاصطناعي حول DeepSeek V3
translated: true
---

يتناول النص هنا استكشاف نموذج DeepSeek v3، مع الإشارة إلى الفيديو "Multi-Head Latent Attention and Multi-token Prediction in Deepseek v3" [https://youtu.be/jL49fLOJYNg?si=4uE2kfe-BlKC1ngO](https://youtu.be/jL49fLOJYNg?si=4uE2kfe-BlKC1ngO). تم استخدام خدمة تحويل الكلام إلى نص من Google لنسخ الفيديو، بالإضافة إلى بعض الأكواد المساعدة لتنظيم النص.

---

أ: أهلاً بكم مرة أخرى في Deep tag. سنقوم اليوم بالتعمق في عالم نماذج اللغة الكبيرة. تحديدًا، DeepSeek V3.

ب: يبدو جيدًا. إنه نموذج يحتوي على 671 مليار معامل، ويحظى باهتمام كبير بسبب نهجه الفريد في الكفاءة والأداء، أليس كذلك؟

أ: وقد شاركت ورقة أكاديمية توضح بنيته.

ب: نعم.

أ: وكخبير في تعلم الآلة، تسعى لفهم كيف يحقق DeepSeek V3 أداءً عاليًا وتدريبًا اقتصاديًا في نفس الوقت.

ب: نعم، هذا صحيح.

أ: أوه، مرحبًا، كيف الحال؟

ج: MLA، التفاصيل، MLA وكيف تعمل.

أ: أوه، بالتأكيد. هذه فكرة رائعة. نعم، يمكننا بالتأكيد الغوص أكثر في الانتباه الكامن متعدد الرؤوس، أو MLA. إذن أنت مهتم بالتفاصيل الدقيقة لـ MLA. حسنًا، دعونا نفكك هذا. ذكرنا أن أحد مفاتيح كفاءة DeepSeek V3 هو بنية الخليط من الخبراء، أو MoE، أليس كذلك؟ حيث يتم تنشيط جزء فقط من المعاملات لكل رمز. ويأخذنا DeepSeek V3 خطوة أبعد مع MLA و DeepSeek Mo.

ب: هذا صحيح. لذا دعونا نركز حقًا على MLA الآن.

أ: حسنًا. إذن في التطبيقات في الوقت الحقيقي، السرعة أمر بالغ الأهمية.

ب: نعم. وذاكرة التخزين المؤقت للمفاتيح والقيم المطلوبة أثناء الاستدلال يمكن أن تشكل عقبة كبيرة.

أ: بالضبط. هنا يأتي دور MLA. حسنًا، إذن آلية الانتباه التقليدية تتطلب تخزين الكثير من المعلومات حول الرموز السابقة.

ب: نعم، وهو كما يمكنك أن تتخيل، يصبح مشكلة مع سلاسل النص الطويلة، أليس كذلك؟

أ: لكن MLA تضغط هذه المعلومات بذكاء، حسنًا، لتقليل تدفق الذاكرة المؤقتة بشكل كبير، مما يجعل الاستدلال أسرع بكثير. إذن الأمر يشبه أخذ موسوعة ضخمة وتكثيفها إلى النقاط الرئيسية فقط.

ب: إنها تشبيه رائع. إنه يحتفظ بالمعلومات الأساسية دون الأوزان غير الضرورية. نعم، لذا فهو مفيد حقًا للتطبيقات في الوقت الحقيقي.

أ: نعم. الآن دعونا نتحدث عن كيفية عمله فعليًا. حسنًا، إذن كيف يحقق MLA هذا الضغط؟

ب: حسنًا، يستخدم ضغطًا مشتركًا منخفض الرتبة لمفاتيح وقيم الانتباه.

أ: حسنًا، إذن هو يضغط المفاتيح والقيم، ولكن ماذا يعني هذا بالضبط؟ إذن دعونا نتعمق قليلاً من الناحية الفنية. حسنًا، تأخذ آلية MLA تمثيلًا خفيًا للإدخال، ثم ت投影ه إلى نواقل استعلام ومفاتيح وقيم. حسنًا، هنا يصبح الأمر مثيرًا للاهتمام. يفصل MLA الاستعلام إلى جزأين.

ب: حسنًا، جزأين؟

أ: نعم. جزء واحد يستخدم للمحتوى، والجزء الآخر يستخدم للمعلومات الموضعية باستخدام شيء يسمى Rope.

ب: Rope؟ يبدو ذلك تقنيًا جدًا.

أ: إنه يعني تضمينات الموضع الدورانية، وتساعد النموذج على فهم موضع الرموز في التسلسل. حسنًا، ثم يتم ضغط المفاتيح والقيم في فضاء كامن ذي أبعاد أقل. إذن الأمر يشبه تقليص البيانات، مما يوفر في الذاكرة.

ب: تمامًا. إذن يتم حفظ المعلومات الأكثر أهمية، بينما يتم التخلص من الحجم غير الضروري. نعم، وهذا التمثيل المضغوط يسمح بذاكرة تخزين مؤقت أصغر بكثير للمفاتيح والقيم أثناء الاستدلال، مما يسرع الأمور.

أ: ويستخدم أيضًا معالجة متعددة الرؤوس.

ب: نعم، تمامًا مثل الانتباه التقليد