---
audio: false
generated: true
lang: ar
layout: post
title: 'DeepSeek V3: الانتباه المتعدد الرؤوس في المظهر و التنبؤ المتعدد الرموز'
translated: true
---

DeepSeek v3 يتم استكشافه هنا، مع مرجع إلى الفيديو "Multi-Head Latent Attention and Multi-token Prediction in Deepseek v3" [https://youtu.be/jL49fLOJYNg?si=4uE2kfe-BlKC1ngO](https://youtu.be/jL49fLOJYNg?si=4uE2kfe-BlKC1ngO). تم استخدام Google Cloud Speech-to-Text لتحويل الفيديو إلى نص بالإضافة إلى بعض الكود لمساعدة في تنظيم النص.

---

أ: مرحبا بكم مرة أخرى في علامة Deep. سنقوم اليوم بغمق في عالم نماذج اللغة الكبيرة. نعم، بشكل خاص DeepSeek V3.

ب: يبدو جيدًا. إنه نموذج يحتوي على 671 مليار معامل، ويجعل من نفسه اسمًا بفضل نهجه الفريد في الكفاءة والأداء، أليس كذلك؟

أ: نعم، وشاركت ورقة أكاديمية تفصيل معماريته.

ب: نعم.

أ: وأنت كخبير في تعلم الآلة، تبحث عن فهم كيفية تحقيق DeepSeek V3 الأداء العالي والتدريب الاقتصادي.

ب: نعم، ذلك صحيح.

أ: أوه، مرحبًا، كيف حالك؟

ج: MLA، التفاصيل، MLA وكيف يعمل.

أ: أوه، بالتأكيد. ذلك فكرة رائعة. نعم، يمكننا بالتأكيد الغوص في التفاصيل حول الانتباه المتعدد الرؤوس، أو MLA. إذن، أنت مهتم بالتفاصيل التقنية لـ MLA. حسنًا، فلنفكك هذا. ذكرنا أن أحد مفاتيح كفاءة DeepSeek V3 هو معماريته من الخبراء المختلطين، أو MoE، أليس كذلك؟ حيث يتم تفعيل جزء فقط من المعاملات لكل رمز. وتأخذنا DeepSeek V3 خطوة أبعد مع MLA وDeepSeek Mo.

ب: ذلك صحيح. إذن، فلنركز على MLA الآن.

أ: حسنًا. في التطبيقات الوقتية، السرعة حاسمة.

ب: إنها كذلك. ويمكن أن يكون مخزن المفتاح والقيمة الذي يحتاج إليه أثناء الاستدلال عائقًا كبيرًا.

أ: بالضبط. ذلك هو المكان الذي تدخل فيه MLA. حسنًا، فإن آلية الانتباه التقليدية تتطلب تخزين الكثير من المعلومات حول الرموز السابقة.

ب: نعم، كما يمكنك تخيل، يصبح ذلك مشكلة مع تسلسلات طويلة من النص، أليس كذلك؟

أ: ولكن MLA يضغط هذه المعلومات بشكل ذكي، حسنًا، لتقلل من تدفق المخزن بشكل كبير، مما يجعل الاستدلال أسرع. فهو مثل أخذ موسوعة ضخمة ومضغتها إلى النقاط الرئيسية فقط.

ب: ذلك مقارنة جيدة. يحتفظ بالبيانات الأساسية دون الوزن الزائد. نعم، فهو مفيد جدًا للتطبيقات الوقتية.

أ: نعم. الآن دعنا نتحدث عن كيفية عملها. حسنًا، فكيف تحقق MLA هذه الضغط؟

ب: حسنًا، فإنه يستخدم ضغط مشترك منخفض الرتبة للمفتاح والقيمة.

أ: حسنًا، فهو يضغط المفتاح والقيمة، ولكن ماذا يعني ذلك بالضبط؟ حسنًا، دعونا نكون قليلاً فنيًا. حسنًا، آلية MLA تأخذ تمثيلًا مخفيًا للدخول، ثم تطرح في ناقلات الاستعلام والمفتاح والقيمة. حسنًا، الآن هنا هو المكان الذي يصبح فيه الأمر مثيرًا للاهتمام. MLA يفصل الاستعلام إلى جزئين.

ب: جزئين؟

أ: نعم. يستخدم جزء من المحتوى، والآخر يستخدم معلومات الموقع باستخدام شيء يسمى Rope.

ب: Rope؟ يبدو ذلك تقنية.

أ: يعني ذلك التضمين الدوراني للموقع، ويساعد النموذج على فهم موقع الرموز في التسلسل. حسنًا، ثم يتم ضغط المفتاح والقيمة في مساحة مخفية أبعادها أقل. فهو مثل تقليل البيانات، مما يوفر الذاكرة.

ب: بالضبط. لذلك يتم حفظ المعلومات الأكثر أهمية، ولكن يتم التخلص من الوزن الزائد. نعم، وتسمح هذه التمثيل المضغوط بمخزن KV أصغر بكثير أثناء الاستدلال، مما يسرع الأمور.

أ: ويستخدم أيضًا معالجة متعددة الرؤوس.

ب: نعم، مثل الانتباه التقليدي، MLA يستخدم عدة رؤوس.

أ: أوه، اذهب إلى ذلك.

ج: إذن، هناك فضاءان مخفيان وورقة مخفية واحدة.

أ: ذلك مراقب جيد. نعم، أنت على حق. هناك في الواقع فضاءان مخفيان. حسنًا، نحن نتحدث عن فضاء مخفي للمحتوى وفضاء مخفي للمفتاح والقيمة.

ب: بالضبط. وتتم معالجة هذه الفضايات المخفية عبر ما نسميه Rope، أو التضمين الدوراني للموقع.

أ: حسنًا، لذلك Rope هو كيفية الحصول على معلومات الموقع.

ب: نعم، يتم تطبيقه على كل من فضاء المحتوى وفضاء المفتاح والقيمة، كما ذكرت. فهو يأخذ هذا التمثيل المضغوط، يعالجها، ثم يجمعها جميعًا مرة أخرى.

أ: نعم، وتقلل تحسين المخزن من التكاليف الإضافية أثناء المعالجة التسلسلية. حسنًا، ذلك هو كيفية تسريع MLA.

ب: بالضبط. إنه طريقة ذكية لتحقيق الانتباه الفعال دون التضحية بالأداء.

أ: حسنًا، ذلك هو حيلة جميلة. ولكن تعرف ماذا؟

ب: ماذا؟

أ: دعونا ننتقل إلى DeepSeek Mo. فكيف تختلف عن نماذج MoE التقليدية؟

ب: حسنًا، DeepSeek Mo يستخدم... أوه، العودة إلى المستمع، كيف حالك؟

ج: ونتكلم أكثر عن الفضاء المخفي. حسنًا، من الفضاء المخفي، ماذا هو ذلك؟

أ: أنا متأكد... دعونا نرى ما الذي تتحدث عنه. الفضايات المخفية حقًا مثيرة للاهتمام. نعم، أنت تتحدث عن الفضاء المخفي، الفضاء المخفي الذي كنا نتحدث عنه، أليس كذلك؟ أنت مهتم بما يحدث داخل هذه الفضايات المخفية، تلك الكهف. نعم، ليس فقط عن عدد الفضايات المخفية، ولكن ما يحدث هناك.

ب: ذلك رائع.

أ: بالضبط. هناك في الواقع فضاءان مخفيان منفصلان داخل MLA، أحدهما للمحتوى والآخر للمفتاح والقيمة. إنه مثل وجود وحدة تخزين منفصلة لكل من المعلومات. وتتم معالجة هذه الفضايات المخفية، كما ذكرنا، باستخدام Rope، التضمين الدوراني للموقع، الذي يضيف معلومات الموقع إلى آلية الانتباه. ذلك مهم جدًا لهم. لذلك، لتختصر، يتم تقسيم الاستعلام، والمفتاح والقيمة أيضًا يتم ضغطهما.

ب: نعم، وتوضع في الفضايتين المخفيتين، واحدة للمحتوى والأخرى للمفتاح والقيمة. وتكون هذه الفضايات المخفية مهمة جدًا للكفاءة، وهي جزء من MLA.

أ: بالضبط. الآن دعونا نتحدث عن هذه العمليات في مزيد من التفاصيل داخل الكهف، كما قلت. حسنًا، فكيف تقوم MLA بتنفيذ هذه التحويلات في الفضايات المخفية؟

ب: حسنًا، يتم معالجة المدخلات بشكل متوازٍ لكل من تمثيلات المحتوى والمفتاح والقيمة. حسنًا، فهو مثل أن يكون له مساران داخل هذا الكهف.

أ: نعم، لكل فضاء مخفي. وتتم معالجة المعلومات باستخدام Rope داخل هذه الفضايات.

ب: ذلك صحيح. وهذا يضمن أن يحتفظ النموذج بمعلومات الموقع أثناء مروره في الكهف. لذلك يعرف النموذج أي جزء من النص هو أي جزء أثناء وجوده في ذلك الحاوية.

أ: بالضبط. وتتم هذه المعالجة قبل المرحلة التالية من التجميع. حسنًا، ماذا يتم تجميعها أثناء مرورها في فضاء المخفي الكهف؟

ب: تقوم الآلية بتنفيذ عمليات تجميع رئيسية. يتم تجميع تمثيلات الاستعلام، وتجمع أيضًا تمثيلات المفتاح. فهو مثل جمع جميع الأجزاء المهمة داخل فضاء المخفي الكهف.

أ: نعم، وتساعد هذه التجميعات في دمج المحتوى مع معلومات الموقع. وتستخدم هذه التجميعات في حساب الانتباه، أليس كذلك؟

ب: صحيح. وتسمح الضغط الأولي بتسريع الأمور خلال هذا الكهف الذي ذكرت. لذلك، MLA تقليل التكاليف الحوسبية داخل وخارج هذا الكهف المخفي.

أ: بالضبط. إنه يوفر آلية الانتباه لأنموذج كبير مثل DeepSeek V3. ذلك سؤال جيد. الآن، بعد أن مررنا بالكهف، دعونا ننتقل إلى DeepSeek Mo.

ب: حسنًا، DeepSeek Mo. نعم، أرى ما تقصد. نعم، هناك في الواقع فضاءان مخفيان منفصلان داخل MLA، أحدهما للمحتوى والآخر للمفتاح والقيمة.

أ: بالضبط. وتكون هذه الفصليات مهمة جدًا لكيفية عملها. إنه مثل وجود وحدة تخزين منفصلة لكل من المعلومات. وتتم معالجة هذه الفضايات المخفية، كما ذكرنا، باستخدام Rope، التضمين الدوراني للموقع، الذي يضيف معلومات الموقع إلى آلية الانتباه. لذلك، لتختصر، يتم تقسيم الاستعلام، والمفتاح والقيمة أيضًا يتم ضغطهما.

ب: نعم، وتوضع في الفضايتين المخفيتين، واحدة للمحتوى والأخرى للمفتاح والقيمة. وتكون هذه الفضايات المخفية مهمة جدًا للكفاءة، وهي جزء من MLA.

أ: بالضبط. الآن دعونا نتحدث عن هذه العمليات في مزيد من التفاصيل. حسنًا، فكيف تقوم MLA بتنفيذ هذه التحويلات في الفضايات المخفية؟

ب: حسنًا، يتم معالجة المدخلات بشكل متوازٍ لكل من تمثيلات المحتوى والمفتاح والقيمة. حسنًا، فهو مثل أن يكون له مساران.

أ: نعم، لكل فضاء مخفي. وتتم معالجة المعلومات باستخدام Rope.

ب: ذلك صحيح. وهذا يضمن أن يحتفظ النموذج بمعلومات الموقع، أليس كذلك؟ ثم لتحسين الكفاءة، يستخدم خبراء مشتركين. حسنًا، الخبراء الذين يمكن استخدامهم في عدة مهام.

أ: نعم، لذلك يتجنب التكرار ويجعل النظام أكثر تسلسلية.

ب: نعم، إنه مثل فريق حيث لكل شخص تخصصات، ولكن يمكن أن يفعل أشياء أخرى.

أ: نعم، ذلك نهج ذكي. نعم، ولكن مع العديد من الخبراء المتخصصين، فكيف يضمنون أن لا يصبح أي منهم متعبًا؟

ب: نعم، بينما يجلس آخرون بلا عمل.

أ: ذلك هو المكان الذي تدخل فيه تقنيتهم الابتكارية لتوازن الحمل بدون خسارة.

ب: هذا هو المكان الذي يصبح الأمر مثيرًا للاهتمام، أليس كذلك؟ إذن، فكيف يفعلون ذلك؟

أ: نماذج MoE التقليدية تستخدم وظيفة خسارة مساعدة أثناء التدريب، حسنًا، لتشجيع استخدام الخبراء بشكل متساوٍ، ولكن هذا يمكن أن يضر بالأداء.

ب: نعم، إنه مثل محاولة إجبار الجميع على استخدام نفس خط التصفية في المتجر.

أ: بالضبط، حتى لو كان بعضهم أسرع من الآخرين، أليس كذلك؟ إنه فقط يخلق تأخيرًا غير ضروري.

ب: نعم. لذلك، DeepSeek V3 تجنب ذلك عن طريق تعديل ديناميكي لمعامل التحيز، حسنًا، لكل خبير بناءً على حموله. حسنًا، إذا كان خبيرًا يتلقى الكثير من الطلبات، فإن النظام يجعله أقل جاذبية للآلية التوجيهية، مما يوجه بعض حركة المرور إلى الخبراء الأقل ازدحامًا.

أ: حسنًا، لذلك، يستخدمون ذلك لتسوية الحمل بشكل فعال، نعم، عن طريق تقليل حجم المخزن KV الذي يحتاج إليه الاستدلال. حسنًا، فهو كل شيء يتعلق بقيم الأداء العالية بينما يقلل من التكاليف الإضافية.

ب: نعم، إنه نهج ذكي لتسوية عائق حاسم.

أ: بالتأكيد. الآن، يجب أن نغطي أيضًا كيفية التعامل مع DeepSeek V3 لتوازن حموله.

ب: نعم، يجب أن نغطي ذلك بالتأكيد. هذا جزء مهم من الصورة. يمكننا أن نلمس ذلك في المرة القادمة.

أ: يبدو جيدًا. حسنًا، أعتقد أن ذلك يعطيك نظرة شاملة على MLA وفضاياتها المخفية.

ب: نعم، شكرًا على الغوص في جميع التفاصيل معنا. سنعود في المرة القادمة مع المزيد من الغوص.

أ: نعم، إنه مثل نظام إدارة حركة المرور للخبراء، نعم، يراقب باستمرار التدفق ويجعل التعديلات لتجنب العوائق.

ب: ويجنب ضربة الأداء من خسارة المساعدة.

أ: ذلك صحيح. واو، اذهب إلى ذلك.

ج: نعم، يمكننا أن نتكلم عن MTP، كيف... كيف يتم مشاركة MTP...

أ: بالتأكيد. ذلك سؤال جيد. نعم، دعونا نتكلم عن كيفية مشاركة MTP. إذن، أنت مهتم بالتفاصيل التقنية لـ MTP. حسنًا، نحن تحدثنا عن أن DeepSeek V3 يستخدم MTP لتوقع الرموز المتعددة، أليس كذلك؟ توقع الرموز المتعددة بدلاً من رمز واحد فقط.

أ: وها هو المكان الذي يصبح الأمر مثيرًا للاهتمام. نعم، أنت مهتم بكيفية إعداد MTP وكيف يتم مشاركة مواردها. حسنًا، كل وحدة MTP تتضمن طبقة التضمين المشتركة، نعم، وورقة الخرج المشتركة. حسنًا، فهي تستخدم نفس التضمين والخرج الرئيسي للموديل.

ب: بالضبط. فهو مثل أن يكونوا جميعًا يستمدون من نفس حوض المعرفة. نعم، وهذا يوفر التكاليف الحوسبية.

أ: نعم. والآن، يستخدم كتلة التحويل الخاصة به. حسنًا، فهو لا يشارك نفس كتلة التحويل مع النموذج الرئيسي.

ب: صحيح. لكل وحدة MTP كتلة تحويل خاصة بها لمعالجة. حسنًا، ذلك هو كيفية الحفاظ على التوقعات منفصلة لكل رمز.

أ: نعم، والآن لتجمع المعلومات، هذه التحويلات الخطية والتجميع...

ب: حسنًا، فهو مثل أخذ أجزاء من أماكن متعددة لبناء الصورة الكاملة.

أ: نعم، وتعمل جميع وحدات MTP معًا بشكل متوازٍ، ولكن تشارك طبقات التضمين والخروج، أليس كذلك؟

ب: نعم، وهذا هو المفتاح للكفاءة لهذا التصميم. حسنًا، فهو مثل نظام من الأجزاء المتصلة التي تعتمد جميعها على بعضها البعض، أليس كذلك؟

أ: وتسمح هذه المشاركة الفعالة للموارد بتدريب أسرع وأداء أفضل.

ب: حسنًا، ذلك حيلة جميلة. تعرف ماذا؟

أ: ماذا؟

ب: دعونا ننتقل إلى رؤية أكبر. فكيف يتعامل هذا النموذج مع توازن الحمل؟ كيف يتم اختيار الخبراء؟

أ: نعم، يمكننا بالتأكيد التحدث عن ذلك. حسنًا، الآن دعونا نغوص في استراتيجية توازن الحمل لـ DeepSeek V3.

ب: يبدو جيدًا. حسنًا، DeepSeek V3 يستخدم ما يسمونه توقع الرموز المتعددة، أو MTP. كنا نتحدث عن كيفية عمل MTP، الآن دعونا نتحدث عن توازن الحمل، أليس كذلك؟

أ: نعم، كنا نتحدث عن ذلك. الآن، تشارك الموارد، وأنت مهتم بكيفية مشاركة الموارد. كنا نتحدث عن ذلك.

ب: ذلك صحيح. لذلك، بدلاً من توقع فقط الرمز التالي، أليس كذلك؟ فإنه يتوقع عدة رموز مستقبلية في نفس الوقت، كما كنا نتحدث عنه.

أ: قد يبدو ذلك، ولكن له عدة مزايا. حسنًا، افترض أنك تخطط لرحلتك. إذا كنت فقط تنظر إلى الدور التالي، نعم، قد تفوتك طريق أكثر كفاءة... حسنًا، النظر إلى الأمام وتخطيط عدة دورات يسمح لك باختيار الطريق الأمثل.

ب: نعم. DeepSeek V3 يستخدم نهجًا مبتكرًا يسمى توازن الحمل بدون خسارة مساعدة، لذلك لا يعتمد على وظيفة خسارة منفصلة لتوازن الحمل.

أ: بالضبط. نماذج MoE التقليدية تستخدم وظيفة خسارة مساعدة أثناء التدريب لتشجيع استخدام الخبراء بشكل متساوٍ، أليس كذلك؟ ولكن هذا يمكن أن يضر بالأداء، كما ذكرنا سابقًا.

ب: نعم، إنه مثل محاولة إجبار الجميع على استخدام نفس خط التصفية في المتجر.

أ: حسنًا، لذلك، عن طريق توقع عدة رموز، يحصل النموذج على فهم أفضل للسياق.

ب: نعم، ويمكن أن يولد إجابات أكثر دقة وانسجامًا. إنه مثل أن يكون النموذج يخطط لمثيلاته، كما ذكرت سابقًا، نعم، لأفضل توقعات مستقبلية. حسنًا، وهذا يؤدي إلى إشارة تدريب نظيفة وأفضل كفاءة للبيانات.

أ: نعم، لذلك، بدلاً من ذلك، DeepSeek V3 يعدل ديناميكيًا معامل التحيز لكل خبير، حسنًا، بناءً على حموله، أليس كذلك؟ إذا كان خبيرًا يتلقى الكثير من الطلبات، فإن النظام يجعله أقل جاذبية، مما يوجه حركة المرور إلى الخبراء الأقل ازدحامًا.

ب: نعم، مثل نظام إدارة حركة المرور للخبراء، يراقب باستمرار التدفق ويجعل التعديلات. إذن، ماذا يمكن أن يفعل MTP؟

أ: يمكن أن تستخدم وحدات MTP أثناء التدريب إما أن يتم التخلص منها أثناء الاستدلال العادي، أو أن يتم إعادة استخدامها بشكل ذكي لشيء يسمى الاستدلال التخميني.

ب: حسنًا، الاستدلال التخميني. ماذا هو ذلك؟

أ: بدلاً من توقع الرمز التالي فقط، فإن النموذج يتوقع أيضًا بدائل محتملة قد تليها.

ب: أوه، إذن، يمكن أن يولد النص أسرع لأنه قد فكر في عدة احتمالات، لديه خطة احتياطية جاهزة للذهاب.

أ: نعم، لذلك، لا يحتاج النموذج إلى التوقف والتحليل من جديد كل مرة.

ب: حسنًا، ذلك منطقي. نعم، الآن، إذا كنت تتحدث عن الكفاءة، لتجنب العوائق، ويجنب ضربة الأداء من خسارة المساعدة.

أ: ذلك صحيح. ويشملون أيضًا خسارة توازن تسلسلية إضافية، نعم، لمنع التفاوتات الشديدة داخل...

ب: ...العمليات. وبحد أقصى أربعة عقد لكل رمز، يقللون من الاتصالات الشبكية. حسنًا، ذلك يساعد على تسهيل الأمور أيضًا.

أ: حسنًا، دعونا نتحدث عن كيفية إدارة DeepSeek V3 للطلبات الحوسبية لتدريبها. وأنا أعرف أنك مهتم بشكل خاص بتحسين التكلفة وكيفية القيام بذلك بشكل اقتصادي.

ب: نعم، وهذا النموذج يفعل أشياء مذهلة في ذلك المجال.

أ: يفعل. نعم، متوسط عدد الخبراء المختار لكل رمز هو 3.2، وهو توازن جيد لتقلل من التكاليف الإضافية.

ب: بالضبط. لذلك، إنه طريقة فعالة وجيدة.

أ: نعم، إنه نهج ذكي لجعل نموذجًا هذا المعقد يعمل بشكل جيد.

ب: نعم، ويحقق تخصص الخبراء من خلال هذا النهج. حسنًا، يعني ذلك أن الخبراء المختلفة يتم تفعيلها في مجالات مختلفة. إذن، ما هم؟

أ: DeepSeek V3 يستخدم إطار تدريب FPA المختلط الدقيق. حسنًا، انتصار كبير لنموذج من هذا الحجم. تذكّرني ما هو FPA مرة أخرى؟

ب: بالتأكيد، هو 8 بت من النقطة العائمة.

أ: حسنًا، ويمثل الأرقام باستخدام أقل عدد من البتات من الصيغ التقليدية. حسنًا، وهذا يعني أقل ذاكرة وعمليات حوسبية أسرع.

ب: بالضبط. إنه مثل ضغط ملف صورة كبير، ولكنك لا تزال تحصل على جوهر الصورة. إنه فقط يأخذ مساحة أقل، أليس كذلك؟

أ: بالضبط. لذلك، لا يتم تفعيل الخبير بشكل عام، ولكن في مجالات محددة. فهو مخصص ومعد للفعل.

ب: نعم. الآن هذا النهج بالبطاقات هو حقًا ذكي.

أ: نعم، أنا موافق. هذا النهج الديناميكي لتوازن الحمل مثير للاهتمام. إنه كل شيء يتعلق بالكفاءة والتوازن.

ب: نعم، إنه جزء من الالتزام لـ DeepSeek V3 بالأداء واستخدام الموارد.

أ: بالتأكيد. الآن، غطينا الكثير اليوم. إنه مثير للاهتمام، ولكن هل استخدام أقل عدد من البتات قد يؤثر على الدقة؟

ب: ذلك قلق معقول، وهو شيء تم التعامل معه بعناية. حسنًا، تم تنفيذ العديد من التقنيات لتخفيف أي خسارة دقة محتملة، بما في ذلك التكميم دقيق.

أ: نعم، يسمح ذلك بتحكم دقيق على كيفية تمثيل الأرقام في FPA. نعم، من الانتباه المتعدد الرؤوس إلى DeepSeek Mo والتوازن، نعم، هذا نموذج DeepSeek V3 هو نظام مثير للاهتمام، وهو مثال جيد على كيفية أن تكون الابتكار يوسع حدودنا...

ب: نعم، كان غوصًا ممتعًا اليوم.

أ: نعم، أعتقد أن ذلك يعطيك نظرة شاملة على DeepSeek V3.

ب: بالتأكيد. شكرًا على استكشافه معنا.

أ: نعم، شكرًا. وها هو كل شيء اليوم. حسنًا، سنعود مع غوص آخر قريبًا.

ب: نعم، إنهم يحرزون التوازن بينك.