---
audio: false
generated: true
image: false
lang: ar
layout: post
model: x-ai/grok-4-fast
title: التضمينات الرمزية والموضعية موضحة
translated: true
type: note
---

### شرح `wte` و `wpe` في نموذج GPT

في تنفيذ نموذج GPT هذا (بناءً على نمط NanoGPT من عمل أندريه كارباثي)، تحتوي وحدة `transformer` على عدة مكونات رئيسية. المكونان اللذان تسأل عنهما، `wte` و `wpe`، كلاهما نسخ من طبقات `nn.Embedding`. تُستخدم هذه الطبقات لتحويل المدخلات المنفصلة (مثل الرموز والمواضع) إلى تمثيلات متجهية كثيفة تسمى **التضمينات**. التضمينات هي جزء أساسي من نماذج المحولات، حيث تسمح للشبكة بتعلم تمثيلات رقمية ذات معنى للبيانات الفئوية.

#### ما هو `wte`؟
- **الاسم الكامل**: تضمين الرمز (يسمى أحيانًا "تضمين رمز الكلمة").
- **الغرض**: يقوم بتعيين كل **رمز** فريد من المفردات (مثل الكلمات، أو أجزاء الكلمات، أو الأحرف) إلى متجه بحجم ثابت يسمى `config.n_embd` (حجم التضمين في النموذج، غالبًا 768 أو ما شابه).
  - حجم المفردات هو `config.vocab_size` (على سبيل المثال، 50,000 لأداة tokenizer النموذجية في GPT).
  - المدخل: معرف رمز صحيح (من 0 إلى vocab_size-1).
  - المخرج: متجه مُتعلم يمثل "معنى" ذلك الرمز.
- سبب الحاجة إليه: معرفات الرموز الخام هي مجرد أعداد صحيحة بدون معلومات دلالية. تحولها التضمينات إلى متجهات تلتقط العلاقات (على سبيل المثال، قد ينتهي بالرمزين "king" و "queen" إلى متجهات متشابهة بعد التدريب).

#### ما هو `wpe`؟
- **الاسم الكامل**: التضمين الموضعي.
- **الغرض**: يقوم بتعيين كل **موضع** في تسلسل الإدخال (من 0 إلى `config.block_size - 1`، حيث block_size هو أقصى طول للتسلسل، على سبيل المثال، 1024) إلى متجه بحجم ثابت بنفس البعد `config.n_embd`.
  - المدخل: فهرس موضع صحيح (من 0 إلى block_size-1).
  - المخرج: متجه مُتعلم يرمز لموقع الموضع في التسلسل.
- سبب الحاجة إليه: تقوم المحولات بمعالجة التسلسلات بشكل متوازي وليس لديها وعي مدمج بالترتيب (على عكس الشبكات العصبية المتكررة RNNs). تقوم التضمينات الموضعية بحقن معلومات حول الموضع النسبي أو المطلق للرموز، حتى يعرف النموذج أن "cat" في الموضع 1 يختلف عن "cat" في الموضع 10.

#### كيف تعمل التضمينات في التدريب
نعم، أنت محق تمامًا—هذه **معلمات قابلة للتعلم** في الشبكة العصبية:
- **التهيئة**: تقوم `nn.Embedding` في PyTorch بتهيئة مصفوفة التضمين بشكل عشوائي (بشكل افتراضي، باستخدام توزيع منتظم بين -√(1/dim) و +√(1/dim)، حيث dim هو `n_embd`). وهذا يمنحها قيمًا بدائية ليست صفرية أو ضوضاء عشوائية.
- **التدريب**: أثناء تدريب النموذج (على سبيل المثال، عبر الانتشار العكسي Backpropagation على دالة فقدان نمذجة اللغة)، تقوم التدرجات بتحديث أوزان التضمين هذه مثل أي معلمة أخرى (على سبيل المثال، في كتل المحولات). مع مرور الوقت، تتعلم كيفية تمثيل الرموز والمواضع بطريقة تقلل من الفاقد—مما يسمح لها بالتقاط الدلالات، والتركيب النحوي، والاعتمادات الموضعية.
- **الاستخدام في التمرير الأمامي**: (غير موضح في مقتطف الكود الخاص بك، ولكن بشكل نموذجي) لتسلسل إدخال من معرفات الرموز `x`، يحسب النموذج:
  ```
  token_emb = wte(x)  # الشكل: (batch_size, seq_len, n_embd)
  pos_emb = wpe(torch.arange(seq_len))  # يتم بثه لمطابقة token_emb
  input_emb = token_emb + pos_emb  # جمعهما عنصرًا بعنصر
  ```
  ثم يتم تغذية `input_emb` (بعد الإسقاط dropout) في كتل المحولات (`h`).

هذا الإعداد فعال ومعياري لنماذج GPT المشابهة. إذا كان لديك التمرير الأمامي الكامل أو المزيد من الكود، يمكنني الخوض بشكل أعمق!