---
audio: false
generated: true
image: false
lang: ar
layout: post
model: x-ai/grok-4-fast
title: مقدمة في الذاكرة طويلة المدى قصيرة المدى
translated: true
type: note
---

## ما هو LSTM؟

الذاكرة الطويلة قصيرة المدى (LSTM) هي نوع متخصص من بنية الشبكة العصبية المتكررة (RNN) مصممة للتعامل مع البيانات المتسلسلة والتعلم منها، مثل السلاسل الزمنية، واللغة الطبيعية، أو الكلام. تم تقديمها في عام 1997 من قبل سيپ هوكريتر ويورجن شميدهوبر، وتعالج LSTM القيود الرئيسية للشبكات العصبية المتكررة التقليدية، ولا سيما صراعها مع التبعيات طويلة المدى في البيانات.

في جوهرها، LSTM هي خلية شبكة عصبية تعالج تسلسلات الإدخال خطوة بخطوة مع الحفاظ على "ذاكرة" للمدخلات السابقة. تسمح هذه الذاكرة لها بالتقاط الأنماط على فترات ممتدة، مما يجعلها قوية للمهام التي يهم فيها السياق من الماضي البعيد في التسلسل. تُستخدم LSTM على نطاق واسع في أطر التعلم العميق مثل TensorFlow وPyTorch، مشكلة العمود الفقري للعديد من النماذج المتطورة في الذكاء الاصطناعي.

## الخلفية: لماذا كانت LSTM مطلوبة

تعالج الشبكات العصبية المتكررة التقليدية التسلسلات عن طريق تمرير المعلومات من خطوة زمنية إلى أخرى من خلال حالة خفية. ومع ذلك، فإنها تعاني من مشكلتين رئيسيتين:

- **مشكلة التلاشي في التدرج (Vanishing Gradient Problem)**: أثناء الانتشار الخلفي عبر الزمن (BPTT)، يمكن أن تتقلص التدرجات بشكل أسي، مما يجعل من الصعب تعلم التبعيات طويلة المدى. إذا حدث حدث ذو صلة منذ 50 خطوة، فقد "ينسى" الشبكة ذلك.
- **مشكلة الانفجار في التدرج (Exploding Gradient Problem)**: على العكس من ذلك، يمكن أن تنمو التدرجات بشكل كبير جدًا، مما يتسبب في تدريب غير مستقر.

هذه المشاكل تحد من الشبكات العصبية المتكررة الأساسية إلى التسلسلات القصيرة. تحل LSTM هذه المشكلة عن طريق إدخال **حالة الخلية (cell state)** — وهي بنية تشبه الحزام الناقل تمر عبر التسلسل بأكمله، مع تفاعلات خطية طفيفة للحفاظ على المعلومات عبر مسافات طويلة.

## كيف تعمل LSTM: المكونات الأساسية

تعمل وحدة LSTM على تسلسلات المدخلات \\( x_t \\) في الخطوة الزمنية \\( t \\)، وتقوم بتحديث حالاتها الداخلية بناءً على الحالة الخفية السابقة \\( h_{t-1} \\) وحالة الخلية \\( c_{t-1} \\). الابتكار الرئيسي هو استخدام **البوابات (gates)** — وهي شبكات عصبية مُفعلة بواسطة دالة السيجمويد تقرر أي معلومات يجب الاحتفاظ بها أو إضافتها أو إخراجها. تعمل هذه البوابات كـ "منظمات" لتدفق المعلومات.

### البوابات الثلاث الرئيسية

1.  **بوبة النسيان (\\( f_t \\))**:
    *   تقرر أي معلومات يجب التخلص منها من حالة الخلية.
    *   الصيغة: \\( f_t = \sigma(W_f \cdot [h_{t-1}, x_t] + b_f) \\)
    *   المخرجات: متجه من القيم بين 0 (نسيان كامل) و 1 (الاحتفاظ كامل).
    *   هنا، \\( \sigma \\) هي دالة السيجمويد، \\( W_f \\) و \\( b_f \\) هما الأوزان والانحيازات القابلة للتعلم.

2.  **بوبة الإدخال (\\( i_t \\)) والقيم المرشحة (\\( \tilde{c}_t \\))**:
    *   تقرر أي معلومات جديدة يجب تخزينها في حالة الخلية.
    *   بوابة الإدخال: \\( i_t = \sigma(W_i \cdot [h_{t-1}, x_t] + b_i) \\)
    *   القيم المرشحة: \\( \tilde{c}_t = \tanh(W_c \cdot [h_{t-1}, x_t] + b_c) \\) (باستخدام الظل الزائدي للقيم بين -1 و 1).
    *   تخلق هذه تحديثات محتملة لحالة الخلية.

3.  **بوبة الإخراج (\\( o_t \\))**:
    *   تقرر أي أجزاء من حالة الخلية يجب إخراجها كحالة خفية.
    *   الصيغة: \\( o_t = \sigma(W_o \cdot [h_{t-1}, x_t] + b_o) \\)
    *   الحالة الخفية تكون بعد ذلك: \\( h_t = o_t \odot \tanh(c_t) \\) (حيث \\( \odot \\) هو الضرب العنصر بحسب العنصر).

### تحديث حالة الخلية

يتم تحديث حالة الخلية \\( c_t \\) على النحو التالي:
\\[ c_t = f_t \odot c_{t-1} + i_t \odot \tilde{c}_t \\]
*   الحد الأول: ينسى المعلومات غير ذات الصلة من الماضي.
*   الحد الثاني: يضيف معلومات جديدة ذات صلة.

يساعد هذا التحديث الجمعي (بدلاً من الضربي كما في الشبكات العصبية المتكررة) في تدفق التدرجات بشكل أفضل، مما يخفف من مشاكل التلاشي.

### التمثيل المرئي

تخيل حالة الخلية كطريق سريع: بوابة النسيان هي إشارة مرور تقرر أي سيارات (معلومات) تسمح بالمرور من المقطع السابق، وتضيف بوابة الإدخال سيارات جديدة تندمج من طريق جانبي، وتقوم بوابة الإخراج بتصفية ما يخرج إلى الطريق السريع التالي (الحالة الخفية).

## نظرة رياضية عامة

للتعمق أكثر، إليك المجموعة الكاملة من المعادلات لخلية LSTM أساسية:

\\[
\begin{align*}
f_t &= \sigma(W_f x_t + U_f h_{t-1} + b_f) \\
i_t &= \sigma(W_i x_t + U_i h_{t-1} + b_i) \\
\tilde{c}_t &= \tanh(W_c x_t + U_c h_{t-1} + b_c) \\
o_t &= \sigma(W_o x_t + U_o h_{t-1} + b_o) \\
c_t &= f_t \odot c_{t-1} + i_t \odot \tilde{c}_t \\
h_t &= o_t \odot \tanh(c_t)
\end{align*}
\\]

*   \\( W \\) المصفوفات تربط المدخلات بالبوابات؛ \\( U \\) تربط الحالات الخفية.
*   يتضمن التدريب تحسين هذه المعاملات عبر نزول التدرج.

## مزايا LSTM

*   **الذاكرة طويلة المدى**: تتفوق في التعامل مع التسلسلات التي تصل إلى آلاف الخطوات، على عكس الشبكات العصبية المتكررة القياسية.
*   **المرونة**: تتعامل مع مدخلات ذات أطوال متغيرة ومع المعالجة ثنائية الاتجاه (معالجة التسلسلات للأمام وللخلف).
*   **القدرة على التفسير**: توفر البوابات نظرة ثاقبة حول ما "يتذكره" النموذج أو "ينساه".
*   **المتانة**: أقل عرضة للإفراط في التخصيص على البيانات المتسلسلة ذات الضوضاء مقارنة بالنماذج الأبسط.

تشمل العيوب ارتفاع التكلفة الحسابية (المزيد من المعاملات) وتعقيد ضبطها.

## المتغيرات والتطورات

*   **وحدة متكررة ذات بوابة (GRU)**: بديل أخف وزنًا (2014) يدمج بوابتي النسيان والإدخال في بوابة تحديث واحدة، مما يقلل عدد المعاملات مع الاحتفاظ بأداء LSTM في الغالب.
*   **اتصالات Peephole**: متغير قديم حيث تطل البوابات على حالة الخلية.
*   **LSTM ثنائي الاتجاه (BiLSTM)**: LSTM (أمامي و خلفي) للحصول على سياق أفضل في مهام مثل الترجمة الآلية.
*   التكاملات الحديثة: LSTM في المحولات (مثل النماذج الهجينة) أو LSTM المعززة بالانتباه.

## التطبيقات

تتفوق LSTM في المجالات ذات البنية الزمنية أو المتسلسلة:

*   **معالجة اللغة الطبيعية (NLP)**: تحليل المشاعر، الترجمة الآلية (مثل الإصدارات المبكرة من Google Translate)، توليد النص.
*   **التنبؤ بالسلاسل الزمنية**: أسعار الأسهم، توقع الطقس، كشف الشذوذ في بيانات أجهزة الاستشعار.
*   **التعرف على الكلام**: تحويل الصوت إلى نص (مثلًا في Siri أو Alexa).
*   **تحليل الفيديو**: التعرف على الأفعال من خلال معالجة تسلسل الإطارات.
*   **الرعاية الصحية**: توقع نتائج المرضى من السجلات الطبية المتسلسلة.
*   **توليد الموسيقى**: تأليف الألحان من خلال نمذجة تسلسل النوتات.

في السنوات الأخيرة (اعتبارًا من 2025)، غالبًا ما يتم دمج LSTM مع المحولات (transformers) للكفاءة الهجينة، لكنها تبقى أساسية للأجهزة الطرفية ذات القدرة الحسابية المحدودة.

## البدء مع LSTM

للتجربة، استخدم Python مع Keras/TensorFlow:

```python
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense

model = Sequential()
model.add(LSTM(50, input_shape=(timesteps, features)))  # 50 وحدة
model.add(Dense(1))  # طبقة الإخراج
model.compile(optimizer='adam', loss='mse')
# التدريب على بياناتك المتسلسلة
```

الموارد للتعلم العملي: دروس TensorFlow الرسمية أو الورقة العلمية الأصلية.

أحدثت LSTM ثورة في نمذجة التسلسل، مهددة الطريق لتطورات الذكاء الاصطناعي اليوم. إذا كنت تتجه للتعمق، ركز على فهم البوابات من خلال أدوات التصور مثل مخططات انسياب LSTM.

## المراجع
*   [الورقة الأصلية لـ LSTM: Long Short-Term Memory](https://www.bioinf.jku.at/publications/older/2604.pdf)
*   [مدونة كولاه: Understanding LSTM Networks](https://colah.github.io/posts/2015-08-Understanding-LSTMs/)
*   [توجهاً لعلوم البيانات: A Gentle Introduction to LSTMs](https://towardsdatascience.com/understanding-lstm-and-its-diagrams-37e2f46f1714)