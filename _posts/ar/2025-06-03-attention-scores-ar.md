---
audio: false
generated: true
lang: ar
layout: post
title: تثبيت درجات الانتباه في المحولات
translated: true
type: note
---

إن قسمة الجذر التربيعي لبُعد المفتاح، \\(\sqrt{d_k}\\)، في آلية الانتباه ذات الضرب النقطي المعياري (كما في الصيغة \\(\text{Scores المعيارية} = \frac{Q K^T}{\sqrt{d_k}}\\)) هو اختيار مقصود لتحقيق استقرار المتجهات التدرجية ومنع القيم الكبيرة، خاصة في سياق المحولات (Transformers). إليك سبب استخدام \\(\sqrt{d_k}\\) بدلاً من \\(d_k\\) فقط:

1. **تباين الضرب النقطي**:
   - يحسب الضرب النقطي \\( Q K^T \\) التشابه بين متجهات الاستعلام (\\( Q \\)) والمفتاح (\\( K \\))، حيث يكون لكل متجه بُعد \\( d_k \\). إذا افترضنا أن عناصر \\( Q \\) و \\( K \\) مستقلة ولها متوسط 0 وتباين 1 (شائع بعد التهيئة أو المعايرة)، فإن للضرب النقطي \\( Q_i \cdot K_j \\) (لزوج واحد من متجهات الاستعلام والمفتاح) تباينًا مقداره \\( d_k \\). هذا لأن تباين مجموع \\( d_k \\) من حاصل ضرب متغيرين طبيعيين قياسيين مستقلين يتناسب خطيًا مع \\( d_k \\).
   - بدون قياس، يزداد حجم \\( Q K^T \\) مع زيادة \\( d_k \\)، مما يؤدي إلى قيم كبيرة جدًا لـ \\( d_k \\) الكبيرة (شائع في المحولات، حيث قد يكون \\( d_k \\) 64 أو 128 أو أكبر). يمكن أن تسبب القيم الكبيرة في درجات الانتباه مشاكل عند تمريرها عبر دالة softmax.

2. **استقرار Softmax**:
   - يتم إدخال درجات الانتباه \\( \frac{Q K^T}{\sqrt{d_k}} \\) في دالة softmax لحساب أوزان الانتباه. إذا كانت الدرجات كبيرة جدًا (كما ستكون بدون قياس أو بقياس غير كافٍ)، يمكن أن تنتج دالة softmax توزيعات حادة جدًا، حيث يهيمن عنصر واحد (يقترب من 1) بينما تكون العناصر الأخرى قريبة من 0. هذا يؤدي إلى تلاشي المتجهات التدرجية لمعظم العناصر، مما يجعل من الصعب على النموذج أن يتعلم بفعالية.
   - يضمن القسمة على \\(\sqrt{d_k}\\) أن تباين الدرجات المعيارية يساوي تقريبًا 1، مما يحافظ على الدرجات في نطاق تتصرف فيه دالة softmax بشكل جيد، منتجة أوزان انتباه أكثر توازنًا ومتجهات تدرجية مستقرة.

3. **لماذا لا يتم القسمة على \\( d_k \\)؟**:
   - إن القسمة على \\( d_k \\) بدلاً من \\(\sqrt{d_k}\\) ستؤدي إلى قياس مفرط للضرب النقطي، مما يقلل تباين الدرجات إلى \\( \frac{1}{d_k} \\). بالنسبة لـ \\( d_k \\) الكبيرة، سيجعل هذا الدرجات صغيرة جدًا، مما يتسبب في إنتاج softmax لتوزيعات شبه موحدة (لأن المدخلات الصغيرة لـ softmax تؤدي إلى مخرجات قريبة من \\( \frac{1}{n} \\)).
   - سيؤدي القياس المفرط باستخدام \\( d_k \\) إلى تخفيف قدرة آلية الانتباه على التركيز على المفاتيح ذات الصلة، حيث سيتم تقليل الفروق بين الدرجات.
   - يمكن أن يؤدي القياس المفرط باستخدام \\( d_k \\) أيضًا إلى عدم استقرار عددي في بعض الحالات، حيث قد تكون القيم الصغيرة جدًا أصعب في المعالجة بدقة في الحساب ذي الفاصلة العائمة.

4. **لماذا \\(\sqrt{d_k}\\)؟**:
   - تقوم القسمة على \\(\sqrt{d_k}\\) بتطبيع تباين الضرب النقطي ليكون approximately 1، حيث أن \\( \text{Var}\left(\frac{Q K^T}{\sqrt{d_k}}\right) = \frac{\text{Var}(Q K^T)}{d_k} = \frac{d_k}{d_k} = 1 \\). هذا يحافظ على الدرجات في نطاق معقول، مما يضمن أن تنتج softmax أوزان انتباه ذات معنى دون أن تكون حادة جدًا أو موحدة جدًا.
   - ينشأ الجذر التربيعي بشكل طبيعي من الخاصية الرياضية لتباين الضرب النقطي. إنه يحقق توازنًا بين منع القيم المتفجرة (والتي قد تحدث بدون قياس) وتجنب كبت الدرجات بشكل مفرط (والذي قد يحدث مع \\( d_k \\)).

5. **الفعالية التجريبية**:
   - تم تقديم اختيار \\(\sqrt{d_k}\\) في ورقة المحول الأصلية ("Attention is All You Need" بواسطة Vaswani et al., 2017) وتم التحقق منه تجريبيًا عبر العديد من النماذج القائمة على المحولات. إنه يعمل بشكل جيد في الممارسة لمجموعة واسعة من قيم \\( d_k \\)، مما يضمن تدريبًا مستقرًا وأداءً جيدًا.

باختصار، فإن القسمة على \\(\sqrt{d_k}\\) هو اختيار مدعوم رياضياً لتحقيق استقرار تباين درجات الانتباه، وضمان عمل softmax في نظام جيد التصرف، وتجنب مشاكل المتجهات التدرجية المتفجرة أو المتلاشية. إن القسمة على \\( d_k \\) ستقلل من الدرجات بشكل مفرط، مما يؤدي إلى أوزان انتباه أقل تمييزًا وقد يضر بأداء النموذج.