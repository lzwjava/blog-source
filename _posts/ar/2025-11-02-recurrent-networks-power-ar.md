---
audio: false
generated: true
image: false
lang: ar
layout: post
model: x-ai/grok-4-fast
title: القوة المفاجئة للشبكات العصبية المتكررة
translated: true
type: note
---

هذه المدونة التي كتبها أندريج كارباثي عام 2015 تُعد مقدمة كلاسيكية للشبكات العصبية المتكررة (RNNs)، وتسليط الضوء على قوتها المدهشة في التعامل مع المتتابعات مثل توليد النص. يشارك كارباثي حماسه بعد تدريب الشبكات العصبية المتكررة لمهام مثل التسمية التوضيحية للصور، والتي أنتجت مخرجات متماسكة مع حد أدنى من الضبط. ويطلق كودًا مفتوح المصدر لنماذج اللغة على مستوى الحرف باستخدام LSTMs (نوع من RNN) ويوضح "السحر" من خلال تجارب توليد النص المتنوعة. أدناه ملخص منظم للأقسام الرئيسية.

## المقدمة
يصف كارباثي الشبكات العصبية المتكررة بأنها "فعالة بشكل غير معقول" للبيانات المتسلسلة، مقارنة إياها بالشبكات التقليدية ذات التغذية الأمامية التي تتعامل مع مدخلات/مخرجات ذات حجم ثابت. فهو يدرب شبكات عصبية متكررة بسيطة على نصوص للتنبؤ بتوليد الأحرف، متسائلاً عن كيفية استيعابها لأنماط اللغة بهذه البراعة. تتضمن المشاركة كودًا على GitHub لتكرار العروض التوضيحية.

## المفاهيم الأساسية: كيف تعمل الشبكات العصبية المتكررة
تتفوق الشبكات العصبية المتكررة في التعامل مع المتتابعات (مثل الجمل، مقاطع الفيديو) من خلال الحفاظ على "حالة" داخلية (متجه مخفي) تنقل المعلومات عبر الخطوات الزمنية. على عكس الشبكات الثابتة، تطبق نفس التحويل بشكل متكرر:

- **أنواع الإدخال/الإخراج**: إدخال ثابت إلى مخرجات متسلسلة (مثال: صورة إلى تسمية توضيحية)؛ متسلسلة إلى مخرجات ثابتة (مثال: جملة إلى شعور)؛ متسلسلة إلى متسلسلة (مثال: ترجمة).
- **الآلية الأساسية**: في كل خطوة، الحالة الجديدة \\( h_t = \tanh(W_{hh} h_{t-1} + W_{xh} x_t) \\)، حيث \\( x_t \\) هو الإدخال، والمخرج \\( y_t \\) يُشتق من الحالة. يتم التدريب عبر الانتشار الخلفي عبر الزمن (BPTT).
- **العمق والمتغيرات**: ضع الطبقات فوق بعضها للعمق؛ استخدم LSTMs للتعامل مع التبعيات طويلة المدى بشكل أفضل من الشبكات العصبية المتكررة العادية.
- **ملاحظة فلسفية**: الشبكات العصبية المتكررة كاملة حسب تورينج، وهي في الأساس "برامج تعلُم" وليست مجرد دوال.

مقتطف بسيط من Python يوضح دالة الخطوة:
```python
def step(self, x):
    self.h = np.tanh(np.dot(self.W_hh, self.h) + np.dot(self.W_xh, x))
    y = np.dot(self.W_hy, self.h)
    return y
```

## نمذجة اللغة على مستوى الحرف
المثال الأساسي: التدريب على نص للتنبؤ بالحرف التالي (مشفر بطريقة one-hot)، وبناء توزيعات احتمالية فوق مفردات (مثال: 65 حرفًا للإنجليزية). يعمل التوليد بأخذ عينات من التنبؤات وإعادة تغذيتها. يتعلم السياق عبر الاتصالات المتكررة—مثال: التنبؤ بـ 'l' بعد "hel" مقابل "he". يتم التدريب باستخدام SGD على دفعات صغيرة ومحسنات مثل RMSProp.

## العروض التوضيحية: النص المُولد بواسطة RNN
جميعها تستخدم كود char-rnn الخاص بالمؤلف على ملفات نصية مفردة، تظهر التقدم من نتائج غير مترابطة إلى مخرجات متماسكة.

- **مقالات بول جراهام** (~1 ميجابايت): يحاكي أسلوب نصائح الشركات الناشئة. نموذج: "The surprised in investors weren’t going to raise money... Don’t work at first member to see the way kids will seem in advance of a bad successful startup."
- **شكسبير** (4.4 ميجابايت): ينتج حوارات تشبه المسرحيات. نموذج: "PANDARUS: Alas, I think he shall be come approached and the day When little srain would be attain'd into being never fed..."
- **ويكيبيديا** (96 ميجابايت): يولد نصوصًا تشبه المقالات مع markdown، وروابط، وقوائم. نموذج: "Naturalism and decision for the majority of Arab countries' capitalide was grounded by the Irish language by [[John Clair]]..."
- **الجبر الهندسي LaTeX** (16 ميجابايت): يخرج براهين رياضية قابلة للترجمة تقريبًا. نموذج: "\begin{proof} We may assume that $\mathcal{I}$ is an abelian sheaf on $\mathcal{C}$..."
- **كود Linux Kernel بلغة C** (474 ميجابايت): دوال واقعية مع تعليقات وبناء جملة. نموذج: "static int indicate_policy(void) { ... if (ss->segment < mem_total) unblock_graph_and_set_blocked(); ... }"
- **أسماء الأطفال** (8 آلاف اسم): اختراعات جديدة مثل "Rudi Levette" أو "Hany Mareanne".

"درجة الحرارة" تتحكم في أخذ العينات: منخفضة لنص آمن لكن متكرر؛ عالية لنص مبدع لكن غير منتظم.

## التحديات والقيود
- التبعيات طويلة المدى: تنسى السياق المبكر (مثال: علامات LaTeX غير متطابقة).
- الحفظ مقابل التعميم: جيدة في بناء الجملة والهيكل لكن تهلوس في المحتوى.
- مكثفة الحسابياً: تحتاج إلى وحدات معالجة الرسومات؛ الشبكات العصبية المتكررة العادية غير مستقرة بدون LSTMs.
- الإصلاحات المستقبلية: الذاكرة الخارجية (آلات تورينج العصبية)، آليات الانتباه.

## الخاتمة
تتفوق الشبكات العصبية المتكررة في معالجة اللغة الطبيعية (الترجمة، الكلام)، والرؤية (التسمية التوضيحية)، وما بعدها، متطورة من الهراء إلى التوليد المنظم. يقوم كارباثي بتصور التنشيطات (مثال: الخلايا العصبية التي تكتشف علامات الاقتباس) ويُمهد لشبكة RNN تم تدريبها على المشاركة نفسها، مخرجة نصوصًا meta مثل "I've the RNN with and works...". يوصي بموارد قراءة وبرمجة إضافية، ويتنبأ بدور الشبكات العصبية المتكررة في الذكاء الاصطناعي الأذكى.

[التأثير غير المعقول للشبكات العصبية المتكررة](http://karpathy.github.io/2015/05/21/rnn-effectiveness/)