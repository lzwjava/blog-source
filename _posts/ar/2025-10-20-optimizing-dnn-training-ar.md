---
audio: false
generated: true
image: false
lang: ar
layout: post
model: x-ai/grok-4-fast
title: تحسين تدريب الشبكات العصبية العميقة
translated: true
type: note
---

### الفصل الثامن: التحسين لتدريب النماذج العميقة

يركز هذا الفصل من كتاب *التعلم العميق* لإيان جودفيلو، ويوشوا بينجيو، وآرون كورفيل على تقنيات التحسين القائمة على التدرج والمصممة خصيصًا لتدريب الشبكات العصبية العميقة. وهو يبني على المفاهيم الأساسية من الفصل الرابع ويؤكد على تقليل دالة التكلفة \\( J(\theta) \\) لإيجاد المعلمات المثلى \\( \theta \\)، حيث تجمع \\( J(\theta) \\) عادةً بين الخسارة على بيانات التدريب وعبارات التنظيم. الهدف هو تقريب المخاطر الحقيقية \\( J^*(\theta) = \mathbb{E}_{(x,y) \sim p_{data}} L(f(x;\theta), y) \\)، ولكن عمليًا، يتم ذلك عبر المخاطر التجريبية على مجموعة التدريب.

#### كيف يختلف التعلم عن التحسين الخالص
تحسين تعلم الآلة لا يتعلق بتقليل دالة التكلفة مباشرة بل بتحسين الأداء على البيانات غير المرئية بشكل غير مباشر (مثل دقة مجموعة الاختبار). تشمل الاختلافات الرئيسية:
- **الأهداف غير المباشرة**: تعمل دالة التكلفة \\( J(\theta) \\) كبديل لمقياس معقد مثل خسارة 0-1. تُستخدم الخسائر البديلة (مثل اللوغاريتم السالب للاحتمالية للتصنيف) لأن الخسائر الحقيقية تفتقر غالبًا إلى التدرجات المفيدة.
- **القابلية للتحليل**: تقوم \\( J(\theta) \\) بحساب المتوسط على الأمثلة، مما يتيح تقليل المخاطر التجريبية: \\( J(\theta) \approx \frac{1}{m} \sum_{i=1}^m L(f(x^{(i)};\theta), y^{(i)}) \\).
- **مخاطر الإفراط في التخصيص**: يمكن للنماذج عالية السعة حفظ بيانات التدريب، لذا فإن التوقف المبكر (بناءً على أداء التحقق) أمر بالغ الأهمية، حتى لو استمرت خسارة التدريب في الانخفاض.
- **استراتيجيات الدُفعات**:
  - **الطرق الدفعية**: تستخدم مجموعة البيانات الكاملة للحصول على تدرجات دقيقة (حتمية ولكنها بطيئة للبيانات الكبيرة).
  - **نزول التدرج العشوائي**: يستخدم أمثلة فردية (تحديثات سريعة ولكن بها ضوضاء).
  - **طرق الدُفعات المصغرة**: توازن بين الاثنين، شائعة في التعلم العميق (أحجام مثل 256-32). تساعد الضوضاء من الدُفعات الصغيرة في التنظيم؛ يمنع الخلط التحيز.

يقترب التعلم عبر الإنترنت (بيانات البث) من تدرجات المخاطر الحقيقية دون تكرار.

#### التحديات في تحسين التعلم العميق
تدريب النماذج العميقة مكثف حسابيًا (من أيام إلى أشهر على المجموعات) وأصعب من التحسين الكلاسيكي بسبب:
- **التعقيد الحسابي**: الخسائر غير القابلة للاشتقاق والإفراط في التخصيص في تقليل المخاطر التجريبية.
- **الحجم**: مجموعات البيانات الكبيرة تجعل التدرجات الدفعية الكاملة غير عملية؛ يقدم أخذ العينات تباينًا (مقياس الخطأ \\( 1/\sqrt{n} \\)).
- **مشاكل البيانات**: التكرار، والارتباطات (يتم إصلاحها بالخلط)، والتحيز من إعادة أخذ العينات.
- **حدود الأجهزة**: مقيدة أحجام الدفعات بالذاكرة؛ يساعد التوازي غير المتزامن ولكن يمكن أن يقدم عدم اتساق.
- عقبات خاصة بالشبكات العصبية (مفصلة لاحقًا): سوء التكييف، الحدود الدنيا المحلية، الهضاب، والتدرجات المتلاشية/المتفجرة.

تتحمل طرق الرتبة الأولى (القائمة على التدرج فقط) الضوضاء بشكل أفضل من طرق الرتبة الثانية (القائمة على الهسيان)، والتي تضخم الأخطاء في الدُفعات المصغرة.

#### خوارزميات التحسين
يراجع الفصل الخوارزميات لتقليل \\( J(\theta) \\)، بدءًا من نزول التدرج العشوائي الأساسي وامتدادًا إلى المتغيرات:
- **نزول التدرج العشوائي**: تحديث الدفعة المصغرة الأساسي: \\( \theta \leftarrow \theta - \epsilon \hat{g} \\)، حيث \\( \hat{g} \\) هو تقدير تدرج الدفعة المصغرة و \\( \epsilon \\) هو معدل التعلم. يتقارب أسرع من الطرق الدفعية بسبب هروب الضوضاء من الحدود الدنيا المحلية الضعيفة.
- **الزخم ومتغيراته**: يضيف مصطلحات سرعة للتسريع عبر المناطق المسطحة وتبديد التذبذبات.
- **الطرق التكيفية**: تضبط معدلات التعلم لكل معلمة (مثل AdaGrad, RMSProp, Adam) للتعامل مع التدرجات المتناثرة والمقاييس المختلفة.
- **تقريبات الرتبة الثانية**: تستخدم معلومات الانحناء (مثل التدرج المرافق، التدرج الطبيعي) لتحسين التكييف، ولكنها مكلفة.
- **نزول الإحداثيات وغيرها**: لبنيات محددة.

تشمل النصائح العملية جدولة معدل التعلم، التهيئة (مثل Xavier)، وجمع الطرق (مثل التكييف المسبق).

#### أجزاء رئيسية أخرى
- **تهيئة المعلمات**: بالغة الأهمية لتجنب التدرجات المتلاشية؛ تناقش التهيئة المتناظرة/المتعامدة والتدريب المسبق طبقة بطبقة.
- **المفاضلات**: توازن بين سرعة التقارب، الاستقرار، والتعميم؛ تؤكد على الضبط الموجه بالتحقق.
- **مواضيع متقدمة**: تغطي التدرجات المتأخرة في الإعدادات غير المتزامنة، التحسين الموزع، والروابط بطرق التطور.

بشكل عام، يؤكد الفصل على عدم هيمنة خوارزمية واحدة — يعتمد النجاح على الضبط الخاص بالمشكلة، مع اعتبار نزول التدرج العشوائي وامتداداته كخيول عمل للتعلم العميق.

[كتاب التعلم العميق - الفصل الثامن: التحسين](https://www.deeplearningbook.org/contents/optimization.html)