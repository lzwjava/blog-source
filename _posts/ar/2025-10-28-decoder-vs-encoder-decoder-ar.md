---
audio: false
generated: true
image: false
lang: ar
layout: post
model: x-ai/grok-4-fast
title: المحوّلات من نوع مفكك الشفرة فقط مقابل مفكك الشفرة-مشفر
translated: true
type: note
---

### الفروق الرئيسية بين محولات فك التشفير فقط ومحولات التشفير-فك التشفير

المحولات (Transformers) هي بنيات شبكات عصبية تُستخدم على نطاق واسع في معالجة اللغة الطبيعية (NLP) وما بعدها. المحول الأصلي (الذي تم تقديمه في ورقة 2017 "Attention is All You Need") يستخدم إعداد **مُشفر-فك تشفير**، بينما نماذج اللغة الكبيرة (LLMs) الحديثة مثل GPT غالبًا ما تستخدم تصميم **فك تشفير فقط**. لقد تم دفع التحول نحو نماذج فك التشفير فقط بسبب بساطتها وفعاليتها في المهام الانحدارية الذاتية (على سبيل المثال، توليد النص). أدناه، سأقوم بتحليل الفروق الرئيسية.

#### الفروق المعمارية الأساسية
- **مُشفر-فك تشفير**:
  - يتكون من كومتين متماثلتين: **مُشفر** (يعالج تسلسل الإدخال بالكامل بشكل متوازٍ، باستخدام الانتباه الذاتي (self-attention) لالتقاط السياق ثنائي الاتجاه) و **فك تشفير** (يولد المخرجات بشكل انحداري ذاتي، باستخدام الانتباه الذاتي مع إخفاء سببي (causal masking) بالإضافة إلى الانتباه المتقاطع (cross-attention) لمخرجات المُشفر).
  - الأفضل للمهام **من تسلسل إلى تسلسل (seq2seq)** حيث يكون الإدخال والإخراج منفصلين (على سبيل المثال، الترجمة الآلية: الإنجليزية → الفرنسية).
  - يتعامل مع السياق ثنائي الاتجاه في الإدخال ولكن أحادي الاتجاه (من اليسار إلى اليمين) في الإخراج.

- **فك التشفير فقط**:
  - يستخدم فقط مكون فك التشفير، مع تعديل الانتباه الذاتي بواسطة **الإخفاء السببي** (حيث يمكن لكل رمز (token) الاهتمام بالرموز السابقة فقط، مما يمنع "الاطلاع" على الرموز المستقبلية).
  - يعامل التسلسل بالكامل (الإدخال + الإخراج) كتدفق واحد للتنبؤ الانحداري الذاتي (على سبيل المثال، التنبؤ بالرمز التالي في نمذجة اللغة).
  - مثالي للمهام **التوليدية** مثل روبوتات الدردشة، إكمال القصص، أو توليد الكود، حيث يتنبأ النموذج برمز واحد في كل مرة بناءً على السياق السابق.

#### جدول المقارنة

| الجانب              | محولات فك التشفير فقط                  | محولات التشفير-فك التشفير                  |
|---------------------|--------------------------------------------|-----------------------------------------------|
| **المكونات**     | كومة واحدة من طبقات فك التشفير (انتباه ذاتي + إخفاء سببي). | كومتان مزدوجتان: المُشفر (انتباه ذاتي ثنائي الاتجاه) + فك التشفير (انتباه ذاتي، إخفاء سببي، انتباه متقاطع). |
| **أنواع الانتباه**| انتباه ذاتي مقنّع (أحادي الاتجاه) فقط. | انتباه ذاتي (ثنائي الاتجاه في المُشفر)، انتباه ذاتي مقنّع (في فك التشفير)، وانتباه متقاطع (فك التشفير يهتم بالمُشفر). |
| **معالجة الإدخال/الإخراج** | الإدخال والإخراج في تسلسل واحد؛ توليد انحداري ذاتي. | تسلسلات منفصلة للإدخال (مشفرة) والإخراج (مفكوكة)؛ تسمح بالتشفير المتوازي. |
| **التعقيد**     | أبسط: معلمات أقل، أسهل في التوسع والتدريب على بيانات غير موسومة ضخمة. | أكثر تعقيدًا: عدد أعلى من المعلمات، يتطلب بيانات إدخال-إخراج مقترنة للتدريب. |
| **هدف التدريب** | عادةً التنبؤ بالرمز التالي (نمذجة اللغة السببية). | غالبًا فقدان الانتروبيا المتقاطعة على تسلسل الإخراج، باستخدام الإجبار بالمعلم (teacher forcing). |
| **نقاط القوة**      | يتفوق في التوليد مفتوح النهاية؛ يتوسع جيدًا (مثل GPT-3/4). فعال في نمذجة السياق الطويل. | أفضل للمهام المنظمة ذات تعيينات واضحة للإدخال-الإخراج (مثل التلخيص، الترجمة). يتعامل مع فك التشفير غير الانحداري الذاتي. |
| **نقاط الضعف**     | سياق ثنائي الاتجاه محدود (لا يمكن "رؤية" الإدخال الكامل دفعة واحدة)； أصعب للمهام التي تحتاج إلى تشفير صريح للإدخال. | أكثر تكلفة من الناحية الحسابية؛ أقل مرونة للتدريب المسبق التوليدي البحت. |
| **أمثلة**       | سلسلة GPT، LLaMA، PaLM.                  | T5، BART، المحول الأصلي للترجمة. |

#### لماذا التفضيل لنماذج فك التشفير فقط؟
اكتسبت نماذج فك التشفير فقط شعبية لأنها أسهل في التدريب المسبق على كميات هائلة من البيانات النصية (مجرد التنبؤ بالكلمة التالية) واضبطها لمهام متنوعة لاحقة. هذا النهج "نموذج واحد ليديرهم جميعًا" يبسط التطوير مقارنة بمشفرات-فك التشفير، التي تكون أكثر تخصصًا. ومع ذلك، تبرز مشفرات-فك التشفير في السيناريوهات التي تتطلب محاذاة قوية بين الإدخال والإخراج (على سبيل المثال، التوليد الشرطي).

إذا كنت تنفذ هذا أو لديك حالة استخدام محددة في الاعرض، أعلمني للحصول على نصائح أكثر تخصيصًا!

#### المراجع
- [Attention is All You Need (ورقة المحول الأصلية)](https://arxiv.org/abs/1706.03762)
- [Language Models are Few-Shot Learners (GPT-3، مثال فك تشفير فقط)](https://arxiv.org/abs/2005.14165)
- [Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer (T5، مثال مُشفر-فك تشفير)](https://arxiv.org/abs/1910.10683)