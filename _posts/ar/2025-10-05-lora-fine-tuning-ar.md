---
audio: false
generated: true
image: false
lang: ar
layout: post
model: x-ai/grok-4-fast
title: دليل ضبط لورا والمشاريع
translated: true
type: note
---

### فهم LoRA من المدونة
منشور المدونة الذي أشرت إليه يناقش "LoRA بدون ندم"، مسلطًا الضوء على التكيف ذي الرتبة المنخفضة (LoRA) كطريقة فعالة لضبط النماذج اللغوية الكبيرة (LLMs) بدقة عن طريق تحديث المصفوفات ذات الرتبة المنخفضة فقط بدلاً من النموذج الكامل. يغطي المزايا مثل الخدمة متعددة المستأجرين (على سبيل المثال، عبر أدوات مثل vLLM و SGLang)، واحتياجات ذاكرة أقل للتدريب، وأداء غالبًا ما يكون على قدم المساواة مع الضبط الدقيق الكامل لمجموعات البيانات النموذجية. لا يتعمق في مشاريع مبتدئة محددة ولكنه يذكر موارد مثل ورقة Punica لخدمة محولات LoRA المتعددة.

### كيفية العثور على مشروع لتشغيله باستخدام LoRA
إن العثور على مشروع LoRA هو أمر مباشر نظرًا لأنه تقنية شائعة في مجتمع التعلم الآلي مفتوح المصدر. إليك دليل خطوة بخطوة:

1.  **البحث على GitHub**: استخدم كلمات رئيسية مثل "LoRA fine-tuning" أو "LoRA LLM" أو "PEFT LoRA" في شريط بحث GitHub. رشّح النتائج حسب عدد النجوم (الشعبية)، والتفرعات (الاستخدام المجتمعي)، ومدى الحداثة (تم التحديث خلال السنة الماضية). استهدف المستودعات التي تحتوي على ملفات README واضحة، ودفاتر ملاحظات مثال، ونماذج مدربة مسبقًا.

2.  **استكشاف Hugging Face Hub**: ابحث عن "LoRA" في علامة تبويب النماذج (Models). العديد من المستودعات توفّر روابط لمحولات جاهزة للتشغيل (مضبوطة بدقة على مهام محددة مثل الدردشة أو التلخيص). يمكنك تنزيلها ودمجها مع النماذج الأساسية باستخدام مكتبة `peft`.

3.  **التحقق من مستودعات نماذج محددة**: ابحث عن أدلة الضبط الدقيق الرسمية من مبتكري النماذج (مثل Mistral، Llama) على صفحات GitHub الخاصة بهم — فهي غالبًا ما تتضمن أمثلة على LoRA.

4.  **منتديات المجتمع**: تصفح Reddit (r/MachineLearning أو r/LocalLLaMA)، أو X (المعروف سابقًا باسم Twitter) باستخدام هاشتاغ #LoRA، أو Papers with Code للعثور على تنفيذات مرتبطة بأوراق البحث العلمي.

5.  **المتطلبات للتشغيل**: معظم المشاريع تحتاج إلى Python، وPyTorch، ومكتبات مثل `transformers` و `peft`. ابدأ باستخدام وحدة معالجة الرسومات (GPU) (على سبيل المثال، عبر Google Colab للاختبار المجاني) ومجموعة بيانات مثل Alpaca لضبط النموذج على التعليمات.

هذا النهج يجب أن ينتج مشاريع قابلة للتشغيل بسرعة — توقع أوقات إعداد تتراوح بين 10-30 دقيقة للأساسيات.

### مشاريع مفتوحة المصدر جيدة لـ LoRA
إليك ثلاثة مشاريع مفتوحة المصدر صلبة ومناسبة للمبتدئين تركز على الضبط الدقيق باستخدام LoRA. إنها محفوظة جيدًا، ولها أمثلة، وتغطي حالات استخدام مختلفة:

-   **Microsoft's LoRA (التنفيذ الأصلي)**: المستودع الأساسي من مؤلفي ورقة LoRA. يتضمن حزمة `loralib` للتكامل مع PyTorch وأمثلة لنماذج Hugging Face. رائع لفهم الرياضيات الأساسية والتكيف مع الإعدادات المخصصة. النجوم: ~3k+.

-   **Alpaca-LoRA**: برنامج نصي بسيط لضبط نماذج LLaMA على التعليمات باستخدام أجهزة المستهلك العادية عبر LoRA. يعيد إنتاج نتائج Stanford's Alpama بشفرة دنوية — مثالي لتجربتك الأولى. يتضمن أوزانًا مدربة مسبقًا ويعمل على وحدة معالجة رسومات واحدة. النجوم: ~5k+.

-   **Mistral-Finetune**: رسمي من Mistral AI للضبط الدقيق الفعال في استخدام الذاكرة لنماذجهم (مثل Mistral-7B) باستخدام LoRA. يدعم ميزات متقدمة مثل التكميم (quantization)؛ مثالي إذا كنت تستهدف مهام الدردشة/التعليمات. النجوم: ~1k+.

ابدأ بـ Alpaca-LoRA إذا كنت جديدًا — انسخ المستودع، وقم بتثبيت التبعيات (`pip install -r requirements.txt`)، وشغّل `python generate.py` مع مجموعة بيانات. بالنسبة لجميعها، تحقق من ملف README لإعداد مجموعة البيانات.

### المراجع
-   [LoRA Without Regret](https://thinkingmachines.ai/blog/lora/)
-   [LoRA: Low-Rank Adaptation of Large Language Models](https://github.com/microsoft/LoRA)
-   [alpaca-lora: Instruct-tune LLaMA on consumer hardware](https://github.com/tloen/alpaca-lora)
-   [mistralai/mistral-finetune](https://github.com/mistralai/mistral-finetune)