---
audio: false
generated: true
image: false
lang: ar
layout: post
model: x-ai/grok-4-fast
title: 'المحولات: النشأة، التصميم، والمستقبل'
translated: true
type: note
---

### ملخص "كيف توقفت عن القلق وأحببت المحول" بقلم أشيش فاسواني

هذه المحادثة التي استمرت 45 دقيقة، والتي أُلقيت في 7 نوفمبر 2023 كجزء من سلسلة CS25 Transformers United بجامعة ستانفورد، هي غوص عميق تأملي في أصول وتصميم وتطور ومستقبل بنية المحول (Transformer). باعتباره مؤلفًا مشاركًا للورقة البحثية الرائدة عام 2017 "الانتباه هو كل ما تحتاجه"، يشارك فاسواني قصصًا شخصية من فترة عمله في Google Brain، ويكشف النقاب عن القرارات الرئيسية، ويقدم رؤى متفائلة لكنها واقعية للمرحلة التالية من الذكاء الاصطناعي. وهي مُنظمة حول السياق التاريخي، والابتكارات الأساسية، والتطورات اللاحقة للمحول، والأفكار التطلعية – مثالية لفهم سبب تحول المحولات إلى العمود الفقري للذكاء الاصطناعي الحديث.

#### الخلفية التاريخية والشرارة التي أدت إلى ظهور المحولات
يبدأ فاسواني بالإشارة إلى مؤتمر دارتموث عام 1956، حيث حلم رواد الذكاء الاصطناعي بآلة موحدة تحاكي الذكاء البشري عبر الرؤية واللغة والمزيد – باستخدام الأنظمة القائمة على القواعد وافتراض تحقيق انتصارات سريعة. وبعد 70 عامًا: على الرغم من فترات الشتاء التي مر بها الذكاء الاصطناعي، فإننا نعود إلى نقطة البداية مع محولات تشغل نماذج متعددة الوسائط. ويقارن هذا مع معالجة اللغة الطبيعية في العقد الأول من القرن الحادي والعشرين، والتي كانت عبارة عن خليط غير منظم من خطوط الأنابيب لمهام مثل الترجمة الآلية (مثل محاذاة الكلمات، واستخراج العبارات، إعادة التسجيل العصبي). بحلول عام 2013، كان المجال مجزأ إلى أقسام منعزلة مثل تحليل المشاعر أو الحوار، حيث كان التقدم مدفوعًا بالتمويل بدلاً من نظرية موحدة.

نقطة التحول؟ التمثيلات الموزعة (مثال: "ملك - رجل + امرأة ≈ ملكة" في word2vec) ونماذج التسلسل إلى تسلسل (seq2seq) (2014-2015)، والتي دمجت مهامًا متنوعة في أطر التشفير وفك التشفير (encoder-decoder). لكن الشبكات المتكررة مثل LSTM كانت مصدر إزعاج: المعالجة التسلسلية قتلت التوازي، حالات الخفية (hidden states) شكلت اختناقًا للمعلومات، والتبعيات طويلة المدى كانت ضعيفة. ساعدت الالتفافات (convolutions) (مثل ByteNet, ConvS2S) في السرعة لكنها عانت مع الاتصالات البعيدة.

**قصة داخلية:** أثناء العمل على الترجمة الآلية العصبية من جوجل (GNMT) في عام 2016، تخلص فريق فاسواني من خطوط الأنابيب واعتمد على LSTM بحتة، محققين أحدث المستويات مع بيانات هائلة. ومع ذلك، شعرت LSTM بأنها "محبطة" – بطيئة على وحدات معالجة الرسومات، وصعبة التوسع. كانت الفكرة المفاجئة هي التوق إلى التوازي الكامل: تشفير المدخلات وفك تشفير المخرجات دون عناء خطوة بخطوة. فشلت أحلام عدم الانحدار الذاتي المبكرة (توليد كل شيء دفعة واحدة، ثم التحسين) لأن النماذج لم تستطع تعلم الترتيب دون التوجيه من اليسار إلى اليمين، والذي يقوم بشكل طبيعي بتقليم المسارات غير المحتملة.

#### خيارات التصميم الأساسية: بناء المحول الأصلي
تخلت المحولات عن التكرار والالتفافات لصالح الانتباه (attention) الخالص، مما مكن من محادثات مباشرة بين الرموز (tokens) عبر التشابه في المحتوى – مثل سحب بقع الصور المتشابهة في مهام الرؤية (مثال: إزالة الضوضاء بوسائل غير محلية). الانتباه الذاتي (Self-attention) ثابت بالنسبة للتبديلات (permutation-invariant) لكنه مناسب للتوازي، مع تعقيد O(n² d) وهو مثالي لوحدات معالجة الرسومات عندما لا تكون التسلسلات لا نهائية.

لبنات البناء الرئيسية:
- **الانتباه القائم على الضرب النقطي المعياري (Scaled Dot-Product Attention):** إسقاطات Q, K, V من المدخلات؛ النقاط كـ softmax(QK^T / √d_k) موزونة على V. تم تحجيمها لتجنب اختفاء التدرجات (vanishing gradients) (بافتراض تباين وحدة). القناع السببي (Causal masking) لفك التشفير يمنع النظر إلى المستقبل. تم اختياره على الانتباه الجمعي (additive attention) لسرعة ضرب المصفوفات.
- **الانتباه متعدد الرؤوس (Multi-Head Attention):** الرأس الواحد يوسط كثيرًا (مثال: طمس أدوار "القطة لحست اليد"). تقسم الرؤوس الأبعاد إلى فضاءات فرعية – مثل آلات تورينج متعددة الأشرطة – للحصول على فضاءات فرعية مركزة (مثال: رأس واحد يثبت الاحتمال 1 على تفاصيل محددة). لا حاجة لحساب إضافي، انتقائية تشبه الالتفاف.
- **التشفير الموضعي (Positional Encoding):** دوال الجيب (Sinusoids) تحقن الترتيب، بهدف الوصول إلى المواضع النسبية (قابلة للتحلل حسب المسافة). لم تتعلم النسبية بشكل كامل في البداية، لكنها نجحت.
- **التراص والاستقرار:** مكدسات المشفر-فك التشفير مع البواقي (residuals) وتطبيع الطبقة (layer norm) (التطبيع المسبق pre-norm لاحقًا للشبكات الأعمق). الشبكات التغذوية الأمامية (Feed-forward) تتوسع وتنقبض مثل ResNets. المشفر: انتباه ذاتي؛ فك التشفير: انتباه ذاتي مقنع + انتباه متقاطع (cross-attention).

سحقت النتائج المرجعية لـ WMT بثمانية أضعاف عدد العمليات الحسابية العائمة (flops) الأقل من مجموعات LSTM، وعممت على التحليل، وأشارت إلى إمكانات متعددة الوسائط. القابلية للتفسير؟ تخصصت الرؤوس (بعضها طويل المدى، وأخرى محلية تشبه الالتفاف)، لكن فاسواني يمزح قائلاً إنها "قراءة فنجان شاي" – واعدة لكن ضبابية.

#### التطور: إصلاحات وانتصارات القياس
"ثبتت" المحولات لأنها بسيطة، لكن التحسينات الصغيرة عززتها:
- **المواضع 2.0:** دوال الجيب قصرت في النسبية؛ عززت التضمينات النسبية (التحيزات لكل زوج) الترجمة والموسيقى. ALiBi (تحيزات المسافة المتعلمة) تستقرئ الأطوال؛ RoPE (التدويرات الممزجة للمطلق والنسبي) هو الآن الملك – يوفر الذاكرة، ويتقن النسبية.
- **السياقات الطويلة:** لعنة التربيعية (Quadratic curse)؟ النوافذ المحلية، الأنماط المتفرقة (ممتدة/عالمية)، التجزئة (Reformer)، الاسترجاع (Memorizing Transformer)، حيل الرتبة المنخفضة. Flash Attention تتخطى كتابة الذاكرة للسرعة؛ Multi-Query تقطع رؤوس KV للاستدلال. النماذج الكبيرة تخفف من تكلفة الانتباه على أي حال.
- **تحسينات أخرى:** التطبيع المسبق (Pre-norm) يثبت؛ فك التشفير التخميني (speculative decoding) (مسودة سريعة، تحقق بطيء) يحاكي سرعة عدم الانحدار الذاتي في الإنتاج.

**إفادة داخلية:** اختراق الانتباه النسبي الفعال كان "تمارين رياضية للمصفوفات"، لكن فيزياء العتاد (مثال: الضرب النقطي للمسرعات) وجهت الخيارات.

#### الاتجاهات المستقبلية: ما وراء القياس
فاسواني متفائل: العمالقة غير الخاضعة للإشراف الذاتي يمكن وكلاء في السياق (in-context agents)، مما يعيد صدا آلة دارتموث الموحدة. قوانين القياس (Scaling laws) تحكم، لكن راقب إحياء RNN أو معماريات أفضل. الأولويات:
- **وكلاء متعددو الوسائط:** برمجة الآلاف بسرعة؛ الأدوات كجسور (استيعاب البسيط منها، والتعاون على المعقد).
- **البيانات والبنية التحتية:** مكاسب مضاعفة من بيانات أفضل؛ FP8/INT8 لعرض النطاق الترددي، تدريب على نطاق InfiniBand.
- **الذكاء التكيفي:** النماذج الصغيرة + المخططين/تمثيلات البيانات تُطابق الكبيرة؛ التعلم بالقلة (few-shot) أثناء الاستدلال؛ إشارات عدم اليقين؛ بناء المهارات (مثال: روبوتات ماينكرافت).
- **السحر الشامل (Full-Stack):** حلقات التغذية الراجعة لسير العمل (مثال: تحليل البيانات كتنقيب عن "المعرفة المظلمة" dark knowledge).
- **الظواهر الناشئة (Emergents):** من الاستبصار (Grokking) في الألعاب إلى غموض GPT-4 – دراسة الصغير لفهم الكبير.
- **الأنظمة:** عرض النطاق الترددي للذاكرة هو عنق الزجاجة؛ الاسترجاع لكل شيء.

يمزح عن شركته الناشئة مع المؤلف المشارك نوام شازير: أتمتة سير العمل من تحليل البيانات، شاملة (full-stack) لسحر التغذية الراجعة. أدوات حتى "المحولات في كل مكان" (Transformers all the way down)، مستفيدة من ذكاء البرمجيات.

#### أبرز الأسئلة والأجوبة
في الدردشة بعد المحادثة:
- عقبات عدم الانحدار الذاتي: تحتاج إلى "ترتيب إرشادي (oracle ordering)" للاستقلالية؛ الفضاءات الكامنة تساعد لكن تعدد الوسائط يعض.
- العالم من النص: نماذج اللغة الكبيرة + التعلم المعزز من التغذية الراجعة البشرية (RLHF) تستخرج مخططي الروبوتات؛ مزج بيانات التدريب يثير إبداعًا "جديدًا".
- الوحدانية (Modularity): MoE تحرض الخبراء؛ تنسيق الوكلاء المتعددين (الأهداف، التحديثات، التحقق) هو المفتاح.
- النصيحة: اطلب حلقة مغلقة بين الإنسان والذكاء الاصطناعي؛ المجال مفتوح على مصراعيه للمفاجآت.

أشياء خالدة – إيحاءات تاريخية، عمق تقني، إلهام دون ضجيج. إذا كنت في مجال الذكاء الاصطناعي، فهي مشاهدَة ضرورية لفهم السياق حول سبب فوز المحولات.

[كيف توقفت عن القلق وأحببت المحول (يوتيوب)](https://www.youtube.com/watch?v=1GbDTTK3aR4)