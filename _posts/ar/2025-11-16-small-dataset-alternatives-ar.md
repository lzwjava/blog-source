---
audio: false
generated: true
image: false
lang: ar
layout: post
model: x-ai/grok-4-fast
title: بدائل مجموعة البيانات الصغيرة عالية الجودة
translated: true
type: note
---

نعم، OpenWebText ضخم جدًا (~9 مليار رمز، ملف 17 جيجابايت بتنسيق uint16) ويتطلب وقتًا طويلاً للمعالجة المسبقة ومساحة كبيرة على القرص. إليك بدائل عالية الجودة وأصغر حجمًا بكثير تُستخدم عادةً للاختبار، التصحيح، أو تدريب النماذج الصغيرة بسرعة:

| مجموعة البيانات           | عدد الرموز التقريبي | الحجم النهائي لـ .bin (uint16) | سبب الإعجاب بها                                                                   | كيفية الحصول عليها بسرعة                                |
|---------------------------|---------------------|--------------------------------|-----------------------------------------------------------------------------------|----------------------------------------------------------|
| **OpenWebText-10k**       | ~40 مليون           | ~80 ميجابايت                  | نفس توزيع OpenWebText الكامل بالضبط، فقط أول 10 ألف مستند                         | `load_dataset("openwebtext", split="train[:10k]")`       |
| **OpenWebText-100k**      | ~400 مليون          | ~800 ميجابايت                 | لا يزال تمثيليًا جدًا، ينتهي الترميز في دقائق قليلة                               | `split="train[:100k]"`                                   |
| **عينة FineWeb-Edu**      | 50 مليون–1 مليار    | 100 ميجابايت–2 جيجابايت       | جودة أعلى من OWT (تصفية على غرار LLama)، شائع جدًا مؤخرًا                         | `load_dataset("HuggingFaceFW/fineweb-edu", name="sample-10BQ", split="train")` → ~50 مليون رمز |
| **Shakespeare**           | ~1 مليون            | ~2 ميجابايت                   | مجموعة بيانات صغيرة كلاسيكية، مثالية للفحوصات السريعة                             | `load_dataset("tiny_shakespeare")` أو تنزيل ملف .txt المفرد |
| **PG-19 (كتب)**           | كامل 2.8 مليار      | ~5.5 جيجابايت                 | كتب في المجال العام نظيفة جدًا، ولكن يمكنك أخذ جزء منها فقط                       | `load_dataset("pg19", split="train[:5%]")` → ~140 مليون رمز |
| **C4 (مجموعة فرعية)**     | متغير               | متغير                         | Common Crawl تم تنظيفها بواسطة فريق T5، باللغة الإنجليزية فقط                     | `load_dataset("allenai/c4", "en", split="train[:1%]")` streaming=True |
| **عينة RedPajama**        | 5 مليون–200 مليون   | 10 ميجابايت–400 ميجابايت      | مزيج من المصادر (CommonCrawl، Wikipedia، كتب، إلخ)، نفس وصفة LLaMA                | `load_dataset("togethercomputer/RedPajama-Data-1T", name="sample", split="train")` |

### توصياتي الشخصية بناءً على هدفك

1. تريد فقط شيئًا يعمل في أقل من 5 دقائق ويتناسب مع أي مكان → **tiny_shakespeare**  
   (مستخدمي nanoGPT يحبون هذا)

2. تريد شيئًا لا يزال "نصًا حقيقيًا من الإنترنت" لكنه صغير جدًا → أول 10 آلاف مستند من OpenWebText  
   ```python
   dataset = load_dataset("openwebtext", split="train[:10000]")
   ```

3. تريد مجموعة البيانات الصغيرة ذات الأعلى جودة حاليًا (2025) → عينة FineWeb-Edu ذات 10 مليار رمز  
   (~50 مليون رمز، ولكن أنظف بكثير من OWT، جودة مطابقة تقريبًا لما تستخدمه أفضل المعامل)

   ```python
   dataset = load_dataset(
       "HuggingFaceFW/fineweb-edu",
       name="sample-10BQ",  # عينة من 10 مليارات رمز، ~50 مليون لكل شارد إذا قمت بالتقطيع
       split="train"
   )
   ```

4. تريد شيئًا أقل من 500 ميجابايت لكنه لا يزال متنوعًا → عينة RedPajama (نسخة 200 مليون رمز)  
   ```python
   dataset = load_dataset("togethercomputer/RedPajama-Data-1T", name="sample-200M", split="train")
   ```

كل هذه الخيارات تعمل مباشرة مع نفس سكربت المعالجة المسبقة لديك دون أي تغييرات (فقط استبدل سطر load_dataset).

للتكرار السريع، أحتفظ عادةً بثلاثة ملفات ثنائية على القرص:
- `shakespeare.bin`   (~2 ميجابايت)   → اختبارات Overfit
- `owt_10k.bin`       (~80 ميجابايت)  → تصحيح الأخطاء في الكود الجديد
- `fineweb_edu_1B.bin` (~2 جيجابايت) → تدريب النماذج الصغيرة الفعلي

أخبرني بالحجم الذي تريده وسأعطيك الأمر الدقيق المكون من سطر واحد!