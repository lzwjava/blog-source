---
audio: false
generated: true
lang: ar
layout: post
title: الشبكة العصبية المتكررة
translated: true
type: note
---

دعونا نحلل الشبكات العصبية المتكررة (RNNs):

**ما هي الشبكة العصبية المتكررة (RNN)؟**

الشبكة العصبية المتكررة (RNN) هي نوع من الشبكات العصبية الاصطناعية المصممة خصيصًا لمعالجة البيانات المتسلسلة. على عكس الشبكات العصبية الأمامية القياسية التي تعالج كل مدخل بشكل مستقل، تمتلك RNNs "ذاكرة" للمدخلات السابقة. يتم الحفاظ على هذه الذاكرة من خلال حالة مخفية يتم نقلها من خطوة زمنية إلى التي تليها.

إليك طريقة مبسطة للتفكير في الأمر:

* **تسلسل المدخلات:** تأخذ RNN تسلسلًا من المدخلات، مثل الكلمات في جملة، أو أسعار الأسهم عبر الزمن، أو الإطارات في مقطع فيديو.
* **الحالة المخفية:** في كل خطوة زمنية، تقوم RNN بمعالجة المدخل الحالي والحالة المخفية السابقة. يتم استخدام هذه المعلومات المجمعة لحساب الحالة المخفية الجديدة. تعمل الحالة المخفية كملخص للمعلومات التي تم رؤيتها حتى الآن في التسلسل.
* **المخرجات:** بناءً على المدخل الحالي والحالة المخفية، يمكن لـ RNN إنتاج مخرج في كل خطوة زمنية. يمكن أن يكون هذا المخرج تنبؤًا، أو تصنيفًا، أو جزءًا آخر من المعلومات.
* **التكرار:** السمة الرئيسية هي الاتصال المتكرر، حيث يتم إعادة تغذية الحالة المخفية من الخطوة الزمنية السابقة إلى الشبكة للتأثير على معالجة الخطوة الزمنية الحالية. هذا يسمح للشبكة بتعلم الأنماط والتبعيات عبر التسلسل.

**في أي الحالات تعمل RNNs بشكل جيد؟**

تكون RNNs فعالة بشكل خاص في المهام حيث يهم ترتيب وسياق البيانات. فيما يلي بعض الأمثلة:

* **معالجة اللغة الطبيعية (NLP):**
    * **نمذجة اللغة:** التنبؤ بالكلمة التالية في الجملة.
    * **توليد النص:** إنشاء نصوص جديدة، مثل القصائد أو المقالات.
    * **الترجمة الآلية:** ترجمة النص من لغة إلى أخرى.
    * **تحليل المشاعر:** تحديد النغمة العاطفية للنص.
    * **التعرف على الكيانات المسماة:** تحديد وتصنيف الكيانات (مثل أسماء الأشخاص والمنظمات والمواقع) في النص.
* **تحليل السلاسل الزمنية:**
    * **توقع أسعار الأسهم:** التنبؤ بأسعار الأسهم المستقبلية بناءً على البيانات التاريخية.
    * **توقعات الطقس:** التنبؤ بالظروف الجوية المستقبلية.
    * **كشف الشذوذ:** تحديد الأنماط غير العادية في البيانات المستندة إلى الزمن.
* **التعرف على الكلام:** تحويل اللغة المنطوقة إلى نص.
* **تحليل الفيديو:** فهم المحتوى والديناميكيات الزمنية لمقاطع الفيديو.
* **توليد الموسيقى:** إنشاء قطع موسيقية جديدة.

في الجوهر، تتفوق RNNs عندما يعتمد المخرج في خطوة زمنية معينة ليس فقط على المدخل الحالي ولكن أيضًا على تاريخ المدخلات السابقة.

**ما المشاكل التي تواجهها RNNs؟**

على الرغم من فعاليتها في العديد من المهام المتسلسلة، تعاني RNNs التقليدية من عدة قيود رئيسية:

* **اختفاء وانفجار التدرجات:** هذه هي المشكلة الأكثر أهمية. أثناء عملية التدريب، يمكن أن تصبح التدرجات (التي تستخدم لتحديث أوزان الشبكة) إما صغيرة جدًا (تختفي) أو كبيرة جدًا (تنفجر) عند انتشارها العكسي عبر الزمن.
    * **اختفاء التدرجات:** عندما تصبح التدرجات صغيرة جدًا، تواجه الشبكة صعوبة في تعلم التبعيات طويلة المدى. تضيع المعلومات من الخطوات الزمنية السابقة، مما يجعل من الصعب على الشبكة تذكر السياق عبر التسلسلات الطويلة. هذا هو جوهر مشكلة "التبعية طويلة المدى" المذكورة في المطالبة.
    * **انفجار التدرجات:** عندما تصبح التدرجات كبيرة جدًا، يمكن أن تسبب عدم استقرار في عملية التدريب، مما يؤدي إلى تحديثات للأوزان كبيرة جدًا وتجعل الشبكة تتباعد.
* **صعوبة تعلم التبعيات طويلة المدى:** كما ذكر أعلاه، تجعل مشكلة اختفاء التدرج من الصعب على RNNs التقليدية تعلم العلاقات بين العناصر في التسلسل التي تفصلها مسافات كبيرة. على سبيل المثال، في الجملة "القط، الذي كان يطارد الفئران طوال الصباح، ذهب أخيرًا إلى النوم"، قد تواجه RNN تقليدية صعوبة في ربط "القط" بـ "ذهب إلى النوم" بسبب الكلمات المتداخلة.
* **التكلفة الحسابية:** يمكن أن يكون تدريب RNNs مكلفًا حسابيًا، خاصة للتسلسلات الطويلة، بسبب الطبيعة المتكررة للحسابات.
* **المعالجة التسلسلية:** تعالج RNNs البيانات بشكل تسلسلي بطبيعتها، مما قد يحد من قدرتها على التوازي ويمكن أن يجعلها أبطأ للتسلسلات الطويلة جدًا مقارنة بالنماذج التي يمكنها معالجة البيانات بالتوازي.

**الحلول الحديثة:**

لمعالجة هذه القيود، خاصة مشكلة اختفاء التدرج وصعوبة تعلم التبعيات طويلة المدى، تم تطوير هياكل أكثر تقدمًا، مثل:

* **شبكات الذاكرة طويلة قصيرة المدى (LSTM):** تقدم هذه الشبكات خلية ذاكرة وآليات بوابة تسمح لها بتذكر أو نسيان المعلومات بشكل انتقائي على فترات طويلة.
* **وحدات متكررة ذات بوابة (GRUs):** GRUs هي نسخة مبسطة من LSTM تستخدم أيضًا آليات بوابة للتحكم في تدفق المعلومات.
* **المحوّلات (Transformers):** على الرغم من أنها ليست RNNs بالمعنى الدقيق، أصبحت المحولات ناجحة للغاية في التعامل مع البيانات المتسلسلة، خاصة في معالجة اللغة الطبيعية (NLP)، من خلال استخدام آليات الانتباه لنمذجة التبعيات بين جميع المواضع في تسلسل المدخلات مباشرة، متغلبًا على قيد المعالجة التسلسلية ومشكلة التبعية طويلة المدى بشكل أكثر فعالية.

باختصار، تعتبر RNNs قوية لمعالجة البيانات المتسلسلة حيث يهم الترتيب، لكنها تواجه تحديات، خاصة مع التبعيات طويلة المدى بسبب مشكلة اختفاء التدرج. تم تطوير هياكل حديثة مثل LSTM و GRUs والمحوّلات للتخفيف من هذه المشكلات وتحقيق أداء أفضل في المهام المتسلسلة المختلفة.