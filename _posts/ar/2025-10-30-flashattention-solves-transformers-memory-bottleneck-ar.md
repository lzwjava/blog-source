---
audio: false
generated: true
image: false
lang: ar
layout: post
model: x-ai/grok-4-fast
title: انتباه الفلاش يحل مشكلة اختناق الذاكرة في المحولات
translated: true
type: note
---

### الخلفية: تحديات الانتباه القياسي في المحولات (Transformers)

تعتمد المحولات (Transformers)، وهي العمود الفقري لنماذج مثل GPT وBERT، بشكل كبير على آلية الانتباه الذاتي (self-attention) لمعالجة التسلسلات. في الانتباه الذاتي، وبالنظر إلى تسلسلات الإدخال الممثلة كمصفوفة الاستعلام \\(Q\\)، ومصفوفة المفتاح \\(K\\)، ومصفوفة القيمة \\(V\\) (كل منها ذات شكل \\(N \times d\\)، حيث \\(N\\) هو طول التسلسل و \\(d\\) هو بُعد الرأس، عادةً 64-128)، يتم حساب ناتج الانتباه \\(O\\) كما يلي:

\\[
S = QK^T \in \mathbb{R}^{N \times N}, \quad P = \softmax(S) \in \mathbb{R}^{N \times N}, \quad O = PV \in \mathbb{R}^{N \times d},
\\]

حيث يتم تطبيق \\(\softmax\\) بشكل صفي، وغالبًا ما يتم قياس \\(S\\) بواسطة \\(\tau = 1 / \sqrt{d}\\) لتحقيق الاستقرار. العمليات الإضافية مثل الإخفاء السببي (causal masking) (للنماذج الانحدارية الذاتية) والإسقاط (dropout) شائعة.

هذه الصيغة أنيقة ولكنها مكلفة حسابيًا. المصفوفات الوسيطة \\(S\\) و \\(P\\) هي ذات أبعاد \\(N \times N\\)، مما يؤدي إلى **تعقيد زمني وذاكرة تربيعي** \\(O(N^2)\\) في طول التسلسل \\(N\\). بالنسبة للسياقات الطويلة (مثل \\(N = 4096\\) في GPT-2 أو حتى 128k في النماذج اللغوية الكبيرة الحديثة)، يصبح هذا عنق زجاجة حاد:

- **الجوع للذاكرة**: في وحدات معالجة الرسومات (GPUs)، تكون ذاكرة النطاق الترددي العالي (HBM) هي التخزين الأساسي، ولكن تجسيد \\(S\\) و \\(P\\) يمكن أن يتجاوز سعة HBM المتاحة (مثلاً، 40-80 جيجابايت على A100/H100). عند \\(N=4096\\), \\(d=64\\)، يستهلك هذا وحده ~1-2 جيجابايت فقط للقيم الوسيطة، بالإضافة إلى المدخلات/المخرجات/التنشيطات، مما يتسبب غالبًا في أخطاء نفاد الذاكرة (OOM).
- **قيود السرعة**: الانتباه مقيد بالذاكرة، وليس بالحساب. وحدات معالجة الرسومات الحديثة (مثل NVIDIA A100) لديها ~1.5 تيرابايت/ثانية من عرض النطاق الترددي لـ HBM ولكن ~19 تيرافلوبس من القدرة الحاسوبية—ومع ذلك، تتطلب عمليات مثل softmax قراءة/كتابة مصفوفة \\(N^2\\) الكاملة عدة مرات (مثلاً، 4-6 وصولات HBM لكل عنصر في المرورتين الأمامية والخلفية). هذا يؤدي إلى أوقات تنفيذ فعلية تتدرج تربيعيًا: مثلاً، المرور الأمامي ~36 مللي ثانية عند \\(N=4096\\) في PyTorch، والمرور الخلفي ~88 مللي ثانية.
- **عوائق التدريب/التوليد**: أثناء التدريب، تتطلب التدرجات (gradients) تخزين \\(P\\) للمرور الخلفي، مما يضاعف الذاكرة. بالنسبة للاستدلال، تكون السياقات الطويلة (مثل 64k رمز) غير مجدية دون تقريبات مثل الانتباه المتفرق (sparse attention) أو الطرق منخفضة الرتبة (مثل Reformer, Linformer)، والتي تبادل الدقة بالكفاءة ولكنها غالبًا ما تقدم أداءً أقل بسبب تجاهل تكاليف الإدخال/الإخراج (I/O).

يتعامل FlashAttention (الذي قدمه Tri Dao وآخرون في 2022) مع هذه المشاكل من خلال إعادة التفكير في الخوارزمية لتكون **واعية بالإدخال/الإخراج (I/O-aware)**، مستفيدًا من التسلسل الهرمي لذاكرة وحدة معالجة الرسومات (ذاكرة الوصول العشوائي الساكنة السريعة SRAM ~20 ميجابايت مقابل HBM البطيئة) دون تقريبات.

### الأفكار الأساسية: التجزئة (Tiling)، دمج النواة (Kernel Fusion)، و Softmax عبر الإنترنت (Online)

يحسب FlashAttention الانتباه **الدقيق** (بدون تقريبات) عن طريق:

1.  **التجزئة (Tiling)**: بدلاً من تجسيد المصفوفات الكاملة \\(N \times N\\)، يقسم \\(Q, K, V\\) إلى كتل صغيرة تتسع في ذاكرة SRAM. يتم تقسيم \\(Q\\) إلى \\(T_r = \lceil N / B_r \rceil\\) كتل صفية بحجم \\(B_r \times d\\) (مثلاً، \\(B_r \approx 64-256\\))، ويتم تقسيم \\(K, V\\) إلى \\(T_c = \lceil N / B_c \rceil\\) كتل عمودية بحجم \\(B_c \times d\\) (مثلاً، \\(B_c \approx 128-1024\\)). يتم اختيار أحجام الكتل ديناميكيًا بناءً على سعة SRAM \\(M\\) (مثلاً، \\(B_c \approx M / (4d)\\)) لتعظيم إعادة الاستخدام.

2.  **دمج النواة (Kernel Fusion)**: يتم دمج جميع العمليات (ضرب المصفوفات لـ \\(S\\)، الإخفاء، softmax، الإسقاط، ضرب المصفوفات لـ \\(O\\)) في نواة CUDA واحدة. هذا يتجنب كتابة القيم الوسيطة إلى HBM، مما يقلل الإدخال/الإخراج بنسبة ~50-70%. تقوم النواة بتحميل الكتل من HBM إلى SRAM، وتحسب على الشريحة (on-chip)، وتكتب فقط المجاميع الجزئية مرة أخرى—مثلاً، قراءة/كتابة واحدة لـ HBM لكل كتلة بدلاً من كل عنصر.

3.  **Softmax عبر الإنترنت مع الإحصائيات**: لا يمكن حساب softmax بشكل جزئي دون الصف الكامل، لذلك يستخدم FlashAttention **تحليلًا ترابطيًا (associative decomposition)** للحساب التدريجي. بالنسبة لصف مقسم إلى كتل \\(x = [x^{(1)}; x^{(2)}]\\)، تتبع الإحصائيات الجارية:
    - القيمة القصوى للصف \\(m_i = \max_j S_{ij}\\)،
    - المجموع الصفي للأسس \\(\ell_i = \sum_j \exp(S_{ij} - m_i)\\).

    يتم التحديث لكتلة جديدة \\(x^{(t)}\\) مع إحصائيات محلية \\(\tilde{m}_t, \tilde{\ell}_t\\):
    \\[
    m_i^{\new} = \max(m_i, \tilde{m}_t), \quad \ell_i^{\new} = e^{m_i - m_i^{\new}} \ell_i + e^{\tilde{m}_t - m_i^{\new}} \tilde{\ell}_t.
    \\]
    ثم يكون softmax الجزئي هو \\(\tilde{P}_{ij} = \exp(S_{ij} - m_i^{\new})\\)، ويتراكم الناتج كـ \\(O_i \leftarrow \frac{\ell_i}{\ell_i^{\new}} e^{m_i - m_i^{\new}} O_i + \frac{\tilde{\ell}_t}{\ell_i^{\new}} e^{\tilde{m}_t - m_i^{\new}} \tilde{P}_{ij} V_j\\).

    هذا مستقر عدديًا (يطابق softmax المدمج) ودقيق، كما تم إثباته بالاستقراء: بعد جميع الكتل، \\(O = \softmax(S) V\\).

هذه الأفكار تقلل **الذاكرة إلى \\(O(N)\\)** (المدخلات + المخرجات + إحصائيات \\(O(N)\\) مثل \\(m, \ell\\)) و **وصولات HBM إلى \\(O(N^2 d / M)\\)**—أقل من التربيعي، حيث يتم قراءة كل عنصر من \\(K/V\\) مرة واحدة، ويتم قراءة \\(Q/O\\) بمقدار \\(T_c \approx N d / M\\) مرات.

### المرور الأمامي: الحساب كتلة تلو كتلة

المرور الأمامي (كود زائف في الخوارزمية 2 في الورقة) يتكرر على الكتل العمودية لـ \\(K, V\\):

- التهيئة: \\(O = 0^{N \times d}\\), \\(m = -\infty^N\\), \\(\ell = 0^N\\) في HBM.
- لكل كتلة عمودية \\(j = 1\\) إلى \\(T_c\\):
  - تحميل \\(K_j, V_j\\) إلى SRAM (إعادة استخدام عبر الصفوف).
  - لكل كتلة صفية \\(i = 1\\) إلى \\(T_r\\):
    - تحميل \\(Q_i, O_i, m_i, \ell_i\\) إلى SRAM.
    - حساب \\(S_{ij} = \tau Q_i K_j^T\\) محليًا (\\(B_r \times B_c\\)).
    - الإخفاء: \\(S_{ij}^{\masked} = \mask(S_{ij})\\) (مثلاً، سببي: المثلث السفلي إلى \\(-\infty\\)).
    - إحصائيات softmax المحلية: \\(\tilde{m}_{ij} = \rowmax(S_{ij}^{\masked})\\), \\(\tilde{P}_{ij} = \exp(S_{ij}^{\masked} - \tilde{m}_{ij})\\), \\(\tilde{\ell}_{ij} = \rowsum(\tilde{P}_{ij})\\).
    - تحديث الإحصائيات العامة والناتج باستخدام الصيغ أعلاه، مع تطبيق الإسقاط على \\(\tilde{P}_{ij}\\).
    - كتابة \\(O_i, m_i, \ell_i\\) المحدثة إلى HBM.

يدمج هذا كل شيء: إجمالي عمليات الفلوپ (FLOPs) تبقى \\(O(N^2 d)\\)، لكن الإدخال/الإخراج ينخفض بشكل كبير (مثلاً، أقل بمقدار 9 مرات من المعياري). بالنسبة للانتباه السببي، يكون الإخفاء رخيصًا (متجهي). يستخدم الإسقاط حالة عشوائية مشتركة \\(R\\) محفوظة للمرور الخلفي.

### المرور الخلفي: حساب التدرج عبر إعادة الحساب (Recomputation)

المرور الخلفي (الخوارزمية 4) أكثر تعقيدًا، حيث تعتمد التدرجات على \\(P\\):

\\[
dP = dO \cdot V^T, \quad dS = P \odot (dP - \rowsum(dO \odot O)), \quad dQ = dS \cdot K, \quad dK = Q^T \cdot dS, \quad dV = P^T \cdot dO.
\\]

سيؤدي تخزين \\(P\\) إلى \\(O(N^2\\)، لذلك يقوم FlashAttention **بإعادة حساب الكتل على الطاير** (إعادة حساب انتقائية، مثل checkpointing ولكن مجزأ):

- التكرار بشكل مشابه: لكل \\(j\\)، تحميل \\(K_j, V_j\\)؛ تهيئة \\(dK_j, dV_j = 0\\) محليًا.
- لكل \\(i\\): إعادة حساب \\(S_{ij}, P_{ij}\\) باستخدام \\(m_i, \ell_i\\) المحفوظة؛ إعادة إنشاء قناع الإسقاط من \\(R\\).
- حساب التدرجات المحلية: \\(dV_j += P_{ij}^{dropped^T} dO_i\\), \\(dP_{ij} = dO_i V_j^T \odot Z_{ij}\\) (قناع الإسقاط), \\(dS_{ij} = P_{ij} \odot (dP_{ij} - D_i)\\) حيث \\(D_i = \rowsum(dO_i \odot O_i)\\).
- التراكم: \\(dQ_i += \tau dS_{ij} K_j\\), \\(dK_j += \tau Q_i^T dS_{ij}\\).

يستخدم هذا \\(O(N^2 d)\\) إضافية من عمليات الفلوپ (FLOPs) ولكن فقط \\(O(N)\\) ذاكرة إضافية (لا تخزين لـ \\(P\\)). إجمالي الأمامي + الخلفي: ~2-3 أضعاف عمليات الفلوپ (FLOPs) للمعياري ولكن أسرع بمقدار 2-4 مرات بسبب توفير الإدخال/الإخراج.

### الوعي بالإدخال/الإخراج وتحسينات وحدة معالجة الرسومات

لوحدات معالجة الرسومات تسلسل هرمي: السجلات/SRAM (سريعة، صغيرة) >> HBM (بطيئة، كبيرة). الانتباه المعياري يثقل كاهل HBM بـ \\(\Theta(N^2)\\) وصولات لكل مرور. تضمن تجزئة FlashAttention:
- تحميل \\(K, V\\) مرة واحدة (\\(O(N d)\\)).
- تحميل \\(Q, O\\) بمقدار \\(T_c \approx N / B_c \approx N d / M\\) مرات (\\(O(N^2 d / M)\\)).
- الحد الأدنى: لا توجد خوارزمية دقيقة تتغلب على \\(\Omega(N^2 d^2 / M)\\) لنطاق متوسط لـ \\(M\\).

عمليًا: على A100، تهيمن توقفات HBM على وقت التشغيل؛ يقلل FlashAttention منها بنسبة 50-80%، ليصل إلى نظام مقيد بالحساب. وهو يدعم التفرق الكتلي (block-sparsity) (تخطي كتل الإخفاء الصفرية) لمكاسب أكبر (2-4x فوق الكثيف).

### الفوائد: السرعة، الذاكرة، والتأثير اللاحق

- **الذاكرة**: خطية \\(O(N d)\\)، مما يمكن من تسلسلات 64k+ على وحدات معالجة الرسومات المفردة (مقابل 2k-4k معياري). مثلاً، 13 جيجابايت عند \\(N=65k\\) مقابل 200+ جيجابايت معياري.
- **السرعة**: 2-4x من البداية إلى النهاية في تدريب GPT/BERT؛ حتى 7x على الانتباه الخام. مثلاً، الأمامي/الخلفي مجتمعين: 0.43 مللي ثانية عند \\(N=128\\) إلى 9 ثوانٍ عند \\(N=65k\\) (مقابل OOM في PyTorch).
- **الجودة**: دقيق، لذلك لا يوجد انخفاض في الحيرة (perplexity). يمكن من سياقات أطول: مكسب 0.7 نقطة في الحيرة على GPT-2 عند طول 4x؛ أحدث الأداء في مهام المستندات الطويلة (مثلاً، 63% على Path-256 عند 64k تسلسل).
- **الامتدادات**: FlashAttention-2 (2023) يضيف توازيًا أفضل (أسرع حتى 2x)؛ FlashAttention-3 (2024) يستخدم عدم التزامن/دقة منخفضة لـ H100 (أسرع 50%).

### التكامل مع PyTorch: استخدام سلس

PyTorch 2.0+ يدمج FlashAttention بشكل أصلي عبر `torch.nn.functional.scaled_dot_product_attention(q, k, v)`، والتي توجه تلقائيًا إلى النواة المدمجة إذا تحققت الشروط (مثلاً، موترات متجاورة، أنواع بيانات مدعومة). لا حاجة لـ CUDA مخصص:

```python
import torch
from torch.nn.functional import scaled_dot_product_attention

q, k, v = torch.randn(1, 8, 1024, 64, device='cuda')  # batch, heads, seq, dim
with torch.backends.cuda.sdp_kernel(enable_flash=True, enable_math=False):
    out = scaled_dot_product_attention(q, k, v, attn_mask=mask, dropout_p=0.1)
```

هذا يتعامل مع الإخفاء/الإسقاط؛ يعود إلى نواة math إذا لم يكن مدعومًا. للنماذج المخصصة، فهو بديل مباشر لحلقات الانتباه اليدوية، مما يعطي تسريعًا بمقدار 2-3x مباشرة.

أحدث FlashAttention ثورة في تدريب السياقات الطويلة، مما دعم نماذج مثل Llama-2 (4k→70B) وأكثر.

[FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness](https://arxiv.org/abs/2205.14135)  
[PyTorch 2.2: FlashAttention-v2 Integration](https://pytorch.org/blog/pytorch2-2/)  
[GitHub: Dao-AILab/flash-attention](https://github.com/Dao-AILab/flash-attention)