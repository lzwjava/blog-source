---
audio: false
generated: true
image: false
lang: ar
layout: post
model: x-ai/grok-4-fast
title: شرح مُحسِّن آر إم إس بروب والمُحسِّنات المشابهة
translated: true
type: note
---

### ما هو RMSProp؟

RMSProp (انتشار الجذر التربيعي المتوسط) هو خوارزمية تحسين تكيفية تُستخدم في التعلم الآلي لتدريب الشبكات العصبية عبر النزول التدرجي. يتناول تحديات التدرجات المتلاشية أو المنفجرة من خلال الحفاظ على متوسط متحرك للتدرجات المربعة لتطبيع معدل التعلم لكل معامل. هذا يجعله فعالاً بشكل خاص للأهداف غير الثابتة، مثل تلك الموجودة في الشبكات العصبية المتكررة (RNNs). تم تقديمه من قبل جيفري هينتون، وهو نوع مختلف من Adagrad يستخدم متوسطًا متضائلًا أسيًا بدلاً من تجميع جميع التدرجات السابقة، مما يمنع معدل التعلم من الانكماش بقوة كبيرة مع مرور الوقت.

### محسنات مشابهة لـ RMSProp

المحسنات "المشابهة" لـ RMSProp هي عادةً طرق تكيفية تضبط معدلات التعلم ديناميكيًا بناءً على تاريخ التدرج. وهي تبني على أفكار من النزول التدرجي مع الزخم لكنها تركز على التكيف لكل معامل للتعامل مع البيانات المتفرقة أو الضوضائية. أدناه مقارنة بين المحسنات الرئيسية المشابهة:

| المحسن | الميزات الرئيسية | أوجه التشابه مع RMSProp | الاختلافات عن RMSProp |
|-----------|--------------|--------------------------|---------------------------|
| **Adagrad** | يتراكم مجموع التدرجات المربعة لتكيف معدلات التعلم؛ مثالي للبيانات المتفرقة. | كلاهما يكيف معدلات التعلم لكل معامل باستخدام مقادير التدرج. | Adagrad يجمع *كل* التدرجات السابقة، مما يتسبب في انخفاض معدلات التعلم بشكل رتيب (غالبًا بسرعة كبيرة)؛ بينما يستخدم RMSProp متوسطًا متحركًا لتكيف أكثر استقرارًا. |
| **Adadelta** | امتداد لـ Adagrad يستخدم نافذة متحركة للتحديثات التدرجية؛ لا حاجة لضبط معدل التعلم يدويًا. | يشارك في تطبيع الجذر التربيعي المتوسط (RMS) للتدرجات لمعدلات تكيفية. | يقدم متوسطًا متحركًا منفصلًا لتحديثات المعاملات (ليس فقط التدرجات)، مما يجعله أكثر متانة تجاه التهيئة ويقلل من حساسية المعاملات الفائقة. |
| **Adam** (التقدير التكيفي للعزوم) | يجمع بين الزخم (العزم الأول للتدرجات) مع التكيف الشبيه بـ RMSProp (العزم الثاني)؛ مصحح الانحياز لتدريب أفضل في المراحل المبكرة. | يستخدم متوسطًا متضائلًا أسيًا للتدرجات المربعة، تمامًا مثل RMSProp، للقياس لكل معامل. | يضيف مصطلح الزخم لتسريع التقارب؛ يتضمن تصحيح الانحياز وغالبًا ما يتفوق على RMSProp في مجموعات البيانات الكبيرة، على الرغم من أنه قد يعمم بشكل أسوأ قليلاً في بعض الحالات. |
| **AdamW** | نوع مختلف من Adam مع تضاؤل وزن منفصل للتنظيم. | يرث القياس التكيفي لـ RMSProp عبر تقديرات العزم الثاني. | يفصل تضاؤل الوزن عن معدل التعلم التكيفي، مما يحسن التعميم مقارنة بـ Adam القياسي (وبالتالي مقارنة بـ RMSProp) في مهام مثل الضبط الدقيق للمحولات (transformers). |
| **SGD مع زخم نيستيروف** | النزول التدرجي العشوائي الأساسي مع زخم "النظر إلى الأمام" لتوقع التدرجات. | يمكنه محاكاة السلوك التكيفي في الممارسة العملية للحصول على تحديثات أكثر سلاسة. | ليس تكيفيًا حقًا (معدل تعلم ثابت)؛ يعتمد على الزخم للتسريع بدلاً من القياس لكل معامل، لكنه غالبًا ما يعمم بشكل أفضل من الطرق التكيفية البحتة مثل RMSProp. |

هذه المحسنات يتم تنفيذها بشكل شائع في أطر العمل مثل PyTorch أو TensorFlow (مثل `torch.optim.RMSprop`). ابدأ باستخدام RMSProp أو Adam لمعظم مهام التعلم العميق، حيث أنهما يتعاملان مع نطاق واسع من المشاكل بشكل جيد. للبيانات المتفرقة، جرب Adagrad؛ للشبكات العصبية المتكررة/ LSTM، يبرز كل من RMSProp أو Adam.

### المراجع
- [Understanding Deep Learning Optimizers: Momentum, AdaGrad, RMSProp & Adam](https://towardsdatascience.com/understanding-deep-learning-optimizers-momentum-adagrad-rmsprop-adam-e311e377e9c2)
- [Which Optimizer should I use for my ML Project?](https://www.lightly.ai/blog/which-optimizer-should-i-use-for-my-machine-learning-project)
- [A Comprehensive Guide on Deep Learning Optimizers](https://www.analyticsvidhya.com/blog/2021/10/a-comprehensive-guide-on-deep-learning-optimizers/)