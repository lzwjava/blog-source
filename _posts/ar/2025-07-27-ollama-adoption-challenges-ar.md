---
audio: false
generated: true
image: false
lang: ar
layout: post
title: 'أدوات الذكاء الاصطناعي المحلي: عوائق التبني'
translated: true
type: note
---

### لماذا لا يستخدم الكثير من الناس Ollama لتشغيل نماذج الذكاء الاصطناعي محليًا (وواجهات مثل Open-WebUI للدردشة)

Ollama هو أداة مفتوحة المصدر شائعة لتشغيل نماذج اللغة الكبيرة (LLMs) محليًا على جهازك، مما يسمح باستخدام الذكاء الاصطناعي مع التركيز على الخصوصية أو دون اتصال بالإنترنت. Open-WebUI هي واجهة ويب ذاتية الاستضافة تعمل مع Ollama (أو خلفيات عمل مشابهة) لتوفير تجربة دردشة مشابهة لـ ChatGPT. بينما اكتسبت هذه الأدوات شعبية بين المتحمسين للتكنولوجيا والمطورين، يظل اعتمادها محدودًا بين عامة الناس. بناءً على مناقشات المستخدمين والتعليقات والتحليلات، إليك الأسباب الرئيسية التي تجعل استخدامها غير واسع الانتشار:

*   **متطلبات الأجهزة المرتفعة**: يتطلب تشغيل نماذج لغة كبيرة قادرة محليًا قوة حوسبة كبيرة، مثل وحدة معالجة رسومية (GPU) قوية بسعة ذاكرة وصول عشوائي للفيديو (VRAM) لا تقل عن 16 جيجابايت (مثل سلسلة NVIDIA RTX) وذاكرة نظام (RAM) بسعة 32 جيجابايت أو أكثر. معظم المستخدمين العاديين يمتلكون أجهزة كمبيوتر محمولة أو مكتبية عادية لا يمكنها التعامل مع النماذج الكبيرة دون تباطؤ شديد أو تعطل. على سبيل المثال، لا تزال النماذج المُكَثَّفة (المضغوطة للاستخدام المحلي) تتطلب ترقيات أجهزة باهظة الثمن، وبدونها، يكون الأداء غير قابل للاستخدام لأي شيء يتجاوز المهام الأساسية. هذا يجعلها غير متاحة لغير اللاعبين أو المستخدمين العاديين.

*   **أداء أبطأ وأقل موثوقية**: غالبًا ما يتم تكميم النماذج المحلية (تقليل الدقة) لتناسب أجهزة المستهلكين، مما يؤدي إلى نتائج أقل جودة مقارنة بالخدمات السحابية مثل ChatGPT أو Grok. يمكن أن تكون بطيئة (من 10 إلى 30 ثانية لكل رد مقابل ردود سحابية فورية تقريبًا)، وعرضة للأخطاء، والهلوسة، والمخرجات المتكررة، وضعف اتباع التعليمات. غالبًا ما تفشل مهام مثل البرمجة، أو الرياضيات، أو معالجة المستندات الطويلة، حيث أن النماذج المحلية (مثل إصدارات 32B معلمة) أصغر حجمًا وأقل قدرة بكثير من النماذج السحابية الضخمة (مئات المليارات من المعاملات).

*   **تعقيد الإعداد والتكوين**: بينما يعد التثبيت الأساسي لـ Ollama مباشرًا، فإن تحسينه للحصول على نتائج جيدة يتضمن تعديل إعدادات مثل نوافذ السياق (غالبًا ما يكون الإعداد الافتراضي صغيرًا جدًا عند 2k-4k رمز، مما يتسبب في "نسيان" النموذج للمطالبات)، أو تنفيذ إضافات مثل التوليد المعزز بالاسترجاع (RAG) لتحسين الدقة، أو التعامل مع مستويات التكميم. تضيف Open-WebUI طبقة أخرى، غالبًا ما تتطلب Docker، وتكوين المنافذ، واستكشاف الأخطاء وإصلاحها. هناك نقص في الأدلة الشاملة والملائمة للمبتدئين، مما يؤدي إلى الإحباط. يبلغ العديد من المستخدمين عن مواجهة أخطاء، أو مشاكل في الذاكرة، أو الحاجة إلى خبرة في سطر الأوامر، مما يثني غير التقنيين.

*   **راحة البدائل السحابية**: خدمات مثل OpenAI، أو Google Gemini، أو Grok جاهزة للاستخدام فورًا — لا حاجة لتنزيلات، لا قلق بشأن الأجهزة، ومتاحة دائمًا بسرعة وذكاء فائقين. للدردشة أو الإنتاجية، لماذا نزعج أنفسنا بالإعداد المحلي عندما تكون الخيارات السحابية مجانية أو رخيصة (مثل 0.005 دولار لكل 100 ألف رمز) وتتعامل مع الاستفسارات المعقدة بشكل أفضل؟ تبرز الأدوات المحلية في حالات الخصوصية أو الاستخدام دون اتصال، لكن معظم الناس يفضلون السهولة على هذه المزايا.

*   **المبالغة في الترويج وخيبة الأمل**: غالبًا ما تروج وسائل التواصل الاجتماعي ويوتيوب للنماذج المحلية على أنها "قاضية على ChatGPT"، لكن الاختبارات الواقعية تظهر فجوات كبيرة في الجودة. على سبيل المثال، تواجه النماذج المحلية صعوبة في المعايير البسيطة (مثل حساب الأحرف في الكلمات) التي تتقنها النماذج السحابية. هذا يؤدي إلى تجربة المستخدمين لـ Ollama، وشعورهم بخيبة الأمل، والعودة للخدمات السحابية. تساعد Open-WebUI في توفير واجهة أفضل، لكنها لا تحل قيود النموذج الأساسية.

*   **حالات استخدام محدودة لمعظم الناس**: الإعدادات المحلية رائعة للمطورين الذين يصنعون النماذج الأولية للوكلاء، أو المهام الحساسة للخصوصية (مثل معالجة البيانات السرية)، أو السيناريوهات دون اتصال. ومع ذلك، للدردشة اليومية، أو البحث، أو الترفيه، فإنها مبالغ فيها وتقدم أداءً أقل. مشاكل مثل دعم النماذج القديمة في Ollama أو تكامل الأدوات الضعيف يحدان further من جاذبيتها.

باختصار، بينما تمكّن Ollama و Open-WebUI المستخدمين المتقدمين، فإن حواجز الأجهزة، والأداء، والجهد تجعلها أدوات متخصصة. مع تحسن الأجهزة وزيادة كفاءة النماذج، قد ينمو الاعتماد، لكن في الوقت الحالي، تهيمن الخدمات السحابية بسبب بساطتها وقوتها.

### المراجع
*   [Why Are All Local AI Models So Bad? No One Talks About This!](https://www.reddit.com/r/ollama/comments/1idqxto/why_are_all_local_ai_models_so_bad_no_one_talks/)
*   [Why I Stopped Using Ollama and Local Models (And Switched Back to OpenAI)](https://medium.com/@Shamimw/why-i-stopped-using-ollama-and-local-models-and-switched-back-to-openai-2d125f303e1c)
*   [Just Wanted to Run a Large AI Model, But It Turned Out Not So Simple](https://medium.com/@rifewang/just-wanted-to-run-a-large-ai-model-but-it-turned-out-not-so-simple-a6701823c4e4)
*   [DT4 - AI - Ollama for local LLM - Recommendations?](https://discourse.devontechnologies.com/t/dt4-ai-ollama-for-local-llm-recommendations/82597)
*   [Local AI on M-Chip Macbooks?](https://forum.level1techs.com/t/local-ai-on-m-chip-macbooks/220407)
*   [Does anyone have a video or written article that would get one up to ...](https://news.ycombinator.com/item?id=40977760)
*   [How to get CrewAI to properly use Ollama models](https://community.crewai.com/t/how-to-get-crewai-to-properly-use-ollama-models/5117)