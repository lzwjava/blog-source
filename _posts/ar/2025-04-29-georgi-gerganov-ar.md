---
audio: false
generated: true
lang: ar
layout: post
title: جورجي غيرغانوف
translated: true
type: note
---

ظهر جورجي جيرجانوف، المطور البلغاري ذو الخلفية في الفيزياء الطبية، كشخصية محورية في ديمقراطية الذكاء الاصطناعي من خلال إنشائه لمكتبة **llama.cpp**، وهي مكتبة C/C++ عالية الكفاءة لتشغيل النماذج اللغوية الكبيرة مثل نموذج LLaMA من Meta AI. بدأت رحلته بشكوك حول الشبكات العصبية، لكن فضوله التقني ومهارته في التحسين أديا إلى إسهامات رائدة في الاستدلال المحلي للذكاء الاصطناعي.

### الخلفية والعمل المبكر
- **التعليم والإنجازات**: درس جيرجانوف في كلية الفيزياء بجامعة صوفيا، متخصصًا في الفيزياء الطبية. أظهر موهبة مبكرة حيث حصل على الميدالية الفضية في أولمبياد الفيزياء الدولي 2006 وعلى مسابقة في البرمجة عام 2008 نظمها الاتحاد البلغاري لشركات البرمجيات.
- **الشكوك الأولية تجاه الذكاء الاصطناعي**: قبل عام 2022، كان جيرجانوف يصف نفسه بـ"غير المؤمن بالذكاء الاصطناعي"، متشككًا في إمكانات الشبكات العصبية، ومفضلًا نظرة محافظة للتكنولوجيا.
- **Whisper.cpp**: كان مشروعه الرئيسي الأول في الذكاء الاصطناعي هو **whisper.cpp** (2022)، وهو إصدار بلغة C/C++ من نموذج Whisper الخاص بـ OpenAI، وهو نموذج تحويل الكلام إلى نص. قام هذا المشروع، الذي ألهمه التوقيت الجيد والحظ، بتحسين Whisper لتشغيله على وحدات المعالجة المركزية، مما جعله متاحًا على الأجهزة التي لا تحتوي على وحدات معالجة رسومية، مثل أجهزة الكمبيوتر المحمولة أو حتى الهواتف الذكية. اكتسب المشروع شعبية لتمكينه من النسخ الصوتي والترجمة بكفاءة.

### ولادة llama.cpp
- **السياق**: في فبراير 2023، أطلقت Meta AI عائلة نماذج LLaMA، وهي نماذج لغوية كبيرة كفؤة (من 7 إلى 65 مليار معامل) للأغراض البحثية، لكن تشغيلها تطلب موارد حاسوبية كبيرة، عادةً وحدات معالجة رسومية.
- **التحدي**: مستلهمًا نجاحه مع whisper.cpp، شرع جيرجانوف في جعل LLaMA يعمل على أجهزة المستهلك العادية، وتحديدًا جهاز MacBook، "من أجل المتعة فقط". في مارس 2023، طور **llama.cpp**، وهو تنفيذ بسيط بلغة C/C++ لشفرة الاستدلال الخاصة بـ LLaMA بدون أي تبعيات خارجية.
- **الابتكار الرئيسي**: استفاد جيرجانوف من مكتبة **GGML** الخاصة به، وهي إطار عمل لجبر الموترات مبني على لغة C بدأه في سبتمبر 2022، مستلهمًا من LibNC الخاص بـ Fabrice Bellard. ركزت GGML على إدارة الذاكرة الصارمة وتعددية المسارات، مما مكّن من الاستدلال الفعال المعتمد على وحدة المعالجة المركزية.
- **الطفرة في التكميم**: كانت الميزة الأساسية في llama.cpp هي التكميم 4-بت، الذي يضغط أوزان النموذج لتقليل استخدام الذاكرة وتسريع الاستدلال، مع فقدان ضئيل في الدقة (على سبيل المثال، زيادة 4% فقط في الالتباس عند 4-بت). سمح ذلك لنموذج LLaMA بحجم 7B بالعمل على أجهزة لا تحتوي سوى على 4 جيجابايت من ذاكرة الوصول العشوائي، بما في ذلك هواتف Android ولوحات Raspberry Pi.

### الأثر والنمو
- **إمكانية الوصول**: جعلت llama.cpp النماذج اللغوية الكبيرة في متناول الهواة والمطورين الذين لا يمتلكون أجهزة متخصصة. يمكنها العمل على أجهزة MacBooks وهواتف Pixel وحتى لوحات Raspberry Pi 4 (وإن كان ببطء، حوالي 1 رمز/الثانية). أشعل هذا موجة من التجارب، حيث قام الهاكرز والباحثون بتشغيل LLaMA على منصات متنوعة.
- **المجتمع والحجم**: انفجر المشروع في الشعبية، حيث جمع أكثر من 69,000 نجمة على GitHub، وأكثر من 2,600 إصدار، وأكثر من 900 مساهم. طبيعته مفتوحة المصدر وبساطته (مثل دعم CUDA في ملف C++ واحد) شجعت على التعاون، بما في ذلك ميزات مثل دعم ROCm لأجهزة AMD والاستدلال الموزع عبر MPI.
- **تنسيق GGUF**: في أغسطس 2023، قدم جيرجانوف تنسيق **GGUF**، خلفًا لـ GGML. قام GGUF بدمج أوزان النموذج والبيانات الوصفية والرموز في ملف ثنائي واحد، مدعومًا التكميم من 2-بت إلى 8-بت وضمان التوافق مع الإصدارات السابقة. هذا أدى إلى مزيد من تحسين تخزين النموذج وتحميله.
- **الدعم متعدد الوسائط**: بحلول أكتوبر 2023، أضاف llama.cpp دعمًا للنماذج متعددة الوسائط مثل LLaVA، مما وسع نطاقه beyond النص إلى المهام القائمة على الرؤية.

### المساهمات التقنية
- **تقنيات التحسين**: استخدم جيرجانوف تعليمات متجهات SIMD لتحويل وحدات المعالجة المركزية إلى "وحدات معالجة رسومية مصغرة" لعمليات المصفوفات، مما عزز الأداء. أظهرت معاييره على Apple Silicon مزاياها في عرض النطاق الترددي للذاكرة لاستدلال النماذج اللغوية الكبيرة.
- **التحول الفلسفي**: غيرت Llama.cpp منافسة الذكاء الاصطناعي من الأداء الخام للنموذج إلى التحسين وإمكانية الوصول، مما مكّن من الاستدلال المحلي وقلل الاعتماد على وحدات المعالجة الرسومية السحابية.
- **الذكاء الاصطناعي على الحافة**: تماشى المشروع مع رؤية الذكاء الاصطناعي على الجهاز نفسه، حيث أظهرت تجارب مثل الاستدلال الموزع لنموذج LLaMA 65B عبر ست لوحات Raspberry Pi إمكاناته للذكاء الاصطناعي منخفض التكلفة واللامركزي.

### التأثير الأوسع
- **ggml.ai**: أسس جيرجانوف **ggml.ai**، بدعم من Nat Friedman و Daniel Gross، لدعم تطوير GGML و llama.cpp. تقوم الشركة بتوظيف المساهمين لتطوير الاستدلال على الجهاز.
- **الأثر الثقافي**: أطلق عليه اسم "المبرمج الأوروبي المحنك" على X، جعلت سرعة تطويره وأخلاقيات المصدر المفتوح منه بطلاً شعبيًا في مجتمع الذكاء الاصطناعي. ألهم عمله مقارنات بالإصدار المفتوح لـ Stable Diffusion، مما يمثل "لحظة Stable Diffusion" للنماذج اللغوية الكبيرة.
- **الديمقراطية**: بتمكين النماذج اللغوية الكبيرة من العمل على أجهزة المستهلك، خفضت llama.cpp الحواجز أمام تجربة الذكاء الاصطناعي، مما fostered الابتكار في الذكاء الاصطناعي على الحافة وتحدى هيمنة الأطر المعتمدة على وحدات المعالجة الرسومية.

### القصة باختصار
قصة جورجي جيرجانوف مع llama.cpp هي قصة براعة تقنية وتوقيت مناسب. بدءًا من كونه متشككًا في الذكاء الاصطناعي، اغتنم اللحظة عندما تم إصدار LLaMA، مطبقًا خبرته في لغة C/C++ وتحسين الموترات لجعل أحدث النماذج اللغوية الكبيرة تعمل على الأجهزة العادية. من خلال llama.cpp، لم يحل فقط تحدٍ تقني، بل أيضًا حفز حركة نحو ذكاء اصطناعي يمكن الوصول إليه ومحلي، مثبتًا أن مطورًا واحدًا من بلغاريا يمكنه إعادة تشكيل مشهد الذكاء الاصطناعي. يستمر عمله في التطور، حيث تظل llama.cpp شاهدًا على قوة الابتكار مفتوح المصدر.