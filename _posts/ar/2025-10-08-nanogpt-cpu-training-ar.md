---
audio: false
generated: true
image: false
lang: ar
layout: post
model: x-ai/grok-4-fast
title: تهيئة تدريب nanoGPT على المعالج المشرح
translated: true
type: note
---

### تفصيل الأمر

الأمر الكامل يشغّل نص `train.py` من مستودع nanoGPT باستخدام Python 3.11، محمّلاً الإعدادات الافتراضية من تكوين مجموعة بيانات شكسبير على مستوى الحرف، ويتجاوز معايير محددة لتشغيل تدريب أصغر يعتمد على وحدة المعالجة المركزية. أدناه، سأشرح كل مكون ومعيار خطوة بخطوة.

- **python3.11**: يحدد إصدار مفسر Python (3.11) لتنفيذ النص البرمجي. nanoGPT يتطلب Python 3.8+؛ وهذا يضمن التوافق مع الميزات الحديثة.

- **train.py**: نص التدريب الرئيسي في nanoGPT. يتعامل مع تحميل البيانات، تهيئة النموذج، حلقة التدريب (تمريرات أمامية/خلفية، التحسين)، التقييم، التسجيل، وحفظ النقاط المرجعية.

- **config/train_shakespeare_char.py**: ملف تكوين يضع الإعدادات الافتراضية الخاصة بمجموعة البيانات (مثل `dataset = 'shakespeare_char'`، `vocab_size = 65`، معدل التعلم الأولي، إلخ). يحدد المهمة: التدريب على نص من أعمال شكسبير على مستوى الحرف. جميع الأعلام اللاحقة `--` تتجاوز القيم من هذا التكوين.

#### معايير التجاوز
هذه أعلام سطر أوامر يتم تمريرها إلى `train.py` عبر argparse، مما يسمح بالتخصيص دون تعديل الملفات. تتحكم في الأجهزة، وسلوك التدريب، وهندسة النموذج، والتنظيم.

| المعيار | القيمة | الشرح |
|-----------|-------|-------------|
| `--device` | `cpu` | يحدد جهاز الحوسبة: `'cpu'` يشغل كل شيء على وحدة المعالجة المركزية للمضيف (أبطأ ولكن لا حاجة لوحدة معالجة الرسومات). الإعداد الافتراضي هو `'cuda'` إذا كانت وحدة معالجة الرسومات متاحة. مفيد للاختبار أو الإعدادات محدودة الموارد. |
| `--compile` | `False` | يمكّن أو يعطل تحسين `torch.compile()` الخاص بـ PyTorch على النموذج (تم تقديمه في PyTorch 2.0 لتنفيذ أسرع عبر تجميع الرسم البياني). اضبطه على `False` لتجنب مشاكل التوافق (مثلًا على الأجهزة القديمة أو الأجهزة غير CUDA). الإعداد الافتراضي هو `True`. |
| `--eval_iters` | `20` | عدد التمريرات الأمامية (التكرارات) التي سيتم تشغيلها أثناء التقييم لتقدير خسارة التحقق. القيم الأعلى تعطي تقديرات أكثر دقة ولكنها تستغرق وقتًا أطول. الإعداد الافتراضي هو 200؛ هنا تم تقليله للفحص السريع. |
| `--log_interval` | `1` | التكرار (في عدد التكرارات) الذي تتم فيه طباعة خسارة التدريب إلى وحدة التحكم. اضبط على 1 لإخراج مفصل في كل خطوة؛ الإعداد الافتراضي هو 10 لتقليل الضوضاء. |
| `--block_size` | `64` | الحد الأقصى لطول السياق (طول التسلسل) الذي يمكن للنموذج معالجته دفعة واحدة. يؤثر على استخدام الذاكرة وكمية التاريخ التي "يتذكرها" النموذج. الإعداد الافتراضي في التكوين هو 256؛ 64 أصغر للتدريب الأسرع على الأجهزة المحدودة. |
| `--batch_size` | `12` | عدد التسلسلات التي تتم معالجتها بالتوازي في كل خطوة تدريب (حجم الدفعة). الدُفعات الأكبر تستخدم ذاكرة أكثر ولكن يمكنها تسريع التدريب عبر استخدام أفضل لوحدة معالجة الرسومات/وحدة المعالجة المركزية. الإعداد الافتراضي هو 64؛ 12 تم تصغيره لوحدة المعالجة المركزية. |
| `--n_layer` | `4` | عدد طبقات فك شيفرة المحولات (عمق الشبكة). المزيد من الطبقات تزيد السعة ولكنها تزيد من خطر التجاوز وتتطلب حوسبة أكثر. الإعداد الافتراضي هو 6؛ 4 يصنع نموذجًا أصغر. |
| `--n_head` | `4` | عدد رؤوس الانتباه متعدد الرؤوس في كل طبقة. يتحكم في التوازي في حساب الانتباه؛ يجب أن يقسم بالتساوي إلى `n_embd`. الإعداد الافتراضي هو 6؛ 4 يقلل التعقيد. |
| `--n_embd` | `128` | بُعد تضمينات النموذج والحالات المخفية (عرض النموذج). القيم الأكبر تزيد القوة التعبيرية ولكن تزيد احتياجات الذاكرة/الحوسبة. الإعداد الافتراضي هو 384؛ 128 يخلق نموذجًا صغيرًا جدًا (~0.5M معامل). |
| `--max_iters` | `2000` | العدد الإجمالي لتكرارات التدريب (الخطوات). كل تكرار يعالج دفعة واحدة. يتوقف التدريب بعد هذا العدد ما لم يتم إيقافه مبكرًا. الإعداد الافتراضي هو 5000؛ 2000 يقصر التشغيل للتجربة. |
| `--lr_decay_iters` | `2000` | عدد التكرارات التي يتحلل خلالها معدل التعلم خطيًا إلى الصفر (جدول تحلل جيب التمام أو خطي). يطابق `max_iters` هنا للتحلل الكامل بالنهاية. الإعداد الافتراضي هو 5000؛ يربط التحلل بالخطوات الإجمالية. |
| `--dropout` | `0.0` | احتمالية الإسقاط (جزء من الوحدات التي يتم تعيينها عشوائيًا إلى الصفر أثناء التدريب) للتنظيم. 0.0 تعطله بالكامل (لا يوجد تنظيم)، مفيد لمجموعات البيانات/النماذج الصغيرة. الإعداد الافتراضي هو 0.2. |

هذا الإعداد يدرب نموذجًا صغيرًا يشبه GPT-2 (4 طبقات، 128 بُعد) على نص شكسبير لمدة 2000 خطوة على وحدة المعالجة المركزية، مع تسجيل متكرر وبدون إسقاط — مثالي للنماذج الأولية السريعة أو تصحيح الأخطاء.

[مستودع nanoGPT](https://github.com/karpathy/nanoGPT)