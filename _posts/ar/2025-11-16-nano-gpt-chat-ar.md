---
audio: false
generated: true
image: false
lang: ar
layout: post
model: x-ai/grok-4-fast
title: كارباثيز نانو جي بي تي و نانو شات
translated: true
type: note
---

### نظرة عامة
nanoGPT وnanochat هما مشروعان مفتوحا المصدر أنشأهما أندريه كارباثي، مصممان لجعل تدريب نماذج اللغة الكبيرة (LLM) في متناول الجميع وتعليمياً. تركز nanoGPT (التي صدرت في 2023) على أساسيات تدريب نماذج بطريقة GPT من الصفر، بينما تبني nanochat (التي صدرت في أكتوبر 2025) عليها كمسار شامل "كامل" لإنشاء روبوت محادثة مشابه لـ ChatGPT. الاختلافات الرئيسية تكمن في النطاق، ومراحل التدريب، وتعقيد قاعدة التعليمات البرمجية، وقابلية الاستخدام من البداية إلى النهاية — حيث تتطور nanochat بشكل أساسي من nanoGPT إلى نظام كامل شبيه بالأنظمة الجاهزة للإنتاج للذكاء الاصطناعي المحادث.

### الاختلافات الرئيسية في كود التدريب
كود التدريب في nanochat هو امتداد وتحسين لنهج nanoGPT، ولكنه يدمج مراحل إضافية وتحسينات وتكاملات مصممة خصيصًا لتطبيقات الدردشة. إليك تفصيل لذلك:

| الجانب                  | nanoGPT                                                                 | nanochat                                                                 |
|-------------------------|-------------------------------------------------------------------------|--------------------------------------------------------------------------|
| **التركيز الأساسي**      | التدريب المسبق لنموذج GPT قائم على Transformer على بيانات النص الخام (مثل OpenWebText أو Shakespeare). يعلّم المفاهيم الأساسية مثل Tokenization، وهندسة النموذج، وحلقات التدريب الأساسية. | مسار كامل: التدريب المسبق + التدريب المتوسط (المحادثات/أسئلة الاختيار من متعدد) + الضبط الدقيق تحت الإشراف (SFT) + التعلم المعزز الاختياري (RLHF عبر GRPO) + التقييم + الاستدلال. يبني روبوت دردشة قابلاً للنشر. |
| **مراحل التدريب**    | - مرحلة واحدة للتدريب المسبق.<br>- تقييم أساسي (مثل perplexity). | - **التدريب المسبق**: مشابه لـ nanoGPT ولكن على مجموعة بيانات FineWeb.<br>- **التدريب المتوسط**: على SmolTalk (حوارات مستخدم-مساعد)، وأسئلة الاختيار من متعدد، وبيانات استخدام الأدوات.<br>- **SFT**: الضبط الدقيق لمحاذاة الدردشة، يتم تقييمه على معايير مثل MMLU, ARC-E/C, GSM8K (الرياضيات), HumanEval (الكود).<br>- **RL**: RLHF اختياري على GSM8K لمحاذاة التفضيلات.<br>- إنشاء تقرير تقييم آلي مع مقاييس (مثل درجة CORE). |
| **حجم وهيكل قاعدة التعليمات البرمجية** | ~600 سطر إجمالاً (مثال: `train.py` ~300 سطر, `model.py` ~300 سطر). PyTorch بسيط وقابل للتعديل؛ يفضل البساطة على الاكتمال. أصبح قديماً لصالح nanochat. | ~8,000 سطر من كود PyTorch النظيف والمعياري. يتضمن Tokenizer مبني بلغة Rust، ومحرك استدلال فعال (KV cache, prefill/decode)، وتكامل أدوات (مثل Python sandbox)، وواجهة مستخدم ويب. أكثر تماسكاً ولكنه لا يزال قابلاً للتفرع. |
| **المحسن والمعايير** | AdamW القياسي؛ معدلات التعلم مضبوطة للنماذج متوسطة الحجم (مثال: GPT-2 124M معامل). | هجين Muon + AdamW (مستوحى من modded-nanoGPT)؛ معدلات تعلم تكيفية (مثال: أقل لمجموعات البيانات الصغيرة لتجنب الإفراط في التطابق). يتغير الحجم عبر علامة `--depth` لحجم النموذج. |
| **معالجة البيانات**      | مجموعات نصوص خام؛ Tokenizer BPE أساسي للتدريب. | محسن: تدريب Tokenizer مخصص (حجم المفردات ~65K)؛ يستخدم مجموعات بيانات Hugging Face (FineWeb للتدريب المسبق, SmolTalk للمحادثات). يدعم البيانات الاصطناعية لحقن الشخصية. |
| **وقت وتكلفة التدريب** | ~4 أيام على 8xA100 لنموذج مكافئ لـ GPT-2 (~$500+). يركز على التشغيل التعليمي. | ~4 ساعات على 8xH100 لنموذج أساسي بـ 560M معامل (~$100)؛ ~12 ساعة تتجاوز أداء GPT-2؛ يتدرج إلى ~$1,000 للنماذج الأقوى (مثال: 40% في MMLU بعد 24 ساعة). |
| **الاستدلال والنشر** | توليد نص أساسي؛ لا يوجد دردشة مدمجة أو واجهة مستخدم. | محرك مُحسّن مع KV cache؛ دردشة عبر CLI؛ واجهة مستخدم ويب بطريقة ChatGPT؛ استخدام أدوات (مثل تنفيذ الكود). النماذج "ساذجة/طريفة" ولكنها مسلية للحجوم الصغيرة. |
| **الهدف التعليمي**   | بناء "الأحشاء" لنموذج لغة (الدماغ). رائع للمبتدئين الذين يجربون الهياكل. | بناء نسخة كاملة من ChatGPT (من الدماغ إلى الواجهة). مشروع ختامي للدورات المتقدمة مثل LLM101n؛ معيار بحثي محتمل. |

### متى تستخدم كل منهما
- **اختر nanoGPT** إذا كنت جديدًا في مجال نماذج اللغة الكبيرة وتريد مقدمة خفيفة الوزن للتدريب المسبق (مثال: تعديل الهياكل أو مجموعات البيانات). إنها أبسط ولكنها محدودة بالنماذج غير المحادثة.
- **اختر nanochat** لتطوير روبوت الدردشة من البداية إلى النهاية. يتفوق على nanoGPT في مهام الدردشة (مثال: تحسن بنسبة 58% في بعض اختبارات RLHF) ويتضمن كل ما تحتاجه للانتقال من البيانات إلى واجهة مستخدم عاملة في مستودع واحد.

كلا المشروعين يؤكدان على قابلية التعديل — على سبيل المثال، تسمح nanochat بتغيير الحجم بسهولة (تغيير عمق النموذج) أو التخصيص (إضافة بيانات اصطناعية للشخصية). nanochat甚至 تمنح النموذج الوعي الذاتي (مثال: يعرف حجمه وقيوده بنفسه).

لاستكشاف عملي:
- [nanoGPT على GitHub](https://github.com/karpathy/nanoGPT)
- [nanochat على GitHub](https://github.com/karpathy/nanochat)

إذا كنت تقوم بتشغيل هذه المشاريع، ابدأ بـ `speedrun.sh` الخاص بـ nanochat على وحدة معالجة رسومية سحابية مثل Lambda Labs لتجربة كاملة.