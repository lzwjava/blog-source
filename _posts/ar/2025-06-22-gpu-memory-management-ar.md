---
audio: false
generated: true
lang: ar
layout: post
title: إدارة ذاكرة وحدة معالجة الرسوم للنماذج الكبيرة
translated: true
type: note
---

تشغيل ملف GGUF سعة 64 جيجابايت على كمبيوتر مكتبي محلي مزود ببطاقة رسومات NVIDIA سعة 12 جيجابايت يمثل تحديًا ولكنه ممكن التحقيق من خلال الإدارة الدقيقة للذاكرة، والتكميم، وتفريغ الطبقات. ومع ذلك، لا توجد أدوات محددة تسمى "executor batch" مصممة حصريًا للتحكم الدقيق في ذاكرة وحدة معالجة الرسومات في هذا السياق. بدلاً من ذلك، يمكنك الاستفادة من الأطر الحالية مثل `llama.cpp` أو `Oobabooga's Text-Generation-WebUI` مع تقنيات التكميم والتفريغ لإدارة ذاكرة وحدة معالجة الرسومات. أدناه، سأوضح مدى الجدوى والتحديات والخطوات لمحاولة تحقيق هذا.

### الجدوى والتحديات
1.  **قيود الذاكرة**:
    *   يمثل ملف GGUF سعة 64 جيجابايت عادة نموذج لغة كبير (مثل نموذج 70B معامل عند تكميم Q4_K_M). حتى مع التكميم، غالبًا ما تتجاوز البصمة памяти النموذج أثناء الاستدلال سعة 12 جيجابايت من VRAM لوحدة معالجة الرسومات الخاصة بك.
    *   لتشغيل مثل هذا النموذج، ستحتاج إلى تفريغ معظم الطبقات إلى ذاكرة الوصول العشوائي للنظام و/أو وحدة المعالجة المركزية، مما يبطئ الاستدلال بشكل كبير بسبب النطاق الترددي المنخفض لذاكرة الوصول العشوائي (60-120 جيجابايت/ثانية) مقارنة بـ VRAM لوحدة معالجة الرسومات (مئات الجيجابايت/ثانية).
    *   مع 12 جيجابايت من VRAM، يمكنك تفريغ عدد صغير فقط من الطبقات (مثال: 5-10 طبقات لنموذج 70B)، مع ترك الباقي لذاكرة الوصول العشوائي للنظام. وهذا يتطلب ذاكرة وصول عشوائي للنظام كبيرة (يفضل أن تكون 64 جيجابايت أو أكثر) لتجنب الـ swapping، مما قد يجعل الاستدلال بطيئًا بشكل لا يطاق (دقائق لكل رمز).

2.  **التكميم (Quantization)**:
    *   تدعم نماذج GGUF مستويات تكميم مثل Q4_K_M أو Q3_K_M أو حتى Q2_K لتقليل استخدام الذاكرة. بالنسبة لنموذج 70B، قد يتطلب Q4_K_M ذاكرة إجمالية تبلغ ~48-50 جيجابايت (VRAM + RAM)، بينما قد ينخفض Q2_K إلى ~24-32 جيجابايت ولكن مع فقدان كبير في الجودة.
    *   قد يسمح التكميم المنخفض (مثل Q2_K) بتلائم المزيد من الطبقات في VRAM ولكنه يتسبب في تدهور أداء النموذج، مما قد يجعل المخرجات أقل تماسكًا.

3.  **عدم وجود أداة "executor batch" دقيقة لذاكرة وحدة معالجة الرسومات**:
    *   لا توجد أداة مخصصة تسمى "executor batch" للتحكم الدقيق في ذاكرة وحدة معالجة الرسومات في هذا السياق. ومع ذلك، تسمح لك أطر العمل مثل `llama.cpp` وأطر العمل المماثلة بتحديد عدد الطبقات التي يتم تفريغها إلى وحدة معالجة الرسومات (`--n-gpu-layers`)، مما يتحكم بشكل فعال في استخدام VRAM.
    *   لا تقدم هذه الأدوات تخصيصًا دقيقًا للذاكرة (مثال: "استخدم بالضبط 11.5 جيجابايت من VRAM") ولكنها تسمح لك بموازنة استخدام VRAM و RAM من خلال تفريغ الطبقات والتكميم.

4.  **الأداء**:
    *   مع 12 جيجابايت من VRAM والكثير من التفريغ إلى RAM، توقع سرعات استدلال بطيئة (مثال: 0.5-2 رمز/ثانية لنموذج 70B).
    *   تصبح سرعة ذاكرة الوصول العشوائي للنظام وأداء وحدة المعالجة المركزية (مثل أداء النواة الأحادية، نطاق ذاكرة الوصول العشوائي) عنق زجاجة. تساعد ذاكرة الوصول العشوائي السريعة (مثل DDR4/DDR5 بتردد 3600 ميجاهرتز) ووحدة المعالجة المركزية الحديثة ولكنها لن تضاهي سرعات وحدة معالجة الرسومات.

5.  **متطلبات الأجهزة**:
    *   ستحتاج إلى 64 جيجابايت على الأقل من ذاكرة الوصول العشوائي للنظام لتحميل النموذج بالكامل (VRAM + RAM). مع ذاكرة وصول عشوائي أقل، قد يلجأ النظام إلى الـ swapping على القرص، مما يتسبب في تباطؤ شديد.
    *   تحسن وحدة المعالجة المركزية الحديثة (مثل Ryzen 7 أو Intel i7) ذات أداء النواة الأحادية العالي والعديد من النوى من الاستدلال المعتمد على وحدة المعالجة المركزية.

### هل هو ممكن؟
نعم، من الممكن تشغيل نموذج GGUF سعة 64 جيجابايت على بطاقة رسومات NVIDIA سعة 12 جيجابايت، ولكن مع مفاضلات كبيرة:
*   **استخدم التكميم العالي** (مثل Q2_K أو Q3_K_M) لتقليل البصمة памяти للنموذج.
*   **قم بتفريغ معظم الطبقات** إلى ذاكرة الوصول العشوائي للنظام ووحدة المعالجة المركزية، باستخدام عدد قليل فقط من الطبقات على وحدة معالجة الرسومات.
*   **تقبل سرعات الاستدلال البطيئة** (يحتمل أن تكون 0.5-2 رمز/ثانية).
*   **تأكد من وجود ذاكرة وصول عشوائي كافية في النظام** (64 جيجابايت أو أكثر) لتجنب الـ swapping.

ومع ذلك، قد لا تكون التجربة عملية للاستخدام التفاعلي بسبب أوقات الاستجابة البطيئة. إذا كانت السرعة حرجة، ففكر في نموذج أصغر (مثل 13B أو 20B) أو بطاقة رسومات تحتوي على المزيد من VRAM (مثل RTX 3090 بسعة 24 جيجابايت).

### خطوات محاولة تشغيل ملف GGUF سعة 64 جيجابايت
إليك كيف يمكنك محاولة تشغيل النموذج باستخدام `llama.cpp`، الذي يدعم GGUF وتفريغ وحدة معالجة الرسومات:

1.  **تحقق من الأجهزة**:
    *   تأكد من أن بطاقة الرسومات NVIDIA الخاصة بك تحتوي على 12 جيجابايت من VRAM (مثل RTX 3060 أو 4080 mobile).
    *   تأكد من وجود 64 جيجابايت على الأقل من ذاكرة الوصول العشوائي للنظام. إذا كان لديك أقل (مثل 32 جيجابايت)، فاستخدم التكميم العدواني (Q2_K) واختبر وجود الـ swapping.
    *   تحقق من وحدة المعالجة المركزية (مثل 8+ نوى، سرعة ساعة عالية) وسرعة ذاكرة الوصول العشوائي (مثل DDR4 3600 ميجاهرتز أو DDR5).

2.  **ثبّت التبعيات**:
    *   ثبّت NVIDIA CUDA Toolkit (الإصدار 12.x) و cuDNN لتسريع وحدة معالجة الرسومات.
    *   انسخ وقم ببناء `llama.cpp` مع دعم CUDA:
      ```bash
      git clone https://github.com/ggerganov/llama.cpp
      cd llama.cpp
      make LLAMA_CUDA=1
      ```
    *   ثبّت روابط Python (`llama-cpp-python`) مع CUDA:
      ```bash
      pip install llama-cpp-python --extra-index-url https://wheels.grok.ai
      ```

3.  **حمّل نموذج GGUF**:
    *   احصل على نموذج GGUF سعة 64 جيجابايت (مثال: من Hugging Face، مثل `TheBloke/Llama-2-70B-chat-GGUF`).
    *   إذا أمكن، حمّل إصدارًا مكممًا أقل (مثل Q3_K_M أو Q2_K) لتقليل احتياجات الذاكرة. على سبيل المثال:
      ```bash
      wget https://huggingface.co/TheBloke/Llama-2-70B-chat-GGUF/resolve/main/llama-2-70b-chat.Q3_K_M.gguf
      ```

4.  **اضبط تفريغ الطبقات**:
    *   استخدم `llama.cpp` لتشغيل النموذج، مع تحديد طبقات وحدة معالجة الرسومات:
      ```bash
      ./llama-cli --model llama-2-70b-chat.Q3_K_M.gguf --n-gpu-layers 5 --threads 16 --ctx-size 2048
      ```
      *   `--n-gpu-layers 5`: يقوم بتفريغ 5 طبقات إلى وحدة معالجة الرسومات (اضبط بناءً على استخدام VRAM؛ ابدأ بعدد منخفض لتجنب أخطاء نفاد الذاكرة).
      *   `--threads 16`: يستخدم 16 خيطًا لمعالجة CPU (اضبط وفقًا لعدد نوى وحدة المعالجة المركزية الخاصة بك).
      *   `--ctx-size 2048`: يحدد حجم السياق (اخفضه لتوفير الذاكرة، مثال: 512 أو 1024).
    *   راقب استخدام VRAM باستخدام `nvidia-smi`. إذا تجاوز VRAM 12 جيجابايت، فقلل من `--n-gpu-layers`.

5.  **حسّن التكميم**:
    *   إذا لم يتلائم النموذج أو كان بطيئًا جدًا، فجرب التكميم المنخفض (مثل Q2_K). قم بتحويل النموذج باستخدام أدوات التكميم في `llama.cpp`:
      ```bash
      ./quantize llama-2-70b-chat.Q4_K_M.gguf llama-2-70b-chat.Q2_K.gguf q2_k
      ```
    *   ملاحظة: قد يتسبب Q2_K في تدهور جودة الإخراج بشكل كبير.

6.  **أدوات بديلة**:
    *   استخدم `Oobabooga’s Text-Generation-WebUI` للحصول على واجهة سهلة الاستخدام:
      *   التثبيت: `git clone https://github.com/oobabooga/text-generation-webui`
      *   حمّل نموذج GGUF مع backend الخاص بـ `llama.cpp` و اضبط تفريغ وحدة معالجة الرسومات في الواجهة.
      *   اضبط المعلمات مثل `gpu_layers` في الإعدادات للبقاء ضمن 12 جيجابايت من VRAM.
    *   جرب `LM Studio` لإدارة نماذج GGUF المبسطة، على الرغم من أنها أقل مرونة في ضبط استخدام VRAM بدقة.

7.  **اختبر وراقب**:
    *   شغّل أمرًا بسيطًا (مثال: "ما هو 1+1؟") وتحقق من سرعة توليد الرموز.
    *   إذا كان الاستدلال بطيئًا جدًا (<0.5 رمز/ثانية) أو إذا قام النظام بالـ swapping، ففكر في:
      *   تقليل حجم السياق (`--ctx-size`).
      *   خفض مستوى التكميم أكثر.
      *   ترقية ذاكرة الوصول العشوائي أو استخدام نموذج أصغر.

### التوصيات
*   **نماذج أصغر**: يمكن لنموذج GGUF سعة 13B أو 20B (مثل `Llama-2-13B-chat.Q4_K_M`، ~8-12 جيجابايت) أن يتلائم بالكامل في 12 جيجابايت من VRAM، مما يقدم استدلالاً أسرع (10-25 رمز/ثانية) وجودة أفضل من نموذج 70B مكمم بشدة.
*   **المزيد من VRAM**: إذا أمكن، قم بترقية إلى بطاقة رسومات تحتوي على 16 جيجابايت (RTX 4080) أو 24 جيجابايت (RTX 3090) من VRAM لتفريغ المزيد من الطبقات وتحسين السرعة.
*   **ذاكرة الوصول العشوائي للنظام**: تأكد من وجود 64 جيجابايت+ من ذاكرة الوصول العشوائي للتعامل مع الطبقات المفرغة دون حدوث swapping. إذا كان لديك 32 جيجابايت، فالتزم بتكميم Q2_K.
*   **تجنب Q2_K للاستخدام الحرج**: قد يجعل التكميم Q2_K النموذج غير قابل للاستخدام للمهام المعقدة due to quality loss.

### لماذا لا توجد أداة "executor batch"؟
من المحتمل أن يشير مصطلح "executor batch" إلى أدوات معالجة الدُفعات أو إدارة الذاكرة، ولكن لا توجد مثل هذه الأداة مصممة خصيصًا للتحكم في ذاكرة وحدة معالجة الرسومات في نماذج LLM. توفر أطر العمل مثل `llama.cpp` الوظيفة الأقرب من خلال السماح بتفريغ الطبقات والتكميم. للتحكم الدقيق في الذاكرة، ستحتاج إلى ضبط المعلمات يدويًا (`--n-gpu-layers`، مستوى التكميم) والمراقبة باستخدام `nvidia-smi`. يمكن للمستخدمين المتقدمين استكشاف إدارة الذاكرة المخصصة في PyTorch أو CUDA، ولكن هذا معقد وليس مدعومًا جيدًا لنماذج GGUF.

### الخلاصة
من الممكن تشغيل ملف GGUF سعة 64 جيجابايت على بطاقة رسومات NVIDIA سعة 12 جيجابايت باستخدام `llama.cpp`، والتكميم العالي (Q3_K_M أو Q2_K)، وذاكرة وصول عشوائي كافية في النظام (64 جيجابايت+)، ولكنه سيكون بطيئًا (0.5-2 رمز/ثانية) وقد يضر بالجودة. للحصول على أداء أفضل، فكر في نماذج أصغر أو ترقية بطاقة الرسومات. إذا كنت بحاجة إلى مساعدة في خطوات إعداد محددة أو اختيار النموذج، فأخبرني بنموذج بطاقة الرسومات الدقيق، وذاكرة الوصول العشوائي، وحالة الاستخدام!