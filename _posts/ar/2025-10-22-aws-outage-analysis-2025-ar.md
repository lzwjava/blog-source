---
audio: false
generated: true
image: false
lang: ar
layout: post
model: x-ai/grok-4-fast
title: تحليل حالة توقف الخدمة في AWS أكتوبر 2025
translated: true
type: note
---

### انقطاع خدمة AWS في 20 أكتوبر 2025: تحليل مفصل للحالة

كان انقطاع خدمة AWS في 20 أكتوبر 2025 اضطرابًا كبيرًا نشأ في منطقة US East (N. Virginia) (us-east-1)، وهي أكبر مراكز البيانات التابعة لـ AWS وأكثرها أهمية. بدأ الانقطاع في وقت متأخر من يوم 19 أكتوبر بتوقيت المحيط الهادئ (PDT) واستمر لمدة 16 ساعة تقريبًا، مما أثر على أكثر من 140 خدمة وتسبب في تأثيرات متتالية على مستوى العالم بسبب الاعتماد على هذه المنطقة. سلطت الحادثة الضوء على نقاط الضعف في حل DNS، والترابط بين الخدمات، وعمليات الاستعادة، مما أثر على ملايين المستخدمين عبر التطبيقات والمواقع الإلكترونية والخدمات. فيما يلي تفصيل بناءً على التقرير الرسمي لما بعد الحادث من AWS والتقارير المتزامنة.

#### الجدول الزمني
تطور الانقطاع على مراحل، بدءًا من الاكتشاف وصولاً إلى الأعطال المتتالية قبل الاستعادة التدريجية. أبرز المحطات (جميع الأوقات بتوقيت المحيط الهادئ PDT):

| الوقت | الحدث |
|------|-------|
| 11:49 مساءً (19 أكتوبر) | تم اكتشاف زيادة في معدلات الخطأ وزمن الوصول عبر خدمات AWS متعددة في us-east-1. |
| 12:11 صباحاً (20 أكتوبر) | تعلن AWS عن ارتفاع معدلات الخطأ؛ تتصاعد تقارير المستخدمين الأولية على مواقع المراقبة مثل DownDetector. |
| 12:26 صباحاً | تم تحديد المشكلة على أنها فشل في حل DNS لنقاط نهاية واجهة برمجة تطبيقات DynamoDB في us-east-1. |
| 1:26 صباحاً | تأكيد ارتفاع معدلات الخطأ بشكل خاص لواجهات برمجة تطبيقات DynamoDB، بما في ذلك الجداول العالمية (Global Tables). |
| 2:22 صباحاً | تطبيق تخفيفات أولية؛ تظهر علامات مبكرة على الاستعادة. |
| 2:24 صباحاً | تم حل مشكلة DNS لـ DynamoDB، مما أدى إلى استعادة جزئية للخدمة – لكن ظهرت إعاقات في تشغيل EC2 وإخفاقات في فحوصات صحة موازن الحمل الشبكي (NLB). |
| 3:35 صباحاً | تم التخفيف الكامل من مشكلة DNS؛ نجحت معظم عمليات DynamoDB، لكن إعاقات تشغيل EC2 استمرت عبر مناطق التوفر (AZs). |
| 4:08 صباحاً | استمرار العمل على أخطاء EC2 وتأخيرات استطلاع Lambda لتعيينات مصدر حدث SQS. |
| 5:48 صباحاً | استعادة جزئية لتشغيل EC2 في مناطق توفر محددة؛ تبدأ قوائم الانتظار المتخلفة لـ SQS في التصفية. |
| 6:42 صباحاً | نشر التخفيفات عبر مناطق التوفر؛ تطبق AWS تحديد معدل (Rate Limiting) على عمليات تشغيل حالات EC2 جديدة لتحقيق الاستقرار. |
| 7:14 صباحاً | استمرار أخطاء واجهة برمجة التطبيقات (API) ومشاكل الاتصال عبر الخدمات؛ تصل الأعطال المؤثرة على المستخدمين إلى ذروتها (مثل انقطاعات التطبيقات). |
| 8:04 صباحاً | يركز التحقيق على الشبكة الداخلية لـ EC2. |
| 8:43 صباحاً | تم تحديد السبب الجذري لمشاكل الشبكة: خلل في النظام الفرعي الداخلي لـ EC2 المسؤول عن مراقبة صحة NLB. |
| 9:13 صباحاً | تخفيفات إضافية لفحوصات صحة NLB. |
| 9:38 صباحاً | تم استعادة فحوصات صحة NLB بالكامل. |
| 10:03 صباحاً – 12:15 ظهراً | تحسينات تدريجية في تشغيل EC2؛ تزداد استدعاءات Lambda واستقرار الاتصال على مراحل عبر مناطق التوفر. |
| 1:03 ظهراً – 2:48 ظهراً | تم تقليل التخفيضات (Throttles)؛ معالجة قوائم الانتظار المتخلفة لخدمات مثل Redshift وAmazon Connect وCloudTrail. |
| 3:01 ظهراً | تمت استعادة الحالة التشغيلية الطبيعية الكاملة لجميع الخدمات؛ من المتوقع أن تُصفى قوائم الانتظار المتخلفة الطفيفة (مثل AWS Config وRedshift) في غضون ساعات. |
| 3:53 ظهراً | تعلن AWS عن حل الانقطاع. |

بلغت تقارير المستخدمين على منصات مثل DownDetector ذروتها حوالي الساعة 6 صباحًا بتوقيت المحيط الهادئ، مع أكثر من 5000 حادثة قبل أن تتراجع.

#### السبب الجذري
نشأ الانقطاع من فشل في حل DNS أثر على نقاط نهاية خدمة DynamoDB في us-east-1. تعمل DynamoDB، وهي خدمة قاعدة بيانات NoSQL، كـ "طائرة تحكم" (Control Plane) حاسمة للعديد من ميزات AWS – حيث تتعامل مع البيانات الوصفية (Metadata) والجلسات والتوجيه. عندما فشل DNS في حل هذه النقاط الطرفية، شهدت واجهات برمجة تطبيقات DynamoDB ارتفاعًا في زمن الوصول والأخطاء.

تم حل هذه المشكلة الأولية بسرعة، لكنها أطلقت شلالًا من التأثيرات:
- فشلت عمليات تشغيل حالات EC2 بسبب اعتمادها على DynamoDB لتخزين البيانات الوصفية.
- عطل برمجي كامن في النظام الفرعي الداخلي لـ EC2 (المسؤول عن مراقبة صحة NLB) زاد من حدة مشاكل اتصال الشبكة، مما أدى إلى إعاقات أوسع في موازنة الحمل واستدعاءات واجهة برمجة التطبيقات.
- تضمنت جهود الاستعادة التخفيض (Throttling) (مثل تحديد عمليات تشغيل EC2 واستدعاءات Lambda) لمنع التحميل الزائد، لكن المحاولات المتكررة من الخدمات المعتمدة ضاعفت الضغط.

أكدت AWS أنها لم تكن هجمة إلكترونية، بل كانت مشكلة تقنية متعلقة بالبنية التحتية، ربما مرتبطة بتحديث خاطئ لقاعدة بيانات DNS أو فشل في نظام النسخ الاحتياطي. حدث التأثير العالمي لأن us-east-1 تستضيف طائرات تحكم رئيسية لخدمات مثل IAM وLambda، حتى للموارد في المناطق الأخرى.

#### الخدمات المتأثرة
تم التأثير على أكثر من 142 خدمة من خدمات AWS، primarily تلك المعتمدة على DynamoDB أو EC2 أو نقاط نهاية us-east-1. الفئات الأساسية:

- **قواعد البيانات والتخزين**: DynamoDB (الرئيسي)، RDS، Redshift، SQS (قوائم انتظار متخلفة).
- **الحوسبة والتنسيق**: EC2 (عمليات التشغيل)، Lambda (الاستدعاءات، الاستطلاع)، ECS، EKS، Glue.
- **الشبكات وموازنة الحمل**: موازن الحمل الشبكي (فحوصات الصحة)، API Gateway.
- **المراقبة والإدارة**: CloudWatch، CloudTrail، EventBridge، IAM (التحديثات)، AWS Config.
- **أخرى**: Amazon Connect، Athena، والميزات العالمية مثل DynamoDB Global Tables.

لم تتعطل جميع الخدمات بالكامل – حيث شهد الكثير أخطاء جزئية أو تأخيرات – لكن الطبيعة المترابطة تعني أن حتى المشاكل الطفيفة انتشرت.

#### الآثار
عطل الانقطاع حوالي ثلث التطبيقات المعتمدة على الإنترنت، مما أثر على ما يقدر بـ 100+ مليون مستخدم حول العالم. أمثلة بارزة:
- **التواصل الاجتماعي والإعلام**: Snapchat (فشل في تسجيل الدخول)، Reddit (انقطاعات)، Twitch (مشاكل في البث).
- **الألعاب**: Roblox (تعطل الخوادم)، Fortnite (فشل في Matchmaking).
- **التموال والمدفوعات**: Venmo، بنوك مثل Lloyds (تأخير في المعاملات)، HMRC (خدمات الضرائب البريطانية).
- **التجزئة والتجارة الإلكترونية**: أجزاء من موقع Amazon نفسه للتجزئة؛ تسجيل الوصول للخطوط الجوية (مثل تأخيرات Delta، United).
- **أخرى**: أجهزة Alexa (فشل في الصوت)، Twilio (خلل في الاتصالات).

تقدر الخسائر الاقتصادية بأكثر من 500 مليون دولار، مع ارتفاع بنسبة 300٪ في فحوصات الأمن السيبراني حيث ذعر المستخدمون. وسلط الضوء على مركزية الإنترنت: حيث تتعامل us-east-1 مع ~30٪ من حركة مرور AWS، مما يجعلها نقطة ضعف مفردة على الرغم من التصميم متعدد مناطق التوفر.

#### الحل والدروس المستفادة
حلت AWS المشكلة من خلال تخفيفات مستهدفة: إصلاحات DNS، ورقع للنظام الفرعي لـ EC2/NLB، وتخفيضات تدريجية في التخفيض (Throttling). بعد الحادثة، نصحوا بما يلي:
- إعادة محاولة الطلبات الفاشلة.
- مسح ذاكرة التخزين المؤقت لـ DNS.
- توزيع الموارد عبر مناطق توفر/مناطق متعددة (مثلاً عبر مجموعات التوسع التلقائي Auto Scaling Groups).
- استخدام حصص الخدمة (Service Quotas) والتخزين المؤقت (Caching) لامتصاص الصدمات ضد المحاولات المتكررة.

تشمل الدروس المستفادة الأوسع الحاجة إلى توفر أفضل في طائرات التحكم، والكشف عن الشذوذ مدعومًا بالذكاء الاصطناعي للتقييم السريع، واستراتيجيات سحابية متنوعة. التزمت AWS بتقديم تحليل كامل للسبب الجذري (RCA) للعملاء، مؤكدة أن مثل هذه الأحداث، وإن كانت نادرة، تكشف تحديات القياس (Scaling) على مستوى HyperScale.

كان هذا أكثر انقطاع تعطلي لـ us-east-1 منذ عام 2021، لكن التعافي كان أسرع من السوابق التاريخية بسبب الأدوات الآلية.

#### المراجع
- [لوحة صحة AWS: صحة الخدمة - 20 أكتوبر 2025](https://health.aws.amazon.com/health/status?eventID=arn:aws:health:us-east-1::event/MULTIPLE_SERVICES/AWS_MULTIPLE_SERVICES_OPERATIONAL_ISSUE/AWS_MULTIPLE_SERVICES_OPERATIONAL_ISSUE_BA540_514A652BE1A)
- [تحليل انقطاع AWS: 20 أكتوبر 2025](https://www.thousandeyes.com/blog/aws-outage-analysis-october-20-2025)
- [يشير الانقطاع الهائل لـ AWS إلى نقطة ضعف رئيسية في الإنترنت الحديث](https://finance.yahoo.com/news/amazons-massive-aws-outage-points-to-a-key-weakness-in-the-modern-internet-145418180.html)
- [ما الذي تسبب في انقطاع Amazon's AWS، ولماذا توقفت العديد من التطبيقات الرئيسية؟](https://www.aljazeera.com/news/2025/10/21/what-caused-amazons-aws-outage-and-why-did-so-many-major-apps-go-offline)
- [انقطاع AWS في 20 أكتوبر: كيف عطل خلل DNS في DynamoDB الإنترنت](https://medium.com/@ansababy/aws-outage-on-october-20-how-a-dns-glitch-in-dynamodb-disrupted-the-internet-40e1f0440a12)
- [تحديث - خدمات AWS تعمل بشكل طبيعي](https://www.aboutamazon.com/news/aws/aws-service-disruptions-outage-update)
- [انقطعت خدمة AWS — تحديثات مباشرة تلي الانقطاع الهائل](https://www.tomsguide.com/news/live/amazon-outage-october-2025)