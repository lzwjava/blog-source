---
audio: false
generated: true
image: false
lang: ar
layout: post
model: x-ai/grok-4-fast
title: تفاصيل تهيئة وتفعيل نانو جي بي تي
translated: true
type: note
---

في مشروع **NanoGPT** الخاص بـ Andrej Karpathy (وهو التنفيذ البسيط والشهير لنموذج GPT من أواخر 2022 إلى 2025)، النموذج **لا** يستخدم ReLU. إنه يستخدم **GELU** (وحدة الخطأ الغوسية الخطية) كدالة تنشيط، وهو الخيار القياسي في نماذج GPT الحديثة (مثل GPT-2 و GPT-3 و Llama، إلخ).

### نطاقات تهيئة المعاملات الرئيسية في NanoGPT (حتى أحدث إصدار 2025)

| المعامل / المكون          | طريقة التهيئة                          | المقياس / النطاق النموذجي                               | ملاحظات |
|--------------------------------|------------------------------------------------|-----------------------------------------------------|-------|
| طبقة التضمين (الرمز المميز + الموضع) | `nn.Embedding` → التوزيع الطبيعي         | الانحراف المعياري = 0.02                                          | صغير جدًا للحفاظ على القيم اللوغاريتمية الأولية صغيرة |
| الطبقات الخطية في FFN (الإسقاط)    | `nn.Linear` → التهيئة الافتراضية لـ PyTorch → ثم يتم تحجيمها | الوزن: الانحراف المعياري ≈ 0.02–0.03 بعد التحجيم               | يطبق Karpathy عامل تحجيم بقيمة `0.02 / sqrt(n_embd)` أو ما شابه في بعض الإعدادات |
| رأس LM النهائي (إسقاط الإخراج) | نفس التضمينات (الوزن مقيد)             | الانحراف المعياري = 0.02                                          | مقيد مع تضمين الرمز المميز |
| انحياز LayerNorm                  | أصفار                                          | 0                                                   | قياسي |
| وزن LayerNorm                | آحاد                                           | 1.0                                                 | قياسي |
| تحجيم البقايا (بعد التهيئة)    | يتم ضرب الأوزان بعامل صغير           | غالبًا `weight *= 0.02` أو `weight *= sqrt(2/n_layers)` | خدعة حاسمة لاستقرار التدريب عند التهيئة |
| إسقاط الاهتمام QKV        | يتم تحجيمه كما ورد أعلاه                                | الانحراف المعياري الفعلي ≈ 0.02                                | نفس تحجيم الطبقات الخطية الأخرى |
| إسقاط إخراج الاهتمام    | تحجيم إضافي: `cproj.weight *= 0.02` أو ما شابه | أصغر (لتعويض مسار البقايا)     | مهم جدًا للنماذج العميقة |

### خدعة التهيئة الخاصة في NanoGPT
في ملف `model.py`، بعد إنشاء كل طبقة `nn.Linear`، يفعل Karpathy شيئًا مشابهًا لهذا (القيم الدقيقة تعتمد على الإعدادات):

```python
if isinstance(m, nn.Linear):
    torch.nn.init.normal_(m.weight, mean=0.0, std=0.02)
    if m.bias is not None:
        torch.nn.init.zeros_(m.bias)
    # تحجيم البقايا المهم للشبكات الأعمق
    if "cproj" in name:          # إسقاط الإخراج للاهتمام أو FFN
        m.weight.data *= 0.02    # أو أحيانًا *= 1/sqrt(n_layer)
```

أو في الإصدارات الأحدث (2024–2025)، يستخدم تحجيمًا قائمًا على الإعدادات:

```python
std = 0.02 / math.sqrt(2 * n_layer)   # يساعد في النماذج الأعمق
```

هذا يضمن أن تباين تيار البقايا لا ينفجر أو يختفي مع زيادة العمق.

### ملخص: NanoGPT لا يستخدم ReLU
- دالة التنشيط: **GELU** (وليس ReLU)
- الانحراف المعياري الرئيسي للتهيئة: **~0.02**
- تحجيم إضافي على إسقاطات الإخراج: **0.02 أو أقل** (مهم بشكل خاص)
- لا يوجد انحياز في معظم الطبقات الخطية باستثناء LayerNorm

هذه التهيئة الصغيرة جدًا + تحجيم البقايا هي السبب في أن NanoGPT يتدرب بشكل مستقر حتى بدون مرحلة الإحماء لمعدل التعلم في الإعدادات الأصغر.

إذا كنت تقوم بتدريب NanoGPT الخاص بك من الصفر، انسخ خدع التهيئة هذه بالضبط — فهي أكثر أهمية بكثير مما يعتقده الناس!