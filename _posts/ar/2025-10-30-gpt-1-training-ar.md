---
audio: false
generated: true
image: false
lang: ar
layout: post
model: x-ai/grok-4-fast
title: نظرة عامة على بنية وتدريب GPT-1
translated: true
type: note
---

### نظرة عامة على بنية GPT-1

GPT-1 (المحول التوليدي المدرب مسبقًا 1) هو النموذج الأساسي الذي قدمته OpenAI في عام 2018. لقد كان رائدًا في نهج تدريب نموذج لغة كبير على نص غير موسوم باستخدام هدف نمذجة اللغة، يليه الضبط الدقيق للمهام اللاحقة. البنية هي **محول (Transformer) يحتوي على وحدة فك تشفير فقط**، تم تكييفها من ورقة المحول الأصلية (Vaswani et al., 2017)، ولكن تم تجريدها إلى كومة وحدة فك التشفير فقط من أجل التوليد التلقائي. يتيح هذا التصميم للنموذج توقع الرمز (Token) التالي في التسلسل، مما يجعله مناسبًا للمهام التي تتضمن نصًا متتابعًا.

على عكس النماذج ثنائية الاتجاه مثل BERT، يستخدم GPT-1 **الانتباه الذاتي المقنع** لضمان السببية - حيث يمكن لكل موقع الاهتمام بالمواقع السابقة فقط، مما يمنع تسرب المعلومات من الرموز المستقبلية.

### المكونات الرئيسية والمعاملات الفائقة (Hyperparameters)

- **نوع النموذج**: وحدة فك تشفير محول (Transformer Decoder) متعددة الطبقات مع انتباه ذاتي مقنع متعدد الرؤوس وشب feed-forward موضعية.
- **عدد الطبقات**: 12 كتلة محول (طبقة).
- **آلية الانتباه**: 12 رأس انتباه في كل طبقة، حيث يعالج كل رأس حالات ذات أبعاد 64 (البعد الكلي للنموذج: 768).
- **أبعاد التضمين (Embedding)**:
  - الحجم المخفي (d_model): 768.
  - البعد الداخلي لشبكة feed-forward (d_ff): 3072 (4x الحجم المخفي، المعيار لـ Transformers).
- **الترميز الموضعي (Positional Encoding)**: تضمينات موضعية مُتعلمة تُضاف إلى تضمينات الرموز (لا يتم استخدام الترميز الجيبي).
- **دالة التنشيط**: وحدات خطية للخطأ الغاوسي (GELU) في طبقات feed-forward.
- **المفردات والتجزئة (Tokenization)**: ترميز زوج البايت (BPE) مع 40,000 دمج، تم تدريبه على النص.
- **إجمالي المعاملات**: حوالي 117 مليون.
- **طول التسلسل**: تم التدريب على تسلسلات مكونة من 512 رمز.
- **التنظيم (Regularization)**:
  - الإسقاط (Dropout): 0.1 على البواقي (Residuals)، والتضمينات، والانتباه.
  - اضمحلال الوزن (Weight decay): تنظيم L2 معدل (0.01) على الأوزان غير المتحيزة/غير تابعة لتطبيع الطبقة.
- **التهيئة**: تم تهيئة الأوزان من توزيع طبيعي N(0, 0.02).

### تفاصيل التدريب

- **التدريب المسبق**:
  - **مجموعة البيانات**: BooksCorpus، عبارة عن مجموعة من ~7,000 كتاب غير منشور (إجمالي ~800 مليون كلمة) عبر أنواع مثل الخيال، والرومانسية، والمغامرة. تم تنظيف النص (على سبيل المثال، عبر مكتبة ftfy) وتجزئته باستخدام spaCy.
  - **الهدف**: نمذجة اللغة غير الخاضعة للإشراف (التنبؤ بالرمز التالي).
  - **المحسن (Optimizer)**: Adam مع β1=0.9، β2=0.999، ε=1e-8.
  - **جدول معدل التعلم**: إحماء خطي على مدى 2,000 تحديث إلى حد أقصى 2.5×10⁻⁴، ثم التبريد بجيب التمام إلى 0.
  - **حجم الدفعة (Batch Size)**: 64 تسلسل لكل دفعة مصغرة.
  - **العصور (Epochs)**: 100.
  - **مقياس الأداء**: حقق درجة perplexity قدرها 18.4 على النص التدريبي.
- **الضبط الدقيق**:
  - تغييرات طفيفة: إضافة طبقة إخراج خطية للتنبؤات الخاصة بالمهمة واستخدام رموز محددات (مثل <s>، <e>، $) لتنظيم المدخلات كتسلسلات فردية.
  - فقدان نمذجة اللغة المساعد مُرجح بقيمة 0.5 أثناء الضبط الدقيق.
  - المعاملات الفائقة: معدل تعلم منخفض (6.25×10⁻⁵)، حجم دفعة 32، 3 عصور، مع 0.2% إحماء واضمحلال خطي.
  - سمح هذا النهج للتعلم بالانتقال بأداء قوي في مهام مثل الاستدلال النصي والإجابة على الأسئلة بدون هياكل معمارية خاصة بالمهمة.

### الابتكارات والقيود البارزة

أظهر GPT-1 أن نموذجًا واحدًا مدربًا مسبقًا يمكنه الانتقال بفعالية إلى مهام معالجة اللغة الطبيعية المتنوعة عبر تنسيق إدخال بسيط، متفوقًا على بعض النماذج المضبوطة لمهام محددة في ذلك الوقت. ومع ذلك، كان حجمه صغيرًا نسبيًا مقارنة بنماذج LLM الحديثة (على سبيل المثال، GPT-3 يحتوي على 175 مليار معلمة) ومقتصرًا على سياقات بطول 512 رمز. وضعت البنية الأساس لسلسلة GPT بأكملها، مؤكدة على قابلية التوسع في التدريب المسبق.

[Improving Language Understanding by Generative Pre-Training](https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf)  
[GPT-1 - Wikipedia](https://en.wikipedia.org/wiki/GPT-1)