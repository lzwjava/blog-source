---
audio: false
generated: true
image: false
lang: ar
layout: post
model: x-ai/grok-4-fast
title: ملخص فهم شبكات LSTM
translated: true
type: note
---

تدوينة كريستوفر أولاه في عام 2015 تغوص في شبكات الذاكرة طويلة المدى قصيرة المدى (LSTM)، وهي نوع قوي من الشبكات العصبية المتكررة (RNN) مصممة للتعامل مع البيانات المتسلسلة حيث يهم سياق المعلومات من الماضي. تبدأ بمقارنة كيفية بناء البشر للفهم بمرور الوقت (مثل قراءة جملة) مع الشبكات العصبية التقليدية، التي تعامل المدخلات بشكل مستقل. تقوم الشبكات العصبية المتكررة بإصلاح هذه المشكلة عن طريق إضافة حلقات تسمح للمعلومات بالاستمرار، وتفكيكها إلى سلسلة من الوحدات النمطية لمهام مثل نمذجة اللغة أو تحليل الفيديو.

## لماذا تفشل الشبكات العصبية المتكررة التقليدية
بينما تتفوق الشبكات العصبية المتكررة في التسلسلات القصيرة—مثل توقع كلمة "السماء" بعد "الغيوم في الـ"—فإنها تواجه صعوبة في التبعيات طويلة المدى. على سبيل المثال، في جملة "لقد نشأت في فرنسا... أتحدث الفرنسية بطلاقة"، يجب أن تشير الإشارة المبكرة إلى "فرنسا" على كلمة "الفرنسية"، لكن الشبكات العصبية المتكررة التقليدية غالبًا ما تنسى بسبب تلاشي التدرجات (vanishing gradients) أثناء التدريب. هذا القيد، الذي سلطت عليه الأبحاث المبكرة الضوء، مهد الطريق لشبكات LSTM.

## جوهر LSTM: حالة الخلية والبوابات
تقدم شبكات LSTM **حالة الخلية**—وهي "سير ناقل" يحمل المعلومات مباشرة عبر الخطوات الزمنية مع تغيير طفيف، مما يمكن الذاكرة طويلة المدى. للتحكم في هذا التدفق، هناك ثلاث **بوابات**، كل منها عبارة عن طبقة سيجمويد (تخرج قيم بين 0 و1) تضرب نقطيًا لتقرر ما يجب الاحتفاظ به أو التخلص منه:

- **بابة النسيان**: تنظر إلى الحالة المخفية السابقة والإدخال الحالي لمحو المعلومات القديمة غير ذات الصلة من حالة الخلية. على سبيل المثال، نسيان جنس الفاعل القديم عندما يظهر فاعل جديد في الجملة.
- **بابة الإدخال**: تقرر أي معلومات جديدة يجب إضافتها، مقترنة بطبقة tanh تخلق قيمًا مرشحة. معًا، يقومون بتحديث حالة الخلية عن طريق تحجيم وإضافة بيانات جديدة.
- **بابة الإخراج**: ترشح حالة الخلية (بعد تحجيم tanh) لإنتاج ناتج الحالة المخفية، مما يؤثر على الخطوة التالية.

الرياضيات تتلخص في:  
حالة الخلية الجديدة \\( C_t = f_t \odot C_{t-1} + i_t \odot \tilde{C}_t \\)  
(حيث \\( f_t \\), \\( i_t \\) هما مخرجات البوابة، \\( \odot \\) هو الضرب النقطي).

تظهر المخططات المرافقة هذا: صناديق صفراء للطبقات العصبية، دوائر وردية للعمليات، وخطوط لتدفق البيانات.

## المتغيرات والتوسعات
تغطي التدوينة تعديلات مثل **اتصالات peephole** (السماح للبوابات بالنظر إلى حالة الخلية) و**البوابات المقترنة** (ربط بوابتي النسيان والإدخال للكفاءة). كما تشير أيضًا إلى وحدة GRU، وهي ابن عم أبسط لـ LSTM تدمج البوابات والحالات. تشمل الإشارات الأخرى شبكات RNN ذات البوابات العميقة (Depth Gated RNNs) وشبكات RNN ذات آلية العمل المنتظمة (Clockwork RNNs)، مع دراسات تظهر أن المتغيرات تؤدي أداءً مماثلاً.

## التدريب وأكثر من ذلك
تتدرب شبكات LSTM عبر الانتشار العكسي عبر الزمن (backpropagation through time) على السلسلة المفككة، متجنبةً مشاكل الشبكات العصبية المتكررة التقليدية. تختتم التدوينة بالإشادة بالرواد مثل هوخريتر وشميدهوبر، والإيحاء بأفكار مستقبلية مثل آليات الانتباه (للتركيز على الأجزاء الرئيسية من المدخلات)، والربط بشبكات RNN التوليدية وشبكات LSTM الشبكية (Grid LSTMs).

إجمالاً، إنه دليل مرئي بديهي يجعل شبكات LSTM تبدو أقل مثل الصناديق السوداء—قراءة أساسية لأي شخص يتعامل مع التسلسلات في التعلم العميق.

[Understanding LSTM Networks](https://colah.github.io/posts/2015-08-Understanding-LSTMs/)