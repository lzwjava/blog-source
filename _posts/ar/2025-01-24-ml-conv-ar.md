---
audio: false
lang: ar
layout: post
title: محادثة
translated: true
---

أ: مرحبًا، سمعت الكثير عن تعلم الآلة (ML) و تعلم العميق (DL) و GPT مؤخرًا. هل يمكنك شرح ذلك لي؟

ب: بالطبع! دعنا نبدأ من الأساس. تعلم الآلة هو مجال من علوم الكمبيوتر حيث تتعلم الأنظمة من البيانات لتحسين أدائها دون أن يتم برمجتها بشكل صريح. افترض أن ذلك مثل تعليم الكمبيوتر على التعرف على الأنماط.

أ: فهمت. و تعلم العميق؟

ب: تعلم العميق هو فرع من تعلم الآلة. يستخدم الشبكات العصبية - أساسًا، نماذج حاسوبية مستوحاة من الدماغ البشري - لمعالجة البيانات في طبقات. تساعد هذه الطبقات النموذج على فهم الأنماط المعقدة، مثل التعرف على الوجوه في الصور أو فهم الكلام.

أ: الشبكات العصبية تبدو ممتعة. كيف تعمل؟

ب: افترض شبكة من العقد المتصلة، مثل الخلايا العصبية. يعالج كل عقد قطعة من المعلومات ويمر بها. "العميق" في تعلم العميق يعني أن هناك العديد من الطبقات، مما يسمح للنموذج بتعلم أنماط أكثر تعقيدًا.

أ: وما هو GPT؟ سمعت أنه أمر كبير.

ب: أوه، GPT هو أمر كبير! يعني Generative Pre-trained Transformer. هو عائلة من نماذج اللغة الكبيرة التي طورتها OpenAI. يمكن لـ GPT أن يولد نصًا يشبه النص البشري، يجيب على الأسئلة، وحتى يكتب مقالات.

أ: ذلك مدهش. كيف يعمل؟

ب: يستخدم GPT شيئًا يسمى بنية Transformer، التي تعتمد على آليات الانتباه الذاتي. وهذا يعني أن النموذج يمكن أن يركز على أجزاء مختلفة من النص المدخل لفهم السياق بشكل أفضل. يتم تدريبه على كميات هائلة من بيانات النص ثم يتم تكييفه لمهام محددة.

أ: ما الفرق بين GPT و ChatGPT؟

ب: ChatGPT هو نوع من GPT يتم تكييفه للمحادثات. تم تصميمه للتواصل مع المستخدمين، اتباع التعليمات، وإنتاج إجابات تبدو طبيعية.

أ: فهمت. ما هو الأمر مع "التدريب المتقدم" و "التكييف"?

ب: التدريب المتقدم هو مثل إعطاء النموذج تعليمًا عامًا. يتعلم من مجموعة بيانات هائلة لفهم أنماط اللغة. التكييف هو أكثر مثل تدريب متخصص - يكيّف النموذج لمهمة محددة، مثل إجابة أسئلة العملاء أو ملخص النص.

أ: ذلك منطقي. ما هو هذا "Transformer" الذي ذكرته؟

ب: Transformers هو نوع من بنية الشبكات العصبية التي تم تقديمها في ورقة شهيرة تسمى "Attention Is All You Need." لقد ثورة معالجة اللغة الطبيعية باستخدام آليات الانتباه الذاتي، مما يسمح للنموذج بتقييم أهمية الكلمات المختلفة في الجملة.

أ: الانتباه الذاتي؟ ما هو ذلك؟

ب: هو طريقة للنموذج التركيز على الأجزاء الأكثر أهمية من المدخل. على سبيل المثال، في الجملة "The cat sat on the mat," قد يركز النموذج أكثر على "cat" و "mat" لفهم العلاقة بينهما.

أ: رائع! وكيف يولد GPT النص؟

ب: يستخدم GPT شيئًا يسمى نمذجة اللغة السببية. يوقع الكلمة التالية في التسلسل بناءً على جميع الكلمات السابقة. على سبيل المثال، إذا قمت بكتابة "The sky is," قد يوقع "blue" كالكلمة التالية.

أ: يبدو ذلك بسيطًا، ولكن أعتقد أنه ليس كذلك.

ب: بالضبط! السحر في الحجم. نماذج GPT لديها مليارات من المعلمات، والتي تشبه المفاتيح والمفاتيح التي يعدلها النموذج أثناء التدريب لتعلم الأنماط. كلما زاد عدد المعلمات، زاد عدد الأنماط المعقدة التي يمكن أن يلمسها.

أ: ما هو العيب؟

ب: حسنًا، هذه النماذج تحتاج إلى كميات هائلة من البيانات والحوسبة لتدريبها. تحتاج أيضًا إلى ضبط دقيق لتجنب إنتاج محتوى مائل أو ضار. ذلك هو حيث تدخل تقنيات مثل تعلم التعزيز من ردود الفعل البشرية (RLHF) - لتوازي إنتاجات النموذج مع القيم البشرية.

أ: RLHF؟

ب: نعم، هو طريقة لتدريب النموذج باستخدام ردود الفعل من البشر. على سبيل المثال، قد يدرج الناس إجابات مختلفة، ويتعلم النموذج أن يفضل تلك التي يحبها البشر.

أ: ذلك رائع. وما هو المستقبل من كل هذا؟

ب: المستقبل مثير! نرى نماذج أقوى، طرق أفضل للتعامل مع البيانات متعددة الأوجه (مثل النص والصورة)، وجهود لتجعل الذكاء الاصطناعي أكثر أخلاقية وعدالة. ولكن هناك أيضًا تحديات، مثل التأكد من أن هذه النماذج لا تؤذي المجتمع أو البيئة.

أ: يبدو أن هناك الكثير للتفكير فيه.

ب: بالتأكيد! ولكن هذا أيضًا وقت رائع لاستكشاف الذكاء الاصطناعي. هناك الكثير من إمكانيات حل المشاكل الحقيقية في العالم وإيجاد فرص جديدة.

أ: شكرًا على شرح كل هذا!

ب: لا شكر على الواجب! أخبرني إذا كان لديك أسئلة أخرى - الذكاء الاصطناعي هو موضوع كبير، وهناك دائمًا المزيد للتعلم.