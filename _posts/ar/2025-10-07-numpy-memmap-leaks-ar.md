---
audio: false
generated: true
image: false
lang: ar
layout: post
model: x-ai/grok-4-fast
title: إصلاح تسريبات الذاكرة في ميم ماب لـ NumPy
translated: true
type: note
---

### ما تقوله منشور Stack Overflow

يناقش المنشور مشكلة شائعة مع `np.memmap` في NumPy عند معالجة مجموعات البيانات الكبيرة المخزنة على القرص. يصف السؤال الأصلي التكرار عبر مصفوفة ضخمة (على سبيل المثال، عبر مؤشرات مُعادة الترتيب عشوائيًا) لقراءة الصفوف على شكل قطع، مع ملاحظة أن استخدام ذاكرة الوصول العشوائي والذاكرة الظاهرية يزداد باطراد أثناء العملية، مما يؤدي إلى إبطاء النظام—على الرغم من أن كل صف يتم الوصول إليه مرة واحدة فقط. يريد صاحب السؤال تحديد الحد الأقصى لاستخدام الذاكرة للقطعة الحالية فقط ويطلب بدائل مثل التكرار القائم على المُولِّد أو الإدخال/الإخراج المباشر للملف (مع الإشارة إلى أن الأخير أبطأ).

تحدد الإجابة المرتبطة (المعرف 61472122، المقبولة) "تسرب الذاكرة" على أنه احتفاظ memmap الخاص بـ NumPy بالمصفوفة بأكملها في الذاكرة للتخزين المؤقت، على الرغم من وضع القراءة فقط والوصول ذو المسار الواحد. تقترح الإجابة حلين:

1.  **إعادة إنشاء كائن memmap لكل قطعة**: حذف وإعادة تهيئة `np.memmap` لكل دفعة أو قطعة. هذا يمنع تراكم المصفوفة الكاملة في ذاكرة الوصول العشوائي، مما يحافظ على الاستخدام منخفضًا (مرتبط بحجم القطعة). هناك حمل طفيف على وحدة المعالجة المركزية بسبب إعادة الإنشاء، لكنه ضئيل. مثال على الكود:
    ```python:disable-run
    def process_chunks(data_filename, chunk_size=4096):
        for start in range(0, total_size, chunk_size):
            # إعادة إنشاء memmap جديد في كل مرة
            data = np.memmap(data_filename, dtype=np.float32, mode='r', shape=full_shape)
            # معالجة القطعة الحالية فقط
            chunk = data[start:start + chunk_size]
            # ... القيام بالعمل ...
            del data  # تجاهل صريح
    ```

2.  **mmap مخصص مع نصيحة ذاكرة نظام التشغيل**: الوصول إلى كائن `mmap` الأساسي في memmap واستخدام `madvise` في Python 3.8+ (على سبيل المثال، `MADV_DONTNEED` أو `MADV_DONTDUMP`) لإخبار نظام التشغيل بإطلاق الصفحات غير المستخدمة. هذا أكثر انخفاضًا في المستوى ولكنه يتجنب حمل إعادة الإنشاء. يتضمن مقاطع كود من مصدر NumPy للوصول إلى mmap.

توصي الإجابة بأدوات التحليل مثل `memory-profiler` للتحقق من الإصلاحات.

### لماذا تستخدم NanoGPT هذا النهج

في الكود المقدم من NanoGPT (وهو تنفيذ أدنى لنموذج GPT لتدريب نماذج اللغة)، فإن إعادة إنشاء `np.memmap` داخل دالة `get_batch` تتبع الحل الأول من تلك الإجابة تمامًا—لتجنب تسرب الذاكرة أثناء أخذ عينات متكررة من الدُفعات من الملفات الثنائية الضخمة (على سبيل المثال، بيانات التدريب المُرمَّزة).

تقوم NanoGPT بالتدريب على مجموعات بيانات هائلة (مليارات الرموز مثلًا)، لذا فإن تحميل الملف بالكامل في الذاكرة غير مجدٍ. بدلاً من ذلك، تستخدم memmap للوصول الفعال إلى القرص، ولكن بدون إعادة الإنشاء، ستتضخم الذاكرة على مر العصور حيث يقوم NumPy بتخزين المزيد والمزيد من الملف مؤقتًا. من خلال إعادة التعيين من جديد في كل دفعة (عبر أسطر `if split == 'train': data = np.memmap(...)`)، فإنه يضمن انخفاض واستقرار استخدام ذاكرة الوصول العشوائي—وهو أمر بالغ الأهمية لدورات التدريب الطويلة على أجهزة محدودة مثل وحدة معالجة الرسومات المنفردة. يصرح التعليق صراحةً بفضل منشور Stack Overflow على هذه "الحيلة لتجنب تسرب الذاكرة". هذا يحافظ على خفّة أداة تحميل البيانات مع تمكين أخذ العينات العشوائية (`torch.randint`) لأزواج المُدخلات-والأهداف x/y.

[Stack Overflow: numpy memmap memory usage - want to iterate once](https://stackoverflow.com/questions/45132940/numpy-memmap-memory-usage-want-to-iterate-once/61472122#61472122)
```