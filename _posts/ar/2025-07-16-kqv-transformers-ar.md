---
audio: false
generated: false
image: false
lang: ar
layout: post
title: الشبكة العصبية، المحول، وGPT
translated: true
type: post
---

### جدول المحتويات

1. [كيف تعلمت آلية KQV في الـ Transformers](#how-i-learned-the-kqv-mechanism-in-transformers)
   - مصفوفات Query و Key و Value تمثل تفاعلات التوكن
   - الفهم يتطلب معرفة الأبعاد والأشكال
   - المفاهيم الأولية تصبح أوضح مع مرور الوقت
   - عصر الذكاء الاصطناعي يوفر موارد تعليمية وفيرة
   - القصص الملهمة تحفز التعلم المستمر

2. [من الشبكة العصبية إلى GPT](#from-neural-network-to-gpt)
   - إعادة إنشاء الشبكات العصبية من الصفر للفهم
   - الـ Transformers تعالج النص عبر التضمين والترميز
   - الانتباه الذاتي يحسب التشابهات بين الكلمات
   - مشاهدة المحاضرات التأسيسية وقراءة الكود
   - متابعة الفضول من خلال المشاريع والأوراق البحثية

3. [كيف تعمل الشبكة العصبية](#how-neural-network-works)
   - خوارزمية الانتشار العكسي تحدث الأوزان والانحيازات
   - بيانات الإدخال تنشط عبر طبقات الشبكة
   - التغذية الأمامية تحسب مخرجات الطبقات عبر sigmoid
   - حساب الخطأ يوجه تعديلات التعلم
   - الفهم الأبعاد أمر بالغ الأهمية للاستيعاب


## كيف تعلمت آلية KQV في الـ Transformers

*2025.07.16*

بعد قراءة [آلية K, Q, V في الـ Transformers](https://lzwjava.github.io/notes/2025-06-02-attention-kqv-en)، فهمت بطريقة ما كيف تعمل K, Q, و V.

Q ترمز إلى Query (الاستعلام)، K ترمز إلى Key (المفتاح)، و V ترمز إلى Value (القيمة). بالنسبة للجملة، Query هي مصفوفة تخزن قيمة التوكن التي تحتاج إلى الاستفسار عن توكنات أخرى. Key يرمز إلى وصف التوكنات، و Value يرمز إلى مصفوفة المعنى الفعلي للتوكنات.

لها أشكال محددة، لذلك يجب معرفة أبعادها وتفاصيلها.

فهمت هذا حوالي أوائل يونيو 2025. تعلمت عنها لأول مرة حوالي نهاية عام 2023. في ذلك الوقت، قرأت مقالات مثل [The Illustrated Transformer](https://jalammar.github.io/illustrated-transformer/)، لكني لم أفهم الكثير.

بعد حوالي عامين، وجدت أنه من الأسهل فهمها الآن. خلال هذين العامين، ركزت على العمل في الـ backend والاستعداد لامتحانات شهادتي الجامعية المتوسطة، ولم أقرأ أو أتعلم الكثير عن تعلم الآلة (machine learning). ومع ذلك، كنت أتأمل هذه المفاهيم من وقت لآخر عندما كنت أقود السيارة أو أفعل أشياء أخرى.

يذكرني هذا بتأثير الوقت. قد نتعلم الكثير من الأشياء للوهلة الأولى، حتى لو لم نفهم الكثير. لكن بطريقة ما، فإنها تطلق نقطة بداية لتفكيرنا.

بمرور الوقت، وجدت أن المعرفة والاكتشاف، من الصعب التفكير أو فهم الأشياء في المرة الأولى. ولكن لاحقًا، يبدو التعلم والمعرفة أسهل.

أحد الأسباب هو أنه في عصر الذكاء الاصطناعي، أصبح التعلم أسهل لأنه يمكنك الخوض في أي تفصيل أو جانب لحل شكوكك. هناك أيضًا المزيد من مقاطع الفيديو المتعلقة بالذكاء الاصطناعي المتاحة. والأهم من ذلك، أنك ترى الكثير من الأشخاص يتعلمون ويبنون مشاريع على أساس ذلك، مثل [llama.cpp](https://github.com/ggml-org/llama.cpp).

قصة غيورغي غيرغانوف ملهمة. كمتعلم جديد لتعلم الآلة بدأ حوالي عام 2021، أحدث تأثيرًا قويًا في مجتمع الذكاء الاصطناعي.

هذا النوع من الأشياء سيتكرر مرارًا وتكرارًا. لذا، بالنسبة لتعلم التعزيز وأحدث المعارف في الذكاء الاصطناعي، على الرغم من أنني ما زلت غير قادر على تخصيص الكثير من الوقت لها، أعتقد أنه يمكنني إيجاد بعض الوقت للتعلم السريع ومحاولة التفكير فيها كثيرًا. الدماغ سيعمل.

---

## من الشبكة العصبية إلى GPT

*2023.09.28*

### فيديوهات YouTube

Andrej Karpathy - Let's build GPT: from scratch, in code, spelled out.

Umar Jamil - Attention is all you need (Transformer) - Model explanation (including math), Inference and Training

StatQuest with Josh Starmer - Transformer Neural Networks, ChatGPT's foundation, Clearly Explained!!!

Pascal Poupart - CS480/680 Lecture 19: Attention and Transformer Networks

The A.I. Hacker - Michael Phi - Illustrated Guide to Transformers Neural Network: A step-by-step explanation

### كيف أتعلم

بمجرد أن قرأت نصف كتاب "Neural Networks and Deep Learning"، بدأت في تكرار مثال الشبكة العصبية للتعرف على الأرقام المكتوبة بخط اليد. أنشأت مستودعًا على GitHub، https://github.com/lzwjava/neural-networks-and-zhiwei-learning.

هذا هو الجزء الصعب حقًا. إذا استطاع المرء كتابته من الصفر دون نسخ أي كود، فإنه يفهم جيدًا جدًا.

لا يزال الكود المكرر الخاص بي يفتقر إلى تطبيق update_mini_batch و backprop. ومع ذلك، من خلال الملاحظة الدقيقة للمتغيرات في مرحلة تحميل البيانات، والتغذية الأمامية، والتقييم، حصلت على فهم أفضل بكثير للمتجه والأبعاد والمصفوفة وشكل الكائنات.

وبدأت أتعلم تطبيق GPT والـ Transformer. من خلال تضمين الكلمات (word embedding) والترميز الموضعي (positional encoding)، يتغير النص إلى أرقام. ثم، في الأساس، لا يوجد فرق بينها وبين الشبكة العصبية البسيطة للتعرف على الأرقام المكتوبة بخط اليد.

محاضرة Andrej Karpathy "Let's build GPT" ممتازة جدًا. يشرح الأمور بشكل جيد.

السبب الأول هو أنها حقًا من الصفر. نرى أولاً كيف يتم إنشاء النص. إنه غامض وعشوائي نوعًا ما. السبب الثاني هو أن Andrej استطاع قول الأشياء بشكل بديهي جدًا. قام Andrej بمشروع nanoGPT لعدة أشهر.

جاءتني فكرة جديدة للحكم على جودة المحاضرة. هل يمكن للمؤلف حقًا كتابة هذه الأكواد؟ لماذا لا أفهم وما الموضوع الذي فات المؤلف؟ بالإضافة إلى هذه الرسوم البيانية والرسوم المتحركة الأنيقة، ما هي عيوبها ونقاط ضعفها؟

نعود إلى موضوع تعلم الآلة نفسه. كما يذكر Andrej، الـ dropout، والاتصال المتبقي (residual connection)، والانتباه الذاتي (Self-Attention)، والانتباه متعدد الرؤوس (Multi-Head Attention)، والانتباه المقنع (Masked Attention).

من خلال مشاهدة المزيد من مقاطع الفيديو المذكورة أعلاه، بدأت أفهم قليلاً.

من خلال الترميز الموضعي باستخدام دوال sin و cos، نحصل على بعض الأوزان. من خلال تضمين الكلمات، نغير الكلمات إلى أرقام.

$$
    PE_{(pos,2i)} = sin(pos/10000^{2i/d_{model}}) \\
    PE_{(pos,2i+1)} = cos(pos/10000^{2i/d_{model}})
$$

> The pizza came out of the oven and it tasted good.

في هذه الجملة، كيف تعرف الخوارزمية ما إذا كانت تشير إلى "pizza" أم "oven"؟ كيف نحسب التشابهات لكل كلمة في الجملة؟

نريد مجموعة من الأوزان. إذا استخدمنا شبكة Transformer للقيام بمهمة الترجمة، ففي كل مرة ندخل جملة، يمكنها إخراج الجملة المقابلة بلغة أخرى.

عن حاصل الضرب النقطي (dot product) هنا. أحد الأسباب التي تجعلنا نستخدم حاصل الضرب النقطي هنا هو أن حاصل الضرب النقطي سيأخذ في الاعتبار كل رقم في المتجه. ماذا لو استخدمنا حاصل الضرب النقطي المربع؟ نحسب أولاً مربع الأرقام، ثم نجعلها تقوم بحاصل الضرب النقطي. ماذا لو قمنا ببعض حاصل الضرب النقطي العكسي؟

عن الـ masked هنا، نغير أرقام نصف المصفوفة إلى سالب لا نهاية. ثم نستخدم softmax لجعل القيم تتراوح من 0 إلى 1. ماذا لو غيرنا أرقام الجزء السفلي الأيسر إلى سالب لا نهاية؟

### الخطة

مواصلة قراءة الكود والأوراق البحثية ومشاهدة مقاطع الفيديو. فقط استمتع واتبع فضولي.

https://github.com/karpathy/nanoGPT

https://github.com/jadore801120/attention-is-all-you-need-pytorch

---

## كيف تعمل الشبكة العصبية

*2023.05.30*

دعونا نناقش مباشرة جوهر العمل العصبي. أي خوارزمية الانتشار العكسي (backpropagation):

1. الإدخال x: عيّن التنشيط المقابل $$a^{1}$$ لطبقة الإدخال.
2. التغذية الأمامية (Feedforward): لكل l=2,3,…,L احسب $$z^{l} = w^l a^{l-1}+b^l$$ و $$a^{l} = \sigma(z^{l})$$
3. خطأ الإخراج $$\delta^{L}$$: احسب المتجه $$\delta^{L} = \nabla_a C \odot \sigma'(z^L)$$
4. إعادة نشر الخطأ (Backpropagate the error): لكل l=L−1,L−2,…,2, احسب $$\delta^{l} = ((w^{l+1})^T \delta^{l+1}) \odot \sigma'(z^{l})$$
5. الإخراج: تدرج دالة التكلفة يُعطى بواسطة $$\frac{\partial C}{\partial w^l_{jk}} = a^{l-1}_k \delta^l_j$$ و $$\frac{\partial C}{\partial b^l_j} = \delta^l_j $$

هذا مقتبس من كتاب مايكل نيلسون *Neural Networks and Deep Learning*. هل هو مربك؟ قد يكون في المرة الأولى التي تراه فيها. لكنه ليس كذلك بعد شهر من الدراسة حوله. دعني أشرح.

### الإدخال

هناك 5 مراحل. المرحلة الأولى هي الإدخال. هنا نستخدم الأرقام المكتوبة بخط اليد كإدخال. مهمتنا هي التعرف عليها. يحتوي الرقم المكتوب بخط اليد على 784 بكسلًا، أي 28*28. في كل بكسل، توجد قيمة تدرج رمادي تتراوح من 0 إلى 255. لذا، التنشيط يعني أننا نستخدم دالة ما لتنشيطه، لتغيير قيمته الأصلية إلى قيمة جديدة لسهولة المعالجة.

لنفترض أن لدينا الآن 1000 صورة مكونة من 784 بكسلًا. نقوم الآن بتدريبها للتعرف على الرقم الذي تعرضه. لدينا الآن 100 صورة لاختبار تأثير التعلم هذا. إذا كان البرنامج يستطيع التعرف على أرقام 97 صورة، نقول إن دقته 97%.

لذا، سنكرر على 1000 صورة، لتدريب الأوزان والانحيازات. نجعل الأوزان والانحيازات أكثر صحة في كل مرة نعطيها صورة جديدة للتعلم.

نتيجة تدريب دفعة واحدة تنعكس في 10 خلايا عصبية (neurons). هنا، تمثل 10 خلايا عصبية من 0 إلى 9 وتتراوح قيمتها من 0 إلى 1 للإشارة إلى مدى ثقتها في دقتها.

والإدخال هو 784 خلية عصبية. كيف يمكننا تقليل 784 خلية عصبية إلى 10 خلايا عصبية؟ ها هي الفكرة. لنفترض أن لدينا طبقتين. ماذا تعني الطبقة؟ الطبقة الأولى، لدينا 784 خلية عصبية. في الطبقة الثانية، لدينا 10 خلايا عصبية.

نعطي كل خلية عصبية في الـ 784 خلية عصبية وزنًا، على سبيل المثال،

$$w_1, w_2, w_3, w_4, ... , w_{784}$$

ونعطي الطبقة الأولى، انحيازًا، أي $$b_1$$.

وهكذا بالنسبة للخلية العصبية الأولى في الطبقة الثانية، قيمتها هي:

$$w_1*a_1 + w_2*a_2+...+ w_{784}*a_{784}+b_1$$

لكن هذه الأوزان والانحياز هي للخلية العصبية الأولى في الطبقة الثانية ($$neuron^2_{1}$$). للخلية العصبية الثانية في الطبقة الثانية ($$neuron^2_{2}$$)، نحتاج إلى مجموعة أخرى من الأوزان والانحياز.

ماذا عن دالة sigmoid؟ نستخدم دالة sigmoid لتعيين قيمة ما سبق من 0 إلى 1.

$$
\begin{eqnarray}
  \sigma(z) \equiv \frac{1}{1+e^{-z}}
\end{eqnarray}
$$

$$
\begin{eqnarray}
  \frac{1}{1+\exp(-\sum_j w_j x_j-b)}
\end{eqnarray}
$$

نستخدم أيضًا دالة sigmoid لتنشيط الطبقة الأولى. أي، نغير قيمة التدرج الرمادي تلك إلى النطاق من 0 إلى 1. لذا الآن، كل خلية عصبية في كل طبقة لها قيمة من 0 إلى 1.

لذا الآن لشبكتنا ذات الطبقتين، تحتوي الطبقة الأولى على 784 خلية عصبية، وتحتوي الطبقة الثانية على 10 خلايا عصبية. ندربها للحصول على الأوزان والانحيازات.

لدينا 784 * 10 أوزان و 10 انحيازات. في الطبقة الثانية، لكل خلية عصبية، سنستخدم 784 وزنًا و 1 انحيازًا لحساب قيمتها. الكود هنا يبدو كالتالي:

```python
    def __init__(self, sizes):
        self.num_layers = len(sizes)
        self.sizes = sizes
        self.biases = [np.random.randn(y, 1) for y in sizes[1:]]
        self.weights = [np.random.randn(y, x)
                        for x, y in zip(sizes[:-1], sizes[1:])]
```

### التغذية الأمامية

> التغذية الأمامية (Feedforward): لكل l=2,3,…,L احسب $$z^{l} = w^l a^{l-1}+b^l$$ و $$a^{l} = \sigma(z^{l})$$

لاحظ هنا، نستخدم قيمة الطبقة الأخيرة، أي $$a^{l-1}$$ ووزن الطبقة الحالية، $$w^l$$ وانحيازها $$b^l$$ لأداء دالة sigmoid للحصول على قيمة الطبقة الحالية، $$a^{l}$$.

الكود:

```python
        nabla_b = [np.zeros(b.shape) for b in self.biases]
        nabla_w = [np.zeros(w.shape) for w in self.weights]
        # feedforward
        activation = x
        activations = [x]
        zs = []
        for b, w in zip(self.biases, self.weights):
            z = np.dot(w, activation)+b
            zs.append(z)
            activation = sigmoid(z)
            activations.append(activation)
```
### خطأ الإخراج

> خطأ الإخراج $$\delta^{L}$$: احسب المتجه $$\delta^{L} = \nabla_a C \odot \sigma'(z^L)$$

دعونا نرى ماذا تعني $$\nabla$$.

> Del، أو nabla، هو عامل يستخدم في الرياضيات (خاصة في حساب التفاضل والتكامل المتجهي) كعامل تفاضلي متجهي، يمثل عادة بالرمز nabla ∇.

$$
\begin{eqnarray}
  w_k & \rightarrow & w_k' = w_k-\eta \frac{\partial C}{\partial w_k} \\
  b_l & \rightarrow & b_l' = b_l-\eta \frac{\partial C}{\partial b_l}
\end{eqnarray}
$$

هنا $$\eta $$ هو معدل التعلم. نستخدم المشتقة التي C تحترم الأوزان والانحياز، أي معدل التغيير بينهما. هذا هو `sigmoid_prime` في الأسفل.

الكود:

```python
        delta = self.cost_derivative(activations[-1], y) * \
            sigmoid_prime(zs[-1])
        nabla_b[-1] = delta
        nabla_w[-1] = np.dot(delta, activations[-2].transpose())
```

```python
    def cost_derivative(self, output_activations, y):
        return (output_activations-y)
```

### إعادة نشر الخطأ

> إعادة نشر الخطأ (Backpropagate the error): لكل l=L−1,L−2,…,2, احسب $$\delta^{l} = ((w^{l+1})^T \delta^{l+1}) \odot \sigma'(z^{l})$$

```python
     for l in range(2, self.num_layers):
            z = zs[-l]
            sp = sigmoid_prime(z)
            delta = np.dot(self.weights[-l+1].transpose(), delta) * sp
            nabla_b[-l] = delta
            nabla_w[-l] = np.dot(delta, activations[-l-1].transpose())
        return (nabla_b, nabla_w)
```

### الإخراج

> الإخراج: تدرج دالة التكلفة يُعطى بواسطة $$\frac{\partial C}{\partial w^l_{jk}} = a^{l-1}_k \delta^l_j$$
و $$\frac{\partial C}{\partial b^l_j} = \delta^l_j $$

```python
    def update_mini_batch(self, mini_batch, eta):
        nabla_b = [np.zeros(b.shape) for b in self.biases]
        nabla_w = [np.zeros(w.shape) for w in self.weights]
        for x, y in mini_batch:
            delta_nabla_b, delta_nabla_w = self.backprop(x, y)
            nabla_b = [nb+dnb for nb, dnb in zip(nabla_b, delta_nabla_b)]
            nabla_w = [nw+dnw for nw, dnw in zip(nabla_w, delta_nabla_w)]
        self.weights = [w-(eta/len(mini_batch))*nw
                        for w, nw in zip(self.weights, nabla_w)]
        self.biases = [b-(eta/len(mini_batch))*nb
                       for b, nb in zip(self.biases, nabla_b)]
```

### أخيراً

إنه مقال قصير. وفي معظمه، يعرض فقط الكود والصيغة الرياضية. لكن هذا لا بأس به بالنسبة لي. قبل كتابته، لم أفهمه بوضوح. بعد كتابة أو مجرد نسخ مقتطفات من الكود والكتاب، فهمت معظمه. بعد اكتساب الثقة من المعلم ين وانغ، وقراءة حوالي 30% من كتاب *Neural Networks and Deep Learning*، والاستماع إلى محاضرات أندريه كارباثي في ستانفورد ودورات أندرو إن جي، والمناقشة مع صديقي تشي، والتعديل على مكتبات Anaconda و numpy و Theano لجعل الكود يعمل أعوامًا مضت، أنا الآن أفهمه.

إحدى النقاط الرئيسية هي الأبعاد. يجب أن نعرف أبعاد كل رمز ومتغير. وهو يقوم فقط بالحساب التفاضلي. دعنا ننهي باقتباسات ين وانغ:

> تعلم الآلة مفيد حقًا، قد نقول إنه نظرية جميلة، لأنه ببساطة حساب التفاضل والتكامل بعد إعادة تصميم! إنها النظرية القديمة والعظيمة لنيوتن ولايبنتز، في شكل أبسط وأنيق وقوي. تعلم الآلة هو أساسًا استخدام حساب التفاضل والتكامل لاشتقاق وتركيب بعض الدوال، والتعلم العميق هو تركيب دوال أكثر تعقيدًا.

> لا توجد "ذكاء" في الذكاء الاصطناعي، ولا "عصبية" في الشبكة العصبية، ولا "تعلم" في تعلم الآلة، ولا "عمق" في التعلم العميق. لا يوجد "عمق" في التعلم العميق. ما يعمل حقًا في هذا المجال يسمى "حساب التفاضل والتكامل". لذلك أفضّل تسمية هذا المجال "الحساب التفاضلي"، وعملية بناء النماذج تسمى "البرمجة التفاضلية".