---
audio: false
generated: true
image: false
lang: ar
layout: post
model: x-ai/grok-4-fast
title: أداء ضبط لورا مقابل الضبط الكامل
translated: true
type: note
---

### نظرة عامة على أداء LoRA مقابل الضبط الدقيق الكامل

استنادًا إلى التحليل التجريبي من Thinking Machines Lab، تقدم تقنية LoRA (التكيف منخفض الرتبة) ضبطًا دقيقًا فعالاً من حيث المعاملات وغالبًا ما يضاهي الضبط الدقيق الكامل (FullFT) في السيناريوهات المقيدة بالسعة مثل مجموعات البيانات الصغيرة أو التعلم المعزز (RL)، لكنه يتدهور في مجموعات البيانات الأكبر بسبب قيود ديناميكيات التدريب المتأصلة.[1] يتعمق هذا التوسع في كل نتيجة، موضحًا الآليات والأدلة والآثار العملية لمطوري النماذج.

### التكافؤ في مجموعات بيانات ضبط التعليمات والاستدلال من الصغيرة إلى المتوسطة

تحقق LoRA تكافؤًا في الأداء مع FullFT عند الضبط الدقيق على مجموعات بيانات يصل حجمها إلى حجم معتدل، مثل تلك المستخدمة لمتابعة التعليمات (مثل مجموعات بيانات Alpaca) أو مهام الاستدلال (مثل مسائل الرياضيات GSM8K). ينشأ هذا التكافؤ لأن هذه المجموعات تحتوي عادةً على 10,000 إلى 100,000 مثال، مما يتوافق جيدًا مع سعة التحديد البارامترى منخفض الرتبة في LoRA. تقارب LoRA تحديثات الأوزان على أنها تحلل مصفوفة منخفضة الرتبة (ΔW = B A، حيث B و A مصفوفتان منخفضتا الرتبة)، وهو ما يكفي لالتقاط التحولات السلوكية الضيقة اللازمة لمثل هذه المهام دون الحاجة إلى القدرة التعبيرية الكاملة لتحديث جميع المعاملات.

عمليًا، هذا يعني أن المطورين يمكنهم استخدام LoRA لضبط النماذج الكبيرة (مثل 70B+ معامل) على أجهزة المستهلك أو مثيلات السحابة ذات الذاكرة المحدودة، لتحقيق نفس المقاييس اللاحقة مثل الدقة أو الحيرة (perplexity) كما في FullFT. على سبيل المثال، في مجموعات البيانات مثل Dolly-15k للتعليمات، تعطي LoRA ذات الرتبة 8–16 نتائج لا يمكن تمييزها، مما يوفر ما يصل إلى 99% في المعاملات القابلة للتدريب ووقت التدريب.[1] ومع ذلك، يظل هذا صحيحًا فقط إذا لم تطلب مجموعة البيانات تعميمًا واسعًا يتجاوز توزيع التدريب — حيث تظل مخاطر الإفراط في التجهيز (overfitting) مشابهة لـ FullFT.

### ضعف الأداء على مجموعات البيانات الكبيرة التي تتجاوز سعة LoRA

عندما تتجاوز مجموعات البيانات السعة الفعالة لـ LoRA (مثل الملايين من الأمثلة للتكيف الخاص بمجال معين مثل توليد الكود على The Stack)، تتخلف LoRA عن FullFT. المشكلة الرئيسية ليست "سقف سعة" صلبًا حيث يتوقف الخسارة فجأة؛ بدلاً من ذلك، تظهر LoRA كفاءة تدريب منخفضة، مع تقارب أبطأ للخسارة مرتبط بعدم التطابق بين عنق الزجاجة منخفض الرتبة وحجم مجموعة البيانات.

ينبع هذا من التحيز الاستقرائي لـ LoRA: يقتصر شكل حاصل ضرب المصفوفات (W' = W + γ B A) التحديثات على فضاء جزئي، مما يعمل مع التحولات متفرقة وقليلة الأبعاد ولكنه يكافح مع الإشارات عالية التباين في مجموعات البيانات الكبيرة. تجريبيًا، تظهر منحنيات الخسارة أن LoRA تتطلب 2–5x خطوات أكثر للوصول إلى مستويات قريبة من FullFT، وحتى حينها، يمكن أن يكون الأداء النهائي أسوأ بنسبة 5–10% في معايير قياس مثل HumanEval للبرمجة.[1] العلاقة بارامترية: تنخفض الكفاءة مع زيادة حجم مجموعة البيانات بشكل أسرع من رتبة LoRA (r)، مما يشير إلى أن زيادة r تساعد بشكل هامشي ولكنها لا تعوض بالكامل دون المخاطرة بالإفراط في التجهيز في أنظمة البيانات المنخفضة.

تشمل الآثار implications تفضيل FullFT (أو الهجينة مثل QLoRA) للمجموعات النصية الضخمة، بينما تبرع LoRA في النماذج الأولية التكرارية. يؤكد هذا أيضًا على الحاجة إلى تقدير حجم مجموعة البيانات قبل اختيار الطرق — يمكن لأدوات مثل عدد الرموز (token counts) توجيه ذلك.

### الحساسية لأحجام الدُفعات الكبيرة وتأثيرات التحديد البارامترى

تظهر LoRA عدم تحمل أكبر لأحجام الدُفعات الكبيرة مقارنة بـ FullFT، حيث تظهر عقوبات الخسارة بشكل حاد بعد النقاط المثلى (مثل حجم الدفعة > 512). بينما يتدرج الضوضاء في متدرج FullFT بشكل أكثر رشاقة، فإن إعداد حاصل ضرب المصفوفات في LoRA يضخم التباين في التحديثات منخفضة الرتبة، مما يؤدي إلى تحسين غير مستقر. تستمر هذه العقوبة حتى لو تم زيادة الرتبة، لأنها متجذرة في خصائص Hessian المختلفة لشكل ثنائي الخطية مقابل التحسين المباشر للأوزان.

على سبيل المثال، في التجارب على مجموعات بيانات الاستدلال، ترتفع خسارة LoRA أسرع بنسبة 20–30% مع أحجام دفعات تزيد عن 1k، بينما يثبت FullFT عبر متوسط بارامترى أوسع.[1] تشمل استراتيجيات التخفيف التراكم المتدرج لمحاكاة الدفعات الفعالة الأصغر أو استخدام تقنيات مثل AdamW مع جدولة معدل تعلم دقيقة. يسلط هذا الديناميكي الضوء على مقايضة LoRA: الكفاءة في الذاكرة ولكن الهشاشة في توسيع التوازي الحسابي، مما يجعلها أقل مثالية لتجمعات التدريب عالية الإنتاجية.

### فوائد تطبيق LoRA على جميع الطبقات، خاصة MLPs و MoEs

حتى على مجموعات البيانات الصغيرة، يتفوق تطبيق LoRA عالميًا (على طبقات الانتباه، وMLP، وMixture-of-Experts) على المتغيرات التي تستهدف الانتباه فقط، خاصة عند مطابقة أعداد المعاملات عبر رتب أعلى. إن LoRA التي تستهدف الانتباه فقط، الشائعة في التطبيقات المبكرة، تظهر أداءً أقل بنسبة 3–7% في مهام مثل الاستدلال متعدد القفزات (multi-hop reasoning) لأنها تهمل طبقات التغذية الأمامية (MLPs/MoEs)، التي تتعامل مع معظم التحويلات غير الخطية ودمج المعرفة الخاصة بالمجال.

تستفيد LoRA كاملة الطبقات من بنية النموذج بشكل أكثر شمولاً: تساهم MLPs في ~70% من المعاملات وتلتقط الحسابات الخاصة بالمهمة، بينما تستفيد MoEs (في نماذج مثل Mixtral) من التكيفات الخاصة بالمسار. فشل مطابقة المعاملات عن طريق زيادة رتبة الانتباه وحدها due إلى التكرار في رؤوس الانتباه، مما يؤدي إلى فضاءات جزئية غير فعالة. أفضل الممارسات: استخدام الرتبة 16–64 عبر جميع الطبقات للبيانات الصغيرة، مما ينتج مكاسب في الكفاءة والتقييمات دون حساب إضافي.[1] تشجع هذه النتيجة على اعتماد أوسع في مكتبات مثل PEFT، مما يقلل من "ضريبة LoRA" في البنى المعمارية المتخصصة.

### التكافؤ في التعلم المعزز مع الرتب المنخفضة

تطابق LoRA أداء FullFT في الضبط الدقيق للتعلم المعزز (مثل RLHF أو DPO على مجموعات بيانات التفضيل)، حتى في الرتب المنخفضة جدًا (r=4–8)، due إلى متطلبات السعة المنخفضة المتأصلة في RL. من الناحية النظرية للمعلومات، تركز تحديثات RL على نمذجة المكافأة ومتجهات السياسة عبر المسارات المتفرقة، ونادرًا ما تتجاوز القوة التمثيلية لدلتا منخفضة الرتبة — على عكس التنبؤات الكثيفة للرموز في التعلم الخاضع للإشراف (SL).

كما هو متوقع من النظرية، يظل هذا صحيحًا لأن خسائر RL (مثل مقاطع PPO) تؤكد على التحولات الاتجاهية في السلوك، والتي يشفرها الفضاء الجزئي لـ LoRA بكفاءة دون تحديد بارامترى كامل. تؤكد التجارب التكافؤ على معايير قياس مثل Anthropic's HH-RLHF، حيث تتقارب LoRA بشكل أسرع وتستخدم ذاكرة أقل بمقدار 10x.[1] للممارسين، هذا يجعل LoRA افتراضيًا لمسارات RL، مما يمكن من محاذاة نماذج ذات تريليونات المعاملات على أجهزة متواضعة.

### الآثار الأوسع والتوصيات

تكشف هذه الرؤى أن LoRA هي أداة عملية لمعظم احتياجات ما بعد التدريب، خاصة في البيئات محدودة الموارد، ولكنها ليست بديلاً عالميًا لـ FullFT على البيانات الواسعة. قد يستكشف العمل المستقبلي رتبًا تكيفية أو تحجيمًا خاصًا بالطبقة لسد الفجوات. يجب على المطورين إجراء معايير قياس على مقياس مجموعة بياناتهم ومراقبة تأثيرات الدفعة مبكرًا.[1] بشكل عام، تقوم LoRA بتعميم الضبط الدقيق دون ندم في السيناريوهات المستهدفة، متوافقة مع التحول نحو تطوير الذكاء الاصطناعي الفعال.

الاستشهادات:
[1] https://tinker-docs.thinkingmachines.ai/lora-primer