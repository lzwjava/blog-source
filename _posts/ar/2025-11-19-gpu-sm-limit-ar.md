---
audio: false
generated: true
image: false
lang: ar
layout: post
model: x-ai/grok-4-fast
title: حد SM لوحدة معالجة الرسومات للضبط الآلي الأقصى
translated: true
type: note
---

تحذير "Not enough SMs to use max_autotune_gemm" يأتي من الواجهة الخلفية TorchInductor في PyTorch (المستخدمة في `torch.compile` مع `mode="max-autotune"` أو أوضاع الضبط التلقائي العدوانية المماثلة). يشير إلى أن وحدة معالجة الرسوميات (GPU) الخاصة بك تحتوي على عدد أقل من وحدات المعالجة المتعددة (SMs) من الحد الأدنى المطلوب لخيارات الضبط التلقائي الأكثر شمولاً لعمليات الضرب المصفوفي (GEMM)، والتي تبحث عبر مساحة هائلة من تكوينات نواة Triton/CUTLASS لتحقيق أقصى أداء.

تعتبر PyTorch أن وحدة معالجة الرسوميات "كبيرة بما يكفي" فقط إذا كان لديها عدد كبير من وحدات المعالجة المتعددة (SMs) (عادة ≥68–80، اعتمادًا على إصدار PyTorch الدقيق؛ على سبيل المثال، تعتبر بطاقات RTX 3080/3090 على الحدود، بينما بطاقات A100/H100/RTX 4090 تتجاوز هذا الحد بشكل مريح). على وحدات معالجة الرسوميات الأصغر (مثل RTX 3060، 3070، 2080 Ti، T4، إلخ)، فإنها تعطل مسار `max_autotune_gemm` الكامل لتجنب وقت تجميع مفرط أو خيارات غير مثلى.

### لماذا يحدث هذا وتأثيره
- يقوم الضبط التلقائي بقياس أداء العديد من متغيرات النواة في وقت التجميع. يحتاج الضبط التلقائي الكامل لعمليات GEMM إلى توازي كافٍ (وحدات معالجة متعددة SMs) لجعل القوالب الأكثر عدوانية تستحق العناء.
- هذا التحذير **غير ضار** — لا يزال التجميع ناجحًا، وتحصل على أداء جيد (ولكن ليس الحد الأقصى المطلق). لا يزال الضبط التلقائي الآخر (الأجزاء غير المرتبطة بـ GEMM، والبحث الأقل عدوانية في GEMM) يعمل.
- **لا** يعني أن الحشو أو عدم الكفاءة ناتج عن حجم الدفعة أو بنية النموذج بالطريقة التي قد تعتقدها. تفسير المستخدم المقترح قريب ولكنه غير دقيق تمامًا هنا — هذا التحذير المحدد يتعلق حصريًا بحجم وحدة معالجة الرسوميات وليس بحشو المدخلات/الأشكال.

### كيفية التحسين أو الالتفاف حوله
1. **استخدم وحدة معالجة رسوميات (GPU) تحتوي على المزيد من وحدات المعالجة المتعددة (SMs)** (أفضل حل لأقصى أداء حقيقي):
   - الحد الأدنى الموصى به لتفعيل `max_autotune_gemm` الكامل بشكل موثوق: RTX 4090 (128 SM)، A100 (108 SM)، H100 (132+ SM)، أو بطاقات مراكز بيانات أحدث.
   - البطاقات الاستهلاكية التي تحتوي على أقل من ~80 SM (مثل RTX 3070 = 46 SM، RTX 3080 = 68 SM) ستؤدي إلى ظهور هذا التحذير.

   | مثال لوحدة المعالجة الرسومية | عدد وحدات المعالجة المتعددة | هل يدعم `max_autotune_gemm` الكامل؟ |
   |------------------|----------|--------------------------|
   | RTX 3060/3070    | 46–58   | لا                       |
   | RTX 3080/3090    | 68–82   | على الحدود (نعم أحيانًا) |
   | RTX 4090         | 128     | نعم                      |
   | A100             | 108     | نعم                      |
   | H100             | 132+    | نعم                      |

2. **غيّر وضع `torch.compile`** (لا حاجة لتغيير العتاد):
   - استخدم `mode="max-autotune-no-cudagraphs"` — يحتفظ بمعظم فوائد الضبط التلقائي لكنه يتخطى مخططات CUDA والمسار المشروط بوحدات المعالجة المتعددة لعمليات GEMM. غالبًا ما يكون سريعًا تقريبًا مع أوقات تجميع أقصر بكثير على وحدات معالجة الرسوميات الأصغر.
   - أو `mode="reduce-overhead"` — أخف وزنًا، يستخدم مخططات CUDA لزمن انتقال منخفض، جيد للاستدلال.
   - مثال:
     ```python
     compiled_model = torch.compile(model, mode="max-autotune-no-cudagraphs", fullgraph=True)
     ```

3. **تفعيل عمليات الضرب المصفوفي عالية الدقة** (يساعد في أي وضع/وحدة معالجة رسوميات):
   ```python
   torch.set_float32_matmul_precision("high")  # or "highest"
   ```
   هذا يسمح بـ TensorFloat-32 / نواة cuBLAS أفضل.

4. **فرض ضبط أكثر عدوانية على أي حال** (غير رسمي، غير مدعوم رسميًا):
   - تعديل نقطة الاختيار في مصدر PyTorch (تعديل `torch/_inductor/utils.py` أو التعديل أثناء وقت التشغيل) لخفض حد وحدات المعالجة المتعددة. محفوف بالمخاطر وقد يضر بالأداء أو يطيل وقت التجميع بشكل كبير.

5. **نصائح عامة أخرى لمحاذاة/كفاءة أفضل** (غير مرتبطة بهذا التحذير المحدد ولكنها تعالج مخاوفك المتعلقة بالحشو):
   - احش أبعاد الدفعة/التسلسل لتكون مضاعفات للرقم 8/16/32 إذا أمكن (شائع لوحدات Tensor Cores).
   - زد حجم الدفعة إذا سمح عبء العمل لديك — عمليات GEMM الأكبر تستخدم وحدة معالجة الرسوميات بشكل أفضل.
   - لأعباء العمل المركزة على الاستدلال، فكر في بدائل مثل vLLM أو TensorRT-LLM أو Hugging Face Optimum إذا استمرت قيود `torch.compile`.

في معظم الحالات الواقعية، فإن فرق الأداء الناتج عن فقدان `max_autotune_gemm` الكامل صغير (بضعة في المائة)، ويحقق لك `max-autotune-no-cudagraphs` 95%+ من الطريق نحو الأداء الأمثل على وحدات معالجة الرسوميات الاستهلاكية.

[مصدر PyTorch Inductor utils.py](https://github.com/pytorch/pytorch/blob/main/torch/_inductor/utils.py)  
[مناقشة تحذير Torch.compile](https://discuss.torch.org/t/torch-compile-warning-not-enough-sms-to-use-max-autotune-gemm-mode/184405)  
[تشريح torch.compile](https://themlsurgeon.substack.com/p/dissecting-torchcompile-surgical)