---
audio: false
generated: true
image: false
lang: ar
layout: post
model: x-ai/grok-4-fast
title: شبكات LSTM كانت مجرد شبكات ResNet مدورة
translated: true
type: note
---

في محادثته بجائزة "اختبار الزمن" في مؤتمر NeurIPS 2024 حول ورقة 2014 "التعلم من تسلسل إلى تسلسل باستخدام الشبكات العصبية"، استرجع إيليا سوتسكيفر الرؤى الأساسية والأخطاء من ذلك العصر. إحدى النقاط الرئيسية التي تناولها تحت عنوان "ما أخطأنا فيه" كانت التعقيد المفرط والقيود اللاحقة لشبكات LSTM (شبكات الذاكرة طويلة وقصيرة المدى)، والتي كانت القوة المحركة وراء الإنجازات المبكرة في نمذجة التسلسل مثل الترجمة الآلية.

### سوء الفهم الأساسي حول شبكات LSTM
عاملنا شبكات LSTM على أنها بنية جديدة ومعقدة بشكل أساسي ومصممة خصيصًا للبيانات التسلسلية – شيء "خاص" كان على الباحثين في التعلم العميق هندسته بعناية للتعامل مع التبعيات الزمنية، ومشكلة التلاشي في التدرج، والتكرار. في الواقع، أوضح سوتسكيفر أن شبكات LSTM كانت أبسط من ذلك بكثير: **هي في الأساس شبكة ResNet (شبكة متبقية) مُدوَّرة بزاوية 90 درجة**.

- **شبكات ResNet** (التي تم تقديمها في 2015) أحدثت ثورة في معالجة الصور من خلال إضافة اتصالات القفز (المتبقيات) التي تسمح للمعلومات بالتدفق مباشرة عبر الطبقات، مما يتيح إنشاء شبكات أعمق بكثير دون عدم استقرار في التدريب.
- شبكات LSTM (من عام 1997) فعلت شيئًا مماثلاً ولكن في *البعد الزمني*: بواباتها وحالة الخلية تعمل مثل المتبقيات، مما يسمح للتدرجات والمعلومات بالانتشار عبر التسلسلات الطويلة دون أن تتلاشى. إنه المبدأ نفسه – فقط "مُدوَّر" من التراص المكاني (مثل البكسل في الصورة) إلى التراص الزمني (مثل الكلمات في الجملة).

علق سوتسكيفر بمزحة: "لأولئك غير الملمين، فإن LSTM هي شيء كان الباحثون الفقراء في التعلم العميق يفعلونه قبل المحولات (Transformers). إنها في الأساس ResNet لكنها مُدوَّرة بزاوية 90 درجة... وقد أتت قبلها؛ إنها تشبه ResNet لكنها أكثر تعقيدًا قليلاً، مع مُكامل وبعض عمليات الضرب." هذا التشبيه يؤكد أن شبكات LSTM لم تكن انحرافًا جذريًا؛ بل كانت تطبيقًا مبكرًا وأنيقًا لفكرة المتبقيات على التكرار.

### لماذا كان هذا مهمًا (وما الخطأ الذي حدث)
- **ما نجح بشكل باهر**: كانت شبكات LSTM قابلة للتطوير بشكل مدهش في وقتها، مما مكَّن نموذج seq2seq من التغلب على الطرق الإحصائية التقليدية في مهام الترجمة. جعلت المتبقيات الشبكات التكرارية العميقة قابلة للتدريب، تمامًا كما فعلت لاحقًا مع الشبكات التغذوية الأمامية.
- **ما أخطأنا فيه (ولماذا تلاشت شعبية LSTM)**: قللنا من شأن كيف أن الطبيعة التسلسلية لشبكات LSTM ستشكل عنق زجاجة أمام التطوير. على عكس شبكات ResNet أو المحولات (Transformers) التي يمكن معالجتها بالتوازي، تعالج شبكات LSTM البيانات خطوة بخطوة، مما يجعلها غير فعالة مع مجموعات البيانات الضخمة أو السياقات الطويلة. كما أضافت تعقيدًا غير ضروري (مثل البوابات المتعددة) قامت آليات الانتباه (attention) في المحولات (Transformers) بتبسيطه والتخلص منه. بحلول عام 2017، كشفت المحولات (Transformers) عن هذه الحدود، محولة التركيز نحو الانتباه الذاتي (self-attention) لتحقيق توازي أفضل وتبعيات لأمد أبعد.

بعد فوات الأوان، لم يكن الجزء "الخاطئ" هو اختراع شبكات LSTM نفسها – فقد كانت حلاً بطوليًا مؤقتًا – بل التمسك بها باعتبارها الحل النهائي للنمذجة التسلسلية بدلاً من الاعتراف بالمتبقيات كأداة عامة والدفع نحو هياكل أكثر توازيًا في وقت أبكر.

هذا المنظور يعيد صياغة شبكات LSTM ليس كأثر قديم، بل كخطوة تأسيسية في فهم كيف تفتح اتصالات القفز (skip connections) إمكانيات التعمق عبر الأبعاد المختلفة.

[التعلم من تسلسل إلى تسلسل باستخدام الشبكات العصبية: عقد من الزمن – النص والمقطع المصور](https://kingy.ai/news/sequence-to-sequence-learning-with-neural-networks-what-a-decade-transcript-video/)  
[المحادثة الكاملة لإيليا سوتسكيفر في NeurIPS 2024](https://www.youtube.com/watch?v=1yvBqasHLZs)  
[تأملات من المحادثة الكاملة لإيليا في NeurIPS 2024](https://dlyog.com/papers/one_internet_v1)