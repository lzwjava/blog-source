---
audio: false
generated: true
image: false
lang: ar
layout: post
model: x-ai/grok-4-fast
title: التدريب على سلطة الكلمات العشوائية
translated: true
type: note
---

إذا دربت نموذجًا صغيرًا مثل nanoGPT (أو أي محول مشابه لـ GPT-2 الصغير) على 2-3 غيغابايت من **كلمات إنجليزية عشوائية بالكامل** (أي تسلسلات من كلمات إنجليزية حقيقية مختارة بشكل موحد عشوائيًا، بدون قواعد، بدون معنى، مجرد خليط كلمات)، إليك ما سيحدث فعليًا:

### النموذج سيتعلم أنه **لا يوجد هيكل** ليتعلمه
- سيكتشف بسرعة كبيرة توزيع الرموز (الترددات الأحادية للكلمات).
- سيدرك أن الكلمة التالية مستقلة بشكل أساسي عن الكلمات السابقة (أو قريبة جدًا من الاستقلالية).
- سينخفض الارتباك (perplexity) على بيانات التدريب بسرعة كبيرة جدًا (بعد بضع عصور تدريب سيكون قريبًا من إنتروبيا توزيع الكلمات الموحد).
- سيتحول النموذج بشكل أساسي إلى **عيّنة أحادية** - سيقوم فقط بإخراج كلمات وفقًا لتكرارها الهامشي في بيانات التدريب، مع اعتماد شبه منعدم على السياق.

### النص المُنتج سيكون أفضل قليلاً من الهراء المحض، لكنه سيظل خردة كاملة
مثال على ما ستحصل عليه بعد التدريب:

> the cat quickly under yesterday apple banana elephant running purple however tomorrow cheese lightning quickly zebra sandwich forever mountain bicycle orange elephant yesterday banana

سوف:
- يستخدم كلمات إنجليزية حقيقية ✓
- يحترم ترددات الكلمات (ستظهر الكلمات الشائعة مثل "the"، "and"، "to" كثيرًا) ✓
- يكون الإملاء مثاليًا ✓
- يكون غير متماسك نحويًا ودلاليًا بنسبة 100% ✗

**لن** يتعلم:
- القواعد النحوية أو التركيب النحوي
- أي هيكل عبارات
- وضع الكلمات الوظيفية
- أي نوع من الترابط في المعنى أو الموضوع
- حتى التكرارات الثنائية/الثلاثية (bigrams/trigrams) بما يتجاوز الضوضاء الإحصائية الضعيفة

### المقارنة مع التدريب على نص حقيقي
عندما تدرّب nanoGPT على نص حقيقي (مثل شكسبير أو OpenWebText)، حتى مع 2-3 غيغابايت فقط، تبدأ في رؤية جمل حقيقية تظهر بسرعة مدهشة لأن هناك هيكلًا غنيًا يمكن التنبؤ به.

مع كلمات عشوائية بحتة، لن تتجاوز أبدًا عتبة الترابط - بغض النظر عن مدة التدريب. يتقارب النموذج على الفور تقريبًا إلى "الكلمة التالية عشوائية وفقًا للتردد" ويتوقف عن التحسن.

### تجربة ممتعة يمكنك تجربتها فعليًا
يمكنك إنشاء مجموعة بيانات كهذه بسهولة:

```python
import random
words = open('big-wordlist.txt').read().splitlines()  # e.g., 300k English words
with open('random_3gb.txt', 'w') as f:
    for _ in range(500_000_000):  # ~3 GB
        f.write(random.choice(words) + ' ')
```

درّب nanoGPT عليها لبضع ساعات. سترى انخفاض الخسارة (loss) إلى حوالي 4-5 nats (وهي في الأساس إنتروبيا توزيع الكلمات الإنجليزية) ثم تستوي تمامًا. سيعطيك أخذ العينات لاحقًا خليط الكلمات المذكور أعلاه بالضبط.

### خلاصة القول
نعم، النص المُنتج سيكون تقريبًا هراءً محضًا - مجرد هراء يبدو أكثر "طبيعية" قليلاً مما لو أخذت عينات من الكلمات بشكل موحد من القاموس، لأنه سيطابق ترددات الكلمات الحقيقية. لكنه لن يقول أبدًا أي شيء ذي معنى أو نحوي ولو من بعيد.

إنها في الواقع طريقة رائعة لإثبات لنفسك مدى اعتماد اللغة الحقيقية على التبعيات طويلة المدى والهيكل - لأنه عندما تزيل كل ذلك، حتى بنية قوية مثل GPT تتعلم بشكل أساسي لا شيء مفيد.