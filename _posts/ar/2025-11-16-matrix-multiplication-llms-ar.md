---
audio: false
generated: true
image: false
lang: ar
layout: post
model: x-ai/grok-4-fast
title: تحسين ضرب المصفوفات للنماذج اللغوية الكبيرة
translated: true
type: note
---

نعم، هناك عدة تحسينات لضرب المصفوفات ذات النقطة العائمة (matmul) في تدريب النماذج اللغوية الكبيرة (LLM)، مع وجود بعض أوجه التشابه غير المباشرة مع تقنيات ضرب الأعداد الكبيرة في البرمجة التنافسية (CP). سأقوم بشرح هذا خطوة بخطوة، مع معالجة قضايا الدقة، والإسقاط (dropout)، ولماذا لا يتم تطبيق الإسقاط "أثناء" عملية ضرب المصفوفات أو عمليات الضرب الفردية للنقاط العائمة. ضع في اعتبارك أن تدريب النماذج اللغوية الكبيرة يركز على الكفاءة على نطاق واسع (على سبيل المثال، على وحدات معالجة الرسومات/وحدات المعالجة التنسورية)، مع إعطاء الأولوية للسرعة والذاكرة على الدقة المثلى، على عكس البرمجة التنافسية حيث تتطلب عمليات الأعداد الصحيحة الكبيرة حسابًا دقيقًا.

### تحسينات ضرب المصفوفات ذات النقطة العائمة في تدريب النماذج اللغوية الكبيرة
تعد عملية ضرب المصفوفات عنق زجاجة أساسي في النماذج اللغوية الكبيرة (على سبيل المثال، في طبقات الانتباه والشبكات التغذوية الأمامية)، حيث تمثل غالبًا 80-90٪ من وقت الحساب. ضرب المصفوفات القياسي له تعقيد O(n³)، لكن التحسينات تستفيد من العتاد، وتقليل الدقة، والتعديلات الخوارزمية:

- **تنسيقات الدقة المنخفضة**: لتسريع التدريب وتقليل استخدام الذاكرة، غالبًا ما تستخدم النماذج اللغوية الكبيرة دقة نقطة عائمة مخفضة مثل FP16 (نصف الدقة)، أو BF16، أو FP8، أو حتى FP4 بدلاً من FP32/FP64. يقلل هذا من حجم البيانات (على سبيل المثال، يستخدم FP8 بايتًا واحدًا لكل رقم مقابل 4 بايت لـ FP32) ويمكن من تسريع العتاد عبر Tensor Cores على وحدات معالجة الرسومات من NVIDIA. على سبيل المثال، يمكن لـ FP8 تسريع ضرب المصفوفات بمقدار 2-4x مع فقدان ضئيل في الدقة من خلال التكميم الديناميكي. وبالمثل، تقدم أطر عمل FP4 مقدرات قابلة للاشتقاق للتعامل مع ضوضاء التكميم أثناء الانتشار الخلفي.

- **التدريب بدقة مختلطة**: تجري الحسابات بدقة منخفضة (على سبيل المثال، ضرب مصفوفات بـ FP16)، لكن التجميع (جمع النواتج) يستخدم دقة أعلى (على سبيل المثال، FP32) لتجنب الفيض أو النضوب. يوازن هذا بين السرعة والاستقرار — أدوات مثل AMP في PyTorch تؤتمت هذا. من الشائع في التدريب المسبق للنماذج اللغوية الكبيرة تحقيق تسريع بمقدار 2-3x دون تدهور جودة النموذج.

- **النواة المدمجة وتحسينات العتاد**: تقوم مكتبات مثل cuBLAS أو TensorRT بدمج ضرب المصفوفات مع عمليات أخرى (مثل دوال التنشيط أو التسوية) في نواة واحدة، مما يقلل من عبء الوصول إلى الذاكرة. بالنسبة للنماذج اللغوية الكبيرة، يقوم Flash Attention بدمج ضرب مصفوفات الانتباه مع softmax والإخفاء، مما يقلل استخدام الذاكرة بنسبة تصل إلى 50٪. تقوم التطبيقات المخصصة (على سبيل المثال، بلغة C++ أو Rust) بتحسين محلية الذاكرة المخبأة والتوازي لعتاد محدد.

- **بدائل خوارزمية**: مستوحاة من الضرب السريع في البرمجة التنافسية (مثل خوارزمية Karatsuba أو FFT للأعداد الصحيحة الكبيرة، والتي تقلل التعقيد إلى O(n log n))، يستكشف بعض أبحاث النماذج اللغوية الكبيرة خوارزميات شبيهة بـ Strassen أو تقريبات لضرب المصفوفات. بشكل أكثر جذرية، تستبدل النماذج "الخالية من ضرب المصفوفات" ضرب المصفوفات ذات النقطة العائمة بأوزان ثلاثية (-1, 0, 1) وعمليات bit (على سبيل المثال، BitNet أو النماذج اللغوية الكبيرة ذات 1-bit)، محققة مكاسب في الكفاءة تصل إلى 10x من خلال تجنب عمليات النقطة العائمة تمامًا. هذا يشبه الضرب الدقيق للأعداد الصحيحة في البرمجة التنافسية لكنه يضحّي بالدقة من أجل السرعة — وهو مفيد للاستدلال ويبدأ في الظهور في التدريب.

- **ضرب المصفوفات المتناثر والمنظم**: إذا وجد تَنَاثُر (على سبيل المثال، من التقليم)، استخدم مكتبات ضرب المصفوفات المتناثرة لتخطي حسابات القيم الصفرية. يمكن أن ي induce الإسقاط المنظم التَنَاثُر أثناء التدريب، والتحسين بناءً عليه.

هذه التحسينات مجربة ومختبرة في أطر عمل مثل Hugging Face Transformers أو Lightning AI، وغالبًا ما تحقق تحسينات من 2 إلى 10x في إنتاجية التدريب.

### مشاكل الدقة في ضرب المصفوفات ذات النقطة العائمة
الأعداد ذات النقطة العائمة لها دقة محدودة (على سبيل المثال، FP16 لديها ~11 بت للجزء العشري، مما يعرضها لخطر النضوب في التدرجات الصغيرة أثناء الانتشار الخلفي). في النماذج اللغوية الكبيرة، يتضخم هذا في المصفوفات الضخمة (على سبيل المثال، مليارات المعاملات)، مسببًا:
- **أخطاء التجميع**: جمع العديد من النواتج الصغيرة يمكن أن يفقد التفاصيل أو يفيض.
- **عدم الإبدال**: (a + b) + c ≠ a + (b + c) في النقطة العائمة، مما يؤدي إلى نتائج غير قابلة للتكرار عبر العتاد المختلف.
- **ضوضاء التكميم**: تقدم تنسيقات الدقة المنخفضة أخطاء تقريب، قد تؤدي إلى زعزعة استقرار التدريب.

التخفيفات:
- تحجيم الخسارة: اضرب الخسائر في عامل (على سبيل المثال، 2^15) قبل الانتشار الخلفي، ثم قم بتحجيم التدرجات مرة أخرى.
- تنسيقات Microscaling أو مجمعات دقة عالية مُحاكاة.
- التقريب العشوائي: قم بالتقريب عشوائيًا بدلاً من القطع لتقليل التحيز.

في البرمجة التنافسية، يستخدم ضرب الأعداد الكبيرة (على سبيل المثال، عبر FFT) أعدادًا صحيحة ذات دقة اعتباطية للحصول على نتائج دقيقة، متجنبًا مشاكل النقطة العائمة تمامًا. لا تستطيع النماذج اللغوية الكبيرة تحمل تلك التكلفة الإضافية، لذا فهة تتبنى النقطة العائمة التقريبية مع ضمانات — الدقة "جيدة بما يكفي" لتعميم النموذج، وليست رياضيات دقيقة.

### الإسقاط وعلاقته بضرب المصفوفات
الإسقاط هو أسلوب تنظيم يقوم بإلغاء عناصر عشوائيًا (على سبيل المثال، بمعدل 10-20٪) أثناء التدريب لمنع الإفراط في التخصيص — يتم تطبيقه *بعد* الطبقات، وليس *أثناء* ضرب المصفوفات أو عمليات الضرب الفردية للنقاط العائمة. على سبيل المثال:
- في الكود: `output = dropout(matmul(input, weights))`
- ضرب المصفوفات نفسه هو عملية ذرية (على سبيل المثال، عبر مكتبات BLAS) تحسب حاصل الضرب النقطي الكامل دون انقطاعات.

*لا* تقوم "بإسقاط عناصر أثناء عملية ضرب المصفوفات" لأن:
- ضرب المصفوفات هو جبر خطي منخفض المستوى؛ حقن الإسقاط في منتصف الحساب سيكسر الرياضيات والكفاءة.
- يضيف الإسقاط ضوضاء بعد ضرب المصفوفات، مما يساعد في التنظيم لكنه غير مرتب بإصلاحات الدقة. في الواقع، يمكن أن يفاقم مشاكل الدقة في الدقة المنخفضة للنقطة العائمة عن طريق تضخيم القيم الصغيرة، لكننا نتعامل مع هذا بشكل منفصل (على سبيل المثال، عبر التحجيم).
- أثناء الاستدلال (وليس التدريب)، يتم تعطيل الإسقاط تمامًا — لا إسقاط عشوائي.

ومع ذلك، من أجل التحسين:
- الإسقاط المدمج (على سبيل المثال، في Flash Attention) يجمعه مع ضرب المصفوفات لإخفاء عبء إنشاء الأعداد العشوائية، محسنًا الأداء بنسبة 10-20٪ دون تغيير الدلالات.
- إذا تسبب الإسقاط في ضرر كبير للدقة، يتم استخدام بدائل مثل تسوية الطبقة أو اضمحلال الأوزان، لكنه معيار في تدريب النماذج اللغوية الكبيرة.

باختصار، نعم، أنت محق — نحن لا نطبق الإسقاط أثناء ضرب المصفوفات أو عمليات ضرب النقاط العائمة. إنها عملية ذات مستوى أعلى.

إذا كنت تنفذ هذا (على سبيل المثال، في مدرب مخصص)، ابدأ بالدقة المختلطة والعمليات المدمجة. للإلهام من أسلوب الأعداد الكبيرة في البرمجة التنافسية، ابحث في متغيرات النماذج اللغوية الكبيرة القائمة على الأعداد الصحيحة مثل BitNet للتدريب القوي تجاه الدقة.

[FP8 GEMM LLM Training - Emergent Mind](https://www.emergentmind.com/topics/fp8-gemm-llm-training)  
[Floating-Point 8: An Introduction to Efficient, Lower-Precision AI](https://developer.nvidia.com/blog/floating-point-8-an-introduction-to-efficient-lower-precision-ai-training/)  
[Optimizing Large Language Model Training Using FP4 Quantization](https://arxiv.org/html/2501.17116v1)  
[Beyond Matrix Multiplacations: The New Frontier of LLMs with 10x Efficiency](https://machine-learning-made-simple.medium.com/beyond-matmul-the-new-frontier-of-llms-with-10x-efficiency-fef21ee32eb6)  
[4-bit LLM training and Primer on Precision, data types & Quantization](https://vizuara.substack.com/p/4-bit-llm-training-and-primer-on)  
[Accelerating Large Language Models with Mixed-Precision](https://lightning.ai/pages/community/tutorial/accelerating-large-language-models-with-mixed-precision-techniques/)  
[Microscaling Floating Point Formats for Large Language Models](https://arxiv.org/html/2510.01863v1)  
[Understanding Floating Point Numbers and Precision in the Context of LLMs](https://dhnanjay.medium.com/understanding-floating-point-numbers-and-precision-in-the-context-of-large-language-models-llms-3b4d981a8266)  
[MatMul-Free LLMs: Key Concepts Explained](https://www.datacamp.com/blog/matmul-free-language-models)  
[Reducing the Cost of Dropout in Flash-Attention](https://arxiv.org/html/2410.07531)  
[Efficient Sparse Training with Structured Dropout](https://arxiv.org/html/2411.01238v1)  
[Writing an LLM from scratch, part 10 -- dropout](https://www.gilesthomas.com/2025/03/llm-from-scratch-10-dropout)