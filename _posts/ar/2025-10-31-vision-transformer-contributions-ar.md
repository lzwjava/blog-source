---
audio: false
generated: true
image: false
lang: ar
layout: post
model: x-ai/grok-4-fast
title: مؤلفو ومُساهمات محول الرؤية
translated: true
type: note
---

### مقدمة عن مؤلفي "الصورة تساوي 16x16 كلمة: المحولات للتعرف على الصور على نطاق واسع"

شكلت الورقة البحثية الرائدة "الصورة تساوي 16x16 كلمة: المحولات للتعرف على الصور على نطاق واسع"، التي قدمها أليكسي دوسوفيتسكي وزملاؤه في جوجل عام 2020، تحولاً محورياً في مجال الرؤية الحاسوبية. اقترحت الورقة نموذج Vision Transformer (ViT)، وهو نموذج يطبق بنى المحولات - التي صممت أصلاً لمعالجة اللغة الطبيعية - مباشرة على بقع الصور، محققاً أداءً متطوراً على مجموعات البيانات واسعة النطاق مثل ImageNet عند التدريب المسبق على بيانات ضخمة (مثل JFT-300M). أظهر هذا العمل أن المحولات الخالصة يمكنها التفوق على الشبكات العصبية التلافيفية (CNNs) في الكفاءة والدقة مع توفر قدرة حاسوبية وبيانات كافية، مما أثر على التطورات اللاحقة في الذكاء الاصطناعي متعدد الوسائط ونماذج الرؤية القابلة للتطوير.

كانت الورقة نتيجة جهد تعاوني لـ 12 باحثاً، primarily من فريق جوجل برين في زيورخ، يمزجون بين الخبرة في التعلم العميق، ونمذجة التسلسل، والتدريب على نطاق واسع. فيما يلي نظرة عامة على المؤلفين الرئيسيين، مع تسليط الضوء على خلفياتهم وإسهاماتهم في المجال. (للإيجاز، ركزت على المساهمين البارزين؛ تتضمن القائمة الكاملة ديرك وايسنборن، وتوماس أونترثينر، ومصطفى دهقاني، وماتياس ميندرير، وجورج هيغولد، وسيلفان جيلي، وجاكوب أوسزكوريت - وكلهم خريجو جوجل بجذور عميقة في المحولات، والتحسين، ودمج الرؤية واللغة).

#### المؤلفون الرئيسيون وخلفياتهم

- **أليكسي دوسوفيتسكي** (المؤلف الرئيسي): كالقوة الدافعة وراء ViT، صاغ دوسوفيتسكي الفكرة الأساسية المتمثلة في معالجة الصور كسلاسل من البقع. حاصل على ماجستير ودكتوراه في الرياضيات من جامعة لومونوسوف موسكو الحكومية، تلاها عمل ما بعد الدكتوراه في جامعة فرايبورغ حول التعلم غير الخاضع للإشراف للميزات. انضم إلى جوجل برين في 2019، وقاد تطوير ViT قبل الانتقال إلى Inceptive (شركة ذكاء اصطناعي مقرها برلين) في 2021. يمتد عمله ليشمل الرؤية الحاسوبية، والنماذج التوليدية، والتعلم الآلي المستوحى من علم الأحياء، بأكثر من 136,000 استشهاد.

- **لوكاس باير**: لعب باير دوراً حاسماً في التنفيذ العملي لـ ViT، وتقييمه على المعايير القياسية، وتحسينات الكفاءة. بلجيكي الجنسية، درس الهندسة الميكانيكية في جامعة RWTH آخن، وحصل على دكتوراه في الروبوتات والذكاء الاصطناعي في 2018 مع التركيز على ذكاء الألعاب والتعلم المعزز. انضم إلى جوجل برين في زيورخ بعد الدكتوراه، وترقى إلى منصب باحث علمي رئيسي في جوجل ديبمايند. في 2025، أصبح أحد أهم عمليات التوظيف في ميتا للذكاء الاصطناعي، مستمراً في العمل على محولات الرؤية والتعلم الآلي المرتكز على البيانات.

- **ألكسندر كوليسنيكوف**: ساهم كوليسنيكوف في تجارب تحجيم ViT ورؤى نقل التعلم، مع التركيز على أدائه على مجموعات البيانات متوسطة الحجم. حصل على ماجستير في الرياضيات من جامعة موسكو الحكومية ودكتوراه في التعلم الآلي/الرؤية الحاسوبية من معهد النمسا للعلوم والتكنولوجيا (ISTA) في 2018. بدأ في جوجل برين في 2018، وتقدم إلى مناصب رئيسية في ديبمايند قبل الانضمام إلى OpenAI، ثم في 2025، انضم إلى ميتا - حيث تم جذبه بسبب خبرته في نماذج الرؤية الفعالة.

- **شياوهوا زهاي**: ركز زهاي على استراتيجيات التدريب المسبق لـ ViT وامتداداته متعددة الوسائط، مستمداً ذلك من عمله في تعلم التمثيل. حاصل على دكتوراه في الهندسة الإلكترونية من جامعة بكين، وانضم إلى جوجل كمهندس برمجيات في 2015، ثم انتقل إلى البحث في جوجل برين في 2017 وديبمايند في 2023. يعمل الآن باحثاً في ميتا (عبر OpenAI زيورخ في 2025)، وتمتد إسهاماته لتربط بين الرؤية، واللغة، والتعلم الذاتي الإشراف، بأكثر من 100,000 استشهاد.

- **نيل هولسبي** (المؤلف الأول): كقائد فريق، أشرف هولسبي على التصميم المعماري لـ ViT وآثاره الأوسع على قوانين التحجيم في الرؤية. حصل على زمالة جوجل الأوروبية للدكتوراه حوالي عام 2010 وأكمل درجة الدكتوراه في التعلم الآلي. كان باحثاً في جوجل لفترة طويلة منذ أيام تدريبه، وأدار فرقاً في جوجل برين وديبمايند تعمل على البنى العصبية ونماذج الرؤية واللغة. في 2025، انضم إلى Anthropic ليرأس مكتبهم الجديد في زيورخ، مركزاً على التحجيم الآمن للذكاء الاصطناعي.

استفادت هذه collaboration في جوجل برين (ومعظمها مقرها زيورخ) من قرب الفريق من وحدات TPU لإجراء تجارب ضخمة - أكثر من 25,000 يوم TPU - مما أثبت جدوى المحولات beyond النص. انتقل العديد من المؤلفين منذ ذلك الحين إلى معامل الذكاء الاصطناعي الرائدة مثل ميتا، OpenAI، وAnthropic، مما يعكس التأثير الدائم لـ ViT على المجال.

#### المراجع
- [An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale (arXiv)](https://arxiv.org/abs/2010.11929)
- [Alexey Dosovitskiy Google Scholar Profile](https://scholar.google.com/citations?user=FXNJRDoAAAAJ)
- [Lucas Beyer Google Scholar Profile](https://scholar.google.com/citations?user=p2gwhK4AAAAJ)
- [Alexander Kolesnikov Google Scholar Profile](https://scholar.google.com/citations?user=H9I0CVwAAAAJ)
- [Xiaohua Zhai Google Scholar Profile](https://scholar.google.com/citations?user=8gruapYAAAAJ)
- [Neil Houlsby Google Scholar Profile](https://scholar.google.com/citations?user=sm1-TZMAAAAJ)
- [From Ph.D. Detour to Meta's Top AI Hire: Lucas Beyer (Times of India)](https://timesofindia.indiatimes.com/education/news/lucas-beyer-educational-qualification-from-ph-d-detour-and-google-rejection-to-becoming-metas-top-ai-hire/articleshow/122158414.cms)
- [Zuckerberg Poaches Alexander Kolesnikov from OpenAI (Times of India)](https://timesofindia.indiatimes.com/education/news/zuckerberg-poached-alexander-kolesnikov-from-sam-altmans-openai-where-did-this-ai-genius-study/articleshow/122158654.cms)
- [Anthropic Poaches Neil Houlsby from Google DeepMind (Euronews)](https://www.euronews.com/next/2025/02/13/anthropic-poaches-google-deepmind-talent-to-lead-new-european-office)