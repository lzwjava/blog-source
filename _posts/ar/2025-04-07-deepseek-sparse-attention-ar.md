---
audio: false
generated: true
image: false
lang: ar
layout: post
model: anthropic/claude-4.5-sonnet
title: شرح الانتباه المتفرق من DeepSeek
translated: true
type: note
---

لدي الآن معلومات شاملة عن DeepSeek Sparse Attention (DSA) في DeepSeek-V3.2-Exp. دعني أقدم شرحًا مفصلاً.

## DeepSeek Sparse Attention (DSA) في DeepSeek-V3.2-Exp

DeepSeek Sparse Attention (DSA) هي آلية انتباه متفرق دقيق الحبيبات تم تقديمها في نموذج DeepSeek-V3.2-Exp، مما يمثل تقدمًا كبيرًا في معالجة السياقات الطويلة بكفاءة لنماذج اللغة الكبيرة. **تحقق DSA انتباهًا متفرقًا دقيق الحبيبات لأول مرة، مما يوفر تحسينات كبيرة في كفاءة التدريب والاستدلال للسياقات الطويلة مع الحفاظ على جودة مخرجات النموذج بشكل شبه متطابق**[1]

## البنية الأساسية: نظام ثنائي المكونات

تتكون DSA من مكونين أساسيين يعملان معًا لتحقيق انتباه متفرق فعال:[2]

### 1. **فهرس البرق (Lightning Indexer)**

فهرس البرق هو آلية تسريع سريعة وخفيفة الوزن تقيم بسرعة أهمية الرموز التاريخية للاستعلام الحالي. **يحتفظ الفهرس بذاكرة تخزين مؤقت صغيرة للمفاتيح بأبعاد 128 لكل رمز**[3] (مقارنة بذاكرة التخزين المؤقت الكاملة للمفاتيح-القيم المستخدمة في الانتباه التقليدي).

**كيف يعمل:**
- يحسب فهرس البرق درجات الارتباط بين رمز الاستعلام الحالي وجميع الرموز السابقة في التسلسل
- يستخدم تمثيلات مضغوطة للمفاتيح (128 بُعدًا بدلاً من المفاتيح كاملة الأبعاد) لتقليل متطلبات الذاكرة والحساب بشكل كبير
- **على الرغم من أن فهرس البرق لا يزال لديه تعقيد O(L²)، إلا أنه يتطلب حسابًا أقل بكثير مقارنة بآلية الانتباه الرئيسية**[4]
- يصنف الفهرس الرموز بسرعة حسب الأهمية ويحدد أهم الرموز ذات الصلة الأعلى ترتيبًا (top-K)

**الميزة الرئيسية:** يعمل الفهرس كـ"مرشح مسبق" خفيف الوزن يمكنه المسح السريع عبر السياقات الطويلة دون العبء الحسابي الكامل لحسابات الانتباه الكاملة.

### 2. **آلية اختيار الرموز دقيقة الحبيبات**

بعد أن يحدد فهرس البرق الرموز المهمة، تقوم آلية الاختيار دقيقة الحبيبات بإجراء حساب الانتباه المتفرق الفعلي:

- فقط أهم الرموز ذات الصلة الأعلى ترتيبًا (كما يحددها الفهرس) تتلقى حساب انتباه كامل
- يقلل هذا المعالجة الانتقائية بشكل كبير من حساب الانتباه من O(n²) إلى حوالي O(nk)، حيث k هو عدد الرموز المحددة (أصغر بكثير من n)
- **تحل DSA محل النهج القائم على القوة الغاشمة بمعالجة انتقائية، باستخدام ما تسميه DeepSeek "فهرس البرق" لتسريع الرموز السابقة بسرعة وتحديد أيها الأكثر أهمية لكل استعلام**[2]

## تقليل التعقيد الرياضي

تتطلب آليات الانتباه التقليدية حساب العلاقات بين كل رمز وجميع الرموز الأخرى، مما يؤدي إلى تعقيد حسابي O(n²). **يقلل DeepSeek Sparse Attention (DSA) تعقيد الانتباه الأساسي من O(L²) إلى O(Lk)، حيث k هو عدد الرموز المحددة (أصغر بكثير من L)**[4]

يمثل هذا تحولاً أساسيًا في كيفية حساب الانتباه:
- **الانتباه الكامل التقليدي:** كل استعلام ينتبه إلى كل زوج مفتاح-قيمة → O(n²)
- **انتباه DSA المتفرق:** كل استعلام ينتبه فقط إلى أهم أزواج المفتاح-قيمة الأعلى ترتيبًا → O(nk)
- نظرًا لأن k << n (عادة ما تكون k ثابتة صغيرة أو تنمو ببطء شديد مقارنة بـ n)، فإن هذا يحقق تحجيمًا شبه خطي

## التكامل مع Multi-Latent Attention (MLA)

تتكامل DSA مع بنية Multi-Latent Attention (MLA) الحالية لـ DeepSeek المستخدمة في نماذج V3. تعمل آلية الانتباه المتفرق على قمة تمثيلات المفاتيح-القيم المضغوطة لـ MLA، مما يخلق استراتيجية ضغط ثنائية المرحلة:

1. **المرحلة الأولى (MLA):** ضغط تمثيلات المفاتيح-القيم في مساحات كامنة ذات أبعاد أقل
2. **المرحلة الثانية (DSA):** تقليل الحساب further باختيار فقط الرموز الأكثر صلة للانتباه إليها

يحقق هذا الضغط المزدوج مكاسب في الكفاءة لا يمكن لأي من التقنيتين تحقيقه بمفرده.[3]

## مكاسب الأداء والكفاءة

التحسينات في الكفاءة من DSA كبيرة عبر أبعاد متعددة:

### **تحسينات السرعة:**
- **أسرع بمقدار 2-3× في الاستدلال** لمعالجة النصوص الطويلة[2]
- تسريع كبير في كل من مراحل التدريب والاستدلال
- فعال بشكل خاص للتسلسلات الأطول من 32K رمز

### **تقليل الذاكرة:**
- متطلبات أصغر لذاكرة التخزين المؤقت KV بسبب مفاتيح الفهرس المضغوطة (128 بُعدًا)
- يخزن فقط الانتباه الكامل للرموز المحددة
- يمكن معالجة سياقات أطول ضمن نفس ميزانية الذاكرة

### **تقليل التكلفة:**
تترجم مكاسب الكفاءة مباشرة إلى تخفيضات مذهلة في التكلفة. **انخفضت أسعار API بأكثر من 50%، مع تكاليف إدخال منخفضة تصل إلى 0.07 دولار لكل مليون رمز (ضربة ذاكرة التخزين المؤقت)**[5]

**أسعار API الجديدة:**
- الإدخال: 0.14 دولار لكل مليون رمز (قياسي)، 0.07 دولار لكل مليون رمز (ضربة ذاكرة التخزين المؤقت)
- الإخراج: 0.42 دولار لكل مليون رمز
- يمثل هذا **انخفاضًا بأكثر من 50%** مقارنة بـ V3.1-Terminus[6]

يأتي تخفيض التكلفة من عاملين:
1. تقلل آليات الانتباه المتفرق التكاليف الحسابية بشكل كبير
2. إدخال آليات ذاكرة التخزين المؤقت يقلل الحسابات الزائدة عن الحاجة[5]

## الحفاظ على الأداء

إنجاز حاسم لـ DSA هو الحفاظ على جودة النموذج مع تحقيق مكاسب الكفاءة. تم تدريب DeepSeek-V3.2-Exp بنفس تكوين V3.1-Terminus لتقييم تأثير الانتباه المتفرق بدقة.

**نتائج المعايير:**[1]

| المعيار | V3.1-Terminus | V3.2-Exp (DSA) |
|-----------|--------------|----------------|
| MMLU-Pro | 85.0 | 85.0 |
| GPQA-Diamond | 80.7 | 79.9 |
| LiveCodeBench | 74.9 | 74.1 |
| AIME 2025 | 88.4 | 89.3 |
| HMMT 2025 | 86.1 | 83.6 |

تظهر النتائج أن **V3.2-Exp يُظهر أداءً على قدم المساواة مع V3.1-Terminus عبر المعايير العامة**[1]، مع ظهور تحسينات في بعض المهام. تم تصميم آلية الانتباه المتفرق بعناية للاحتفاظ بأهم اتصالات الانتباه، لذا فإن التأثير على جودة المخرgs ضئيل.

## كيف تختلف DSA عن طرق الانتباه المتفرق الأخرى

### **دقيق الحبيبات مقابل خشن الحبيبات:**
معظم طرق الانتباه المتفرق السابقة تستخدم أنماطًا خشنة الحبيبات (أنماط ثابتة، نوافذ محلية، انتباه متدرج). تحقق DSA **التفرغ دقيق الحبيبات** من خلال تعلم أي رموز محددة للانتباه إليها ديناميكيًا بناءً على صلة المحتوى.

### **الاختيار المتعلم:**
على عكس أنماط التفرغ الثابتة، تتعلم DSA تسجيل الأهمية من خلال فهرس البرق، مما يسمح لأنماط انتباه تكيفية تستجيب للعلاقات الدلالية الفعلية.

### **مُحسَّن للأجهزة:**
تم تصميم DSA من الأساس ليكون فعالاً على أجهزة GPU الحديثة، على عكس بعض الطرق المتفرقة التي تظهر مكاسب نظرية ولكن تسريع محدود في العالم الحقيقي.

### **التفرغ القابل للتدريب:**
نمط الانتباه المتفرق يتم تعلمه أثناء التدريب (قابل للتدريب أصلاً)، وليس تطبيقه فقط في وقت الاستدلال، مما يسمح بتحسين أفضل.

## التنفيذ التقني

يتطلب تنفيذ DSA نواة CUDA متخصصة لأداء مثالي:

- **نواة الفهرس** لاختيار top-K السريع (متوفرة في DeepGEMM)
- **نواة الانتباه المتفرق** للحساب الفعال على الرموز المحددة (متوفرة في FlashMLA)
- دعم الانتباه المقسم إلى صفحات لكفاءة الذاكرة
- التكامل مع أطر الاستدلال الحالية (vLLM, SGLang)[1]

## حالات الاستخدام والمزايا

تتفوق DSA بشكل خاص في السيناريوهات التي تتطلب:

1. **معالجة السياقات الطويلة** (64K+ رمز): تحليل المستندات، فهم الكود، المحادثات متعددة الجولات
2. **التطبيقات عالية الإنتاجية**: حيث تكون التكلفة والسرعة حرجة
3. **النشر مقيد الذاكرة**: حيث يكون حجم ذاكرة التخزين المؤقت KV عنق زجاجة
4. **التطبيقات في الوقت الفعلي**: حيث يهم زمن الاستدلال

## الأهمية الاستراتيجية

**تمثل DeepSeek-V3.2-Exp خطوة وسيطة نحو الجيل التالي من البنية**[1]، وتحديدًا وضع الأساس لـ DeepSeek-V4. يسمح الإصدار التجريبي لـ DeepSeek بـ:

- التحقق من صحة آليات الانتباه المتفرق على نطاق واسع
- جمع بيانات أداء من العالم الحقيقي
- صقل النهج قبل النشر الكامل
- اختبار التكامل مع أنظمة الإنتاج

## القيود والاعتبارات

بينما تقدم DSA مزايا كبيرة، هناك بعض الاعتبارات:

1. **التعقيد:** تنفيذ أكثر تعقيدًا من الانتباه القياسي
2. **النفقات العامة للتسلسلات القصيرة:** قد لا تكون النفقات العامة للفهرس مجدية للسياقات القصيرة جدًا
3. **مقايضات التقريب:** بينما يتم الحفاظ على الجودة، من الممكن نظريًا فقدان بعض المعلومات
4. **متطلبات الأجهزة:** يتطلب نواة محسنة لأفضل أداء

## الآثار المستقبلية

تمثل DSA اتجاهًا تطوريًا مهمًا لبنى LLM. من خلال إثبات أن الانتباه الانتقائي يمكنه الحفاظ على الجودة مع تحسين الكفاءة بشكل كبير، فإنه يفتح إمكانيات لـ:

- معالجة سياقات أطول (1M+ رمز)
- خدمات ذكاء اصطناعي بأسعار معقولة أكثر
- النشر على أجهزة أقل قوة
- المعالجة في الوقت الفعلي للمستندات المعقدة

تضع التكنولوجيا DeepSeek في موقع تنافسي ضد نماذج الحدود الأخرى مع تقليل التكاليف التشغيلية بشكل كبير، مما قد يعطل هيكل تسعير سوق واجهة برمجة تطبيقات الذكاء الاصطناعي.[7]

باختصار، DeepSeek Sparse Attention (DSA) هي آلية ثنائية المرحلة متطورة تجمع بين فهرس برق خفيف الوزن لتسريع أهمية الرموز السريع مع حساب انتباه انتقائي دقيق الحبيبات. تحقق مكاسب كفاءة ملحوظة (تخفيض التكلفة بأكثر من 50%، تحسين السرعة 2-3×) مع الحفاظ على جودة النموذج، مما يمثل تقدمًا كبيرًا في جعل LLMs ذات السياقات الطويلة عملية و مجدية اقتصاديًا.

الاستشهادات:
[1] https://huggingface.co/deepseek-ai/DeepSeek-V3.2-Exp
[2] https://medium.com/@Techsankar381/deepseek-halves-ai-tooling-costs-with-breakthrough-sparse-attention-model-3840fb61a4e0
[3] https://x.com/vllm_project/status/1972617272901644345
[4] https://blogs.novita.ai/deepseek-v3-2-exp-on-novita-ai/
[5] https://dev.to/czmilo/deepseek-v32-exp-complete-analysis-2025-ai-model-breakthrough-and-in-depth-analysis-of-sparse-3gcl
[6] https://api-docs.deepseek.com/news/news250929
[7] https://www.implicator.ai/deepseek-cuts-inference-costs-by-10x/