---
audio: false
generated: true
image: false
lang: ar
layout: post
model: x-ai/grok-4-fast
title: تزويد بيانات النص التدريبية لنماذج اللغة الكبيرة
translated: true
type: note
---

### كيفية الحصول على بيانات نصية كبيرة لتدريب نماذج اللغة

يتطلب تدريب نماذج اللغة الكبيرة (LLMs) كميات هائلة من البيانات النصية المتنوعة عالية الجودة — غالبًا تريليونات الرموز المأخوذة من الكتب والمواقع الإلكترونية والكود والمزيد. التحديات الرئيسية هي المقياس (من التيرابايت إلى البيتابايت)، والجودة (ترشيح الضوضاء والمكررات والمحتوى منخفض القيمة)، والشرعية (احترام حقوق الطبع والنشر، استخدام بيانات المجال العام أو المرخصة). إليك دليل خطوة بخطوة للحصول عليها:

1.  **ابدأ بعمليات الزحف العامة على الويب**: هذه هي العمود الفقري لمعظم تدريبات نماذج اللغة الكبيرة. تلتقط لقطات من الإنترنت.
    *   رشّح للحصول على نص نظيف باستخدام أدوات مثل CC-Net أو Dedup (مكتبات بايثون عبر Hugging Face).
    *   معالجة على شكل قطع للتعامل مع الحجم — استخدم التخزين السحابي (مثل AWS S3) للتنزيلات.

2.  **استخدم مجموعات البيانات المُعدّة مسبقًا**: مجموعات مُرشحة مسبقًا من مجموعات البحث. قم بالتنزيل عبر واجهات برمجة التطبيقات (APIs) أو الروابط المباشرة.
    *   ركز على المجموعات الفرعية متعددة اللغات والمتخصصة في مجال معين (مثل الكود، العلوم) لتتناسب مع احتياجاتك.
    *   أدوات مثل مكتبة Hugging Face Datasets تجعل التحميل سهلاً: `from datasets import load_dataset`.

3.  **أكمل بمصادر متخصصة في مجالات محددة**:
    *   الكتب: Project Gutenberg (المجال العام).
    *   Wikipedia: نسخ اللغات.
    *   الكود: أرشيفات GitHub (عبر BigCode).
    *   إنشاء بيانات اصطناعية: استخدم النماذج الحالية (مثل عبر OpenAI API) لإنشاء سلاسل استدلالية، ولكن نظفها لتجنب التلوث.

4.  **نصائح قانونية وأخلاقية**:
    *   التزم بالتراخيص المفتوحة (مثل CC-BY, MIT).
    *   أزل التكرارات (أدوات مثل MinHash) والمعلومات الشخصية (PII).
    *   للتدريب المخصص، ابدأ صغيرًا (مثل الضبط الدقيق على 1-10 جيجابايت) قبل التوسع.
    *   تكاليف الحساب: توقع مئات الساعات من وحدة معالجة الرسومات (GPU-hours) حتى للتدريب المتواضع؛ استخدم Colab أو RunPod للاختبار.

5.  **خطوات المعالجة**:
    *   تنزيل → تنظيف (إزالة HTML، غير النص) → تحويل إلى رموز (Tokenize) (مثل باستخدام TikToken) → تدريب.
    *   المكتبات: Pandas لأخذ العينات، spaCy/NLTK للمعالجة المسبقة.

مجموعات البيانات العامة مجانية وضخمة — مثالية للهواة أو الباحثين. للإنتاج، غالبًا ما تروج الشركات لترخيص البيانات الخاصة.

### مصادر بيانات التدريب لنماذج محددة

تبقي النماذج الخاصة مثل تلك من OpenAI وAnthropic وDeepSeek الوصفات الدقيقة سرية لأسباب تنافسية، لكنها شاركت تفاصيل عالية المستوى عبر الأوراق البحثية والمدونات والتسريبات. النماذج مفتوحة المصدر (مثل Llama, Mistral) أكثر شفافية، وغالبًا ما تطلق مخططات مجموعات البيانات.

*   **نماذج GPT من OpenAI (مثل GPT-4o)**:
    يتم تدريبها على مزيج من بيانات الإنترنت المتاحة للجمهور (عمليات زحف ويب مُرشحة)، والكتب، والمقالات، والكود. استخدمت نماذج GPT المبكرة Common Crawl بشكل مكثف؛ بينما ركزت النماذج اللاحقة على مصادر عالية الجودة في العلوم والتكنولوجيا والهندسة والرياضيات والبرمجة. الإجمالي: تريليونات الرموز، مع إزالة كثيفة للتكرارات. كما يدمجون بيانات مرخصة وتفاعلات المستخدمين (مع خيار الانسحاب). لا يوجد إصدار عام كامل، لكنه "بشكل أساسي الإنترنت بأكمله" — تم استخراجه وترشيحه وتعزيزه.

*   **نماذج Anthropic (مثل Claude 3.5)**:
    تركز على البيانات الآمنة والمفيدة: نص الويب العام، والكتب، وأمثلة اصطناعية مُنشأة للمحاذاة (مثل Constitutional AI). يستخدمون محادثات المستخدمين من Claude (مع خيار الانسحاب) ومجموعات بيانات RLHF مثل HH-RLHF. التركيز على مصادر متنوعة وغير سامة؛ بعض الجدل حول نصوص YouTube المسربة. المقياس الإجمالي: تريليونات مماثلة، ولكن أكثر تحضيرًا من الناحية الأخلاقية.

*   **نماذج DeepSeek (مثل DeepSeek-V3, R1)**:
    نماذج صينية مفتوحة المصدر إلى حد ما تستخدم صفحات ويب عادية، وكتب إلكترونية، ومستودعات كود. تم التدريب المسبق لـ V3 على 14.8 تريليون رمز دون بيانات اصطناعية مقصودة، لكن R1 يضيف 600 ألف عينة استدلالية اصطناعية عبر الرفض بأخذ العينات (تم إنشاؤها بواسطة نماذج سابقة). المصادر: زحف الويب + المستندات التقنية؛ مزيج خاص، ولكنه شفاف في الأوراق البحثية.

*   **النماذج مفتوحة المصدر (مثل Llama 3, BLOOM, GPT-J)**:
    تستخدم هذه النماذج بشكل صريح مجموعات بيانات عامة مثل The Pile (مزيج متعدد اللغات 800 جيجابايت)، أو C4 (Colossal Clean Crawled Corpus، 750 جيجابايت ويب إنجليزي)، أو OSCAR (Common Crawl متعدد اللغات). استخدم BLOOM مجموعة ROOTS (1.6 تيرابايت، 46 لغة). تتجنب البيانات الخاصة، وتركز على إمكانية إعادة الإنتاج — تحقق من بطاقات النماذج على Hugging Face للحصول على التفاصيل الدقيقة.

باختصار: الجميع يعتمد على بيانات على مستوى الويب، لكن النماذج الخاصة تضيف الترشيح والترخيص والبيانات الاصطناعية لتحسين الجودة. النماذج مفتوحة المصدر تعتمد على المجموعات العامة التي أعدها المجتمع.

### روابط تنزيل مجموعات البيانات النصية الكبيرة العامة

إليك أهم المصادر المجانية القابلة للتنزيل (الأحجام تقريبية؛ تحقق من وجود تحديثات). ابدأ بمجموعات فرعية إذا كانت مساحة التخزين محدودة.

*   **Common Crawl**: لقطات شهرية للويب (إجمالي البيتابايتات). رشّح باستخدام فهارس CC-MAIN. [أرشيف Common Crawl](https://commoncrawl.org/get-started)
*   **The Pile**: 800 جيجابايت نص إنجليزي متنوع (كتب، كود، arXiv، إلخ). [EleutherAI The Pile على Hugging Face](https://huggingface.co/datasets/EleutherAI/pile)
*   **C4 (Colossal Clean Crawled Corpus)**: 750 جيجابايت ويب إنجليزي نظيف (مستخدم في T5/GPT). [TensorFlow Datasets C4](https://www.tensorflow.org/datasets/catalog/c4)
*   **OSCAR (Open Super-large Crawled Aggregated coRpus)**: ويب متعدد اللغات (22 لغة، ~10 تيرابايت). [OSCAR على Hugging Face](https://huggingface.co/datasets/oscar-corpus/OSCAR-2201)
*   **Wikipedia Dumps**: مستخرجات النص الكامل (الإنجليزية: ~20 جيجابايت). [التنزيلات من ويكيميديا](https://dumps.wikimedia.org/enwiki/latest/)
*   **BooksCorpus/OpenWebText**: 11 جيجابايت كتب + 40 جيجابايت Reddit/ويب (عصر GPT-2). [OpenWebText على GitHub](https://github.com/jcpeterson/openwebtext)
*   **RedPajama**: 1+ تريليون رمز، مُستنسخ من أوراق بحثية لـ Llama. [TogetherAI RedPajama على HF](https://huggingface.co/datasets/togethercomputer/RedPajama-Data-1T)
*   **LLMDataHub**: قائمة مُعدّة لأكثر من 100 مجموعة بيانات (دردشة، كود، إلخ). [GitHub LLMDataHub](https://github.com/Zjh-819/LLMDataHub)

للمزيد، تصفح مركز مجموعات بيانات Hugging Face: [Hugging Face Datasets](https://huggingface.co/datasets). تحقق دائمًا من التراخيص!

**المراجع**  
[كيف يتم تطوير ChatGPT ونماذجنا الأساسية](https://help.openai.com/en/articles/7842364-how-chatgpt-and-our-language-models-are-developed)  
[ما تسربه GPT-oss عن بيانات تدريب OpenAI](https://www.lesswrong.com/posts/iY9584TRhqrzawhZg/what-gpt-oss-leaks-about-openai-s-training-data)  
[Anthropic ستستخدم محادثات Claude كبيانات تدريب](https://www.wired.com/story/anthropic-using-claude-chats-for-training-how-to-opt-out/)  
[مجموعة بيانات HH-RLHF من Anthropic](https://huggingface.co/datasets/Anthropic/hh-rlhf)  
[ما الذي دخل في تدريب DeepSeek-R1؟](https://epoch.ai/gradient-updates/what-went-into-training-deepseek-r1)  
[الدليل الكامل لنماذج DeepSeek](https://www.bentoml.com/blog/the-complete-guide-to-deepseek-models-from-v3-to-r1-and-beyond)  
[LLMDataHub: مجموعات بيانات رائعة لتدريب نماذج اللغة الكبيرة](https://github.com/Zjh-819/LLMDataHub)  
[مجموعات بيانات التدريب مفتوحة المصدر لنماذج اللغة الكبيرة](https://kili-technology.com/large-language-models-llms/9-open-sourced-datasets-for-training-large-language-models)  
[بيانات تدريب نماذج اللغة الكبيرة: المصادر العامة الثمانية الرئيسية](https://oxylabs.io/blog/llm-training-data)