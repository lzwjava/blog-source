---
audio: false
generated: true
image: false
lang: ar
layout: post
model: openai/gpt-5
title: خوارزمية الانتباه السريع وفوائدها
translated: true
type: note
---

# شرح عميق لـ FlashAttention (Tri Dao وآخرون، 2022)

ملخص مختصر في البداية: تزيل FlashAttention الحاجة إلى تخصيص مصفوفة الانتباه الكاملة \\(N\times N\\) في ذاكرة الوصول العشوائي الديناميكية (HBM) لوحدة معالجة الرسومات عن طريق حساب الانتباه **كتلة تلو الأخرى** داخل ذاكرة SRAM الموجودة على الشريحة (السجلات / الذاكرة المشتركة)، ودمج خط أنابيب الانتباه بالكامل في نواة واحدة واستخدام تراكم softmax صحيح عدديًا على مستوى الكتل. هذا يقلل بشكل كبير من حركة المرور والذاكرة في HBM من \\(O(N^2)\\) إلى \\(O(N)\\) بشكل فعال، وفي الممارسة العملية يعطي تسريعًا كبيرًا في وقت التنفيذ على وحدات معالجة الرسومات للتسلسلات الطويلة. citeturn0search0turn0search9

---

## المشكلة: لماذا الانتباه القياسي مقيد بالإدخال/الإخراج (IO-bound)
عادة ما يتم تنفيذ الانتباه الذاتي (الضرب النقطي القياسي) في المحولات (Transformer) بثلاث خطوات:

1. حساب النقاط \\(S = Q K^\top\\) (بحجم \\(N\times N\\))؛  
2. حساب softmax لكل صف \\(P = \mathrm{softmax}(S)\\)؛  
3. حساب المخرج \\(O = P V\\).

بشكل ساذج، تقوم بتجسيد \\(S\\) (وغالبًا \\(P\\)) في ذاكرة الوصول العشوائي الديناميكية (DRAM) لوحدة معالجة الرسومات. لطول تسلسل \\(N\\)، يستخدم هذا \\(O(N^2)\\) من الذاكرة ويؤدي إلى مشكلتين في الإدخال/الإخراج:
- بصمة كبيرة في DRAM (غالبًا أول شيء يستنفد ذاكرة وحدة معالجة الرسومات)، و  
- الكثير من عمليات القراءة/الكتابة بين DRAM (HBM) و SRAM/السجلات على الشريحة — ونقلات HBM↔SRAM هذه هي عنق الزجاجة الحقيقي على وحدات معالجة الرسومات الحديثة.

تعيد FlashAttention صياغة الانتباه كمشكلة **إدخال/إخراج**، وليس فقط مشكلة عمليات حسابية (FLOPs)، وتهدف إلى تقليل الوصول إلى HBM. citeturn0search0

---

## الأفكار الأساسية (على مستوى عالٍ)
1. **تقسيم المصفوفات** \\(Q, K, V\\) إلى كتل تتسع في ذاكرة SRAM على الشريحة (الذاكرة المشتركة / السجلات).  
2. **معالجة الانتباه كتلة تلو الأخرى**: بالنسبة لبلاطة \\(Q\\) معينة ومجموعة متدفقة من بلاطات \\(K,V\\)، احسب المساهمات الجزئية في المخرج واجمعها فورًا — لا تقم أبدًا بتجسيد مصفوفة النقاط الكاملة \\(N\times N\\) في DRAM.  
3. **دمج كل شيء في نواة واحدة**: تقوم النواة بتحميل البلاطات إلى SRAM، وتحسب \\(QK^\top\\) لزوج البلاطات هذا، وتطبق منطق softmax وتضرب في بلاطة \\(V\\)، وتكتب المخرجات الجزئية — كل ذلك دون رحلات ذهاب وإياب للمصفوفات الوسيطة الكبيرة إلى DRAM. يقلل دمج النواة من عبء التعليمات والذاكرة.  
4. **تراكم softmax عدديًا صحيح على مستوى الكتل**: لأن softmax عبر الصف بأكمله يحتاج إلى القيمة القصوى والمجموع الكلي، تستخدم FlashAttention قيمة قصوى ومجموعًا مستمرين (بأسلوب log-sum-exp) لدمج مساهمات softmax من بلاطات \\(K\\) المتعددة بدقة وثبات دون تخزين الصف الكامل من النقاط.  
5. **المرور العكسي عن طريق إعادة الحساب**: بدلاً من تخزين القيم الوسيطة الكبيرة للمرور العكسي، أعِد حساب الانتباه الأمامي لكل كتلة أثناء المرور العكسي (مقايضة عمليات حسابية إضافية مقابل نقل إدخال/إخراج أقل بكثير في DRAM). يوفر نقل الإدخال/الإخراج المحفوظ في DRAM عادةً تسريعًا صافيًا لأن نقل الإدخال/الإخراج في DRAM هو المهيمن. citeturn0search2turn0search10

معًا، تنتج هذه الأفكار كلًا من تقليل الذاكرة وتحسينات السرعة في وقت التنفيذ. citeturn0search0

---

## الخوارزمية على مستوى الكتل — خطوة بخطوة (أمامي)
ضع في اعتبارك رأس انتباه واحد بطول تسلسل \\(N\\) وبعد للرأس \\(d\\). اختر حجم بلاطة \\(B\\) بحيث تتناسب كتلة النقاط بحجم \\(B\times B\\) والبلاطات المقابلة لـ \\(Q\\)، \\(K\\)، \\(V\\) في SRAM.

لكل بلاطة استعلام \\(Q_{i}\\) (الصفوف \\(iB:(i+1)B\\)):

1. تهيئة جامع للمخرجات \\(O_i \leftarrow 0\\).  
2. تهيئة حالة التطبيع المستمرة: `row_max` (لكل صف استعلام) إلى \\(-\infty\\)، `row_sum` إلى 0. تتابع هذه المقام العددي الثابت لـ softmax عبر بلاطات K المتعددة.  
3. لكل بلاطة مفتاح/قيمة \\(K_{j}, V_{j}\\) (الأعمدة \\(jB:(j+1)B\\)):
   - تحميل \\(Q_i\\)، \\(K_j\\)، \\(V_j\\) إلى SRAM.  
   - حساب بلاطة النقاط الأولية \\(S_{ij} = Q_i K_j^\top / \sqrt{d}\\) (بشكل متجهي وبشكل \\(B\times B\\)).
   - لكل صف في \\(S_{ij}\\)، احسب القيمة القصوى المحلية للصف \\(m_{ij}\\) والقيم الأسية \\(\exp(S_{ij} - m_{ij})\\).  
   - دمج الأسيات من هذه البلاطة في التطبيع المستمر للصف باستخدام خدعة log-sum-exp:
     - ليكن \\(M = \max(\text{row\_max}, m_{ij})\\).
     - تحديث `row_sum` := `row_sum` · exp(row_max − M) + local_sum · exp(m_{ij} − M).
     - تعيين `row_max` := \\(M\\).
   - حساب مساهمة البلاطة في الجامع باستخدام الأسيات المعاد قياسها بشكل مناسب: تراكم \\(O_i \mathrel{+}= \text{(tile-softmax)} \times V_j\\). (كل ذلك يتم داخل SRAM.)
4. بعد دفق جميع بلاطات K، إنهاء التطبيع باستخدام row_sum و row_max لإنتاج مخرجات softmax صحيحة؛ كتابة \\(O_i\\) إلى DRAM.

النقطة الأساسية: لا تتم كتابة أي مصفوفة \\(N\times N\\) إلى DRAM أبدًا؛ فقط البلاطات الصغيرة والمخرجات النهائية هي التي تكتب. التراكم الصحيح عدديًا باستخدام القيمة القصوى والمجموع المستمرين هو ما يسمح لقطع softmax لكل بلاطة بالدمج بدقة في نفس نتيجة softmax الكاملة عبر الصف. citeturn0search2turn0search10

---

## لماذا يفوز دمج النواة والتقسيم إلى بلاطات في SRAM عمليًا
- **وصول أقل إلى HBM:** الانتباه القياسي يقرأ/يكتب \\(O(N^2)\\) عنصرًا إلى DRAM (النقاط، softmax). تقرأ FlashAttention كل عنصر في \\(Q,K,V\\) عددًا ثابتًا من المرات، وتعيش جميع قيم النقاط/softmax المؤقتة فقط في SRAM. يظهر تحليل الإدخال/الإخراج في الورقة عددًا أقل من عمليات الوصول إلى HBM ونطاقات حيث تكون FlashAttention هي الأمثل من حيث الإدخال/الإخراج نظرًا لحجم SRAM. citeturn0search0  
- **حدود الكمون وعرض النطاق الترددي أهم من العمليات الحسابية (FLOPs):** وحدات معالجة الرسومات سريعة جدًا في الضرب والجمع (FP multiply-accumulate)؛ عندما تهيمن حركة مرور DRAM على وقت التشغيل، فإن تقليل نقلات DRAM يكون أكثر أهمية من تقليل العمليات الحسابية. يزيل دمج النواة حركة المرور الوسيطة لـ DRAM ويقلل من عبء تشغيل النواة. citeturn0search0  
- **مقايضة المرور العكسي:** إعادة حساب الكتل الأمامية أثناء المرور العكسي تزيد العمليات الحسابية ولكنها تتجنب تخزين القيم الوسيطة الكبيرة في DRAM. لأن إعادة الحساب تتم في SRAM وتحد من حركة مرور DRAM، فهي فائدة صافية لوقت التنفيذ في العديد من الحالات. citeturn0search10

تظهر النتائج التجريبية من الورقة والمتابعات تسريعًا بمضاعفات متعددة (على سبيل المثال، 2-7× في المعايير المبلغ عنها حسب النموذج وطول التسلسل) وتخفيضات كبيرة في ذروة الذاكرة. citeturn0search0turn0search10

---

## تفاصيل وتقايض تنفيذية مهمة

- **اختيار حجم البلاطة:** يجب اختيار البلاطة \\(B\\) بحيث تتناسب مجموعة العمل (بلاطات Q، K، V، مخازن مؤقتة للنقاط، جامعات جزئية، بالإضافة إلى مساحة إضافية) في SRAM على الشريحة لكل كتلة خيوط (threadblock). يعتمد \\(B\\) الأمثل على بعد الرأس، وأنواع البيانات (FP16/FP32/FP8)، وبنية وحدة معالجة الرسومات (كمية الذاكرة المشتركة / السجلات). إذا كانت صغيرة جدًا تقلل من كفاءة الحساب؛ إذا كانت كبيرة جدًا لا تتسع في SRAM. citeturn0search2

- **الاستقرار العددي:** تستخدم الخوارزمية القيمة القصوى والمجموع المستمرين لكل صف (دمج log-sum-exp) لضمان أن softmax النهائي يساوي softmax للمصفوفة الكاملة. هذا حاسم: FlashAttention هي **انتباه دقيق** (وليس تقريبي) بسبب ذلك التراكم المستقر. citeturn0search0

- **الإخفاء والسببية:** يتم التعامل مع الإخفاء السببي (ذاتي الانحدار) ببساطة عن طريق تخطي أو تعيين مساهمات المواضع المخفية في البلاطات المُدَفقَة وتحديث التطبيع المستمر وفقًا لذلك. لا يزال المنطق على مستوى الكتل يعمل ولكن قد يحتاج إلى ترتيب دقيق للبلاطات لضمان عدم تلويث العناصر المخفية للجامعات. citeturn0search2

- **المرور العكسي وتخطيط الذاكرة:** تخزن FlashAttention فقط البيانات الوصفية الدنيا (على سبيل المثال، row_max/row_sum لكل كتلة) وتعيد حساب منتجات البلاطات الأمامية أثناء المرور العكسي. يعيد التنفيذ ترتيب العمل بعناية لزيادة إعادة الاستخدام وتقليل ضغط السجل. citeturn0search10

- **الدقة وأنواع البيانات:** يؤثر استخدام FP16/FP8 على اختيارات تخزين البلاطات والتراكم. تضيف بعض الأعمال اللاحقة (FlashAttention-2 / FlashAttention-3) تحسينات للدقة المختلطة وميزات وحدة معالجة الرسومات الأحدث (Hopper, H100) لدفع الاستخدام والإنتاجية (FP throughput) إلى أبعد من ذلك. citeturn0search4turn0search11

- **تعيين التوازي:** تقوم النواة بتعيين warps/CTA blocks إلى بلاطات الاستعلام؛ داخل CTA، تتعاون warps في تحميل بلاطات K/V وحساب ضرب المصفوفات للبلاطات والاختزالات. تعتبر الاختزالات الفعالة على مستوى warp واستخدام تعليمات الضرب والجمع المدمجة (fused multiply-add) مهمة لذروة الإنتاجية. citeturn0search2

---

## FlashAttention مقابل طرق الانتباه الطويل التقريبية
تحتفظ FlashAttention بدلالات الانتباه **الدقيقة** (نفس النتيجة العددية مثل الانتباه الكامل حتى تقريب الفاصلة العائمة)، في حين أن العديد من طرق الانتباه الطويل تقرب الانتباه (التفرق، الرتبة المنخفضة، FAVOR+، إلخ) وتقايض الجودة مقابل الذاكرة/الوقت. بدلاً من ذلك، تقلل FlashAttention تكلفة الذاكرة/الإدخال والإخراج مع الحفاظ على الحساب الدقيق، لذلك لا تتغير جودة النموذج بينما تتحسن الإنتاجية/الذاكرة. هذا هو سبب جاذبيتها على نطاق واسع: لا يوجد مقايضة في الدقة، مجرد نواة منخفضة المستوى أفضل. citeturn0search0

---

## التوفر العملي والنظام البيئي
- أصدر المؤلفون تنفيذًا (CUDA) ومستودعًا محفوظًا بـ FlashAttention ولاحقًا FlashAttention-2. العديد من الأطر (Hugging Face Transformers، XLA/PyTorch forks، تنفيذات تعتمد على Triton) إما تستدعي عامل flash_attn أو توفر نوى مدمجة مماثلة. يمكنك استخدام عامل `flash_attn` أو المكتبات التي تعرضه؛ في PyTorch، تتضمن الإصدارات الحديثة بدائيات انتباه فعالة الذاكرة أيضًا، وتوفر حزم الطرف الثالث `flash_attn` تحسينًا فوريًا في السرعة/الذاكرة للعديد من أعباء العمل. تحقق من المستودع الرسمي للمثبتات وأمثلة API. citeturn0search9turn0search4

تحذير: "لا حاجة لنواة مخصصة" صحيح جزئيًا فقط — FlashAttention *هي* نواة مدمجة مخصصة (العمل في المستودع) التي تستدعيها الأطر. قد تشحن إصدارات PyTorch الحديثة نوى مدمجة مماثلة داخليًا أو تفوض إلى مكتبات البائع، لكن الفكرة الأساسية تتطلب تنفيذ نواة مدمجة (سواء في CUDA أو Triton أو كود البائع). الدرس المهم: أنت (كمستخدم للنموذج) لست مضطرًا لكتابة هذه النوى بنفسك — استخدم العامل المقدم. citeturn0search9turn0search7

---

## الامتدادات والمتابعات
- **FlashAttention-2 (2023):** يحسن التوازي، وتقسيم العمل، وتوسيع نطاق متعدد النوى للحصول على استخدام وإنتاجية أفضل لوحدة معالجة الرسومات. citeturn0search4  
- **FlashAttention-3 وأعمال هندسية أخرى (2024+):** تعديلات إضافية للأجهزة الجديدة (Hopper/H100)، و FP8، واستخدام TFLOP أعلى. تواصل هذه استمرار اتجاه نوى الانتباه المدمجة الواعية للأجهزة. citeturn0search11

---

## متى تساعد FlashAttention أكثر (قواعد عامة)
- **التسلسلات الطويلة** (آلاف متعددة) أو أحجام دفعات/رؤوس كبيرة — توفر معظم الذاكرة وتعطي أكبر تسريع.  
- **عندما يكون عرض نطاق DRAM هو عنق الزجاجة** — على سبيل المثال، النماذج الكبيرة مع \\(N\\) كبيرة حيث كان الانتباه الساذج يرهق DRAM.  
- **التدريب بسياقات كبيرة** لأن المرور العكسي الصديق لإعادة الحساب يقلل من ذروة الذاكرة (يسمح بدفعة/سياق أكبر). citeturn0search0

---

## كود زائف سريع (تصوري)
```
لكل بلاطة استعلام Qi:
    row_max = -inf (لكل صف)
    row_sum = 0    (لكل صف)
    out_acc = 0    (B x d_v)
    لكل بلاطة مفتاح/قيمة Kj, Vj:
        تحميل Qi, Kj, Vj إلى SRAM
        S = Qi @ Kj.T / sqrt(d)
        m = max_row(S)
        expS = exp(S - m)      # أسيات محلية
        local_sum = sum_row(expS)
        # دمج المطبعات بشكل مستقر:
        M = max(row_max, m)
        row_sum = row_sum * exp(row_max - M) + local_sum * exp(m - M)
        row_max = M
        # تراكم المخرج الجزئي مع expS معاد القياس:
        out_acc += (expS / (row_sum بعد الدمج)) @ Vj
    كتابة out_acc إلى DRAM
```
(التنفيذ يخزن المزيد من البيانات الوصفية وينظم القياس الدقيق بعناية؛ ما ورد أعلاه تصوري.) citeturn0search2

---

## المراجع / أين تقرأ بعد ذلك
- ورقة FlashAttention الأصلية (Tri Dao وآخرون، 2022) — الخوارزمية، تحليل الإدخال/الإخراج، المعايير. citeturn0search0turn0search2  
- GitHub الرسمي مع نواة CUDA وارتباطات PyTorch. citeturn0search9  
- ورقة FlashAttention-2 (Tri Dao، 2023) وملاحظات هندسية حول الإصدارات اللاحقة. citeturn0search4turn0search11

---

إذا كنت تريد، يمكنني:
- عرض مقارنة سطرًا بسطر لحلقة انتباه ساذجة مقابل كود زائف لـ FlashAttention على مستوى الكتل (بدون جداول عريضة)،  
- رسم تخطيطي لتنظيم نواة CUDA/Triton وميزانية السجل/الذاكرة المشتركة لمثال ملموس (على سبيل المثال، بعد رأس 64، بلاطة 128)، أو  
- إرشادك إلى استدعاء PyTorch API المحدد ومقتطف مثال لاستبدال `torch.nn.functional.scaled_dot_product_attention` بـ FlashAttention. ما الذي سيكون أكثر فائدة بعد ذلك؟