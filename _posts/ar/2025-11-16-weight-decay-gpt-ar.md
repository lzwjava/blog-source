---
audio: false
generated: true
image: false
lang: ar
layout: post
model: x-ai/grok-4-fast
title: تخفيف الوزن في تدريب نماذج جي بي تي
translated: true
type: note
---

تعتبر تناقص الأوزان (Weight decay) تقنية تنظيم شائعة تُستخدم عند تدريب نماذج GPT (وجميع نماذج اللغة الكبيرة تقريبًا) لمنع التجاوز (overfitting) وتحسين التعميم.

### ما الذي يفعله فعليًا
يضيف تناقص الأوزان مصطلح عقوبة إلى دالة الخسارة التي تدفع أوزان النموذج نحو قيم أصغر. من الناحية الرياضية، إذا كانت خسارتك الأصلية هي ℒ (مثل الانتروبيا المتقاطعة)، مع تناقص الأوزان فإنك تحسن:

ℒ_الإجمالية = ℒ + (λ / 2) × ||w||²

حيث:
- w هي معاملات النموذج (الأوزان)
- ||w||² هي القاعدة L2 التربيعية للأوزان
- λ (لامدا) هو معامل تناقص الأوزان (عادةً 0.01 ~ 0.1 في التدريب على غرار GPT)

يمنع هذا المصطلح الإضافي النموذج من تعلم أوزان كبيرة جدًا ما لم تُقلل بشكل كبير من الخسارة الأصلية.

### كيفية تنفيذه عمليًا (AdamW)
في معظم عمليات تدريب GPT (GPT-2، GPT-3 من OpenAI، LLaMA، Mistral، إلخ)، لا يستخدم الأشخاص Adam العادي مع التنظيم L2 القياسي. بدلاً من ذلك يستخدمون AdamW (Adam مع تناقص الأوزان المنفصل).

الفرق الرئيسي:
- يضيف التنظيم L2 القياسي في Adam العقوبة مباشرة في التدرج (gradient).
- يفصل AdamW ذلك: يطبق تناقص الأوزان كخطوة تحديث منفصلة، مما يعمل بشكل أفضل بكثير مع المحسنات التكيفية مثل Adam.

لذلك عندما ترى شيئًا مثل هذا في تكوينات التدريب:

```python
optimizer = AdamW(model.parameters(), lr=5e-5, weight_decay=0.1)
```

هذا الرقم 0.1 هو λ المذكور أعلاه، وهو يستخدم النسخة المنفصلة الصحيحة.

### القيم النموذجية في نماذج حقيقية على غرار GPT

| النموذج / الورقة البحثية | تناقص الأوزان | ملاحظات                                   |
|----------------------|--------------|-----------------------------------------|
| GPT-2 (OpenAI)       | 0.01         |                                         |
| GPT-3 (OpenAI)       | 0.1          | Brown et al. 2020                       |
| LLaMA 1 & 2 (Meta)   | 0.1          | غالبًا ما يُجمع مع عدم التطبيق على معايير الطبقة أو الانحيازات |
| PaLM (Google)        | 0.1          |                                         |
| Grok-1 (xAI)         | 0.1          | من التكوين المفتوح الإصدار            |
| Mistral / Mixtral    | 0.1          |                                         |

### فارق دقيق مهم: أي المعاملات يطبق عليها تناقص الأوزان؟
الممارسة الحديثة (منذ LLaMA) هي تطبيق تناقص الأوزان فقط على طبقات معينة:
- نعم: الطبقات الخطية (مصفوفات الأوزان)، مصفوفة التضمين (embedding)
- لا: مصطلحات الانحياز (bias)، أوزان LayerNorm، أي معاملات أحادية البعد

يتم ذلك عادةً باستخدام مجموعات المعاملات في PyTorch:

```python
decay = set()
no_decay = set()
for name, param in model.named_parameters():
    if "bias" in name or "layer_norm" in name or param.ndim == 1:
        no_decay.add(param)
    else:
        decay.add(param)

optimizer = AdamW([
    {"params": list(decay),   "weight_decay": 0.1},
    {"params": list(no_decay), "weight_decay": 0.0}
], lr=...)
```

### لماذا هو مهم لنماذج GPT بالحجم الكبير
عندما يكون عدد المعاملات بمليارات أو مئات المليارات، يمكن للنماذج أن تحفظ بيانات التدريب بسهولة. يعد تناقص الأوزان أحد أبسط وأكثر الأدوات فعالية (إلى جانب الإسقاط (dropout)، العمق العشوائي، إلخ) للحفاظ على السعة الفعالة تحت السيطرة وتحسين الأداء اللاحق.

باختصار: تناقص الأوزان ≈ التنظيم L2، ولكن عند تنفيذه بشكل صحيح عبر AdamW وتطبيقه بشكل انتقائي، يصبح مكونًا قياسيًا في virtually جميع وصفات التدريب المسبق الحديثة على غرار GPT.