---
audio: false
generated: true
image: false
lang: ar
layout: post
model: x-ai/grok-4-fast
title: عمارة كيمي للانتباه الهجين الخطي
translated: true
type: note
---

### ما هو كيمي لينيار؟

كيمي لينيار هو نموذج معماري تجريبي للغة من نوع "خليط الخبراء" (MoE) طورته Moonshot AI، وأُطلق في أواخر أكتوبر 2025. صُمم النموذج للتعامل مع سياقات طويلة جدًا (تصل إلى 1 مليون رمز) بكفاءة عالية، مما يجعله مناسبًا بشكل خاص للمهام التي تتضمن استدلالاً ممتدًا، وتوليد نصوص طويلة، وسيناريوهات التعلم المعزز (RL). المعمارية مفتوحة المصدر بموجب ترخيص MIT ومتاحة على Hugging Face تحت أسماء مثل Kimi-Linear-48B-A3B-Instruct.

في صميمها، تستخدم كيمي لينيار **آلية انتباه هجينة** تجمع بين:
*   **انتباه كيمي دلتا (KDA)**: هو نوع من الانتباه الخطي يُعتبر نسخة مُحسّنة من Gated DeltaNet. يستخدم KDA آلية بوابة أكثر كفاءة على ذاكرة RNN ذات الحالة المحدودة، مما يسمح له بمحاكاة الانتباه الكامل مع تقليل النفقات الحسابية بشكل كبير. هذا يجعله "خطيًا" من حيث التعقيد (O(N) بدلاً من O(N²) لطول التسلسل N).
*   **الانتباه الكامن متعدد الرؤوس (MLA)**: مُدمج بشكل عام بنسبة 3:1 (3 أجزاء KDA إلى جزء واحد MLA) لنمذجة أفضل للتبعيات المعقدة.

يحتوي النموذج على 48 مليار معلمة إجمالية ولكن يتم تنشيط 3 مليارات فقط في كل مرور أمامي (وهو ما هو شائع في تصاميم MoE)، وتم تدريبه على 5.7 تريليون رمز. تشمل الفوائد الرئيسية:
*   انخفاض يصل إلى 75٪ في استخدام ذاكرة KV cache.
*   زيادة سرعة فك التشفير بما يصل إلى 6 أضعاف للسياقات الطويلة.
*   أداء متميز في المعايير المرجعية لمهام السياق القصير، واسترجاع السياق الطويل، وقوانين قياس التعلم المعزز.

تم تنفيذ نواة KDA في مكتبة FLA مفتوحة المصدر لسهولة دمجها في محركات الاستدلال مثل llama.cpp أو exLlama.

### كيف يقارن بـ MLA وآليات الانتباه الأخرى؟

كيمي لينيار ليس بديلاً مباشرًا لـ MLA بل يُبنى عليه كنموذج هجين، معالجا بعض قيود MLA في السياقات فائقة الطول. إليك تحليل مقارن:

| الجانب                  | كيمي لينيار (KDA + MLA الهجين) | MLA (الانتباه الكامن متعدد الرؤوس) | الانتباه الكامل التقليدي (مثل MHA) |
|-------------------------|--------------------------------|----------------------------------|---------------------------------------|
| **التعقيد**         | خطي (O(N)) لمعظم الطبقات؛ هجين مع MLA عالمي ومتفرق | دون تربيعي (O(N log N) فعال عبر الضغط الكامن) | تربيعي (O(N²)) – يتعارض مع الطول |
| **الكفاءة (الذاكرة/الإنتاجية)** | ممتازة: ذاكرة KV أقل بنسبة 75٪، أسرع ب6 مرات لمليون رمز؛ يمكن تشغيله على بطاقة GPU واحدة سعة 24GB بدقة منخفضة | جيدة: يقلل المعلمات عبر الكامن المشترك؛ مُستخدم في كيمي K2 (1T معلمة) و DeepSeek-V3 | ضعيفة: تستهلك الذاكرة بشكل كبير للسلاسل الطويلة؛ تحتاج إلى تحسين مكثف |
| **الأداء**        | يتفوق على الانتباه الكامل في السياقات القصيرة/الطويلة/التعلم المعزز؛ قوي في المهام الوكلية/البرمجة | قوي في النمذجة الكثيفة (أفضل من MHA في الالتباس)؛ متميز في السياقات متوسطة المدى | الأساس: أفضل جودة خام لكنه غير فعال؛ يتأخر في القياس |
| **حالات الاستخدام**          | السياق الطويل (1M+ رمز)، التعلم المعزز، استدلال فعال | نماذج LLM عامة مع كفاءة في المعلمات (مثل نماذج MoE مثل كيمي K2) | السياقات القصيرة؛ نماذج قديمة مثل GPT-3 |
| **العيوب**          | بنية معمارية جديدة – أدوات ودعم محدودان في البداية | أقل مثالية للأطوال القصوى بدون الهجينة | تكلفة حاسوبية عالية؛ غير مجدٍ لمليون+ رمز بدون حيل |

*   **مقارنة بـ MLA**: يقوم MLA (الموجود في كيمي K2 من Moonshot و DeepSeek-V3) بضغط الاستعلامات والمفاتيح إلى كامن منخفض الرتبة للكفاءة، لكنه لا يزال يمكن أن يشكل عنق زجاجة في التسلسلات الطويلة جدًا بسبب العناصر التربيعية المتبقية. يقوم كيمي لينيار بالتخفيف من ذلك من خلال دمج KDA الخطي لـ 75٪ من رؤوس الانتباه، محافظًا على نمذجة التبعيات العالمية لـ MLA مع تقليل الذاكرة بشكل كبير. في المقاييس المرجعية، يتفوق الهجين على إعدادات MLA الخالصة في مهام استرجاع المعلومات من سياقات طويلة وفي كفاءة تدريب التعلم المعزز.
*   **مقارنة بالآخرين (مثل MHA، المتغيرات الخطية مثل RWKV)**: يتفوق على الانتباه القياسي متعدد الرؤوس (MHA) في السرعة والمقياس دون فقدان الجودة. مقارنةً بآليات الانتباه الخطية الخالصة (مثل RWKV أو DeltaNet الأساسي)، فإن تحسينات البوابة في كيمي لينيار ومزيج MLA يجعله أكثر تعبيرًا للمهام الدقيقة، متجنبًا "النسيان" في النماذج الخطية التكرارية الخالصة.

بشكل عام، يمثل كيمي لينيار تطورًا نحو آليات الانتباه "الهجينة"، مخلطًا بين القابلية الخطية للقياس والضغط الكامن لنماذج الجيل التالي للسياقات الطويلة. إنه واعد بشكل خاص للنشر مفتوح المصدر حيث تكون قيود الأجهزة مهمة.

**المراجع**
*   [مجموعة كيمي-لينيار على Hugging Face](https://huggingface.co/collections/moonshotai/kimi-linear)
*   [مناقشة Reddit حول إصدار كيمي لينيار](https://www.reddit.com/r/LocalLLaMA/comments/1ojzekg/moonshotaikimilinear48ba3binstruct_hugging_face/)
*   [ورقة Gated DeltaNet (أساس KDA)](https://arxiv.org/abs/2412.06464)