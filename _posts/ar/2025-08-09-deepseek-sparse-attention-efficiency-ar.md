---
audio: false
generated: true
image: false
lang: ar
layout: post
model: anthropic/claude-4.5-sonnet
title: اختراق كفاءة الانتباه المتفرق لـ DeepSeek
translated: true
type: note
---

يمثل الانتباه المتناثر الأصلي من DeepSeek (NSA) طفرة في نمذجة السياقات الطويلة بكفاءة لنماذج اللغة الكبيرة. على عكس آليات الانتباه الكامل التقليدية ذات التعقيد الحسابي التربيعي، يقلل NSA التكاليف الحسابية بذكاء مع الحفاظ على أداء النموذج أو حتى تجاوزه من خلال استراتيجية انتباه متناثر هرمي متطور.[1][2]

## البنية الأساسية وفلسفة التصميم

يتعامل NSA مع التحدي الأساسي لنمذجة السياقات الطويلة: آليات الانتباه القياسية تتطلب حسابات O(n²) حيث n هو طول التسلسل، مما يجعلها مكلفة بدرجة prohibitively للسياقات التي تتجاوز آلاف الرموز المميزة. **يستخدم NSA استراتيجية متناثرة هرمية ديناميكية، تجمع بين ضغط الرموز المميزة على النطاق الخشن مع اختيار الرموز المميزة على النطاق الناعم للحفاظ على كل من الوعي السياقي العالمي والدقة المحلية**[3]

تعمل الآلية على مبدأين أساسيين:

1. **ليس جميع الرموز المميزة تتطلب نفس القدر من الاهتمام** - يمكن ضغط بعضها أو تلخيصه
2. **تحسين الأجهزة أمر ضروري** - كفاءة الخوارزمية لا تعني شيئًا بدون تنفيذ سريع في العالم الحقيقي

## البنية ثلاثية الفروع

يعالج NSA الانتباه من خلال ثلاثة فروع متوازية تعمل معًا لإنشاء نمط انتباه متناثر فعال:[4]

### 1. **فرع الضغط**
يتعامل هذا الفرع مع تجميع السياق على النطاق الخشن من خلال تجميع الرموز المميزة المتتالية في كتل وضغطها في رموز مميزة ممثلة. تقلل آلية الضغط عدد الرموز المميزة التي يجب على النموذج الانتباه إليها من خلال إنشاء تمثيلات ملخصة لمجموعات الرموز المميزة. على سبيل المثال، قد يتم ضغط تسلسل مكون من 32,768 رمزًا مميزًا إلى حوالي 2,046 رمزًا مميزًا مضغوطًا.[5]

يستخدم الضغط آليات بوابة متعلمة لتحديد كيفية تجميع المعلومات من رموز مميزة متعددة في رموز مميزة ممثلة فردية، والحفاظ على الوعي السياقي العالمي دون العبء الحسابي الكامل.

### 2. **فرع الاختيار**
ينفذ هذا الفرع اختيار الرموز المميزة على النطاق الناعم من خلال تحديد أهم الرموز المميزة للانتباه إليها بشكل ديناميكي. بدلاً من الانتباه إلى جميع الرموز المميزة، يحسب النموذج درجات الأهمية وينتبه انتقائيًا فقط إلى الرموز المميزة الأكثر صلة بالاستعلام الحالي. هذا يحافظ على الدقة المحلية وي captures التفاصيل الحرجة التي قد تضيع من خلال الضغط وحده.

يتم تعلم عملية الاختيار أثناء التدريب، مما يسمح للنموذج بتحديد الرموز المميزة التي تحمل أكبر قيمة معلوماتية للسياقات والمهام المختلفة بشكل تكيفي.[6]

### 3. **فرع النافذة المنزلقة**
يحافظ هذا الفرع على السياق المحلي من خلال السماح لكل رمز مميز بالانتباه إلى جيرانه المباشرين ضمن نافذة ثابتة. هذا يضمن capture التبعيات قصيرة المدى دائمًا، بغض النظر عن قرارات الضغط أو الاختيار. تغطي النافذة المنزلقة عادةً الرموز المميزة الحديثة ضمن نصف قطر محدد.

## الأساس الرياضي

يمكن التعبير عن حساب الانتباه في NSA على أنه يعمل على ثلاث مجموعات مفتاح-قيمة متميزة:

- **أزواج KV المضغوطة** من فرع الضغط
- **أزواج KV المحددة** من فرع الاختيار
- **أزواج KV المحلية** من النافذة المنزلقة

بدلاً من حساب الانتباه لجميع الرموز المميزة n، يحسب NSA الانتباه على مجموعة فعالة أصغر بكثير تجمع بين هذه المصادر الثلاثة. **من خلال دمج ضغط الرموز المميزة الهرمي مع اختيار الرموز المميزة blockwise**[3]، تقلل الآلية التعقيد التربيعي إلى تحجيم خطي تقريبًا أو شبه خطي.

## التحسين المحاذي للأجهزة

ابتكار حاسم لـ NSA هو تصميمه الواعي بالأجهزة. فشلت طرق الانتباه المتناثر السابقة غالبًا في تحقيق تسريع في العالم الحقيقي لأنها لم تكن محسنة لبنى GPU الحديثة.[1]

يحقق NSA تسريعًا كبيرًا من خلال:

### **نمط الوصول إلى الذاكرة Blockwise**
ينظم الخوارزمية البيانات في كتل تتماشى مع التسلسلات الهرمية لذاكرة GPU وعمليات Tensor Core. هذا يزيد من تحميل الذاكرة المجمعة إلى أقصى حد ويمكن من الاستخدام الفعال لوحدات حساب GPU.[3]

### **موازنة الشدة الحسابية**
تم تصميم الخوارزمية للحفاظ على شدة حسابية عالية - نسبة الحساب إلى الوصول إلى الذاكرة. هذا يضمن أن تظل وحدات معالجة الرسومات مقيدة بالحساب بدلاً من الذاكرة، مما يزيد من استخدام الأجهزة إلى أقصى حد.

### **تنفيذ Kernel المدمج**
يجمع NSA عمليات متعددة في نواة مدمجة واحدة، مما يلغي عمليات نقل KV cache الزائدة وتجسيد الموتر الوسيط.[5] هذا يقلل بشكل كبير من متطلبات عرض النطاق الترددي للذاكرة.

### **جدولة الحلقة المحسنة**
يؤدي التحسين على مستوى kernel إلى إزالة عمليات الذاكرة الزائدة وزيادة إعادة استخدام السجل إلى أقصى حد.

## مكاسب الأداء

التحسينات في الكفاءة كبيرة:[7]

- **أسرع حتى 9.0× في الحساب الأمامي** مقارنةً بـ FlashAttention-2 أثناء التدريب
- **أسرع 6.0× في المرحلة الخلفية**
- **تسريع 11.6× أثناء فك الترميز** لتسلسلات بطول 64k
- **يحافظ على أداء الانتباه الكامل أو يتجاوزه** عبر المقاييس

التسريع مذهل بشكل خاص للتسلسلات الأطول. لتسلسل مكون من 64k رمزًا مميزًا، يحقق NSA أسرع بحوالي 11.6× أثناء فك الترميز لأنه يقوم بتحميل بيانات KV cache أقل بكثير من الذاكرة.[3]

## القابلية للتدريب الأصلية - تطور حاسم

على عكس العديد من طرق الانتباه المتناثر السابقة التي سارعت فقط من الاستدلال، **يمكن NSA التدريب من طرف إلى طرف، مما يقلل حساب التدريب المسبق دون التضحية بأداء النموذج**[1]. يتم تعلم نمط التبعثر أثناء التدريب بدلاً من أن يكون ثابتًا أو قائمًا على heuristic.

هذا يعني:
- يتعلم النموذج أي الرموز المميزة يضغط وأيها يختار
- تتدفق التدرجات من خلال قرارات الانتباه المتناثر
- تتكيف استراتيجيات الضغط والاختيار مع المهمة وتوزيع البيانات المحدد

هذه القابلية للتدريب الأصلية crucial لأنها تسمح للنموذج باكتشاف أنماط التبعثر المثلى بدلاً من الاعتماد على القواعد المصممة يدويًا.

## المزايا مقارنة بالانتباه التقليدي

**الكفاءة الحسابية**: يقلل التعقيد التربيعي إلى شبه خطي، مما يمكن المعالجة العملية للسياقات ذات 100k+ رمز مميز.

**كفاءة الذاكرة**: يقلل بشكل كبير من متطلبات ذاكرة KV cache أثناء كل من التدريب والاستدلال.

**الحفاظ على الأداء**: تظهر النتائج التجريبية أن النماذج المدربة بـ NSA تطابق أو تتجاوز نماذج الانتباه الكامل عبر المقاييس العامة، ومهام السياق الطويل، والاستدلال القائم على التعليمات.[3]

**تسريع الأجهزة**: على عكس بعض الطرق المتناثرة التي تظهر مكاسب نظرية ولكن تحسين محدود في العالم الحقيقي، يقدم NSA تسريعًا كبيرًا مقاسًا على أجهزة GPU الفعلية.

**التبعثر التكيفي**: أنماط الانتباه المتعلمة تتكيف مع متطلبات المهمة بدلاً من استخدام أنماط ثابتة.

## تفاصيل التنفيذ التقني

يستفيد التنفيذ من عدة تقنيات متطورة:

- **ضغط هرمي ديناميكي** يتكيف مع نسب الضغط بناءً على المحتوى
- **آليات تجميع بوابات** للدمج الذكي للرموز المميزة
- **اختيار الرموز المميزة القائم على النقاط** باستخدام مقاييس الأهمية المتعلمة
- **عمليات الذاكرة المحاذية للكتل** المحسنة للتسلسلات الهرمية لـ GPU cache
- **نواة مخصصة قائمة على Triton** التي تتفوق على التطبيقات القياسية[8]

## التطورات الحديثة

أعلنت DeepSeek مؤخرًا عن DeepSeek-V3.2-Exp، الذي ينفذ نسخة متقدمة تسمى DeepSeek Sparse Attention (DSA). يحقق هذا المتغير الأحدث انتباهًا متناثرًا على النطاق الناعم بأقل تأثير على جودة الإخراج، مما يعزز أداء السياق الطويل مع تقليل التكاليف الحسابية.[9]

## خاتمة

يمثل NSA تحولًا paradigmatic في تصميم آليات الانتباه من خلال تحسين الكفاءة الخوارزمية، واستخدام الأجهزة، وقابلية تدريب النموذج في وقت واحد. من خلال الجمع بين الضغط الهرمي، والاختيار الديناميكي، والنوافذ المنزلقة مع التنفيذ المحاذي للأجهزة، فإنه يجعل نمذجة السياقات الطويلة عملية وفعالة. تظهر الآلية أن التصميم المشترك الدقيق للخوارزميات وتحسين الأجهزة يمكن أن يوفر تحسينات كبيرة في كل من السرعة وكفاءة الذاكرة دون المساس بجودة النموذج.

الاستشهادات:
[1] https://arxiv.org/abs/2502.11089
[2] https://medium.com/data-science-in-your-pocket/deepseek-native-sparse-attention-advanced-attention-mechanism-for-llms-6ac68fc014ff
[3] https://arxiv.org/pdf/2502.11089
[4] https://shchegrikovich.substack.com/p/attention-vs-attention
[5] https://medium.com/@nanda.yugandhar/the-illustrated-guid-to-native-sparse-attention-b657b5e76bbc
[6] https://www.marktechpost.com/2025/02/18/deepseek-ai-introduces-nsa-a-hardware-aligned-and-natively-trainable-sparse-attention-mechanism-for-ultra-fast-long-context-training-and-inference/
[7] https://medium.com/foundation-models-deep-dive/deepseeks-nsa-for-efficient-attention-14b6f01486d5
[8] https://arxiv.org/html/2502.11089v1
[9] https://api-docs.deepseek.com/news/news250929