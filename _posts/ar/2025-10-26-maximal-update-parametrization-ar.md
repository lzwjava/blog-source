---
audio: false
generated: true
image: false
lang: ar
layout: post
model: x-ai/grok-4-fast
title: معاملية التحديث القصوى للشبكات العصبية
translated: true
type: note
---

### ما هو μP (التمثيل البارامتري للتحديث الأقصى)؟

μP، وهو اختصار لـ Maximal Update Parametrization (التمثيل البارامتري للتحديث الأقصى)، هو أسلوب لتمثيل بارامترات الشبكات العصبية طوره غريغ يانغ، عالم الرياضيات وباحث الذكاء الاصطناعي العامل الآن في xAI (وكان سابقًا في Microsoft Research). تم تقديم هذا الأسلوب في ورقته البحثية عام 2022 بعنوان "Tensor Programs V: Tuning Large Neural Networks via Zero-Shot Hyperparameter Transfer"، ويعيد هذا الأسلوب تمثيل أوزان نماذج التعلم العميق لجعلها أكثر قابلية للتوسع وكفاءة في التدريب.

#### الهدف
الهدف الأساسي من μP هو تحقيق استقرار ضبط المعاملات الفائقة (HP) عبر أحجام النماذج المختلفة. في الإعدادات التقليدية (مثل التمثيل البارامتري القياسي، أو SP)، تحتاج المعاملات الفائقة مثل معدلات التعلم إلى إعادة الضبط في كل مرة تقوم فيها بتوسيع نطاق نموذج ما — على سبيل المثال، من الملايين إلى المليارات من البارامترات — لأن التدرجات والتحديثات تصبح غير مستقرة (غالبًا ما تتغير بشكل ترباعي مع عرض أو عمق النموذج). يقوم μP بإصلاح هذه المشكلة عن طريق تحويل البارامترات بحيث يبقى "التحديث الأقصى" (أكبر خطوة تدرج ممكنة) ثابتًا بغض النظر عن المقياس. وهذا يمكن من تحقيق **μTransfer**، وهو سير عمل تقوم فيه بضبط المعاملات الفائقة على نموذج "وكيل" صغير جدًا وتطبيقها مباشرة على نموذج هدف ضخم دون أي تعديلات إضافية.

#### الفوائد الرئيسية
- **توفير هائل في التكلفة**: الضبط على النماذج الصغيرة يكون رخيصًا. على سبيل المثال، نقل المعاملات الفائقة من نموذج وكيل بحجم 13 مليون بارامتر تفوق على النتائج المنشورة لـ BERT-large (350 مليون بارامتر)، وكانت تكلفة الضبط الإجمالية تعادل تكلفة تشغيل تدريب مسبق واحد فقط لـ BERT-large. بالنسبة لـ GPT-3 (6.7 مليار بارامتر)، تفوق نقل المعاملات من نموذج وكيل بحجم 40 مليون بارامتر على النتائج الأساسية بتكلفة بلغت 7% فقط من تكلفة التدريب المسبق الكامل.
- **قابلية التوسع للنماذج الكبيرة**: يعمل بشكل جيد على هندسات مثل Transformers و ResNets، مما يجعله مثاليًا لتدريب الشبكات العصبية الهائلة (مثل تلك المستخدمة في xAI). يضمن هذا الأسلوب "القيم المثلى الثابتة مع المقياس"، مما يعني أن مشهد الخسارة لا يشوه بشكل غير متوقع مع نمو النماذج.
- **سهولة الاستخدام**: متاح كمكتبة لـ PyTorch (`pip install mup`)، وقد تم دمجه في خطوط أنتاج التدريب للنماذج الضخمة للذكاء الاصطناعي.

#### الحدس الرياضي السريع
في التمثيل البارامتري القياسي (SP)، يتباين نمو الأوزان بشكل غير مستقر، مما يؤدي إلى انفجار/اختفاء التدرجات. يعيد μP تمثيل الأوزان (على سبيل المثال، لطبقة خطية، عن طريق القياس بـ fan-in) لتوحيد مصفوفة معلومات فيشر، مما يضمن بقاء تدفقات التدرج متوازنة. هذا يحافظ على بقاء المعاملات الفائقة المثلى (مثل معدلات التعلم) ثابتة تقريبًا عبر المقاييس المختلفة، متجنبًا الحاجة إلى إعادة الضبط لكل حجم.

يُبنى μP على إطار عمل "برامج الموتر" الأوسع ليانغ لفهم حدود العرض/العمق اللانهائية في الشبكات العصبية، ولا يزال يتطور — حيث تؤكد الأعمال الحديثة (اعتبارًا من 2025) على دوره في تعلم السمات الأقصى بما يتجاوز مجرد نقل المعاملات الفائقة.

**المراجع**  
- [Tensor Programs V: Tuning Large Neural Networks via Zero-Shot Hyperparameter Transfer](https://arxiv.org/abs/2203.03466)  
- [The Practitioner's Guide to the Maximal Update Parameterization](https://www.cerebras.ai/blog/the-practitioners-guide-to-the-maximal-update-parameterization)  
- [How To Scale (Blog on μP and Scaling)](https://howtoscalenn.github.io/)  
- [Greg Yang's Professional Page](https://thegregyang.com/)