---
audio: false
generated: true
image: false
lang: ar
layout: post
model: x-ai/grok-4-fast
title: ثورة بيرت في معالجة اللغات الطبيعية
translated: true
type: note
---

### ما هو BERT؟
BERT (تمثيلات المُشفر ثنائي الاتجاه من المحولات) هو نموذج قائم على المحولات قدمته جوجل في 2018. أحدث ثورة في معالجة اللغة الطبيعية (NLP) من خلال تمكين الفهم الثنائي العميق للنص، مما أدى إلى أداء متطور في مهام متنوعة مثل الإجابة على الأسئلة، وتحليل المشاعر، والتعرف على الكيانات المسماة.

### الابتكارات الرئيسية
تكمن إنجازات BERT في استراتيجية التدريب المسبق، والبنية المعمارية، ونهج الضبط الدقيق. إليك تفصيل لذلك:

- **التدريب المسبق ثنائي الاتجاه**:
  على عكس النماذج السابقة مثل GPT (من اليسار إلى اليمين) أو ELMo (ثنائي الاتجاه السطحي)، يقرأ BERT النص في كلا الاتجاهين في وقت واحد. هذا يسمح له بالتقاط تمثيلات سياقية أكثر ثراءً من خلال النظر إلى السياق الكامل حول كل كلمة، وليس فقط اتجاه واحد.

- **نمذجة اللغة المقنعة (MLM)**:
  أثناء التدريب المسبق، يقوم BERT بإخفاء 15% من الكلمات في المدخلات بشكل عشوائي ويقوم بتدريب النموذج على توقعها بناءً على السياق المحيط. تشجع مهمة "ملء الفراغ" هذه النموذج على تعلم علاقات الكلمات الدقيقة والقواعد النحوية دون الاعتماد على التوليد التسلسلي.

- **توقع الجملة التالية (NSP)**:
  للتعامل مع الفهم على مستوى الجملة، يقوم BERT بالتدريب المسبق على أزواج من الجمل: 50% منها متتالية (إيجابية) و 50% عشوائية (سلبية). يتعلم النموذج التنبؤ بما إذا كانت الجملة الثانية تتبع الجملة الأولى منطقياً، مما يحسن اكتشاف الترابط لمهام مثل التلخيص أو المحادثة.

- **بنية المُشفر فقط من المحولات**:
  يستخدم BERT فقط كومة المُشفر من المحول الأصلي (بدون وحدة فك التشفير)، مكدسة في 12 أو 24 طبقة اعتمادًا على المتغير (BERT-base أو BERT-large). هذا يجعله فعالاً لمهام التشفير ويستفيد من آليات الانتباه الذاتي للمعالجة المتوازية للتبعيات طويلة المدى.

- **نقل المعرفة عبر الضبط الدقيق**:
  بعد التدريب المسبق على نصوص غير موسومة هائلة (مثل BooksCorpus وويكيبيديا الإنجليزية)، يتم ضبط BERT بدقة على المهام اللاحقة الموسومة عن طريق إضافة طبقة بسيطة محددة للمهمة. يقلل هذا النهج "المناسب لمعظم الأغراض" بشكل كبير من الحاجة إلى التدريب المحدد للمهمة من الصفر، مما يجعله متعدد الاستخدامات وفعالاً من حيث البيانات.

غيرت هذه الابتكارات مجال معالجة اللغة الطبيعية من النماذج المخصصة للمهام إلى التضمينات المدربة مسبقًا ذات الأغراض العامة، مما مهد الطريق لنماذج مثل RoBERTa و T5. يظهر تأثير BERT جليًا في اعتماده عبر الصناعات لكل شيء من محركات البحث إلى روبوتات الدردشة.

[ورقة BERT العلمية](https://arxiv.org/abs/1810.04805)  
[مستودع BERT على GitHub](https://github.com/google-research/bert)