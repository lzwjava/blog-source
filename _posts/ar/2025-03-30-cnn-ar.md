---
audio: false
generated: true
lang: ar
layout: post
title: الشبكة العصبية التلافيفية
translated: true
type: note
---

### ما هي الشبكة العصبية الالتفافية (CNN)؟

**الشبكة العصبية الالتفافية (CNN)** هي نوع من الشبكات العصبية العميقة تُستخدم بشكل أساسي للمهام المتعلقة بالصور مثل التصنيف، واكتشاف الكائنات، والتجزئة. تم تصميم شبكات CNN لتتعلم تلقائيًا وبشكل تكيفي التسلسلات الهرمية المكانية للميزات من صور الإدخال. فهي فعالة للغاية في التقاط التبعيات المحلية وتقليل عدد المعلمات مقارنة بالطبقات المتصلة بالكامل التقليدية في الشبكات العصبية.

### المكونات الرئيسية لشبكة CNN:
1. **الطبقة الالتفافية**:  
   تطبق هذه الطبقة عمليات الالتفاف على بيانات الإدخال، مما يساعد في استخراج الميزات من صورة الإدخال (مثل الحواف، والقوام، والأنماط). تستخدم عملية الالتفاف مرشحات (تسمى أيضًا النواة) تنزلق فوق صورة الإدخال.

2. **طبقة التجميع**:  
   تُستخدم طبقات التجميع لخفض دقة خرائط الميزات، مما يقلل من أبعادها المكانية ويجعل الشبكة أكثر كفاءة حسابيًا مع المساعدة أيضًا في تحقيق عدم الحساسية للإزاحة (القدرة على التعرف على الكائنات حتى لو تم نقلها في الصورة).

3. **الطبقة المتصلة بالكامل**:  
   بعد طبقات الالتفاف والتجميع، تُستخدم الطبقات المتصلة بالكامل لتصنيف الميزات المستخرجة من الطبقات السابقة. تستخدم طبقة الإخراج النهائية عادةً دالة تنشيط softmax أو sigmoid لمهام التصنيف.

4. **دالة التنشيط (ReLU)**:  
   بعد كل طبقة التفاف أو طبقة متصلة بالكامل، غالبًا ما تُستخدم دالة تنشيط مثل **ReLU** لإدخال عدم الخطية في النموذج، مما يسمح له بتعلم أنماط أكثر تعقيدًا.

### مثال على بنية شبكة CNN:
- **طبقة الإدخال**: صورة أو دفعة من الصور.
- **الطبقة الالتفافية 1**: تطبيق مجموعة من مرشحات الالتفاف (النواة).
- **تنشيط ReLU**: تطبيق ReLU لإدخال عدم الخطية.
- **طبقة التجميع 1**: التجميع الأقصى أو التجميع المتوسط.
- **الطبقة الالتفافية 2**: تطبيق عمليات التفاف إضافية.
- **الطبقة المتصلة بالكامل**: تسطيح الإخراج وتغذيته إلى الطبقات المتصلة بالكامل للتصنيف.
- **طبقة الإخراج**: تنشيط softmax أو sigmoid لنتيجة التصنيف النهائية.

---

### تنفيذ شبكة CNN من الصفر (بدون استخدام أطر عمل مثل TensorFlow/PyTorch)

إليك تنفيذًا بسيطًا لشبكة CNN باستخدام **NumPy**. سيعطيك هذا فكرة أساسية عن كيفية عمل العمليات (الالتفاف، ReLU، التجميع، إلخ) في شبكة CNN.

سننفذ شبكة CNN أساسية تحتوي على:
1. طبقة التفاف
2. طبقة تنشيط ReLU
3. طبقة تجميع
4. طبقة متصلة بالكامل

سنركز على نسخة مبسطة جدًا من شبكة CNN، حيث لا نمتلك ميزات متقدمة مثل تسوية الدفعات، الإسقاط، إلخ.

### الخطوة 1: طبقة الالتفاف

سننفذ عملية **الالتفاف**، التي تتضمن انزلاق مرشح (نواة) فوق صورة الإدخال.

```python
import numpy as np

def convolve2d(input_image, kernel):
    kernel_height, kernel_width = kernel.shape
    image_height, image_width = input_image.shape
    
    # أبعاد الإخراج بعد الالتفاف
    output_height = image_height - kernel_height + 1
    output_width = image_width - kernel_width + 1
    
    output = np.zeros((output_height, output_width))
    
    # انزلاق النواة فوق صورة الإدخال
    for i in range(output_height):
        for j in range(output_width):
            region = input_image[i:i+kernel_height, j:j+kernel_width]
            output[i, j] = np.sum(region * kernel)  # ضرب عنصر بعنصر وجمع النتائج
    return output
```

### الخطوة 2: تنشيط ReLU

يتم تطبيق ReLU بشكل عنصري على إخراج الالتفاف.

```python
def relu(input_image):
    return np.maximum(0, input_image)  # عملية ReLU
```

### الخطوة 3: طبقة التجميع (التجميع الأقصى)

سننفذ طبقة **التجميع الأقصى** البسيطة بنافذة 2x2 وخطوة 2.

```python
def max_pooling(input_image, pool_size=2, stride=2):
    image_height, image_width = input_image.shape
    output_height = (image_height - pool_size) // stride + 1
    output_width = (image_width - pool_size) // stride + 1
    
    output = np.zeros((output_height, output_width))
    
    # تطبيق التجميع الأقصى
    for i in range(0, image_height - pool_size + 1, stride):
        for j in range(0, image_width - pool_size + 1, stride):
            region = input_image[i:i+pool_size, j:j+pool_size]
            output[i // stride, j // stride] = np.max(region)
    
    return output
```

### الخطوة 4: الطبقة المتصلة بالكامل

الطبقة المتصلة بالكامل هي ببساطة طبقة كثيفة تأخذ إخراج الطبقات السابقة وتحسب المجموع الموزون.

```python
def fully_connected(input_image, weights, bias):
    # تسطيح صورة الإدخال (إذا كانت متعددة الأبعاد)
    flattened_input = input_image.flatten()
    
    # حساب إخراج الطبقة المتصلة بالكامل
    output = np.dot(flattened_input, weights) + bias
    return output
```

### الخطوة 5: تجميع كل شيء معًا

الآن، دعنا نحدد مثالًا بسيطًا حيث سننشئ شبكة CNN تأخذ صورة، وتطبق الالتفاف، وReLU، والتجميع، ثم طبقة متصلة بالكامل لإجراء التنبؤ.

```python
# صورة مثال (5x5)
image = np.array([[1, 2, 3, 0, 1],
                  [4, 5, 6, 1, 2],
                  [7, 8, 9, 2, 3],
                  [1, 2, 3, 4, 5],
                  [6, 7, 8, 5, 6]])

# تعريف نواة بسيطة (3x3)
kernel = np.array([[1, 0, -1],
                   [1, 0, -1],
                   [1, 0, -1]])

# عملية الالتفاف
conv_output = convolve2d(image, kernel)
print("إخراج الالتفاف:")
print(conv_output)

# تطبيق تنشيط ReLU
relu_output = relu(conv_output)
print("إخراج ReLU:")
print(relu_output)

# تطبيق التجميع الأقصى
pool_output = max_pooling(relu_output)
print("إخراج التجميع الأقصى:")
print(pool_output)

# الطبقة المتصلة بالكامل (إخراج مسطح، أوزان وانزياح أحادي الأبعاد)
weights = np.random.randn(pool_output.size)  # أوزان عشوائية
bias = np.random.randn()  # انزياح عشوائي
fc_output = fully_connected(pool_output, weights, bias)
print("إخراج الطبقة المتصلة بالكامل:")
print(fc_output)
```

### شرح العملية:

1. **صورة الإدخال**: ننشئ صورة بسيطة 5x5.
2. **الالتفاف**: نطبق مرشح 3x3 على الصورة، مما ينتج عنه إخراج أصغر.
3. **ReLU**: نطبق دالة ReLU لإدخال عدم الخطية.
4. **التجميع**: نخفض دقة الصورة باستخدام التجميع الأقصى بنافذة 2x2 وخطوة 2.
5. **المتصلة بالكامل**: نقوم بتسطيح إخراج التجميع ونطبق طبقة متصلة بالكامل بأوزان وانزياح عشوائيين.

هذه شبكة CNN مبسطة جدًا، ولكن يجب أن تعطيك فكرة عن كيفية عمل طبقات الالتفاف، وReLU، والتجميع، والمتصلة بالكامل.

### الخلاصة

يوفر هذا التنفيذ إطارًا أساسيًا لفهم كيفية عمل شبكة CNN على مستوى منخفض. في الممارسة العملية، تقوم أطر العمل مثل **TensorFlow**، و**Keras**، و**PyTorch** بالتعامل مع العديد من التحسينات، وتحسينات الكفاءة، والميزات الإضافية (مثل تسوية الدفعات، الإسقاط) التي تجعل شبكات CNN أكثر قوة وسهولة في التدريب على مجموعات البيانات الكبيرة.

أعلمني إذا كنت تريد مزيدًا من التفاصيل حول كيفية توسيع هذا أو إذا كنت بحاجة إلى مساعدة في جزء آخر من شبكات CNN!