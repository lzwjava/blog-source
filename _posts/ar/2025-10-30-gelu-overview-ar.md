---
audio: false
generated: true
image: false
lang: ar
layout: post
model: x-ai/grok-4-fast
title: نظرة عامة على دالة التنشيط جيلو
translated: true
type: note
---

### ما هو GELU؟

وحدة الخطأ الغوسي الخطية (GELU) هي دالة تنشيط سلسة تُستخدم في الشبكات العصبية، وخاصة في النماذج القائمة على المحولات (transformer) مثل سلسلة BERT و GPT. تم تقديمها في عام 2016 من قبل الباحثين دان هيندريكس وكيفن جيمبل، وهي تجمع بين فوائد التحويلات الخطية والترجيح الاحتمالي المستوحى من التوزيع الغوسي (الطبيعي). على عكس دوال التنشيط الأبسط مثل ReLU (التي تُخرج صفرًا للمدخلات السالبة)، تسمح GELU بتدفق تدرج صغير للقيم السالبة، مما يؤدي إلى أداء أفضل في الشبكات العميقة من خلال تقليل مشاكل مثل التدرجات المتلاشية وتحسين التقارب.

#### التعريف الرياضي
الصيغة الأساسية لـ GELU هي:

\\[
\text{GELU}(x) = x \cdot \Phi(x)
\\]

حيث \\(\Phi(x)\\) هي دالة التوزيع التراكمي (CDF) للتوزيع الطبيعي المعياري:

\\[
\Phi(x) = \frac{1}{2} \left[ 1 + \erf\left( \frac{x}{\sqrt{2}} \right) \right]
\\]

هنا، \\(\erf\\) هي دالة الخطأ، التي تقيس مدى جودة تقريب الغوسي لدالة الخطوة. هذا يجعل GELU احتمالية: فهي "تسقط" المدخلات السالبة بشكل أقل حدة من ReLU مع الحفاظ على المدخلات الموجبة سليمة.

للكفاءة الحسابية، غالبًا ما تُستخدم التقديرات التقريبية، مثل النسخة القائمة على السيجمويد:

\\[
\text{GELU}(x) \approx x \cdot \sigma(1.702x)
\\]

حيث \\(\sigma\\) هي دالة السيجمويد.

#### لماذا نستخدم GELU؟
- **السلاسة**: يمكن اشتقاقها في كل مكان، مما يساعد في التحسين.
- **عدم الرتابة**: تقمع القيم السالبة قليلاً ولكن لا تلغيها، مما يحافظ على المعلومات.
- **الفوائد العملية**: تتفوق على ReLU و ELU في مهام مثل معالجة اللغة الطبيعية، مع تدريب أسرع ودقة أعلى في النماذج الكبيرة.
- **العيوب**: تكلفتها الحسابية أعلى قليلاً من ReLU بسبب حساب دالة الخطأ أو السيجمويد.

عمليًا، تحتوي مكتبات مثل PyTorch و TensorFlow على تطبيقات مدمجة لـ GELU (مثل `torch.nn.GELU()`).

#### المراجع
- [Gaussian Error Linear Units (GELUs)](https://arxiv.org/abs/1606.08415)
- [GELU Explained](https://www.baeldung.com/cs/gelu-activation-function)
- [GELU Activation](https://medium.com/@shauryagoel/gelu-gaussian-error-linear-unit-4ec59fb2e47c)