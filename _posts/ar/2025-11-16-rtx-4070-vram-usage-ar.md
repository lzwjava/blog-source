---
audio: false
generated: true
image: false
lang: ar
layout: post
model: x-ai/grok-4-fast
title: استخدام ذاكرة RTX 4070 في الواقع الافتراضي
translated: true
type: note
---

### لماذا استخدام ذاكرة بحوالي 6.5 جيجابايت؟
تمتلك RTX 4070 ذاكرة VRAM سعة 12 جيجابايت (12288 ميجابايت، وتظهر كـ 12282 ميجابايت على الأرجح بسبب التقريب/المساحة المحجوزة). تستخدم عملية التدريب الخاصة بك 6582 ميجابايت (~6.4 جيجابايت) إجمالاً، مع احتساب عملية Python لـ 4464 ميجابايت (~4.4 جيجابايت)؛ يشمل الفرق النفقات العامة لسياق CUDA، والنواة المؤقتة، وذاكرة التخزين المؤقت من torch.compile (كما ورد في السجل: "compiling the model..."). هذا رقم مرتفع لنموذج صغير بحجم 30 مليون معامل لأن **ذاكرة GPU أثناء تدريب المحولات (transformer) يهيمن عليها "التنشيطات" (الحسابات الوسيطة في المرورتين الأمامية والخلفية)، وليس فقط المعاملات**. المعاملات وحدها ستستخدم فقط ~0.5 جيجابايت، لكن التنشيطات تتزايد بشكل حاد مع batch_size، وblock_size، وn_embd، وn_layer في إعداداتك. يحتفظ PyTorch's autograd بمخرجات المرور الأمامي في الذاكرة من أجل الانتشار الخلفي (لا يوجد gradient checkpointing في nanoGPT الافتراضي)، وميزات مثل AMP (mixed precision)، وAdamW المدمج، وتجميع النموذج تضيف نفقات عامة.

الأسباب الرئيسية لهذا المستوى من الاستخدام:
- **التنشيطات تهيمن (4-5 جيجابايت هنا)**: كل مرور أمامي خلال طبقات المحولات يولد موترات وسيطة كبيرة (مثل، إسقاطات query/key/value في الاهتمام (attention)، وحالات الخفية للطبقات التغذوية الأمامية). تضاعف المرور الخلفي هذا عن طريق تخصيص مؤقتات التدرج (gradient). كما أن للاهتمام (attention) ذاكرة بقاعدة O(batch_size × num_heads × block_size²) لمصفوفات النقاط (على سبيل المثال، ~50 ميجابايت لكل طبقة قبل التحرير)، على الرغم من أن تنفيذ nanoGPT يعيد استخدام المخازن المؤقتة حيثما أمكن.
- **لا توجد تحسينات للذاكرة**: nanoGPT الافتراضي يعتمد على التخزين الكامل للتنشيطات بدون checkpointing (الذي يُقايض الحساب بالذاكرة عن طريق إعادة حساب المرور الأمامي أثناء الخلفي). يقوم Torch.compile بدمج العمليات ولكن يمكن أن يزيد التخصيص الذروي أثناء التقاط الرسم البياني وتنفيذه.
- **النفقات العامة للدقة المختلطة**: النموذج/التدرجات (grads) بـ FP16 (2 بايت/معامل)، لكن حالات مُحسّن AdamW بـ FP32 (8 بايت لكل من momentum/variance، ~2× المعاملات). دفعات الإدخال (رموز FP16) صغيرة (~16 كيلوبايت)، لكن المؤقتات ليست كذلك.
- **عوامل وقت التشغيل**: تراكم التدرج (gradient accumulation) (steps=4) يعالج batch_size=16 لكل خطوة لكنه لا يضاعف الذاكرة (تتراكم التدرجات في مكانها)؛ ومع ذلك، فإن مراحل التقييم (eval_iters=200) تسبب ارتفاعًا مؤقتًا في الاستخدام. يُظهر سجلك تدريبًا مستقرًا عند iter 1300، لذا هذا هو الخط الأساسي.

باختصار، إنه "مرتفع جدًا" مقارنة بحجم النموذج لأن النماذج الصغيرة مثل هذا لا تزال تتحمل النفقات العامة الكاملة للمحولات لكل رمز (token)، وإعداداتك (batch=16, block=512) تعالج ~8 آلاف رمز لكل خطوة — وهو ما يكفي لملء ذاكرة VRAM بشكل كبير دون تحسين عدواني.

### كيفية تقدير ~6.5 جيجابايت من الإعدادات (Config)
لا يمكنك التنبؤ *بدقة* بدون إجراء التحليل (على سبيل المثال، عبر `torch.utils.bottleneck` أو NVIDIA Nsight)، لأن ذلك يعتمد على إصدار PyTorch، وCUDA، وتفاصيل التنفيذ الدقيقة. ولكن يمكنك التقريب باستخدام الصيغ القياسية لذاكرة تدريب المحولات. تقسم هذه الصيغ ذاكرة VRAM إلى مكونات: المعاملات/المُحسّن (~10-20% من الإجمالي)، التنشيطات (~70-80%)، والنفقات العامة (~10%). جميع الحسابات أدناه تفترض التدريب بـ FP16 (dtype='float16' من GradScaler في السجل) مع AdamW.

#### 1. **ذاكرة المعاملات (سهل التقدير: ~0.06 جيجابايت)**
   - الصيغة: num_params × bytes_per_param (النموذج بـ FP16).
   - من السجل: 29.94M معامل.
   - FP16: 29.94M × 2 بايت = 59.88 ميجابايت (~0.06 جيجابايت).
   - كيفية حساب المعاملات من الإعدادات (صيغة nanoGPT): ≈ 12 × n_layer × n_embd² (كتل المحولات) + n_embd × vocab_size (التضمين + رأس LM).
     - 12 × 6 × 384² = 12 × 6 × 147,456 ≈ 10.6M
     - 384 × 50,304 ≈ 19.3M
     - الإجمالي: ~29.9M (يطابق السجل؛ تم تجاهل الإضافات الصغيرة مثل الانحيازات/تطبيع الطبقات).

#### 2. **ذاكرة التدرجات + المُحسّن (~0.3–0.6 جيجابايت)**
   - التدرجات: نفس المعاملات (FP16): ~0.06 جيجابايت إضافية.
   - المُحسّن (AdamW المدمج، يؤكده السجل): حالتان (momentum, variance) لكل معامل متحلّل، عادةً FP32.
     - المعاملات المتحللة: 30.13M (السجل: 26 موتر، 30,130,176 معامل).
     - الصيغة: decayed_params × 2 × 4 بايت (FP32) = 30.13M × 8 ≈ 241 ميجابايت.
     - غير المتحللة (الانحيازات/تطبيع الطبقات): صغيرة، ~5K معامل، مهملة.
   - الإجمالي الأساسي: المعاملات + التدرجات + المُحسّن ≈ (2 + 8) بايت/معامل = 10 بايت/معامل × 30M ≈ 300 ميجابايت.
     - النطاق: 12–20 بايت/معامل إذا شمل ذلك الأوزان الرئيسية FP32 أو إضافات (شائع في الدقة المختلطة).
   - من الإعدادات: يتغير بشكل مباشر مع n_layer، n_embd (أكبر = المزيد من المعاملات). أحجامك الصغيرة تبقي هذا منخفضًا.

#### 3. **ذاكرة التنشيطات (الأصعب/الأكثر تعقيدًا: ~4–5 جيجابايت)**
   - هذا هو الجزء الأكبر ويختلف حسب التنفيذ. قاعدة حسابها O(batch_size × block_size × n_embd × n_layer) للأجزاء الخطية، بالإضافة إلى O(batch_size × n_head × block_size²) لنقاط الاهتمام (attention scores).
   - **الصيغة الأساسية** (من مقدرات تدريب المحولات):
     ```
     activations_bytes ≈ batch_size × block_size × n_embd × n_layer × multiplier × 2 (بايت FP16)
     ```
     - المضاعف: تجريبي 16–34 للمرور الأمامي (التضمين + مخازن مؤقتة للاهتمام/الطبقات التغذوية الأمامية لكل طبقة) + المرور الخلفي (2–3× الأمامي). القيمة الشائعة: 24 (12 للأمامي، 12 للخلفي؛ يحسب ~4–6 موتر/طبقة مثل Q/K/V/out في الاهتمام، up/down في الطبقات التغذوية الأمامية ببعد وسطي 4×).
     - إعداداتك: batch_size=16, block_size=512, n_embd=384, n_layer=6.
     - الأساس: 16 × 512 × 384 × 6 = 18.87M "عنصر".
     - × 24 × 2 بايت = 18.87M × 48 ≈ 906 ميجابايت (أقل من التقدير).
   - **الارتفاع المحدد للاهتمام** (O(seq²)، مهم عند block_size=512):
     - لكل طبقة: batch_size × n_head × block_size² × 2 بايت (لمصفوفة نقاط QK^T).
     - 16 × 6 × 512 × 512 × 2 ≈ 50.3 ميجابايت/طبقة.
     - × n_layer=6، لكن بشكل تسلسلي (ليس كلها في once): ~50–100 ميجابايت ذروة لكل طبقة أثناء المرور الأمامي، بالإضافة إلى المؤقتات في المرور الخلفي. الإجمالي يضيف ~0.3–0.5 جيجابايت عبر المرورات.
   - **الإجمالي التجريبي المعدّل لإعداداتك**: الصيغة الأساسية تقلل التقدير بمقدار 4–5× بسبب المؤقتات في PyTorch (مثل، مخازن GEMM في الاهتمام/الطبقات التغذوية الأمامية، لا يتم تحريرها حتى نهاية المرور الخلفي) وطبقات nanoGPT القائمة على الحلقات التي تخزن جميع مخرجات المرور الأمامي (~ L × 4–6 × batch × seq × embd بايت). في العالم الحقيقي: ~ batch_size × block_size × n_embd × n_layer × 160 × 2 بايت ≈ 18.87M × 320 ≈ 6 جيجابايت (تم ضبطه ليطابق إجمالي 6.5 جيجابايت الخاص بك؛ يتوافق مع تقارير GPT الصغيرة المماثلة).
     - لماذا 160؟ يشمل المرور الخلفي الكامل (لا يوجد checkpointing)، الوسيط للطبقات التغذوية الأمامية (4× n_embd)، ذواكر التخزين المؤقت للمتبقيات/تطبيع الطبقات، و ~20–30% نفقات عامة لـ PyTorch لكل موتر.
   - من الإعدادات: يتغير خطيًا مع batch_size/block_size (إنتاجية الرموز)، تربيعيًا مع block_size (الاهتمام)، ومع n_embd/n_layer (العمق/العرض). قيمك معتدلة لكنها تتراكم: على سبيل المثال، تخفيض batch_size إلى النصف لـ 8 سيقطع التنشيطات ~50%، موفرًا ~2–3 جيجابايت.

#### 4. **النفقات العامة والمتنوعات (~1 جيجابايت)**
   - CUDA/PyTorch: السياق (~500 ميجابايت)، تشغيل النواة، تجزئة المُخصص.
   - Torch.compile: التقاط الرسم البياني + العمليات المدمجة تضيف 0.5–1 جيجابايت (يظهر السجل التجميع؛ يمكن تحليله بـ `torch._dynamo.config`).
   - البيانات: رموز الدفعة (مهملة)، لكن إذا كان التقييم يعمل، فإن eval_iters=200 يضيف دفعات مؤقتة.
   - الإجمالي: الأساسي (0.4 جيجابايت) + التنشيطات (4.5 جيجابايت) + النفقات العامة (1.5 جيجابايت) ≈ 6.4 جيجابايت.

#### التحقق مقابل إعدادات مماثلة
- NanoGPT/GPT-2 صغير (124M معامل، embd=768=2× خاصتك، layers=12=2×، batch~8–16، seq=1024=2×) يستخدم عادة 6–10 جيجابايت في التدريب بـ FP16. نموذجك بحجم ~1/4 المعاملات لكن بحجم رموز مماثل (16×512 مقابل 16×1024 /2 للقياس)، لذا ~النصف سيكون 3–5 جيجابايت أساسي — الزيادة الإضافية لديك تأتي من عدم وجود checkpointing والتجميع.
- أدوات للتنبؤ الأفضل:
  - تشغيل `python -m torch.utils.bottleneck` أثناء التدريب للتفصيل.
  - استخدام `estimate_memory()` من Hugging Face أو مكتبات مثل `modelscope` لمقدرات LLM (تعديلها لتناسب nanoGPT).
  - الحاسبات الآلية عبر الإنترنت (مثل، من نتائج البحث) أدخل معاملاتك/batch/seq لتقدير ~5–7 جيجابايت.

للتقليل إلى <4 جيجابايت: اضبط batch_size=8 (أو استخدم gradient_accumulation_steps=8 لنفس الدفعة الفعالة=64)، block_size=256، أو أضف gradient checkpointing (عدّل `model.forward` لاستخدام `torch.utils.checkpoint`). إعداداتك مناسبة بشكل مريح في 12 جيجابايت، لكن راقب باستخدام `nvidia-smi -l 1` للارتفاعات المفاجئة.