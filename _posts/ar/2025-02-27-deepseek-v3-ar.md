---
audio: false
generated: true
lang: ar
layout: post
title: ديب سيك V3
translated: true
type: note
---

### نظرة عامة وأبرز النقاط
1. اسم النموذج: DeepSeek-V3، نموذج لغة من نوع خليط الخبراء (MoE) يحتوي على 671 مليار معلمة، يتم تفعيل 37 مليار منها لكل رمز.
2. مجموعة بيانات التدريب: تم التدريب المسبق على 14.8 تريليون رمز متنوع وعالي الجودة.
3. الابتكارات الأساسية: يدمج الاهتمام الكامن متعدد الرؤوس (MLA) وهندسات DeepSeekMoE مع موازنة حمل خالية من الخسائر المساعدة للكفاءة.
4. كفاءة التدريب: يحقق التدريب الكامل باستخدام 2.788 مليون ساعة فقط من معالجات H800 GPU.
5. كفاءة التكلفة: تقدر تكلفة التدريب بـ 5.576 مليون دولار أمريكي، بافتراض 2 دولار لكل ساعة GPU.

---

### الابتكارات المعمارية
6. إطار العمل القائم على المحولات: يحتفظ بهندسة المحولات من أجل القابلية للتوسع والمرونة.
7. الاهتمام الكامن متعدد الرؤوس (MLA): يقلل ذاكرة الاستدلال عن طريق ضغط ذاكرة التخزين المؤقت للمفاتيح والقيم دون فقدان في الأداء.
8. DeepSeekMoE: يستخدم مزيجًا من الخبراء المشتركة والموجهة للتدريب فعال التكلفة والكفاءة الحسابية العالية.
9. موازنة الحمل الخالية من الخسائر المساعدة: تقدم مصطلحات انحياز للحفاظ على أحمال الخبراء المتوازنة دون المساس بالأداء.
10. التنبؤ متعدد الرموز (MTP): يتنبأ بتسلسل برموز متعددة لكل موقع، مما يحسن كفاءة البيانات والتخطيط المسبق للتمثيل.

---

### إطار العمل التدريبي
11. التدريب بدقة مختلطة FP8: يستخدم التكميم الدقيق والتخزين منخفض الدقة لتحسين الذاكرة والحساب.
12. خوارزمية DualPipe: تتداخل مراحل الحساب والاتصال، مما يقلل من فقاعات خط الأنابيب ويحسن التوازي.
13. اتصال فعال عبر العقد: يستخدم نواة محسنة لعمليات all-to-all، مستخدمًا عرض النطاق الترددي لـ NVLink وInfiniBand.
14. حالات المحسن منخفضة الدقة: يخزن حالات المحسن في BF16، مما يقلل استهلاك الذاكرة دون فقدان في الأداء.
15. تقنيات تحسين الذاكرة: يعيد حساب عمليات معينة (مثل RMSNorm) أثناء الانتشار الخلفي لتوفير الذاكرة.

---

### تفاصيل التدريب المسبق
16. عملية تدريب مستقرة: لم تحدث طفرات خسائر غير قابلة للاسترداد أو تراجعات أثناء التدريب المسبق.
17. تمديد طول السياق: تم تمديد طول السياق إلى 32 ألف ثم إلى 128 ألف في مرحلتين.
18. تكاليف التدريب: تطلب التدريب المسبق 2.664 مليون ساعة GPU، وتمديد السياق 119 ألف ساعة GPU، وبعد التدريب 5 آلاف ساعة GPU.
19. كفاءة الرمز: تم ضمان كفاءة التدريب عن طريق تقليل ساعات GPU لكل تريليون رمز.
20. بيانات عالية الجودة: تمت تنقية مجموعة بيانات التدريب المسبق للتنوع والملاءمة.

---

### التحسينات بعد التدريب
21. الضبط الدقيق المشرف (SFT): يمحص مخرجات النموذج مع تفضيلات البشر.
22. التعلم المعزز (RL): يستخدم تحسين السياسة النسبية الجماعية للضبط الدقيق.
23. تقطير المعرفة: يدمج قدرات التفكير من نماذج DeepSeek-R1.
24. التحكم في نمط الإخراج: يوازن بين الدقة وطول النص المنتج وأسلوبه.
25. صقل الأداء: يحسن التدريب اللاحق نتائج المعايير القياسية بشكل أكبر.

---

### أداء المعايير القياسية
26. MMLU (معايير تعليمية): يحقق 88.5، متفوقًا على نماذج المصدر المفتوح الأخرى.
27. GPQA (المعرفة العامة): يسجل 59.1، بمستوى可比 مع GPT-4o وClaude-3.5-Sonnet.
28. معايير الرياضيات: أداء رائد في مهام التفكير الرياضي.
29. مسابقات البرمجة: يتفوق في معايير البرمجة مثل LiveCodeBench.
30. المعرفة الواقعية: يظهر نتائج فائقة في معايير الواقعية الإنجليزية والصينية.

---

### الاستدلال والنشر
31. مرحلة الملء المسبق: تجمع بين التوازي الموتر (TP4)، والتوازي التسلسلي (SP)، والتوازي الخبير (EP32) للكفاءة.
32. مرحلة فك الترميز: تستخدم EP320 مع IBGDA للاتصال منخفض الكمون.
33. التكرار الديناميكي: يضبط أحمال الخبراء ديناميكيًا لتحسين استخدام الموارد.
34. فصل المراحل: يتم فصل مراحل الملء المسبق وفك الترميز لتعزيز الإنتاجية.
35. استخدام الأجهزة: محسن لمعالجات H800 GPU مع وصلات NVLink وInfiniBand.

---

### الابتكارات في موازنة الحمل وفك الترميز
36. التوجيه القائم على الانحياز: يقدم مصطلحات انحياز لضمان أحمال خبراء متوازنة ديناميكيًا.
37. فك الترميز التخميني: يعزز كمون التوليد باستخدام وحدات MTP.
38. الخبراء الزائدة عن الحاجة: تكرر الخبراء عالية الحمل لموازنة أحمال GPU.
39. التوجيه المحدود بالعقد: يقيد توجيه الرمز إلى 4 عقد كحد أقصى لتقليل عبء الاتصال.
40. عدم إسقاط الرموز: يضمن الاحتفاظ بجميع الرموز أثناء التدريب والاستدلال.

---

### التفاصيل التقنية
41. تكوين العنقود: تم التدريب على عنقود به 2048 من معالجات NVIDIA H800 GPU.
42. التوازي في خط الأنابيب: يستخدم مخطط توازي 16 اتجاهًا للتوسع.
43. البصمة الذاكرة: يتجنب التوازي الموتر المكلف عن طريق تحسين استخدام الذاكرة.
44. النواة المخصصة: يطور نواة اتصال متخصصة للتعامل مع العمليات عبر العقد بكفاءة.
45. تحسين الدقة المختلطة: يجمع بين تنسيقات FP8 وBF16 لديناميكيات تدريب مثالية.

---

### التقييم والنتائج
46. معايير شاملة: تم التقييم عبر مجالات متنوعة تشمل التعليم والبرمجة والتفكير.
47. ريادة المصدر المفتوح: يبرز كأقوى نموذج أساسي مفتوح المصدر في فئته.
48. المقارنة مع النماذج مغلقة المصدر: أداء可比 مع GPT-4o وClaude-3.5-Sonnet.
49. القوة في المعرفة الصينية: يتفوق على النماذج الرائدة في معايير الواقعية الصينية.
50. التعامل مع السياق الطويل: يتفوق في المهام التي تتطلب معالجة سياق ممتد.

---

### الاتجاهات المستقبلية
51. استكشاف التكرار الديناميكي: التحقيق في استراتيجيات تكرار أكثر تكيفًا.
52. توسيع فك الترميز التخميني: استكشاف استخدامات إضافية لـ MTP لتسريع الاستدلال.
53. التصميم المشترك للأجهزة: التكيف مع معالجات GPU من الجيل التالي لتحسين الأداء.
54. تغطية أوسع للمعايير: توسيع التقييمات لمهام أكثر تنوعًا.
55. الاستدامة: تقليل تكاليف التدريب بشكل أكبر من خلال التحسينات الخوارزمية والأجهزة.

---

يقدم هذا المستند ملخصًا شاملاً لـ DeepSeek-V3، متضمنًا هندسته ومنهجيات تدريبه وأداء معاييره القياسية وآفاقه المستقبلية. دعني أعلم إذا كنت بحاجة إلى مزيد من الشرح لأقسام محددة أو نقاط إضافية!