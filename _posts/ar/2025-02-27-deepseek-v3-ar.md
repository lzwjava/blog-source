---
audio: false
generated: true
lang: ar
layout: post
title: DeepSeek V3
translated: true
---

### نظرة عامة وميزات رئيسية
1. اسم النموذج: DeepSeek-V3، وهو نموذج لغة من نوع Mixture-of-Experts (MoE) يحتوي على 671 مليار معامل، منها 37 مليار يتم تنشيطها لكل رمز.
2. مجموعة بيانات التدريب: تم تدريبها مسبقًا على 14.8 تريليون رمز متنوع وممتاز.
3. الابتكارات الأساسية: تضم Multi-Head Latent Attention (MLA) و DeepSeekMoE مع توازن الحمل بدون خسارة مساعدة للفعالية.
4. فعالية التدريب: يحقق التدريب الكامل مع 2.788 مليون ساعة GPU H800 فقط.
5. فعالية التكلفة: تقدر تكلفة التدريب بـ 5.576 مليون دولار، مع افتراض 2 دولار لكل ساعة GPU.

---

### الابتكارات المعمارية
6. إطار Transformer-Based: يحتفظ بنظام Transformer للتمدد والتكيف.
7. Multi-Head Latent Attention (MLA): يقلل من الذاكرة أثناء الاستدلال عن طريق ضغط مخازن المفتاح والقيمة دون خسارة الأداء.
8. DeepSeekMoE: يستفيد من مجموعة من الخبراء المشترك والموجه لتحقيق تدريب فعال وتكلفة عالية.
9. توازن الحمل بدون خسارة مساعدة: يقدم مصطلحات التحيز للحفاظ على توازن الحمل الخبراء دون تضرر الأداء.
10. التنبؤ متعدد الرموز (MTP): يتنبأ بالرموز المتتالية بشكل متسلسل، مما يحسن كفاءة البيانات وتخطيط التمثيل.

---

### إطار التدريب
11. تدريب FP8 Mixed Precision: يستفيد من التكميم الدقيقة والتخزين منخفض الدقة لتحسين الذاكرة والحساب.
12. خوارزمية DualPipe: يتداخل مراحل الحوسبة والتواصل، مما يقلل من فقاعات الأنبوب ويحسن التوازي.
13. التواصل الفعال بين العقد: يستخدم نواة مخصصة لجميع العمليات من كل إلى كل، واستخدام عرض النطاق NVLink و InfiniBand.
14. حالات المحسنات منخفضة الدقة: تخزين حالات المحسنات في BF16، مما يقلل من استهلاك الذاكرة دون خسارة الأداء.
15. تقنيات تحسين الذاكرة: إعادة حساب بعض العمليات (مثل RMSNorm) أثناء التخلف للاحتفاظ بالذاكرة.

---

### تفاصيل التدريب
16. عملية التدريب المستقرة: لم تحدث أي انهيارات أو استرجاعات أثناء التدريب.
17. توسيع طول السياق: تم توسيع طول السياق إلى 32K ثم إلى 128K في مرحلتين.
18. تكلفة التدريب: استغرق التدريب 2.664 مليون ساعة GPU، و 119K ساعة GPU لتوسيع السياق، و 5K ساعة GPU بعد التدريب.
19. كفاءة الرموز: تضمنت كفاءة التدريب عن طريق تقليل ساعات GPU لكل تريليون رمز.
20. بيانات عالية الجودة: تم اختيار مجموعة بيانات التدريب لتنوعها ودراستها.

---

### تحسينات بعد التدريب
21. تدريب دقيق مع إشراف (SFT): يوجه مخرجات النموذج مع تفضيلات الإنسان.
22. تعلم تقويتي (RL): يستخدم تحسين سياسات النسبية للمجموعة لتدقيق.
23. تكرير المعرفة: يدمج قدرات التفكير من نماذج DeepSeek-R1.
24. التحكم في نمط المخرجات: يوازن الدقة مع طول و نمط التوليد.
25. تحسين الأداء: يحسن النتائج المرجعية بعد التدريب.

---

### أداء المرجعية
26. MMLU (مراجعات التعليمية): يحقق 88.5، متفوقًا على نماذج المصدر المفتوح الأخرى.
27. GPQA (المعرفة العامة): يحقق 59.1، متقارب مع GPT-4o و Claude-3.5-Sonnet.
28. مراجعات الرياضيات: أداء قياسي في مهام التفكير الرياضي.
29. مسابقات البرمجة: يتفوق في مراجعات البرمجة مثل LiveCodeBench.
30. المعرفة الفعلية: يثبت نتائج أفضل في مراجعات الفعلية الإنجليزية والصينية.

---

### الاستدلال والتوزيع
31. مرحلة التعبئة: يجمع بين التوازي التنسوري (TP4) والتوازي التسلسلي (SP) والتوازي الخبراء (EP32) للفعالية.
32. مرحلة التشفير: يستخدم EP320 مع IBGDA للاتصال منخفض التأخير.
33. التكرار الديناميكي: يعدل حمولات الخبراء بشكل ديناميكي لتحسين استغلال الموارد.
34. فصل المراحل: يتم فصل مراحل التعبئة والتشفير لتحسين الإنتاجية.
35. استغلال الأجهزة: مخصص لـ H800 GPUs مع NVLink و InfiniBand.

---

### الابتكارات في توازن الحمل والتشفير
36. توجيه التحيز: يقدم مصطلحات التحيز للحفاظ على توازن الحمل الخبراء بشكل ديناميكي.
37. التشفير التخميني: يحسن تأخير التوليد باستخدام وحدات MTP.
38. الخبراء المتكررون: يكرر الخبراء عالية الحمل لتوازن حمولة GPU.
39. توجيه محدود العقد: يقيد توجيه الرموز إلى 4 عقدة كحد أقصى لتقليل تكلفة الاتصال.
40. عدم إلقاء الرموز: يضمن الاحتفاظ بكل الرموز أثناء التدريب والاستدلال.

---

### التفاصيل الفنية
41. تكوين العنقود: تم تدريبها على عنقود يحتوي على 2048 GPU NVIDIA H800.
42. التوازي الأنبوبي: يستخدم نظام توازي 16-طريقة للتمدد.
43. بصمة الذاكرة: تجنب التوازي التنسوري الغالي عن طريق تحسين استخدام الذاكرة.
44. النواة المخصصة: تطوير نواة اتصال مخصصة لتسوية العمليات بين العقد بشكل فعال.
45. تحسين الدقة المختلطة: يجمع بين FP8 و BF16 لتسوية ديناميكية التدريب.

---

### التقييم والنتيجة
46. مراجعات شاملة: تم تقييمها عبر مجالات متنوعة بما في ذلك التعليم والبرمجة والتفكير.
47. القيادة المصدر المفتوح: يظهر كأقوى نموذج أساس مفتوح المصدر في فئةه.
48. المقارنة مع نماذج المصدر المغلقة: الأداء متقارب مع GPT-4o و Claude-3.5-Sonnet.
49. القوة في المعرفة الصينية: يتفوق على النماذج الرائدة في مراجعات الفعلية الصينية.
50. معالجة السياق الطويل: يتفوق في المهام التي تتطلب معالجة السياق المتطاولة.

---

### الاتجاهات المستقبلية
51. استكشاف التكرار الديناميكي: استكشاف استراتيجيات تكرار أكثر تطبيعًا.
52. توسيع التشفير التخميني: استكشاف استخدامات إضافية لـ MTP لتسريع الاستدلال.
53. تصميم الأجهزة المشترك: التكيف مع الجيل القادم من GPUs لتحسين الأداء.
54. تغطية مراجعات أوسع: توسيع التقييمات إلى مهام أكثر تنوعًا.
55. الاستدامة: تقليل تكلفة التدريب بشكل أكبر من خلال تحسينات خوارزمية و الأجهزة.