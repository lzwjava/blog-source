---
audio: false
generated: true
image: false
lang: ar
layout: post
model: x-ai/grok-4.1-fast
title: نظرة عامة على الذكاء المتجسد
translated: true
type: note
---

### الذكاء المتجسد: نظرة عامة

يشير الذكاء المتجسد، المعروف أيضًا باسم الذكاء الاصطناعي المتجسد، إلى أنظمة الذكاء الاصطناعي التي تتواجد ماديًا في العالم من خلال جسم (مثل الروبوت) وتتعلم وتفكر وتتصرف عبر التفاعل المباشر مع بيئتها. على عكس الذكاء الاصطناعي التقليدي، الذي يعالج بيانات مجردة بمعزل عن العالم (مثل نماذج اللغة مثل GPT)، يؤكد الذكاء الاصطناعي المتجسد على أن الذكاء الحقيقي ينبثق من حلقات حسية حركية: إدراك العالم، والتأثير عليه، والتكيف بناءً على التغذية الراجعة. يستمد هذا النموذج من علم الإدراك، حيث يُنظر إلى الإدراك على أنه متجذر في التجسيد المادي بدلاً من الحوسبة المجردة.

تشمل المبادئ الأساسية:
- **الإدراك متعدد الوسائط**: دمج الرؤية، واللمس، والحس العميق، وأحيانًا اللغة أو الصوت.
- **التعلم القائم على التفاعل**: تتحسن الوكلاء من خلال التجربة والخطأ في العالم الحقيقي أو محاكاة عالية الدقة (نقل المعرفة من المحاكاة إلى الواقع).
- **التعميم والتكيف**: التعامل مع بيئات غير منظمة وديناميكية بمهام طويلة الأمد، وتعدد الوسائط (مثل الجمع بين الرؤية واللغة)، والقدرة على مواجهة الاضطرابات.

اعتبارًا من عام 2025، انتشر الذكاء الاصطناعي المتجسد بشكل كبير بسبب النماذج الأساسية (نماذج اللغة والرؤية كبيرة الحجم المدربة مسبقًا)، وتقنيات الانتشار، ومجموعات البيانات الضخمة مثل Open X-Embodiment. إنه يقود التقدم في humanoid robots، والتلاعب، والملاحة، والتفاعل بين الإنسان والروبوت. لا تزال التحديات قائمة في الأداء في الوقت الفعلي، والسلامة، والفجوة بين المحاكاة والواقع، والتوسع لمهام العالم المفتوح. تشمل الجهود الرائدة سلسلة RT من Google، وOpenVLA، وpoliticas قائمة على الانتشار، بهدف الوصول إلى humanoid robots للأغراض العامة.

### التقنيات الأساسية: Diffusion Policy، وRT-2، وACT

تمثل هذه التقنيات الثلاث أحدث الأساليب لتعلم سياسات الروبوتات (العمليات التي تربط الملاحظات بالإجراءات) من خلال التعلم بالتقليد – التدريب على عروض توضيحية بشرية أو خبيرة بدون مكافآت صريحة.

#### ACT (Action Chunking with Transformer)
- **الأصل**: تم تقديمه في عام 2023 بواسطة Tony Zhao وآخرون (Covariant.ai، سابقًا من UC Berkeley) كجزء من نظام ALOHA للتلاعب ثنائي الأيدي منخفض التكلفة.
- **الفكرة الأساسية**: سياسة قائمة على المحولات (transformer) تتنبأ بقطع (chunks) من الإجراءات المستقبلية (مثل 100 خطوة في once) بدلاً من إجراء واحد في كل خطوة زمنية. يقلل هذا من الأخطاء الزمنية (تراكم الأخطاء على المدى الطويل) ويمكن من التحكم السلس عالي التردد (مثل 50Hz).
- **الهيكل**: يستخدم Variational Autoencoder (VAE) أو هيكل transformer. الإدخال: صور RGB متعددة المناظر + الحس العميق (حالات المفاصل). الإخراج: أوضاع/سرعات المفاصل المجزأة.
- **نقاط القوة**:
  - فعال للغاية من حيث العينات (يتعلم مهام معقدة من حوالي 50 عرضًا توضيحيًا).
  - قادر على العمل في الوقت الفعلي على أجهزة المستهلك العادية.
  - يتفوق في المهام الدقيقة والبارعة (مثل خياطة إبرة أو طي الملابس) باستخدام humanoid robots منخفضة التكلفة.
- **القيود**: يعتمد أساسًا على التقليد؛ يدعم بشكل أقل الأوامر اللغوية أو التعميم على نطاق الويب بدون امتدادات.
- **التأثير في العالم الحقيقي**: يشغل أنظمة مثل ALOHA (المتلاعبات المتنقلة) وتم اعتماده على نطاق واسع للمهام ثنائية الأيدي.

#### Diffusion Policy
- **الأصل**: ورقة بحثية عام 2023 بواسطة Cheng Chi وآخرون (Columbia University، Toyota Research Institute، MIT). تم توسيعه في أعمال مثل 3D Diffusion Policy وScaleDP (حتى 1B معامل في 2025).
- **الفكرة الأساسية**: تعامل إجراءات الروبوت كعينات توليدية من نموذج انتشار (مستوحى من مولّدات الصور مثل Stable Diffusion). ابدأ بإجراءات مشوشة، وقم بإزالة التشويش عنها بشكل متكرر بناءً على الملاحظات لإنتاج تسلسلات إجراءات عالية الجودة ومتعددة الوسائط.
- **الهيكل**: نموذج انتشار إزالة تشويش شرطي (غالبًا باستخدام المحولات). يتعلم "دالة النقاط" (انحدار توزيع الإجراء). يستخدم الاستدلال تحكمًا بأفق تراجعي: خطط لتسلسل، ونفذ الإجراء الأول، أعد التخطيط.
- **نقاط القوة**:
  - يتعامل مع السلوكيات **متعددة الوسائط** بشكل طبيعي (مثل وجود طرق متعددة صالحة للإمساك بجسم – يقوم النموذج بأخذ عينة واحدة بشكل متماسك دون حساب متوسط).
  - قوي ضد الإجراءات عالية الأبعاد والعروض التوضيحية المشوشة.
  - أحدث التقنيات في المعايير القياسية (تحسن بنسبة 46٪+ عن الأساليب السابقة في 2023؛ لا يزال منافسًا في 2025).
  - تستخدم الامتدادات مثل 3D Diffusion Policy سحب النقاط (point clouds) لفهم ثلاثي الأبعاد أفضل.
- **القيود**: استدلال أبطأ (10–100 خطوة إزالة تشويش)، على الرغم من أن التحسينات (مثل خطوات أقل، التقطير) تجعله قابلاً للتطبيق في الوقت الفعلي.
- **التأثير في العالم الحقيقي**: مستخدم على نطاق واسع للتلاعب الحركي البصري؛ مدمج في أنظمة مثل PoCo (تكوين السياسة) والنماذج الموسعة.

#### RT-2 (Robotics Transformer 2)
- **الأصل**: 2023 بواسطة Google DeepMind (بناءً على RT-1). جزء من عائلة Vision-Language-Action (VLA).
- **الفكرة الأساسية**: التدريب المشترك الدقيق لنموذج لغة ورؤية كبير مدرب مسبقًا (مثل PaLM-E أو PaLI-X، حتى 55B معامل) على مسارات الروبوتات. يتم تحويل الإجراءات إلى رموز (tokenized) كسلاسل نصية، مما يسمح للنموذج بإخراج الإجراءات مباشرة مع الاستفادة من معرفة نطاق الويب (الصور + النص).
- **الهيكل**: محول (Transformer) يأخذ صور + تعليمات لغة → إجراءات ممثلة برموز. مهارات ناشئة من التدريب المسبق على الويب (مثل التفكير في الرموز، سلسلة الفكر).
- **نقاط القوة**:
  - **التعميم الدلالي**: يفهم أوامر جديدة (مثل "التقط الحيوان المنقرض" → يلتقط لعبة ديناصور) بدون تدريب محدد للروبوت.
  - ينقل معرفة الويب إلى humanoid robotics (مثل التعرف على القمامة من صور الإنترنت).
  - أفضل حتى 3 مرات في المهارات الناشئة مقارنة بنماذج الروبوتات السابقة.
- **القيود**: النماذج الكبيرة → حوسبة أعلى؛ أقل دقة للتحكم البارع منخفض المستوى مقارنة بـ ACT/Diffusion (أفضل للتفكير عالي المستوى).
- **التأثير في العالم الحقيقي**: يشغل جمع بيانات أسطول الروبوتات في Google (AutoRT)؛ تطور إلى RT-X وتم دمجه مع أنظمة لاحقة.

### جدول المقارنة

| الجانب                  | ACT                                      | Diffusion Policy                          | RT-2                                      |
|-------------------------|------------------------------------------|-------------------------------------------|-------------------------------------------|
| **الطريقة الأساسية**    | Transformer + تجزئة الإجراء (حتمي/تراجعي) | انتشار إزالة التشويش (توليدي)            | VLA (إجراءات معالجة بالرموز في LLM/VLM)  |
| **الإدخال**             | صور متعددة المناظر + الحس العميق         | صور/سحب نقاط + الحس العميق               | صور + تعليمات لغة                        |
| **الإخراج**             | إجراءات مفصلية مجزأة                     | تسلسلات إجراءات مُزالة التشويش           | سلاسل إجراءات معالجة بالرموز             |
| **نقطة القوة الرئيسية** | كفاءة العينات، الدقة، الوقت الفعلي      | تعدد الوسائط، المتانة، التعبيرية         | التفكير الدلالي، التعميم من بيانات الويب |
| **سرعة الاستدلال**     | سريع (تمريرة واحدة)                      | أبطأ (إزالة تشويش متكررة)                | متوسط (محول تنبؤ تلقائي)                 |
| **كفاءة البيانات**     | عالية جدًا (~50 عرض توضيحي/مهمة)         | عالية                                     | متوسط (يستفيد من التدريب المسبق على الويب) |
| **الأفضل لـ**           | التلاعب البارع الدقيق                    | المهام المعقدة متعددة الوسائط            | المهام الموجهة باللغة، المهام الجديدة/الناشئة |
| **معدل النجاح النموذجي (مقارنة بالأساليب الأساسية)** | قوي في معايير الروبوت الحقيقي            | +46% (معايير 2023)                       | 2–3× في المهارات الناشئة                 |
| **حالة 2025**           | أساسي للأجهزة منخفضة التكلفة             | تم توسيعه إلى مليارات المعاملات         | أساس لخلفاء OpenVLA/RT-X                 |

### كيف تتلاءم مع الذكاء المتجسد

- **ACT وDiffusion Policy**: سياسات حركية بصرية خالصة – تتقن المهارات الحسية الحركية منخفضة المستوى، "الجسم" للذكاء الاصطناعي المتجسد. تمكن الروبوتات من التصرف بسلاسة في العالم المادي من البكسل الخام.
- **RT-2 (ونماذج VLA)**: جسر نحو الإدراك الأعلى – يضيف اللغة للتفاعل الشبيه بالإنسان والتفكير، دافعًا نحو وكلاء متجسدين للأغراض العامة.
- **الهجينة هي المستقبل**: توجهات 2025 تجمع بينهم (مثل OpenVLA = شبيه بـ RT-2 + رؤوس انتشار؛ OCTO = نماذج صغيرة فعالة مع عناصر انتشار). الهدف النهائي: humanoid robots تدرك بشكل متعدد الوسائط، وتفكر باستخدام LLMs/نماذج العالم، وتتصرف بدقة عبر الانتشار/التجزئة.

لقد جعلت هذه الأساليب humanoid robotics البارعة في متناول الجميع: المهام التي كانت تتطلب سنوات من الهندسة تنجح الآن بعشرات العروض التوضيحية البشرية. مع نمو مجموعات البيانات (مثل Open X-Embodiment) وتحسن الأجهزة (humanoid robots مثل Figure 01، Tesla Optimus)، يقترب الذكاء الاصطناعي المتجسد بسرعة من humanoid robots متعددة الاستخدامات والمفيدة في العالم الحقيقي.

[صفحة مشروع Diffusion Policy](https://diffusion-policy.cs.columbia.edu/)  
[إعلان RT-2](https://deepmind.google/discover/blog/rt-2-new-model-translates-vision-and-language-into-action/)  
[ورقة ALOHA / ACT](https://tonyzhaozh.github.io/aloha/)  
[مجموعة بيانات Open X-Embodiment](https://robotics-transformer-x.github.io/)