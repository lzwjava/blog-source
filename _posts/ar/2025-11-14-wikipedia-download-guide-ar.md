---
audio: false
generated: true
image: false
lang: ar
layout: post
model: x-ai/grok-4-fast
title: دليل تحميل ويكيبيديا الإنجليزية
translated: true
type: note
---

هذا عرض دليل من أحد مرايا التحميل الرسمية لويكيميديا (عادةً https://dumps.wikimedia.org/enwiki/latest/).  
ويحتوي على جميع الطرق المختلفة التي يمكنك من خلالها تحميل مقالات ويكيبيديا الإنجليزية الحالية (اعتبارًا من أوائل نوفمبر 2025).

### الصيغ الرئيسية وماذا تحتوي عليه فعليًا

| نمط اسم الملف | ما هو | الحجم التقريبي (غير مضغوط) | الأفضل لتدريب نماذج اللغة الكبيرة؟ | ملاحظات |
| --- | --- | --- | --- | --- |
| `enwiki-latest-pages-articles.xml.bz2` | ملف واحد ضخم يحتوي على **جميع** المقالات + صفحات النقاش، القوالب، التوجيهات، إلخ. | ~85-90 جيجابايت غير مضغوط | نعم، شائع الاستخدام جدًا | الأسهل إذا كان لديك مساحة وعرض نطاق |
| `enwiki-latest-pages-articles1.xml-p1p41242.bz2`  … حتى … `enwiki-latest-pages-articles27.xml-…` | نفس البيانات، ولكن مقسمة إلى 27 جزءًا أصغر (multistream) | كل منها ~200-600 ميجابايت مضغوط → الإجمالي لا يزال ~85-90 جيجابايت غير مضغوط | نعم، الخيار الأكثر شيوعًا | يسمح بالتحميل المتوازي واستئناف التحميل بسهولة |
| `enwiki-latest-pages-articles-multistreamX.xml.bz2` (مثل multistream27) | ملفات البيانات المضغوطة الضخمة الفعلية التي تنتمي إلى النسخة المقسمة أعلاه | 300-600 ميجابايت لكل منها مضغوط | هذه هي ملفات البيانات الحقيقية التي تريدها | تحتاج إلى هذه + ملفات الفهرس |
| `enwiki-latest-pages-articles-multistreamX.xml.bz2.md5` / `.meta` | ملفات التحقق من السلامة وبيانات وصفية صغيرة | < 1 كيلوبايت | غير مطلوبة للنص | فقط للتحقق من عمليات التحميل |
| `enwiki-latest-pages-articles-multistream-indexX.xml.bz2` | ملفات الفهرس التي تخبرك بأي مقال موجود في أي إزاحة بايت في ملفات multistream الكبيرة | ~30-60 ميجابايت لكل منها مضغوط | مطلوبة إذا كنت تستخدم multistream | ضرورية للوصول العشوائي السريع؛ معظم نصوص المعالجة تتوقعها |

### التوصية: ما يستخدمه الجميع تقريبًا لتدريب النماذج في 2025

الطريقة القياسية الفعلية (المستخدمة من قبل LLaMA, Mistral, Qwen, DeepSeek, Gemma, Phi, إلخ) هي:

قم بتحميل **27 ملف multistream + 27 ملف فهرس خاص بها**  
(أي كل شيء يبدأ بـ `enwiki-latest-pages-articlesXX.xml-p…p….bz2` ومطابق `…-indexXX…bz2`)

أو حتى أبسط: قم بتحميل **الملف الضخم المفرد** فقط إذا كان بإمكان اتصالك وقرصك التعامل معه:

```
enwiki-latest-pages-articles.xml.bz2    ← 24.6 جيجابايت مضغوط، ~85-90 جيجابايت غير مضغوط
```

كلاهما يحتوي على نفس بيانات XML الأولية بالضبط (جميع مقالات النطاق الرئيسي + الكثير من الصفحات الإضافية).

### لماذا لا الملفات الصغيرة جدًا (multistream3.xml-p…, multistream4.xml-p… إلخ)؟
تلك هي أقسام قديمة جدًا من 2010-2015 تحتوي فقط على جزء صغير جدًا من ويكيبيديا (المقالات التي وقعت ضمن نطاق معرف الصفحة ذلك الوقت). يتم الاحتفاظ بها فقط لأسباب تاريخية وهي بلا فائدة تقريبًا اليوم.

### أوامر البدء السريع (إفريز 2025)

التحميل المتوازي للملفات الـ 54 الموصى بها (27+27) (الأسرع):

```bash
# احصل على قائمة الملفات
wget -q https://dumps.wikimedia.org/enwiki/latest/ -O - | \
  grep -E 'enwiki-latest-pages-articles[0-9]+[02468]\.xml-p[0-9]+p[0-9]+\.bz2"' | \
  sed -e 's/.*href="//' -e 's/".*//' | \
  xargs -n1 -P10 wget https://dumps.wikimedia.org/enwiki/latest/

# قم أيضًا بتحميل ملفات الفهرس المطابقة
wget -q https://dumps.wikimedia.org/enwiki/latest/ -O - | \
  grep -E 'enwiki-latest-pages-articles-multistream-index' | \
  sed -e 's/.*href="//' -e 's/".*//' | \
  xargs -n1 -P10 wget https://dumps.wikimedia.org/enwiki/latest/
```

أو فقط الملف المفرد (الأبسط):

```bash
wget https://dumps.wikimedia.org/enwiki/latest/enwiki-latest-pages-articles.xml.bz2
```

بعد ذلك، تقوم عادةً بمعالجته بأدوات مثل:
- wikiextractor (Python) → لاستخراج النص النظيف
- محمل Hugging Face `datasets` لويكيبيديا (يفعل كل شيء لك بالفعل)
- نصوص المعالجة من DeepSeek أو EleutherAI، إلخ.

باختصار:  
قم بتحميل إما الملف المفرد `enwiki-latest-pages-articles.xml.bz2` بحجم 24 جيجابايت، أو الملفات الـ 27 المقسمة + فهارسها. كل شيء آخر في تلك القائمة هو إما ملفات تحقق من السلامة، أو بيانات قديمة غير مجدية، أو بيانات وصفية لا تحتاجها للتدريب.