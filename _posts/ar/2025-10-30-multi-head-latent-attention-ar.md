---
audio: false
generated: true
image: false
lang: ar
layout: post
model: x-ai/grok-4-fast
title: كفاءة الانتباه الكامن متعدد الرؤوس موضحة
translated: true
type: note
---

### الانتباه الكامن متعدد الرؤوس (MLA)

الانتباه الكامن متعدد الرؤوس (MLA) هو آلية انتباه فعالة طورتها DeepSeek AI، تم تقديمها لأول مرة في نموذج DeepSeek-V2 وتم تحسينها في DeepSeek-V3. صممت لتقليل العبء الحسابي للانتباه متعدد الرؤوس التقليدي في نماذج اللغة الكبيرة، خاصة أثناء الاستدلال (مثل توليد الردود)، من خلال ضغط ذاكرة التخزين المؤقت للمفاتيح والقيم (KV) إلى متجهات "كامنة" منخفضة الأبعاد.

#### آلية العمل (مبسطة)
- في الانتباه القياسي، تكون الاستعلامات والمفاتيح والقيم عبارة عن موترات عالية الأبعاد تنمو مع طول التسلسل، مما يؤدي إلى تكاليف عالية في الذاكرة والحساب.
- يضغط MLA أزواج KV من رؤوس متعددة في فضاء كامن مشترك منخفض الرتبة (مثل البعد \\(d_c \ll d_{\text{model}}\\)).
- أثناء التمريرات الأمامية:
  1. إسقاط الاستعلامات ومدخلات KV إلى الفضاء الكامن باستخدام طبقات خطية.
  2. حساب درجات الانتباه مباشرة على هذه الكيانات المضغوطة.
  3. فك ضغط المخرجات فقط إلى الأبعاد الكاملة.
- هذا يتجنب تخزين أو معالجة ذواكر KV الكاملة، مما يمكن استدلالًا أسرع ومعالجة سياق أطول دون التضحية بكثير من الأداء.

#### الفوائد الرئيسية
- **الكفاءة**: تسريع في سرعة الاستدلال يصل إلى 2-3 أضعاف وتقليل استخدام الذاكرة مقارنة بالانتباه المجمع للاستعلامات (GQA).
- **القابلية للتوسع**: يدعم النماذج الضخمة (مثل 405B معامل في DeepSeek-V3) مع تكامل MoE (خليط الخبراء) لتفعيل متفرق.
- **المفاضلات**: انخفاض طفيف في الدقة في بعض المهام، ولكن يتم التخفيف منه بالتدريب على التمثيلات المضغوطة.

يبني MLA على أفكار مثل التكيف منخفض الرتبة (LoRA) لكنه يطبقها ديناميكيًا على الانتباه. إنه مفتوح المصدر ويمكن دمجه في أطر عمل مثل vLLM أو Hugging Face.

#### المراجع
- [التقرير التقني لـ DeepSeek-V3](https://arxiv.org/html/2412.19437v1)
- [شرح الانتباه الكامن متعدد الرؤوس](https://medium.com/data-science/deepseek-v3-explained-1-multi-head-latent-attention-ed6bee2a67c4)