---
audio: false
date: 2025-09-14
generated: true
image: false
lang: ar
layout: post
model: openai/gpt-oss-120b
title: شرح سرعة جروك بدون الشبكات العصبية
translated: true
type: note
---

## إجابة مختصرة

**لا – GROQ نفسه لا يشغل شبكة عصبية على نمط المحولات (لا يوجد انتباه "مفتاح-قيمة-استعلام"، لا حساب موتر-مصفوفة، لا استدلال على نمط GPT).**  
تأتي سرعته من حيل كلاسيكية لمحرك قواعد البيانات (تخطيط مخزن الأعمدة، التصفية مدفوعة بالفهرس، خطط الاستعلام المترجمة، التنفيذ المتدفق، العاملون المتوازيون، والكاش العدواني).

إذا كان مشروع Sanity *يستخدم* بالفعل خدمة قائمة على المحولات (مثل تضمينات البحث الدلالي، إكمالات على نمط OpenAI، أو "حقل ذكاء اصطناعي" مخصص)، فإن هذا الجزء من خط المعالجة **منفصل** عن محرك GROQ وسيكون له خصائص الكمون الخاصة بأي خدمة استدلال تعلم آلي أخرى. يمكن الجمع بين الاثنين، لكن الإحساس "الفوري" الذي تحصل عليه عند تشغيل استعلام GROQ عادي **ليس** بسبب أن محولًا هو من يقوم بالعمل الشاق.

أدناه شرح أعمق يوضح:

1. **ما هو GROQ في الواقع ولماذا هو سريع** (ملخص للإجابة السابقة، مقتصر على الأساسيات).  
2. **لماذا يختلف استدلال المحولات ويكون أبطأ بشكل عام** من الاستعلام النقي المعتمد على الفهرس.  
3. **متى وكيف *قد* ترى المحولات في سير عمل Sanity**، وما الحيل التي يستخدمها المزودون لتسريع هذا الجزء.  
4. **جدول مقارنة سريع** يوضح المقايضات النموذجية للكمون بين استعلامات GROQ النقية، والبحث الدلالي القائم على المحولات، والنهج "الهجينة".  

---

## 1. GROQ = لغة استعلام مُترجمة، مخزن أعمدة (لا شبكات عصبية)

| المكون | ما يفعله | لماذا هو سريع (مقارنة بالنموذج) |
|-----------|--------------|-----------------------------|
| **بحيرة المحتوى** (مخزن ثنائي التعبئة، موجه للأعمدة) | يخزن كل حقل في عموده المضغوط المرتب الخاص به. | يمكن تلبية عامل التصفية بمسح عمود صغير واحد؛ لا حاجة لإزالة تسلسل كائنات JSON كاملة. |
| **ترجمة الاستعلام** | يحلل سلسلة GROQ مرة واحدة، يبني شجرة تجريدية للتركيب، وينشئ خطة تنفيذ قابلة لإعادة الاستخدام. | يتم عمل التحليل المكلف مرة واحدة فقط؛ المكالمات اللاحقة تعيد استخدام الخطة فقط. |
| **التصفية والإسقاط المدفوعان للأسفل** | يقيم الشروط أثناء قراءة العمود، ويسحب فقط الأعمدة التي تطلبها. | يتم تقليل الإدخال/الإخراج إلى الحد الأدنى؛ المحرك لا يلمس البيانات التي لن تظهر في النتيجة. |
| **خط أنابيب متدفق** | المصدر → التصفية → التعيين → التقطيع → المُسلسل → استجابة HTTP. | تصل الصفوف الأولى إلى العميل بمجرد أن تصبح جاهزة، مما يعطي إحساسًا "فوريًا". |
| **عاملون متوازون، بلا خادم** | ينقسم الاستعلام عبر أجزاء عديدة ويعمل على العديد من نوى المعالج في وقت واحد. | تنتهي مجموعات النتائج الكبيرة في ≈ عشرات الميللي ثانية بدلاً من الثواني. |
| **طبقات الكاش** (كاش الخطة، شبكة CDN الطرفية، كاش الأجزاء) | يخزن الخطط المترجمة وأجزاء النتائج المستخدمة بشكل متكرر. | تتخطى الاستعلامات المتطابقة اللاحقة几乎所有 العمل تقريبًا. |

كل هذه عمليات **حتمية، موجهة للأعداد الصحيحة** تعمل على وحدة المعالجة المركزية (أو أحيانًا كود مُسرّع بـ SIMD). لا توجد **ضرب مصفوفات، انتشار عكسي، أو عمليات حسابية ثقيلة للفاصلة العائمة** متضمنة.

---

## 2. استدلال المحولات – لماذا هو أبطأ (حسب التصميم)

| الخطوة في خدمة نموذجية قائمة على المحولات | التكلفة النموذجية | السبب في كونها أبطأ من مسح الفهرس النقي |
|---------------------------------------------|--------------|-------------------------------------------|
| **التجزئة** (نص → معرفات الرمز المميز) | ~0.1 ميللي ثانية لكل 100 بايت | لا تزال رخيصة، لكنها تضيف عبئًا. |
| **البحث عن التضمين / توليد التضمين** (ضرب المصفوفات) | 0.3 – 2 ميللي ثانية لكل رمز على وحدة المعالجة المركزية؛ < 0.2 ميللي ثانية على وحدة معالجة الرسومات/TPU | يتطلب جبر خطي للفاصلة العائمة على مصفوفات الأوزان الكبيرة (غالبًا 12 – 96 طبقة). |
| **الانتباه الذاتي (مفتاح-قيمة-استعلام) لكل طبقة** | O(N²) لكل طول تسلسل الرموز (N) → ~1 – 5 ميللي ثانية للجمل القصيرة على وحدة معالجة الرسومات؛ أكثر بكثير للتسلسلات الأطول. | يجعل القياس التربيعي المدخلات الطويلة مكلفة. |
| **الشبكة الأمامية + تطبيع الطبقة** | ~0.5 ميللي ثانية إضافية لكل طبقة | المزيد من عمليات الفاصلة العائمة. |
| **فك الترميز (إذا كان يولد نصًا)** | 20 – 100 ميللي ثانية لكل رمز على وحدة معالجة الرسومات؛ غالبًا > 200 ميللي ثانية على وحدة المعالجة المركزية. | التوليد التلقائي الانحداري تسلسلي بطبيعته. |
| **كمون الشبكة (نقطة النهاية السحابية)** | 5 – 30 ميللي ثانية ذهابًا وإيابًا (حسب المزود) | يضاف إلى إجمالي الكمون. |

حتى **المحول المُحسّن للغاية والمُكمّـّـل** (مثل 8-بت أو 4-بت) الذي يعمل على وحدة معالجة رسومات حديثة يستغرق عادةً **عشرات الميلي ثوانٍ** لطلب تضمين واحد، **بالإضافة إلى وقت قفزة الشبكة**. هذا *أبطأ بمراحل* من مسح الفهرس النقي الذي يمكن إنجازه في بضع ميلي ثوانٍ على نفس الأجهزة.

### خلاصة الفيزياء

* **بحث الفهرس** → O(1)–O(log N) قراءات لبضعة كيلوبايت → < 5 ميللي ثانية على وحدة المعالجة المركزية النموذجية.  
* **استدلال المحولات** → O(L · D²) عمليات فاصلة عائمة (L = عدد الطبقات، D = الحجم المخفي) → 10-100 ميللي ثانية على وحدة معالجة الرسومات، > 100 ميللي ثانية على وحدة المعالجة المركزية.

لذلك عندما ترى ادعاءً بأن **"GROQ سريع"**، فهذا *ليس* لأن Sanity استبدلت رياضيات الانتباه بطريقة مختصرة سرية؛ بل لأن المشكلة التي يحلها (تصفية وإسقاط المحتوى المهيكل) *أكثر ملاءمة بكثير* لتقنيات قواعد البيانات الكلاسيكية.

---

## 3. عندما *تستخدم* المحولات مع Sanity – النمط "الهجين"

Sanity هو **نظام إدارة محتوى بلا رأس**، وليس منصة تعلم آلي. ومع ذلك، يشجع النظام البيئي على بعض الطرق الشائعة لإدخال الذكاء الاصطناعي في سير عمل المحتوى:

| حالة الاستخدام | كيف يتم توصيلها نموذجيًا | من أين يأتي الكمون |
|----------|-----------------------------|------------------------------|
| **البحث الدلالي** (مثل "العثور على مقالات حول *react hooks*") | 1️⃣ تصدير المستندات المرشحة → 2️⃣ توليد التضمينات (OpenAI، Cohere، إلخ) → 3️⃣ تخزين التضمينات في قاعدة بيانات متجهة (Pinecone، Weaviate، إلخ) → 4️⃣ في وقت الاستعلام: تضمين الاستعلام → 5️⃣ بحث التشابه المتجهي → 6️⃣ استخدام المعرفات الناتجة في عامل تصفية **GROQ** (`*_id in $ids`). | الجزء الثقيل هو الخطوات 2-5 (توليد التضمين + بحث التشابه المتجهي). بمجرد حصولك على المعرفات، الخطوة 6 هي استدعاء GROQ عادي و*فوري*. |
| **مساعدات توليد المحتوى** (ملء حقل تلقائيًا، صياغة نسخة) | الواجهة الأمامية ترسل مطالبة إلى نموذج لغوي كبير (OpenAI، Anthropic) → تستقبل النص المُولد → تعيد الكتابة إلى Sanity عبر واجهة برمجته. | يهيمن كمون استدلال النموذج اللغوي الكبير (عادة 200 ميللي ثانية-2 ثانية). عملية الكتابة اللاحقة هي طفرة مدفوعة بـ GROQ (سريعة). |
| **وضع العلامات / التصنيف التلقائي** | يشغل خطاف ويب عند إنشاء المستند → تستدعي دالة بلا خادم نموذج مصنف → تعيد كتابة العلامات. | وقت استدلال المصنف (غالبًا محول صغير) هو الاختناق؛ مسار الكتابة سريع. |
| **صورة إلى نص (توليد نص بديل)** | نفس النمط أعلاه، لكن النموذج يعالج بايتات الصورة. | المعالجة المسبقة للصورة + استدلال النموذج يهيمن على الكمون. |

**النقطة الأساسية:** *جميع* الخطوات الثقيلة في الذكاء الاصطناعي هي **خارج** محرك GROQ. بمجرد حصولك على البيانات المشتقة من الذكاء الاصطناعي (المعرفات، العلامات، النص المُولد)، تعود إلى GROQ للحصول على الجزء السريع المعتمد على الفهرس.

### كيف يجعل المزودون جزء الذكاء الاصطناعي "أسرع"

إذا كنت تحتاج حقًا إلى أن تكون خطوة الذكاء الاصطناعي منخفضة الكمون، يستخدم المزودون مزيجًا من الحيل الهندسية:

| الحيلة | التأثير على الكمون |
|-------|-------------------|
| **تكميم النموذج (int8/4-بت)** | يقلل عمليات الفاصلة العائمة → تسريع بمقدار 2-5× على نفس الأجهزة. |
| **التقديم بوحدة معالجة الرسومات/TPU مع تحسين حجم الدفعة = 1** | يزيل عبئ تطبيع الدفعة؛ يحافظ على دفء وحدة معالجة الرسومات. |
| **النواة المترجمة (TensorRT، ONNX Runtime، XLA)** | يزيل عبء مستوى Python، ويدمج العمليات. |
| **الاستدلال الطرفي (مثل Cloudflare Workers-AI، Cloudflare AI Compute)** | يقطع الذهاب والإياب في الشبكة إلى < 5 ميللي ثانية للنماذج الصغيرة. |
| **كاش للتضمينات الحديثة** | إذا كررت العديد من الاستعلامات نفس النص، يمكنك تقديم التضمين من مخزن مفتاح-قيمة سريع (Redis، Cloudflare KV). |
| **فهارس "الجار الأقرب التقريبي" الهجينة (ANN)** | قواعد البيانات المتجهة مثل Qdrant أو Pinecone تستخدم HNSW/IVF-PQ التي تجيب على استعلامات التشابه في < 1 ميللي ثانية لملايين المتجهات. |

حتى مع هذه الحيل، **لا تزال خطوة الذكاء الاصطناعي أبطأ بمرحلة واحدة** من بحث الفهرس النقي لـ GROQ. لهذا يبدو تدفق "البحث الدلالي + GROQ" النموذجي كالتالي:

```
العميل ──► تضمين الاستعلام (≈30 ميللي ثانية) ──► بحث التشابه في قاعدة البيانات المتجهة (≈5 ميللي ثانية)
          │
          └─► استقائمة قائمة المعرفات ──► عامل تصفية GROQ (≈2 ميللي ثانية) ──► النتائج النهائية
```

إجماليًا ≈ 40 ميللي ثانية – سريع بما يكفي للعديد من تجارب واجهة المستخدم، لكن **الاختناق واضح في خطوة التضمين**، وليس في محرك GROQ.

---

## 4. مقارنة سريعة للكمون (أرقام توضيحية)

| السيناريو | الكمون النموذجي (الوسيط) | ما يهيمن |
|----------|--------------------------|----------------|
| **عامل تصفية وإسقاط GROQ نقي** (`*[_type=="post" && publishedAt < now()]{title,slug}`) | 30-10 ميللي ثانية (بارد)، 12-5 ميللي ثانية (دافئ) | قراءات الأعمدة + كاش الخطة |
| **GROQ + بحث متجهي (دلالي)** (`*[_id in $ids]{title,slug}` بعد التضمين) | 60-35 ميللي ثانية إجمالاً | توليد التضمين + بحث قاعدة البيانات المتجهة |
| **بحث نصي كامل باستخدام نموذج لغوي كبير خارجي** (`openai embeddings → vector DB → GROQ`) | 150-80 ميللي ثانية | استدلال النموذج اللغوي الكبير + قفزة الشبكة |
| **حلال GraphQL الذي يجلب كل منشور من قاعدة بيانات علائقية** | 300-120 ميللي ثانية (غالبًا N+1) | جولات متعددة ذهابًا وإيابًا إلى قاعدة البيانات، لا يوجد مخزن أعمدة |
| **استعلام Elasticsearch مع تجميعات** | 120-50 ميللي ثانية (يعتمد على عدد القطع) | تسجيل Lucene + تجميع الشبكة |

*الأرقام تقريبية وتختلف حسب حجم مجموعة البيانات والأجهزة والتخزين المؤقت.*

---

## 5. ملخص سريع لسؤالك الدقيق

- **لا يستخدم GROQ محولات، أو انتباه مفتاح-قيمة-استعلام، أو أي حساب موتر-مصفوفة.**  
- تأتي سرعته من **تحسينات محرك قاعدة البيانات** (مخزن الأعمدة، التصفية مدفوعة الفهرس، الخطط المترجمة، التدفق، العاملون المتوازون، التخزين المؤقت).  
- **إذا أضفت محولًا إلى خط المعالجة** (مثل التضمينات للبحث الدلالي)، فسيكون هذا الجزء **أبطأ** من استعلام GROQ النقي؛ الكمون الإجمالي هو مجموع الاثنين.  
- ادعاء Sanity بـ "السرعة" **ينطبق فقط على جزء GROQ**. أي خطوة ثقيلة الذكاء الاصطناعي هي منفصلة وستكون لها خصائص الكمون الخاصة بالبنية التحتية الأساسية للنموذج والتقديم.  

---

### ما التالي؟

1. **حدد الجزء من سير العمل الخاص بك الذي يحتاج حقًا إلى محول** (بحث دلالي، توليد تلقائي، تصنيف).  
2. **اعزل هذه الخطوة**: شغلها على نقطة نهاية ذكاء اصطناعي مخصصة (OpenAI، Anthropic، نموذج مكمّل مستضاف ذاتيًا).  
3. **احتفظ بجانب GROQ نقيًا**: بعد حصولك على المعرفات/العلامات المشتقة من الذكاء الاصطناعي، احصل على البيانات النهائية باستعلام GROQ *عادي* – ستحتفظ بكمون أقل من 30 ميللي ثانية لجزء استرجاع البيانات.  
4. **قم بقياس** المرحلتين بشكل منفصل (استخدم `console.time` أو أداة تتبع). إذا كان كمون الذكاء الاصطناعي هو المهيمن، جرب التكميم، أو الاستدلال الطرفي، أو تخزين التضمينات مؤقتًا.  

من خلال فصل الاهتمامين، تحصل على أفضل ما في العالمين: **البصيرة المدعومة بالذكاء الاصطناعي** عندما تحتاجها، و**استرجاع المحتوى الفوري** لكل شيء آخر.