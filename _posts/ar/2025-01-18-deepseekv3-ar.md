---
audio: true
lang: ar
layout: post
title: 'DeepSeek V3: الانتباه الكامن متعدد الرؤوس والتنبؤ متعدد الرموز'
translated: true
---

تم استكشاف DeepSeek v3 هنا، مع الإشارة إلى الفيديو "Multi-Head Latent Attention and Multi-token Prediction in Deepseek v3" [https://youtu.be/jL49fLOJYNg?si=4uE2kfe-BlKC1ngO](https://youtu.be/jL49fLOJYNg?si=4uE2kfe-BlKC1ngO). تم استخدام Google Cloud Speech-to-Text لنسخ الفيديو بالإضافة إلى بعض الأكواد لمساعدة في تنظيم النص.

---

أ: مرحبًا بكم مرة أخرى في Deep tag. سنقوم اليوم بالغوص العميق في عالم نماذج اللغة الكبيرة. حسنًا، وبالتحديد DeepSeek V3.

ب: يبدو جيدًا. إنه نموذج يحتوي على 671 مليار معلمة، ويحقق ضجة كبيرة بسبب نهجه الفريد في الكفاءة والأداء، أليس كذلك؟

أ: وقد شاركت ورقة أكاديمية توضح بنيته.

ب: نعم.

أ: وكخبير في تعلم الآلة، أنت تتطلع إلى فهم كيف يحقق DeepSeek V3 أداءً عاليًا وتدريبًا اقتصاديًا في نفس الوقت.

ب: نعم، هذا صحيح.

أ: أوه، مرحبًا، كيف الحال؟

ج: MLA، التفاصيل، MLA وكيف تعمل.

أ: أوه، بالتأكيد. هذه فكرة رائعة. نعم، يمكننا بالتأكيد الغوص بشكل أعمق في الانتباه الكامن متعدد الرؤوس، أو MLA. إذن أنت فضولي بشأن التفاصيل الدقيقة لـ MLA. حسنًا، دعونا نفكك هذا. لقد ذكرنا أن أحد مفاتيح كفاءة DeepSeek V3 هو بنية "خليط الخبراء"، أو MoE، أليس كذلك؟ حيث يتم تنشيط جزء فقط من المعلمات لكل رمز. ويأخذنا DeepSeek V3 خطوة أبعد مع MLA و DeepSeek Mo.

ب: هذا صحيح. لذا دعونا نركز حقًا على MLA الآن.

أ: حسنًا. إذن في التطبيقات في الوقت الفعلي، السرعة أمر بالغ الأهمية.

ب: نعم. وذاكرة التخزين المؤقت للقيم الرئيسية المطلوبة أثناء الاستدلال يمكن أن تكون عقبة رئيسية.

أ: بالضبط. هذا هو المكان الذي يأتي فيه MLA. حسنًا، إذن آلية الانتباه التقليدية تتطلب تخزين الكثير من المعلومات حول الرموز السابقة.

ب: نعم، والتي كما يمكنك أن تتخيل، تصبح مشكلة مع تسلسلات النصوص الطويلة، أليس كذلك؟

أ: لكن MLA يقوم بضغط هذه المعلومات بذكاء، حسنًا، لتقليل تدفق الذاكرة المؤقتة بشكل كبير وجعل الاستدلال أسرع بكثير. لذا فهو يشبه أخذ موسوعة ضخمة وتكثيفها إلى النقاط الرئيسية فقط.

ب: إنها تشبيه رائع. إنه يحتفظ بالمعلومات الأساسية دون الوزن غير الضروري. نعم، لذا فهو مفيد حقًا للتطبيقات في الوقت الفعلي.

أ: نعم. الآن دعونا نتحدث عن كيفية عملها بالفعل. حسنًا، إذن كيف يحقق MLA هذا الضغط؟

ب: حسنًا، يستخدم ضغطًا مشتركًا منخفض الرتبة للمفاتيح والقيم في الانتباه.

أ: حسنًا، إذن هو يضغط المفاتيح والقيم، ولكن ماذا يعني ذلك بالضبط؟ لذا دعونا نتعمق قليلاً في الجانب التقني. حسنًا، تأخذ آلية MLA تمثيلًا مخفيًا للإدخال، ثم تقوم بإسقاطه في ناقلات الاستعلام، والمفتاح، والقيمة. حسنًا، الآن هنا حيث تصبح الأمور مثيرة للاهتمام. يقوم MLA بفصل الاستعلام إلى جزأين.

ب: حسنًا، جزأين؟

أ: نعم. جزء واحد يستخدم للمحتوى، والجزء الآخر يستخدم للمعلومات الموضعية باستخدام شيء يسمى Rope.

ب: Rope؟ هذا يبدو تقنيًا جدًا.

أ: إنه يعني "التضمينات الموضعية الدورانية"، ويساعد النموذج على فهم موقع الرموز في التسلسل. حسنًا، ثم يتم ضغط المفاتيح والقيم في فضاء كامن ذي أبعاد أقل. لذا فهو يشبه تقليص البيانات، مما يوفر في الذاكرة.

ب: بالضبط. لذا يتم حفظ المعلومات الأكثر أهمية، ولكن يتم التخلص من الحجم غير الضروري. نعم، وهذا التمثيل المضغوط يسمح بذاكرة تخزين مؤقت أصغر بكثير للقيم الرئيسية أثناء الاستدلال، مما يجعل الأمور أسرع.

أ: ويستخدم أيضًا معالجة متعددة الرؤوس.

ب: نعم، تمامًا مثل الانتباه التقليدي، يستخدم MLA رؤوسًا متعددة.

أ: أوه، تفضل.

ج: إذن هناك فضاءان كامنان ومدخل مخفي واحد.

أ: هذه ملاحظة رائعة. نعم، أنت محق. هناك بالفعل فضاءان كامنان. حسنًا، إذن نحن نتحدث عن فضاء كامن للمحتوى وفضاء كامن للقيم الرئيسية.

ب: بالضبط. ويتم معالجة هذه الفضاءات الكامنة من خلال ما نسميه Rope، أو التضمينات الموضعية الدورانية.

أ: حسنًا، إذن هذا Rope هو كيف يحصلون على المعلومات الموضعية.

ب: نعم، يتم تطبيقه على كل من الفضاءات الكامنة للمحتوى والقيم الرئيسية، كما أشرت. لذا فهو يأخذ هذا التمثيل المضغوط، يعالجه، ثم يجمع كل شيء معًا مرة أخرى.

أ: نعم، وتحسين التخزين المؤقت يقلل بشكل أكبر من الحمل الزائد أثناء المعالجة التسلسلية. حسنًا، إذن هذه هي الطريقة التي يجعل بها MLA الأمور أسرع.

ب: بالضبط. إنها طريقة ذكية لتحقيق انتباه فعال دون التضحية بالأداء.

أ: حسنًا، هذه خدعة رائعة. ولكن هل تعلم ماذا؟

ب: ماذا هناك؟

أ: دعنا ننتقل إلى DeepSeek Mo. كيف يختلف هذا عن نماذج MoE التقليدية؟

ب: حسنًا، DeepSeek Mo يستخدم... أوه، عد إلى مستمعنا، كيف الحال؟

ج: ونتحدث أكثر عن الفضاء المخفي. حسنًا، من الفضاء المخفي، ما هذا؟

أ: أنا بالتأكيد... دعنا نرى ما تعنيه. الفضاءات المخفية مثيرة للاهتمام حقًا. نعم، أنت تسأل عن الفضاء المخفي، الفضاء الكامن الذي كنا نتحدث عنه للتو، أليس كذلك؟ أنت فضولي بشأن ما يحدث داخل تلك الفضاءات الكامنة، ذلك الكهف. نعم، الأمر لا يتعلق فقط بعدد الفضاءات الكامنة، ولكن بما يحدث هناك.

ب: هذا رائع.

أ: بالضبط. هناك بالفعل فضاءان كامنان متميزان داخل MLA، واحد للمحتوى وواحد للقيم الرئيسية. إنه مثل وجود وحدتي تخزين منفصلتين للمعلومات. وهذه الفضاءات الكامنة، كما ناقشنا، تخضع لعمليات Rope، أليس كذلك؟ التضمينات الموضعية الدورانية، التي تضمن المعلومات الموضعية في آلية الانتباه. هذا مهم جدًا بالنسبة لهم. لذا لتلخيص الأمر، يتم فصل الاستعلام، ويتم أيضًا ضغط المفاتيح والقيم.

ب: نعم، ويتم وضعها في الفضاءين الكامنين المنفصلين، واحد للمحتوى وواحد لأزواج القيم الرئيسية. وهذه الفضاءات الكامنة مهمة حقًا للكفاءة وكل ذلك كجزء من MLA.

أ: بالضبط. الآن دعونا نتحدث عن هذه العمليات بتفصيل أكثر داخل الكهف، كما قلت. حسنًا، إذن كيف يقوم MLA فعليًا بإجراء تحويلات الفضاء الكامن هذه؟

ب: حسنًا، يخضع الإدخال لمعالجة متوازية لكل من تمثيلات المحتوى والقيم الرئيسية. حسنًا، إذن لديه مساران داخل ذلك الكهف.

أ: نعم، واحد لكل فضاء كامن. وداخل تلك الفضاءات، يتم معالجة المعلومات باستخدام Rope.

ب: هذا صحيح. هذا يضمن أن النموذج يحتفظ بالمعلومات الموضعية أثناء مروره عبر الكهف. لذا يعرف النموذج أي جزء من النص هو أي جزء أثناء وجوده داخل ذلك الكهف.

أ: بالضبط. ويتم هذه المعالجة قبل المرحلة التالية من التسلسل. حسنًا، ما الذي يتم تسلسله أثناء مروره عبر كهف الفضاء المخفي؟

ب: تقوم الآلية بعمليتي تسلسل رئيسيتين. يتم تسلسل تمثيلات الاستعلام، ويتم أيضًا تسلسل تمثيلات المفاتيح. لذا فهو مثل جمع كل القطع المهمة معًا داخل كهف الفضاء المخفي.

أ: نعم، وتساعد هذه التسلسلات في دمج المحتوى مع المعلومات الموضعية. ويتم بعد ذلك استخدام هذه التمثيلات المسلسلة لحساب الانتباه، أليس كذلك؟

ب: صحيح. وبسبب الضغط الأولي، يكون الأمر أسرع بكثير عبر ذلك الكهف الذي ذكرته. لذا يقلل MLA بشكل كبير من التكاليف الحسابية داخل وخارج ذلك الكهف المخفي.

أ: بالضبط. إنه يحسن آلية الانتباه للنماذج الكبيرة مثل DeepSeek V3. هذا سؤال رائع. الآن، بعد أن مررنا عبر الكهف، دعنا ننتقل إلى DeepSeek Mo.

ب: حسنًا، DeepSeek Mo. هذا صحيح. أرى ما تعنيه. نعم، هناك بالفعل فضاءان كامنان متميزان داخل MLA، واحد للمحتوى وواحد للقيم الرئيسية.

أ: بالضبط. وهذا الفصل هو المفتاح لكيفية عمله. إنه مثل وجود وحدتي تخزين منفصلتين للمعلومات. وهذه الفضاءات الكامنة، كما ناقشنا، تخضع لعمليات Rope، أليس كذلك؟ التضمينات الموضعية الدورانية، التي تضمن المعلومات الموضعية في آلية الانتباه. لذا لتلخيص الأمر، يتم فصل الاستعلام، ويتم أيضًا ضغط المفاتيح والقيم.

ب: نعم، ويتم وضعها في الفضاءين الكامنين المنفصلين، واحد للمحتوى وواحد لأزواج القيم الرئيسية. وهذه الفضاءات الكامنة مهمة حقًا للكفاءة وكل ذلك كجزء من MLA.

أ: بالضبط. الآن دعونا نتحدث عن هذه العمليات بتفصيل أكثر. حسنًا، إذن كيف يقوم MLA فعليًا بإجراء تحويلات الفضاء الكامن هذه؟

ب: حسنًا، يخضع الإدخال لمعالجة متوازية لكل من تمثيلات المحتوى والقيم الرئيسية. حسنًا، إذن لديه مساران.

أ: نعم، واحد لكل فضاء كامن. وداخل تلك الفضاءات، يتم معالجة المعلومات باستخدام Rope.

ب: هذا صحيح. هذا يضمن أن النموذج يحتفظ بالمعلومات الموضعية، أليس كذلك؟ ثم لتعزيز الكفاءة، يستخدم خبراء مشتركين. حسنًا، لذا خبراء يمكن استخدامهم عبر مهام متعددة.

أ: نعم، لذا يتجنب التكرار ويجعل النظام أكثر انسيابية.

ب: نعم، إنه مثل وجود فريق حيث لدى الأشخاص تخصصات ولكن يمكنهم أيضًا القيام بأشياء أخرى.

أ: نعم، هذا نهج ذكي جدًا. نعم، ولكن مع وجود العديد من الخبراء المتخصصين، كيف يتأكدون من عدم إرهاق أي منهم؟

ب: نعم، بينما يجلس الآخرون دون عمل.

أ: هذا هو المكان الذي يأتي فيه التوازن الديناميكي للحمل بدون خسارة مساعدة.

ب: هنا تصبح الأمور مثيرة للاهتمام حقًا، أليس كذلك؟ إذن كيف يفعلون ذلك؟

أ: نماذج MoE التقليدية تستخدم دالة خسارة مساعدة أثناء التدريب، حسنًا، لتشجيع استخدام الخبراء بشكل متساوٍ، ولكن هذا يمكن أن يضر بالأداء بالفعل.

ب: نعم، إنه مثل محاولة إجبار الجميع على استخدام نفس خط الخروج في متجر البقالة.

أ: بالضبط، حتى إذا كان بعضهم يتحرك أسرع من الآخرين، أليس كذلك؟ إنه يخلق تأخيرات غير ضرورية فقط.

ب: نعم. لذا يتجنب DeepSeek V3 ذلك عن طريق ضبط مصطلح انحياز ديناميكي، حسنًا، لكل خبير بناءً على حمله. حسنًا، لذا إذا كان خبير ما يتلقى الكثير من الطلبات، فإن النظام يجعله أقل جاذبية قليلاً لآلية التوجيه، مما يحول بعض الحركة إلى خبراء أقل انشغالاً.

أ: حسنًا، إذن يستخدم كل هذا للتعامل بكفاءة مع التسلسلات الطويلة، نعم، عن طريق تقليل حجم ذاكرة التخزين المؤقت للقيم الرئيسية المطلوبة للاستدلال. حسنًا، إذن الأمر كله يتعلق بالحفاظ على الأداء العالي مع تقليل الحمل الزائد.

ب: صحيح. إنه نهج ذكي جدًا لمعالجة عقبة حرجة.

أ: بالتأكيد. الآن، يجب أن نغطي أيضًا كيفية تعامل DeepSeek V3 مع توازن الحمل.

ب: نعم، بالتأكيد يجب أن نفعل ذلك. هذه أيضًا قطعة مهمة جدًا في اللغز. يمكننا التطرق إلى ذلك بعد ذلك.

أ: يبدو جيدًا. حسنًا، أعتقد أن هذا يمنحك نظرة عامة رائعة على MLA وفضائه الكامن.

ب: نعم، شكرًا على الغوص في كل التفاصيل معنا. سنعود في المرة القادمة مع غوص أعمق.

أ: نعم، إنه مثل نظام إدارة حركة المرور للخبراء، نعم، يراقب التدفق باستمرار ويقوم بالتعديلات لتجنب الاختناقات.

ب: وهذا يتجنب الضربة الأدائية للخسارة المساعدة.

أ: هذا صحيح. و أوه، تفضل.

ج: نعم، يمكننا التحدث عن MTP، كيف... كيف تشارك وحدات MTP تضمينها وكل الأشياء الساخنة...

أ: بالتأكيد. إنه سؤال رائع. نعم، دعونا نتحدث عن كيفية مشاركة وحدات MTP للموارد. إذن أنت فضولي بشأن التفاصيل الدقيقة لتطبيق MTP.

ب: نعم، دعونا نفكك هذا. لذا ذكرنا أن DeepSeek V3 يستخدم MTP للتنبؤ متعدد الرموز، أليس كذلك؟ التنبؤ بعدة رموز بدلاً من رمز واحد فقط.

أ: وهنا تصبح الأمور مثيرة للاهتمام حقًا. نعم، أنت مهتم بكيفية إعداد وحدات MTP وكيفية مشاركتها لمواردها. حسنًا، إذن تتضمن كل وحدة MTP طبقة تضمين مشتركة، نعم، ورأس إخراج مشترك. حسنًا، لذا يستخدمون نفس التضمين ورأس الإخراج مثل النموذج الرئيسي.

ب: بالضبط. لذا فهو مثل أنهم جميعًا يستمدون من نفس مجموعة المعرفة. نعم، وهذا يوفر في التكاليف الحسابية.

أ: نعم. الآن يستخدم كتلة تحويل خاصة به. حسنًا، لذا فهو لا يشارك نفس كتلة التحويل مثل النموذج الرئيسي.

ب: صحيح. كل وحدة MTP لديها كتلة تحويل خاصة بها للمعالجة. حسنًا، لذا هذه هي الطريقة التي يحافظون بها على تنبؤات مميزة لكل رمز.

أ: نعم، ولجمع المعلومات، هذه الإسقاطات الخطية والتسلسل...

ب: حسنًا، إذن إنه مثل أخذ قطع من أماكن متعددة لبناء الصورة الكاملة.

أ: نعم، وكل وحدات MTP تعمل معًا بشكل متوازٍ، لكنها تشارك طبقات التضمين ورؤوس الإخراج، أليس كذلك؟

ب: نعم، وهذا هو المفتاح لكفاءة هذا التصميم. حسنًا، لذا فهو مثل نظام من الأجزاء المترابطة التي تعتمد جميعها على بعضها البعض، أليس كذلك؟

أ: وهذا المشاركة الفعالة للموارد تسمح بتدريب أسرع وأداء أفضل.

ب: حسنًا، هذه خدعة رائعة. هل تعلم ماذا؟

أ: ماذا هناك؟

ب: دعنا ننتقل إلى نظرة عامة أكبر. كيف يتعامل هذا النموذج مع توازن الحمل؟ كيف يتم اختيار هؤلاء الخبراء؟

أ: نعم، يمكننا بالتأكيد التحدث عن ذلك. حسنًا، الآن دعونا نتعمق في استراتيجية توازن الحمل لـ DeepSeek V3.

ب: يبدو جيدًا. حسنًا، إذن يستخدم DeepSeek V3 ما يسمى بالتنبؤ متعدد الرموز.

ج: أوه نعم، نتحدث أكثر عن ذيول MTP.

أ: بالتأكيد... أنا سعيد لأنك مهتم بالغوص بشكل أعمق في MTP. نعم، يمكننا بالتأكيد التفصيل في التنبؤ متعدد الر