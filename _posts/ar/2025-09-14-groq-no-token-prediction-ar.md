---
audio: false
date: 2025-09-14
generated: true
image: false
lang: ar
layout: post
model: openai/gpt-oss-120b
title: لا تتوقع جروك الرموز المميزة
translated: true
type: note
---

## إجابة مختصرة

**GROQ نفسه لا "يتنبأ بالرمز التالي" مطلقًا.**  
إنها **لغة استعلام على نمط قواعد البيانات** تختار وتصفّي وتعيد تشكيل **مستندات JSON المنظمة** التي قمت بتخزينها في Sanity.  
القوة "الدلالية" التي تراها في المنتجات القائمة على Sanity تأتي من **شيئين منفصلين**:

| ما تتساءل عنه | كيف يتم ذلك فعليًا |
|--------------------------|--------------------------|
| **فهم معنى المحتوى الذي خزنته (مثال: "أرني جميع المنشورات حول React hooks")** | يستخدم GROQ **الفهارس ومسح مخزن الأعمدة والمُحَدِّدات المنطقية (Boolean predicates)** للعثور على المستندات الصحيحة. يتم ترميز "المعنى" في **الحقول الصريحة** التي تمتلكها (الوسوم، الفئات، المراجع، المؤلف، إلخ) – لا حاجة لشبكة عصبية. |
| **إنشاء نص جديد أو إكمال جملة ("اكتب ملخصًا لهذه المقالة")** | يجب أن تستدعي **نموذج لغة كبير (LLM)** – مثل OpenAI أو Anthropic أو Cohere أو نموذج محول (transformer) مستضاف ذاتيًا، إلخ. نموذج LLM هو من يقوم بحسابات المصفوفات الموترية (tensor-matrix) الثقيلة (الانتباه K-Q-V، softmax، إلخ). ثم يتم **كتابة النتيجة مرة أخرى** إلى Sanity (أو استخدامها كموجه prompt) عبر استدعاء API عادي. |

إذن الإجابة المختصرة هي: **سرعة Sanity تأتي من محرك استعلام كلاسيكي مُحَسَّن للغاية؛ أي "تنبؤ بالرمز التالي" أو فهم دلالي عميق يتم تنفيذه **خارج** نطاق GROQ، بواسطة خدمة خارجية قائمة على المحولات (transformers).**

أدناه شرح مفصل أكثر يوضح سبب وجود هذا الفصل، وكيف يتلاءم الجزءان معًا، وماذا يمكنك أن تفعل إذا كنت تحتاج *كلاً* من استرجاع المحتوى السريع **و** تنبؤات نموذج اللغة عالي الجودة.

---

## 1. ما الذي يفعله GROQ *فعليًا*

| الميزة | كيف يبدو للمطور | كيف يعمل تحت الغطاء (بدون موترات tensors) |
|---------|--------------------------------------|------------------------------------------|
| **التصفية** (`*[_type == "post" && publishedAt < now()]`) | "أعطني كل منشور تم نشره بالفعل." | يمسح **أعمدة** `_type` و `publishedAt` في "بحيرة المحتوى" (Content Lake). يستخدم فهرس B-tree/bitmap للقفز مباشرة إلى الصفوف المطابقة. |
| **الإسقاط/التخطيط** (`{title, slug, author->{name}}`) | "أرجع هذه الحقول فقط، وحل مرجع المؤلف." | يقرأ فقط أعمدة `title` و `slug` و `author._ref`، ثم يقوم **بدمج** من خلال البحث في مستند المؤلف المشار إليه (مرة أخرى، بطريقة الأعمدة). |
| **الترتيب والتقطيع** (`\|order(publishedAt desc)[0...10]`) | "أعطني أحدث 10 منشورات." | يستخدم العمود المُرتَّب `publishedAt` لإنتاج **دفق مُرتَّب مسبقًا**؛ يتوقف بعد 10 عناصر (لا حاجة لتحضير البقية). |
| **مطابقة النص الكامل** (`title match "react*"`) | "ابحث عن العناوين التي تبدأ بـ 'react'." | يستفيد من **فهرس نصي** (فهرس مقلوب) يعيش بجانب مخزن الأعمدة، مشابه لكيفية عمل Elasticsearch، ولكنه مدمج مباشرة في البحيرة. |
| **البث** | تبدأ النتائج في الوصول بعد أن تكون أولى الصفوف جاهزة. | المحرك يعمل بطريقة خط الأنابيب: المصدر → التصفية → التعيين → المُسلسل → استجابة HTTP، مرسلاً البايتات بمجرد إنتاجها. |

جميع هذه العمليات **حتمية، قائمة على الأعداد الصحيحة، ومحدودة بالإدخال/الإخراج (I/O‑bounded)** – لا تتطلب أبدًا ضرب المصفوفات أو حسابات التدرج (gradient). هذا هو السبب في أن استعلام GROQ الخالص ينتهي عادةً في **بضعة ميلي ثانية إلى بضع عشرات من الميلي ثانية**.

---

## 2. من أين تأتي القدرة "الدلالية" و"التنبؤ بالرمز التالي"

| حالة الاستخدام | أين يوجد نموذج LLM | التدفق النموذجي (مركزي Sanity) |
|----------|---------------------|------------------------------|
| **التلخيص** | `POST https://api.openai.com/v1/chat/completions` (أو أي نقطة نهاية LLM أخرى) | 1️⃣ استخدم GROQ لجلب نص المقالة. <br>2️⃣ أرسل هذا النص كموجه (prompt) إلى نموذج LLM. <br>3️⃣ استلم الملخص المُنشأ واكتبه مرة أخرى (`PATCH /documents/{id}`) عبر Sanity API. |
| **البحث الدلالي** | قاعدة بيانات المتجهات (Vector‑DB) (مثل Pinecone, Weaviate, Qdrant) + نموذج للتضمين (embeddings model) (مثل OpenAI `text‑embedding‑ada‑002`، إلخ) | 1️⃣ قم بتصدير المستندات المرشحة → قم بتضمينها مرة واحدة (بلا اتصال). <br>2️⃣ خزن المتجهات في قاعدة بيانات المتجهات. <br>3️⃣ في وقت الاستعلام: قم بتضمين استعلام المستخدم → ابحث عن أقرب الجيران → احصل على قائمة `_id` → **GROQ** `*[_id in $ids]{title,slug}` للحصول على الحمولة النهائية. |
| **وضع الوسوم/التصنيف التلقائي** | نموذج تصنيف صغير (يمكن أن يكون نموذج محول (transformer) مُدرب بدقة أو حتى انحدارًا لوجستيًا فوق المتجهات) | 1️⃣ يتم تشغيل خطاف ويب (webhook) عند إنشاء المستند. <br>2️⃣ تستدعي دالة Serverless المصنِّف → تستقبل الوسوم. <br>3️⃣ تقوم الدالة بتعديل المستند بالوسوم (تحويل GROQ سريع). |
| **مساعد دردشة يشير إلى محتواك** | نموذج LLM للحوار + GROQ لجلب السياق | 1️⃣ يسأل المستخدم "ماذا قلنا حول التخزين المؤقت؟" <br>2️⃣ يقوم الخلفي (Backend) بتشغيل استعلام GROQ يسحب جميع الأقسام ذات الصلة. <br>3️⃣ يتم إدخال هذه الأقسام في الموجه (prompt) المرسل إلى نموذج LLM. <br>4️⃣ يعيد نموذج LLM استجابة؛ يمكن عرض الاستجابة أو تخزينها. |

**النقطة الأساسية:** العمل الشاق *الدلالي* (التضمينات، الانتباه، تنبؤ الرمز) يتم تنفيذه **بواسطة خدمة نموذج LLM**، وليس بواسطة GROQ. وظيفة GROQ في تلك الخطوط هي ببساطة **جلب القطع الصحيحة من البيانات المنظمة** (أو تخزين الناتج).

---

## 3. لماذا يكون الفصل منطقيًا (الأداء والهيكلية)

| السبب | الشرح |
|--------|--------------|
| **التخصص** | مخزن المحتوى مُحَسَّن لمطابقات *دقيقة*، ومسح النطاقات، والإسقاط السريع. النموذج المحول (transformer) مُحَسَّن لنمذجة اللغة *الاحتمالية*. محاولة جعل نظام واحد يقوم بالأمرين جيدًا تؤدي إلى حلول وسط. |
| **التحكم في التكلفة** | تشغيل نموذج محول (transformer) في كل طلب سيكون مكلفًا (دقائق وحدة معالجة الرسوميات GPU). بإبقاء GROQ رخيصًا، تدفع فقط مقابل جزء الذكاء الاصطناعي عندما تحتاجه فعليًا. |
| **القابلية للتوسع** | يمكن تخزين استعلامات GROQ مؤقتًا عند حواف شبكة CDN، وتقسيمها عبر العديد من العمال (workers)، وتقديمها بزمن انتقال أقل من 30 مللي ثانية. يمكن تفريغ استدلال (inference) نموذج LLM إلى مجموعة منفصلة قابلة للتوسع التلقائي قد تكون محددة بمنطقة. |
| **المرونة** | يمكنك تبديل مورد نموذج LLM (من OpenAI إلى Anthropic إلى مستضاف محليًا) دون تغيير أي كود في GROQ. تبقى لغة الاستعلام مستقرة. |
| **الأمان والامتثال** | يبقى المحتوى المنظم في مشروع Sanity الخاص بك؛ فقط المتجهات *المشتقة* أو الموجهات (prompts) تغادر النظام، مما يسهل تدقيق تدفقات البيانات. |

---

## 4. إذا كنت *ترغب* حقًا في تنبؤ على مستوى الرمز داخل Sanity، كيف سيبدو؟

1. **أنشئ دالة Serverless** (مثل Vercel أو Cloudflare Workers أو AWS Lambda) تستقبل طلبًا مثل `POST /api/generate`.  
2. داخل تلك الدالة:
   ```js
   // 1️⃣ اسحب السياق الذي تحتاجه
   const ctx = await sanityClient.fetch(`*[_id == $docId]{title, body}[0]`, {docId});

   // 2️⃣ ابني موجهًا (ربما مع أمثلة قليلة few‑shot)
   const prompt = `لخص هذه المقالة:\n\n${ctx.body}\n\nالملخص:`;

   // 3️⃣ استدع نموذج LLM (مثال OpenAI)
   const resp = await fetch('https://api.openai.com/v1/chat/completions', {
     method: 'POST',
     headers: { 'Authorization': `Bearer ${process.env.OPENAI_KEY}`, 'Content-Type': 'application/json' },
     body: JSON.stringify({
       model: 'gpt-4o-mini',
       messages: [{role: 'user', content: prompt}],
       temperature: 0.2
     })
   });
   const {content} = (await resp.json()).choices[0].message;

   // 4️⃣ خزن النتيجة مرة أخرى في Sanity (تحويل GROQ سريع)
   await sanityClient.patch(docId).set({summary: content}).commit();
   ```
3. **زمن الانتقال الذي تراه** في العميل هو مجموع:
   - رحلة ذهاب وإياب عبر الشبكة إلى نموذج LLM (غالبًا 30-120 مللي ثانية إذا كان نموذج LLM في موقع حافة سريع).
   - وقت استدلال (inference) نموذج LLM (يعتمد على حجم النموذج؛ النماذج الصغيرة < 50 مللي ثانية، نماذج كبيرة من فئة GPT‑4 > 300 مللي ثانية).
   - تحويل GROQ (≈ 5‑10 مللي ثانية).

إذا كنت تحتاج إلى زمن انتقال إجمالي أقل من 100 مللي ثانية لواجهة مستخدم *دردشة*، فعادةً ما **تشغل النموذج عند الحافة** (مثل Cloudflare Workers‑AI أو Lambda@Edge أو نموذد 8‑bit مستضاف محليًا) بحيث تصبح قفزة الشبكة صفرًا أساسًا. يبقى جزء GROQ ضئيلاً.

---

## 5. كيف تحصل على دلالات *جيدة* دون الدفع مقابل نموذج محول (transformer) كامل القوة في كل طلب

| التقنية | ما الذي تكتسبه | التكلفة التقريبية (اعتبارًا من 2025) |
|-----------|------------------|------------------------------|
| **متجهات مُحَسَّبة مسبقًا + قاعدة بيانات متجهات** | عمليات بحث سريعة عن "التشابه الدلالي" (< 5 مللي ثانية لكل استعلام). | تكلفة تضمين لمرة واحدة (≈ $0.0001 لكل 1 ألف رمز مع OpenAI)، بالإضافة إلى تخزين قاعدة بيانات المتجهات الرخيص. |
| **نموذج صغير مُدرب بدقة (مثل `distilbert-base-uncased` مُكمَّى إلى int8)** | تصنيف/وضع وسوم جيد على الجهاز، استدلال < 10 مللي ثانية على وحدة المعالجة المركزية الحديثة. | مجاني إذا قمت باستضافته بنفسك؛ وإلا $0.02‑$0.05 لكل 1 ألف رمز على الخدمات المدارة. |
| **هندسة الموجهات (Prompt‑engineering) + أمثلة قليلة (few‑shot)** | يحسن دقة نموذج LLM *الكبير* دون تدريب إضافي بدقة. | نفس تكلفة الرمز الخاصة بنموذج LLM، ولكن يمكنك إبقاء الموجهات قصيرة (≤ 200 رمز) للبقاء برخص. |
| **الجيل المعزز بالاسترجاع الهجين (Hybrid retrieval‑augmented generation RAG)** | اجمع بين المطابقة الكلمة المفتاحية الدقيقة (GROQ) مع نموذج LLM صغير للإجابة النهائية. | تستدعي نموذج LLM فقط على عدد *قليل* من المستندات المسترجعة، مما يقلل عدد الرموز بشكل كبير. |

**أفضل الممارسات:**  
- **استخدم GROQ أولاً** *لتصفية* مجموعة المرشحين (مثال: اجلب جميع المنشورات من نوع معين، أو جميع المستندات التي تحتوي على وسم محدد).  
- **إذا كنت تحتاج إلى ترتيب "دلالي"**، فشغِّل هؤلاء المرشحين عبر خطوة *تشابه متجهات* (لا تزال رخيصة).  
- **عندها فقط** استدع نموذج LLM للإنشاء أو التلخيص على العناصر *الأعلى N*.

---

## 6. الأسئلة المتداولة "ماذا عن التنبؤ بالرمز التالي؟"

| السؤال | الإجابة |
|----------|--------|
| *"هل يمكن لـ GROQ الإكمال التلقائي لحقل أثناء كتابتي؟"* | لا. GROQ يُرجع بيانات؛ لا يولد رموزًا جديدة. ستحتاج إلى خدمة إكمال تلقائي منفصلة (مثل OpenAI `gpt‑3.5‑turbo` مع `stream:true`) تستدعيها من واجهة المستخدم الخاصة بك، ثم تكتب القيمة المختارة مرة أخرى إلى Sanity باستخدام تحويل (mutation). |
| *"هل تستخدم مكونات واجهة المستخدم المضمنة في Sanity الذكاء الاصطناعي لاقتراح محتوى؟"* | محرر Studio الافتراضي **لا** يستخدم نموذج LLM. بعض الإضافات المجتمعية (مثل `sanity-plugin-asset-source-llm`) تستدعي نماذج خارجية لتوليد نص بديل أو عناوين، ولكن هذا اختياري وخارجي. |
| *"إذا قمت بتخزين ناتج نموذج لغة في Sanity، هل سيفهمه GROQ بشكل أفضل؟"* | ناتج النموذج هو مجرد نص منظم أكثر. يمكن لـ GROQ أن يصفيه (مثال: `summary match "react"`)، ولكن أي فهم *دلالي* لا يزال يأتي من كيفية **تنظيمك** للبيانات (الوسوم، الفئات، المتجهات) وليس من GROQ نفسه. |
| *"هل يمكنني تدريب نموذج داخل Sanity؟"* | لا. Sanity هو نظام إدارة محتوى (CMS)، وليس منصة تدريب. يمكنك تخزين بيانات التدريب (مجموعات البيانات، JSON الشرح) وتشغيل وظائف التدريب الخارجية عبر خطافات الويب (webhooks)، ولكن النموذج يعمل في مكان آخر. |

---

## 7. قائمة المراجعة المختصرة لمنتج "سريع، واعي دلاليًا"

| ✅ الهدف | كيف تحققه |
|--------|-------------------|
| **بحث فوري عن المحتوى** | استخدم استعلامات GROQ خالصة مع فهارس مناسبة (`_type`, `publishedAt`, حقول مخصصة). |
| **التشابه/البحث الدلالي** | احسب المتجهات مسبقًا → خزنها في قاعدة بيانات متجهات → في وقت الاستعلام: ضمّن استعلام المستخدم → ابحث في المتجهات → أطعم معرفات الناتج إلى عامل تصفية في GROQ. |
| **إنشاء النص على الطاير (ملخصات، كتابة نصوص، دردشة)** | استدع نموذج LLM خارجي (OpenAI, Anthropic, مستضاف ذاتيًا) *بعد* أن تكون قد جلبت السياق المطلوب باستخدام GROQ. اكتب النتيجة مرة أخرى عبر تحويل (mutation) عادي. |
| **زمن انتقال منخفض للجزء الخاص بالذكاء الاصطناعي** | • كمِّن النموذج (Quantise) أو استخدم نموذج مستضاف صغير.<br>• انشر الاستدلال (inference) عند الحافة (Cloudflare Workers‑AI, Vercel Edge Functions).<br>• خزن مؤقتًا أحدث المتجهات أو استجابات نموذج LLM عندما يكون ذلك ممكنًا. |
| **الحفاظ على تكاليف يمكن التنبؤ بها** | • افصل الطبقتين (GROQ = رخيص، نموذج LLM القائم على الرموز = الدفع حسب الاستخدام).<br>• جمّع المتجهات بلا اتصال (مرة واحدة لكل مستند).<br>• استخدم `limit`/`slice` في GROQ لتجنب سحب حمولات هائلة إلى نموذج LLM. |

---

### خلاصة القول

- **GROQ ≠ نموذج محول (transformer).** إنه *محرك استعلام حتمي* يمكنه استرجاع البيانات في بضعة ميلي ثانية لأنه يعمل على مخزن أعمدة مع فهارس وتنفيذ متدفق (streaming execution).
- **الدلالات الدقيقة أو توليد الرمز التالي** هما **خارج نطاق** GROQ؛ يأتيان من خدمات *منفصلة* لنماذج اللغة تستدعيها عندما تحتاجها.
- من خلال **الجمع** بين الاثنين (GROQ السريع لربط البيانات + نموذج LLM حديث للمهام التوليدية أو التشابه) تحصل على نظام يكون **عالي الأداء** و **قويًا دلاليًا** في نفس الوقت.

إذا كان لديك سير عمل محدد ترغب في تحسينه – مثال: "الإكمال التلقائي لأوصاف المنتج أثناء كتابة المحرر" أو "البحث في قاعدة معرفتي باستعلامات اللغة الطبيعية" – فأخبرني وسأتمكن من رسم مخطط هيكلي ملموس ورمز نموذجي. أتمنى لك بناءً سعيدًا!