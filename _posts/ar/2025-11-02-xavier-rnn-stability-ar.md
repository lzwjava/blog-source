---
audio: false
generated: true
image: false
lang: ar
layout: post
model: x-ai/grok-4-fast
title: تهيئة زافيير لاستقرار الشبكات العصبية المتكررة
translated: true
type: note
---

### لماذا التهيئة زافير في الشبكات العصبية المتكررة؟

تعتمد الشبكات العصبية المتكررة (RNNs) على تهيئة زافير (المسماة أيضًا جلوروت) لمصفوفات الأوزان الخاصة بها—مثل `Wxh` (من الإدخال إلى المخفي)، و`Whh` (من المخفي إلى المخفي، الأوزان المتكررة)، و`Why` (من المخفي إلى الإخراج)—للحفاظ على ديناميكيات تدريب مستقرة. الشكل المحدد في الكود الخاص بك يستخدم توزيع غاوسي (طبيعي) مع قياسه بـ `sqrt(1 / fan_in)`، حيث `fan_in` هو عدد الخلايا العصبية الداخلة إلى تلك الطبقة. هذا يحافظ على تباين الأوزان حول `1 / fan_in`، مما يضمن أن الإشارات الواردة لا يتم تضخيمها أو سحقها بقوة شديدة.

إليك السبب الذي يجعل هذا بالغ الأهمية للشبكات العصبية المتكررة، ولماذا قد يؤدي السحب العشوائي المنتظم البسيط من [0, 1] إلى حدوث مشاكل:

#### 1. **الحفاظ على تباين الإشارة عبر الطبقات وخطوات الزمن**
   - في الشبكات التغذوية الأمامية، تساعد تهيئة زافير في الحفاظ على *تباين التفعيلات* ثابتًا تقريبًا أثناء انتشار الإشارات إلى الأمام (والانحدارات إلى الخلف). بدونها، يمكن أن تشهد الطبقات العميقة انفجار التفعيلات (أي تصبح هائلة) أو تلاشيها (تنخفض إلى接近 الصفر)، مما يجعل التدريب مستحيلاً.
   - الشبكات العصبية المتكررة تشبه الشبكات "العميقة" *المفككة عبر الزمن*: الوزن المتكرر `Whh` يضرب الحالة المخفية في كل خطوة زمنية، مما يخلق سلسلة من عمليات الضرب (على سبيل المثال، لطول التسلسل *T*، يكون الأمر أشبه بـ *T* طبقة عميقة). إذا كان تباين الأوزان في `Whh` >1، فإن الانحدارات تنفجر أسيًا إلى الخلف (سيء للتسلسلات الطويلة). إذا كان <1، فإنها تتلاشى.
   - يضمن قياس زافير (مثل `* sqrt(1. / hidden_size)` لـ `Whh`) أن التباين المتوقع للحالة المخفية يبقى ~1، مما يمنع هذا. بالنسبة للتهيئة المنتظمة [0,1]:
     - المتوسط ~0.5 (منحاز إيجابيًا، مسببًا انحرافات).
     - التباين ~1/12 ≈ 0.083—صغير جدًا لـ `hidden_size` كبيرة (مثل 512)، مما يؤدي إلى تلاشي الإشارات بسرعة.

#### 2. **التكيف مع أبعاد الطبقة**
   - تأخذ تهيئة زافير في الاعتبار *fan_in* (المدخلات إلى الطبقة) وأحيانًا *fan_out* (المخرجات). بالنسبة لـ `Wxh`، فإن القياس بـ `sqrt(1 / input_size)` يعتمد على حجم المفردات أو أبعاد التضمين. بالنسبة لـ `Whh`، فإنه يعتمد على `hidden_size` لتتناسب مع الحلقة المتكررة.
   - التهيئة [0,1] تتجاهل الأبعاد: في طبقة مخفية كبيرة (مثل 1000 وحدة)، فإن جمع العديد من المدخلات [0,1] بدون قياس سيطغى على الطبقة التالية. في الطبقات الصغيرة، سيؤدي إلى تدفق ناقص.

#### 3. **تدفق الانحدار في الانتشار الخلفي عبر الزمن (BPTT)**
   - يستخدم تدريب الشبكات العصبية المتكررة الانتشار الخلفي عبر الزمن (BPTT)، حيث تتدفق الانحدارات إلى الخلف عبر *T* خطوة. تؤدي التهيئة الضعيفة إلى "مشكلة تلاشي الانحدار" (شائعة في الشبكات العصبية المتكررة العادية، ولهذا توجد LSTM و GRU). تخفف تهيئة زافير من هذا عن طريق البدء بمقادير متوازنة.
   - تظهر الدراسات التجريبية (على سبيل المثال، الورقة الأصلية لجلوروت وبنجيو) أن تهيئة زافير تقلل عدد الحقب اللازمة للتقارب بمقدار 2-10x في الشبكات العميقة/المتكررة.

#### البدائل ومتى قد "تعمل" التهيئة [0,1]
- قد "تعمل" التهيئة [0,1] في النماذج المصغرة جدًا (على سبيل المثال، أحادية الطبقة، تسلسلات قصيرة) لكنها تفشل على نطاق واسع—فكر في خسائر منفجرة أو قيم NaN.
- تعديلات حديثة: تهيئة هي (لـ ReLUs، تقيس بـ `sqrt(2 / fan_in)`) أو التهيئة المتعامدة (لـ RNNs، تحافظ على القيم القياسية تمامًا). لكن تهيئة زافير هي الإعداد الافتراضي الآمن لوظائف التفعيل sigmoid/tanh الشائعة في الشبكات العصبية المتكررة.

باختصار، تهيئة زافير تدور حول *الاستقرار أولاً*: فهي تضبط الأوزان صغيرة-ولكن-ليست-صغيرة-جدًا، واعية بالأبعاد، لتسمح للانحدارات بالتدفق بحرية. التهيئة العشوائية [0,1] слишком فظة—مثل دوس دواسة الوقود بدون موازنة الإطارات.

للتعمق أكثر:  
[Understanding the Difficulty of Training Deep Feedforward Neural Networks](http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf) (Glorot & Bengio, 2010)  
[Recurrent Neural Network Regularization](https://arxiv.org/abs/1409.2329) (Zaremba et al., on RNN init issues)