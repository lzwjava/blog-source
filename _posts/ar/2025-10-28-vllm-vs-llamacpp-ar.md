---
audio: false
generated: true
image: false
lang: ar
layout: post
model: x-ai/grok-4-fast
title: مقارنة vLLM و llama.cpp
translated: true
type: note
---

### نظرة عامة
vLLM و llama.cpp هما أداتان مفتوحتا المصدر شائعتان لاستدلال ونماذج اللغة الكبيرة (LLMs) بكفاءة، لكنهما تستهدفان سيناريوهات مختلفة. vLLM هو محرك خدمة مبني على Python مُحسَّن لاستدلال مجمَّع عالي الإنتاجية على وحدات معالجة الرسوميات (GPUs)، مما يجعله مثالياً للتطبيقات على نطاق الإنتاج. بينما llama.cpp هو مكتبة خفيفة الوزن مكتوبة بلغة C++ تركز على الاستدلال المحمول قليل الموارد، وتدعم مجموعة واسعة من الأجهزة بما في ذلك وحدات المعالجة المركزية (CPUs) وأجهزة الحافة. فيما يلي مقارنة مفصلة عبر الأبعاد الرئيسية.

### جدول المقارنة

| الجانب              | vLLM                                                                 | llama.cpp                                                            |
|---------------------|----------------------------------------------------------------------|----------------------------------------------------------------------|
| **الغرض الأساسي** | تقديم أداء عالي لنماذج اللغة الكبيرة (LLMs) مع التجميع وواجهة برمجة تطبيقات متوافقة مع OpenAI للطلبات المتزامنة. | محرك استدلال فعال للنماذج المكممة بتنسيق GGUF، مع التركيز على قابلية النقل واستدلالات منفردة منخفضة الكمون. |
| **التنفيذ**  | Python مع خلفية PyTorch؛ يعتمد على CUDA للتسريع.        | نواة C++ مع روابط للغات مثل Python/Rust وغيرها؛ يستخدم GGML للكمّة والتسريع. |
| **دعم الأجهزة**| وحدات معالجة الرسوميات من NVIDIA (CUDA)； يتفوق في الإعدادات متعددة وحدات معالجة الرسوميات مع التوازي الموتر. دعم محدود لوحدات المعالجة المركزية. | واسع: وحدات المعالجة المركزية، وحدات معالجة الرسوميات من NVIDIA/AMD (CUDA/ROCm)، Apple Silicon (Metal)، وحتى الأجهزة المحمولة/المدمجة. |
| **الأداء**     | متفوق في التزامن العالي: إنتاجية تصل إلى 24x مقابل Hugging Face Transformers؛ 250-350 رمز/ثانية مجمعة على أنظمة متعددة من RTX 3090 لنموذج Llama 70B؛ مكاسب 1.8x على 4x H100. في المعايير على RTX 4090 منفرد (Qwen 2.5 3B)، أسرع بنحو ~25% لـ 16 طلبًا متزامنًا. | قوي للطلبات المنفردة/قليلة التزامن: أسرع قليلاً (~6%) للطلبات المنفردة على RTX 4090 (Qwen 2.5 3B)； تراجع جيد لوحدات المعالجة المركزية لكنه يتأخر في التجميع/التعددية لوحدات معالجة الرسوميات (قد يتدهور الأداء مع زيادة وحدات معالجة الرسوميات بسبب التفريغ التسلسلي). |
| **سهولة الاستخدام**     | متوسط: إعداد سريع لخوادم وحدات معالجة الرسوميات لكنه يتطلب بيئة Docker/PyTorch؛ تبديل النماذج يحتاج إلى إعادة تشغيل. | عالي: واجهة سطر أوامر (CLI) ووضع خادم بسيطان؛ كمّة ونشر سهلان عبر Docker؛ سهل للمبتدئين للتشغيل المحلي. |
| **القابلية للتوسع**     | ممتاز للمؤسسات: يتعامل مع الأحمال العالية مع PagedAttention لذاكرة كاش KV فعالة (يقلل الهدر، يضغط المزيد من الطلبات). | جيد للنطاق الصغير/المتوسط: وضع خادم جاهز للإنتاج، لكنه أقل تحسينًا للتزامن الهائل. |
| **كفاءة الموارد** | مركز على وحدات معالجة الرسوميات: استخدام عالي لـ VRAM لكنه يحتاج إلى أجهزة قوية؛ ليس للإعدادات قليلة الموارد. | خفيف الوزن: يعمل على أجهزة المستهلك/الحافة؛ التكميم يمكن نماذج أقل من 1 جيجابايت على وحدات المعالجة المركزية. |
| **المجتمع والنظام البيئي** | نامٍ (مدعوم من UC Berkeley/PyTorch)； تحديثات متكررة للنماذج/الأجهزة الجديدة. | ضخم (آلاف المساهمين)； يدعم 100+ نموذجًا جاهزًا؛ نشط في تعديلات التكميم. |

### الاختلافات الرئيسية والتوصيات
- **متى تختار vLLM**: اختره في بيئات الإنتاج ذات حركة المرور العالية للمستخدمين (مثل خدمات API، روبوتات الدردشة على نطاق واسع) حيث تكون موارد وحدات معالجة الرسوميات وفيرة. تبرز تحسينات التجميع والذاكرة فيه في أعباء العمل المجمعة والمتزامنة، لكنه مفرط للاستخدام الشخصي أو منخفض الطاقة.
- **متى تختار llama.cpp**: مثالي للتطوير المحلي، الاستدلال دون اتصال بالإنترنت، أو الإعدادات محدودة الموارد (مثل أجهزة الكمبيوتر المحمولة، الخوادم بدون وحدات معالجة رسوميات من الطراز الأول). إنه أكثر تنوعًا عبر الأجهزة وأسهل للتجربة، لكنه قد يتطلب تعديلات مخصصة لأداء الذروة متعدد وحدات معالجة الرسوميات.
- **المفاضلات**: vLLM يُعطي الأولوية للسرعة والنطاق على حساب قابلية النقل؛ llama.cpp يركز على إمكانية الوصول لكنه يضحي ببعض الإنتاجية في السيناريوهات المطلوبة. للاحتياجات الهجينة، يمكن لأدوات مثل Ollama (المبنية على llama.cpp) سد الفجوة لتقديم خدمة أبسط.

### المراجع
- [vLLM vs Llama.cpp vs Ollama: Multi-GPU LLM Performance](https://www.arsturn.com/blog/multi-gpu-showdown-benchmarking-vllm-llama-cpp-ollama-for-maximum-performance)
- [vLLM vs Ollama vs Llama.cpp: Which to Use in Production?](https://www.arsturn.com/blog/vllm-vs-ollama-vs-llama-cpp-production-use)
- [llama.cpp vs vllm performance comparison (GitHub Discussion)](https://github.com/ggml-org/llama.cpp/discussions/15180)
- [vLLM vs llama.cpp (Reddit Thread)](https://www.reddit.com/r/LocalLLaMA/comments/1eamiay/vllm_vs_llamacpp/)