---
audio: false
generated: true
image: false
lang: ar
layout: post
model: x-ai/grok-4-fast
title: الشرح المنهجي لتقنيات الضبط الفعال للمعاملات والضبط المنخفض الرتبة
translated: true
type: note
---

فيما يلي شرح متعمق ومرتب للنص المقدم. سأقوم بتفكيكه خطوة بخطوة، بدءًا من المفاهيم الأساسية وصولاً إلى خصوصيات الضبط الفعال للمعاملات (PEFT) والتكيف منخفض الرتبة (LoRA). سأستخدم التشبيهات والحدس الرياضي والسياق الواقعي لجعل الشرح شاملاً وسهلاً في نفس الوقت. يستند هذا الشرح مباشرة إلى النص مع التوسع في الأفكار الأساسية والدوافع والآثار المترتبة على تعلم الآلة (ML) بالنسبة لنماذج اللغة الكبيرة (LLMs).

### 1. حجم نماذج اللغة الحديثة: التدريب المسبق وأهميته
يبدأ النص بتسليط الضوء على الحجم الهائل لنماذج اللغة الرائدة اليوم: "تحتوي نماذج اللغة الرائدة اليوم على ما يزيد عن تريليون معلمة، مُدربة مسبقًا على عشرات التريليونات من الرموز. يستمر أداء النموذج الأساسي في التحسن مع زيادة الحجم، حيث أن هذه التريليونات ضرورية لتعلم وتمثيل جميع الأنماط في المعرفة البشرية المكتوبة".

#### ما هي المعاملات والرموز؟
- **المعاملات (Parameters)** هي "الأوزان" في الشبكة العصبية – قيم رقمية يتعلمها النموذج أثناء التدريب. فكر فيها على أنها "ذاكرة" النموذج أو "مقابض المعرفة". يحتوي النموذج الذي به تريليون معلمة (مثل GPT-4 أو PaLM) على حوالي 1000 مليار من هذه القيم، وهو ما يعادل تقريبًا سعة تخزين البيانات لملايين الصور عالية الدقة.
- **الرموز (Tokens)** هي الوحدات الأساسية للنص التي يعالجها النموذج (مثل الكلمات أو أجزاء الكلمات). يتضمن التدريب المسبق إدخال **عشرات التريليونات** من هذه الرموز (مثلًا من الكتب ومواقع الويب ومستودعات الكود) ليتعلم الأنماط العامة مثل القواعد والحقائق والمنطق.

#### لماذا يحسن الحجم الأداء؟
- نماذج LLMs تعتمد على بنية المحولات (Transformer) (التي تم تقديمها في ورقة 2017 "Attention is All You Need")، والتي تتقن التقاط الأنماط المعقدة من خلال طبقات من آليات الانتباه والشبكات الأمامية.
- تظهر قوانين القياس التجريبية (مثل تلك من Kaplan et al., 2020 لـ OpenAI) أن الأداء (مثل الدقة في المهام مثل الإجابة على الأسئلة) يتحسن بشكل يمكن التنبؤ به مع زيادة المعاملات والبيانات وقوة الحوسبة. مضاعفة المعاملات غالبًا ما تنتج مكاسب لوغاريتمية في "القدرات الظاهرة" (مثل أن يصبح النموذج جيدًا فجأة في الرياضيات أو الترجمة).
- **الحدس**: المعرفة البشرية شاسعة ومترابطة. لتمثيلها جميعًا (مثل قواعد كل لغة، الحقائق التاريخية، المبادئ العلمية)، يحتاج النموذج إلى "فضاء معاملات" ضخم لتشفير هذه المعرفة كارتباطات منخفضة المستوى. النماذج الأصغر (مثل 1 مليار معلمة) تبالغ في التكيف مع الأنماط السطحية وتفشل في المهام الدقيقة، بينما النماذج ذات الحجم التريليوني تعمم بشكل أفضل.
- **المفاضلات**: هذا الحجم يتطلب حوسبة ضخمة (مثل آلاف وحدات معالجة الرسومات لأسابيع) وطاقة، لكنه أساس "النماذج الأساسية" مثل Llama أو سلسلة GPT.

باختصار، يبني التدريب المسبق "دماغًا" عامًا من خلال فرض الأنماط من المدونة المكتوبة للبشرية. يؤكد النص على أن هذا هو الأساس قبل أي تخصص.

### 2. ما بعد التدريب (الضبط الدقيق): التركيز الأضيق وتحديات الكفاءة
يقارن النص بين التدريب المسبق و "ما بعد التدريب"، والذي "يتضمن مجموعات بيانات أصغر ويركز بشكل عام على مجالات معرفة أضيق ونطاقات سلوك. يبدو إهدارًا استخدام تيرابت من الأوزان لتمثيل تحديثات من بيانات تدريب بحجم جيجابت أو ميجابت".

#### ما هو ما بعد التدريب/الضبط الدقيق؟
- بعد التدريب المسبق، يتم "ضبط" النموذج الأساسي بدقة على مجموعات بيانات أصغر ومحددة بالمهمة (مثل 1-10 مليون مثال مقابل تريليونات الرموز). هذا يتكيف به للتطبيقات مثل روبوتات الدردشة (مثل اتباع التعليمات)، أو تحليل المشاعر، أو الأسئلة والأجوبة الطبية.
- أمثلة: ضبط GPT-3 على سجلات دعم العملاء لإنشاء مساعد مفيد، أو على النصوص القانونية لمراجعة العقود.
- **لماذا مجموعات البيانات الأصغر؟** يستهدف الضبط الدقيق "تحديثات" أو "تجاوزات" للمعرفة الأساسية – مثل تعليم الأدب أو المصطلحات الخاصة بمجال معين – دون إعادة اختراع فهم اللغة العام.

#### حدس الإهدار
- **عدم التطابق بين حجم البيانات والنموذج**: إذا كان النموذج الأساسي يحتوي على ~1 تريليون معلمة (بحجم تيرابت تقريبًا، حيث أن كل معلمة تساوي roughly 1 بت)، لكن بيانات الضبط الدقيق صغيرة (بحجم جيجابت أو ميجابت)، فإن تحديث *جميع* المعاملات يشبه إعادة كتابة موسوعة كاملة من أجل حاشية سفلية واحدة. معظم أوزان النموذج تظل غير ذات صلة بالمهمة الجديدة.
- **مشاكل الضبط الدقيق الكامل (FullFT)**:
  - **الحمل الزائد في الحوسبة**: يتطلب تحديث جميع المعاملات إعادة حساب المتجهات التفاضلية (إشارات الخطأ) للنموذج بأكمله خلال كل خطوة تدريب. هذا يضاعف تكاليف الذاكرة والوقت بمقدار 10-100x.
  - **النسيان الكارثي**: يمكن أن يؤدي FullFT إلى تدهور القدرات العامة للنموذج (مثل أن ينسى النموذج المضبوط على الرياضيات الشعر).
  - **انتفاخ التخزين**: النماذج المضبوطة بدقة تكون بنفس حجم النموذج الأساسي (تريليونات المعاملات)، مما يجعل النشر مكلفًا (مثلًا، تكاليف السحابة تتدرج مع الحجم).
- **تشبيه**: تخيل ضبط أوركسترا ضخمة لأداء منفرد واحد من خلال إعادة تدريب كل موسيقي. هذا مبالغة عندما يمكنك فقط تدريب العازف المنفرد.

هذه عدم الكفاءة هي التي دفعت إلى **الضبط الفعال للمعاملات (PEFT)**: طرق لتحديث جزء ضئيل فقط (مثل 0.1-1%) من المعاملات مع تحقيق 90-100% من مكاسب أداء FullFT.

### 3. الضبط الفعال للمعاملات (PEFT): الفكرة الكبرى
"PEFT... يضبط شبكة كبيرة عن طريق تحديث مجموعة أصغر بكثير من المعاملات."

- **الدافع الأساسي**: الحفاظ على نقاط قوة النموذج الأساسي مع حقن تحديثات محددة بالمهمة بأقل تغييرات ممكنة. هذا يقلل من الحوسبة والذاكرة والتخزين – وهو أمر بالغ الأهمية لديمقراطية الذكاء الاصطناعي (مثل السماح للفرق الصغيرة بضبط نماذج مثل Llama 2 بدون حواسيب فائقة).
- **تقنيات PEFT الشائعة** (بجانب LoRA، المذكور لاحقًا):
  - **المحولات (Adapters)**: إدراج وحدات "إضافية" صغيرة (مثل طبقات الاختناق) بين طبقات المحولات، وتدريب تلك فقط.
  - **ضبط المطالبة (Prompt Tuning)**: تعلم مطالبات ناعمة (مثل الرموز الافتراضية) تُضاف إلى المدخلات، لتحديث ~0.01% فقط من المعاملات.
  - **ضبط البادئة (Prefix Tuning)**: مشابه، لكنه يضبط البادئات لطبقات الانتباه.
- **لماذا يعمل**: تحديثات الضبط الدقيق غالبًا ما تكون "منخفضة الأبعاد" – فهي تقع في فضاء جزئي من فضاء المعاملات الكامل. لا تحتاج إلى تعديل كل شيء؛ فبعض التغييرات المستهدفة تنتشر عبر الشبكة.
- **النجاح التجريبي**: تتطابق طرق PEFT أو تتجاوز FullFT في المعايير مثل GLUE (فهم اللغة الطبيعية) مع حوسبة أقل بـ 10-100x. مكتبات مثل PETF من Hugging Face تجعل هذا سهل التطبيق.

يحول PEFT النموذج من "تدريب كل شيء" إلى "تحرير جراحي"، متوافقًا مع موضوع الكفاءة في النص.

### 4. التكيف منخفض الرتبة (LoRA): طريقة PEFT الرائدة
"طريقة PEFT الرائدة هي التكيف منخفض الرتبة، أو LoRA. تستبدل LoRA كل مصفوفة وزن W من النموذج الأصلي بإصدار معدل W′ = W + γ B A، حيث أن B و A هما مصفوفتان معًا لهما معاملات أقل بكثير من W، و γ هو عامل قياس ثابت. بشكل فعال، تخلق LoRA تمثيلاً منخفض الأبعاد للتحديثات التي يمنحها الضبط الدقيق."

#### التفكيك الرياضي
تستهدف LoRA مصفوفات الوزن **W** في المحول (مثل في إسقاطات الاستعلام/المفتاح/القيمة للانتباه أو طبقات التغذية الأمامية). هذه عادةً ما تكون مصفوفات ذات أبعاد d × k (مثل 4096 × 4096، ملايين المعاملات لكل منها).

- **الصيغة**: أثناء الضبط الدقيق، بدلاً من تحديث W مباشرة، تحسب LoRA المخرجات كالتالي:
  ```
  h = W x + γ (B A) x  (حيث x هو المدخل)
  ```
  - **W**: الأوزان الأصلية المجمدة (غير متغيرة).
  - **A**: مصفوفة منخفضة الرتبة، مهيأة عشوائيًا (مثل r × k، حيث r << d، مثل r=8-64).
  - **B**: مصفوفة منخفضة الرتبة أخرى (d × r)، مهيأة إلى الصفر (بحيث يكون التحديث الأولي صفرًا، avoiding disruption).
  - **γ (غاما)**: عامل قياس (مثل γ = α / r، حيث α معامل فائق مثل 16) للتحكم في حجم التحديث واستقرار التدريب.
  - الوزن المحدث بالكامل: **W' = W + γ B A**.

- **لماذا "منخفض الرتبة"؟**
  - يمكن تحليل المصفوفات عبر التحليل القيمي المفرد (SVD): أي مصفوفة ≈ U Σ V^T، حيث "الرتبة" هي عدد القيم المفردة المهمة.
  - تحديثات الضبط الدقيق ΔW = W' - W غالبًا ما تكون **منخفضة الرتبة** (r << min(d,k))، مما يعني أنها تلتقط التغييرات في فضاء جزئي مضغوط (مثل بعض الاتجاهات مثل "التركيز على السلامة" أو "التركيز على الكود").
  - **B A** تقارب ΔW برتبة r (المعاملات: d*r + r*k مقابل d*k لـ W الكاملة). بالنسبة لـ r=8 في مصفوفة W بحجم 4096×4096، تستخدم LoRA ~65k معلمة مقابل 16M – تخفيض بنسبة 99.6%!
  - **الحدس**: التحديثات تشبه المتجهات في فضاء عالي الأبعاد؛ تقوم LoRA بإسقاطها على "طريق سريع" منخفض الأبعاد (الرتبة r)، متجاهلة الضوضاء في فضاء المعاملات الشاسع.

- **كيفية عمل التدريب**:
  1. المرور الأمامي: حساب h باستخدام W + γ B A، ولكن تدريب A و B فقط (W مجمدة).
  2. الانتشار العكسي: تتدفق المتجهات التفاضلية فقط إلى A/B، مما يحافظ على انخفاض الذاكرة.
  3. الاستدلال: إما دمج (W' = W + B A) لنموذج واحد أو الإبقاء منفصلاً من أجل الوحدية.
- **من الورقة البحثية (Hu et al., 2021)**: تم تقديم LoRA لنماذج الرؤية واللغة لكنها انتشرت في معالجة اللغات الطبيعية. تفوقت على المحولات في مهام مثل التلخيص مع استخدام ذاكرة أقل. متغيرات مثل QLoRA تقوم بتكميم النموذج الأساسي أكثر لتصغير البصمة further.

في الجوهر، "تخترق" LoRA النموذج بإضافة "دلتا" خفيفة الوزن (B A) تمثل الضبط الدقيق كتحويل خطي مضغوط.

### 5. مزايا LoRA مقارنة بالضبط الدقيق الكامل (FullFT)
يسرد النص الفوائد التشغيلية، مؤكدًا على الجوانب العملية beyond الكفاءة الخام. سأوسع في كل منها.

#### أ. التكلفة وسرعة ما بعد التدريب
- LoRA تدرب أسرع بـ 100-1000x وأرخص لأنها تحدث ~0.1% من المعاملات. على سبيل المثال، ضبط Llama-7B على وحدة معالجة رسومات واحدة A100 (بينما يحتاج FullFT إلى 8+ وحدات معالجة رسومات) يستغرق ساعات مقابل أيام.
- الدقة المنخفضة (مثل bfloat16) تكفي، مما يقلل استخدام الطاقة.

#### ب. الخدمة متعددة المستأجرين
"نظرًا لأن LoRA تدرب محولاً (أي مصفوفتي A و B) مع الحفاظ على الأوزان الأصلية دون تغيير، يمكن لخادم استدلال واحد الاحتفاظ بالعديد من المحولات (إصدارات نماذج مختلفة) في الذاكرة وأخذ عينات منها في وقت واحد بطريقة مجمعة. Punica: Multi-Tenant LoRA Serving (Chen, Ye, et al, 2023) محركات الاستدلال الحديثة مثل vLLM و SGLang تنفذ هذه الميزة."

- **ماذا يعني**: يتم مشاركة الأساس W؛ المحولات صغيرة (ميغابايت مقابل غيغابايت للنماذج الكاملة). يقوم الخادم بتحميل W واحدة + N محول (مثل للبرمجة، الكتابة، الترجمة).
- **تعدد المستأجرين**: خدمة multiple users/models في وقت متوازي دون إعادة تحميل الأساس. تجميع الطلبات عبر المحولات للكفاءة.
- **التأثير الواقعي**: في الإنتاج (مثل Hugging Face Spaces أو Azure ML)، يتيح هذا "شوربة النماذج" أو تبديل الشخصيات على الطاير. Punica (2023) يحسن الذاكرة عبر التقسيم إلى صفحات؛ يستخدم vLLM/SGLang الانتباه المقسم إلى صفحات لإنتاجية أعلى بـ 10x.
- **تشبيه**: مثل محرك واحد (W) مع مجموعات توربو قابلة للتبديل (المحولات) مقابل شراء سيارة جديدة لكل ضبط.

#### ج. حجم التخطيط للتدريب
"عند ضبط النموذج بأكمله بدقة، يجب تخزين حالة المُحسّن (optimizer state) مع الأوزان الأصلية، غالبًا بدقة أعلى. نتيجة لذلك، عادةً ما يتطلب FullFT ترتيبًا من حيث الحجم لوحدات المعالجة أكثر مما تتطلبه أخذ العينات من نفس النموذج... للتدريب، بالإضافة إلى تخزين الأوزان، نحتاج عادةً إلى تخزين المتجهات التفاضلية ولحظات المُحسّن لجميع الأوزان؛ علاوة على ذلك، غالبًا ما يتم تخزين هذه المتغيرات بدقة أعلى (float32) مما يُستخدم لتخزين الأوزان للاستدلال (bfloat16 أو أقل). نظرًا لأن LoRA تدرب أوزانًا أقل بكثير وتستخدم ذاكرة أقل بكثير، فيمكن تدريبها على تخطيط أكبر قليلاً فقط مما يُستخدم لأخذ العينات."

- **تفصيل ذاكرة التدريب**:
  - FullFT: الأوزان (1T معلمة @ bfloat16 = ~2TB) + المتجهات التفاضلية (نفس الحجم) + حالات المُحسّن (مثل Adam: 2 لحظة لكل معلمة @ float32 = ~8TB إجمالي). يحتاج إلى 100s من وحدات معالجة الرسومات في "تخطيط" موزع (مثل توازي البيانات/النموذج).
  - LoRA: فقط A/B (~0.1% من المعاملات) تحصل على متجهات تفاضلية/حالات (~2-10GB إضافية). التدريب على 1-2 وحدة معالجة رسومات، بنفس تخطيط الاستدلال.
- **تفاصيل الدقة**: الاستدلال يستخدم دقة منخفضة (bfloat16/float16) للسرعة؛ التدريب يحتاج float32 لاستقرار المتجهات التفاضلية. تقلل LoRA من هذه النفقات العامة.
- **إمكانية الوصول**: يمكن للهواة/الشركات الناشئة الضبط الدقيق على أجهزة المستهلك (مثل RTX 4090)، مقابل حاجة FullFT إلى مجموعات enterprise. الكفاءة: غالبًا ما تتقارب LoRA أسرع due إلى متغيرات أقل.

#### د. سهولة التحميل والنقل
"مع أوزان أقل لتخزينها، فإن محولات LoRA سريعة وسهلة الإعداد أو النقل بين الأجهزة."

- المحولات هي ملفات <100MB (مقابل 100s GB للنماذج الكاملة)، قابلة للضغط والمشاركة (مثل عبر GitHub أو التورنت).
- النشر: تحميل الأساس مرة واحدة، تبديل المحولات في ثوانٍ. الدمج اختياري للديمومة.
- **الحالات الخاصة**: التحكم في الإصدار (مثل Git للمحولات)، التعلم الموحد (تدريب المحولات على الجهاز، إرسال الدلتا فقط)، أو الضبط التكراري (تراكم عدة محولات LoRA).

### الآثار الأوسع والقيود
- **لماذا تهيمن LoRA**: إنها بسيطة وفعالة وتتكامل بسلاسة (مثل في مكتبة PEFT). مستخدمة في نماذج مثل fine-tunes Stable Diffusion أو Alpaca (Llama المضبوط بالتعليمات).
- **القيود**: ليست دائمًا بنفس أداء FullFT في المهام المتخصصة جدًا؛ اختيار الرتبة r مهم (منخفضة جدًا تقلل من التكيف، عالية جدًا تهدر الكفاءة). لا تتعامل مع جميع أنواع التحديثات (مثل التغييرات المعمارية).
- **الاتجاهات المستقبلية**: امتدادات مثل DoRA (تحليل الأوزان further) أو VeRA (قائم على المتجهات، أكثر تخفيفًا). مع تدرج النماذج إلى 10T+ معلمة، يصبح PEFT/LoRA أساسيًا للاستدامة.

يجسد هذا النص تحولًا محوريًا في الذكاء الاصطناعي: من إعادة التدريب الكامل الجائعة للموارد إلى التحديثات الأنيقة والمعيارية. إذا كنت ترغب في اشتقاقات رياضية، أو أمثلة برمجية (مثلًا في PyTorch)، أو غوص في أوراق بحثية ذات صلة، دعني أعلم!