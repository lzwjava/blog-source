---
audio: false
generated: true
lang: ar
layout: post
title: دوال التفعيل في الشبكات العصبية
translated: true
type: note
---

**Sigmoid** و **ReLU** و **Leaky ReLU** هي جميعًا دوال تفعيل تُستخدم في الشبكات العصبية. لكل منها خصائص فريدة وتكون مناسبة لمهام مختلفة اعتمادًا على نوع المشكلة التي يتم حلها. إليك تفصيل للاختلافات:

### 1. **Sigmoid**:
   - **الصيغة الرياضية**:  
     \\[
     \sigma(x) = \frac{1}{1 + e^{-x}}
     \\]
   - **النطاق**: (0, 1)
   - **الإيجابيات**:
     - تدرج سلس، مما يساعد في عملية التحسين.
     - مُخرجاتها محصورة بين 0 و 1، مما يجعلها جيدة للاحتمالات أو التصنيف الثنائي.
   - **السلبيات**:
     - **مشكلة التدرج المتلاشي**: بالنسبة لقيم الإدخال الكبيرة أو الصغيرة جدًا، يصبح التدرج صغيرًا جدًا (يقترب من الصفر)، مما يمكن أن يبطئ التدريب، خاصة في الشبكات العميقة.
     - المُخرجات ليست مركزة حول الصفر، مما يمكن أن يؤدي إلى مشاكل عندما تهيمن تحديثات التدرج باتجاه واحد.
   - **حالة الاستخدام**: غالبًا ما تُستخدم في طبقة الإخراج لمهام التصنيف الثنائي (مثل الانحدار اللوجستي).

### 2. **ReLU (Rectified Linear Unit)**:
   - **الصيغة الرياضية**:  
     \\[
     f(x) = \max(0, x)
     \\]
   - **النطاق**: [0, ∞)
   - **الإيجابيات**:
     - **سريعة وبسيطة**: يسهل حسابها وفعالة عمليًا.
     - تحل مشكلة التدرج المتلاشي من خلال السماح للتدرج بالانتشار بشكل جيد.
     - تشجع على التخلخل (يمكن للعديد من الخلايا العصبية أن تصبح غير نشطة).
   - **السلبيات**:
     - **مشكلة ReLU المحتضرة**: يمكن للخلايا العصبية أن "تموت" أثناء التدريب إذا كان ناتجها دائمًا صفرًا (أي لقيم الإدخال السالبة). هذا يمكن أن يتسبب في عدم تنشيط بعض الخلايا العصبية مرة أخرى.
   - **حالة الاستخدام**: تُستخدم بشكل شائع جدًا في الطبقات المخفية للشبكات العميقة، خاصة في الشبكات التلافيفية والشبكات العصبية العميقة.

### 3. **Leaky ReLU**:
   - **الصيغة الرياضية**:  
     \\[
     f(x) = \max(\alpha x, x)
     \\]
     حيث أن \\( \alpha \\) ثابت صغير (مثل 0.01).
   - **النطاق**: (-∞, ∞)
   - **الإيجابيات**:
     - تمنع **مشكلة ReLU المحتضرة** من خلال السماح بانحدار سالب صغير لقيم \\( x \\) السالبة.
     - مثل ReLU، تساعد في مشكلة التدرج المتلاشي وتشجع على التخلخل.
   - **السلبيات**:
     - لا تزال لديها إمكانية لإخراج قيم سالبة، لكنها أقل إشكالية من ReLU القياسي.
     - يمكن أن يكون اختيار قيمة \\( \alpha \\) صعبًا، ويتطلب أحيانًا ضبطًا.
   -