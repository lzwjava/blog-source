---
audio: false
generated: true
image: false
lang: ar
layout: post
model: x-ai/grok-code-fast-1
title: مكتبة تيني جراد الخفيفة للتعلم الآلي
translated: true
type: note
---

### مقدمة عن TinyGrad
TinyGrad هي مكتبة تعلم آلي وتعلم عميق خفيفة وبسيطة في Python، مصممة للأغراض التعليمية والبساطة. تنفذ التمايز التلقائي (autograd) مثل PyTorch ولكنها أصغر بكثير—حوالي 1000 سطر من الكود. إنها رائعة لتعلم مفاهيم التعلم الآلي أو بناء نماذج صغيرة بدون تعقيدات المكتبات الأكبر. تدعم الموترات، الشبكات العصبية، والعمليات الأساسية، بما في ذلك التسريع بواسطة وحدة معالجة الرسومات عبر PyTorch أو Metal.

يمكنك العثور على المستودع الرسمي على: [tinygrad GitHub](https://github.com/geohot/tinygrad). ملاحظة: إنها تجريبية وليست قوية مثل PyTorch أو TensorFlow للاستخدام في الإنتاج.

### التثبيت
ثبت TinyGrad عبر pip:

```bash
pip install tinygrad
```

لها تبعيات قليلة ولكنها تستخدم PyTorch بشكل اختياري لبعض الواجهات الخلفية. لدعم وحدة معالجة الرسومات، تأكد من تثبيت PyTorch.

### الاستخدام الأساسي
ابدأ باستيراد المكتبة وضبط السياق (يتطلب TinyGrad تحديد ما إذا كنت تقوم بالتدريب أو الاستدلال، حيث يتم حساب التدرجات بشكل مختلف).

#### الاستيراد والسياق
```python
from tinygrad import Tensor
from tinygrad.nn import Linear, BatchNorm2d  # للشبكات العصبية

# ضع السياق: التدريب (للتدرجات) أو الاستدلال
Tensor.training = True  # تمكين تتبع التدرج
```

#### إنشاء ومعالجة الموترات
الموترات هي بنية البيانات الأساسية، مشابهة لمصفوفات NumPy أو موترات PyTorch.

```python
# إنشاء موترات من قوائم، مصفوفات NumPy، أو حسب الشكل
a = Tensor([1, 2, 3])          # من قائمة
b = Tensor.zeros(3)            # موتر أصفار بالشكل (3,)
c = Tensor.rand(2, 3)          # موتر عشوائي بالشكل (2, 3)

# العمليات الأساسية
d = a + b                      # جمع حسب العنصر
e = d * 2                      # ضرب عددي
f = a @ Tensor([[1], [2], [3]])  # ضرب المصفوفات (a أحادي البعد، يتم نقله ضمنياً)

print(e.numpy())               # التحويل إلى NumPy للطباعة أو الاستخدام الإضافي
```

#### التمايز التلقائي (الانتشار الخلفي)
يحسب TinyGrad التدرجات تلقائياً باستخدام قاعدة السلسلة.

```python
# تمكين تتبع التدرج
Tensor.training = True

x = Tensor([1.0, 2.0, 3.0])
y = (x * 2).sum()             # بعض العمليات؛ y عدد قياسي

y.backward()                  # حساب التدرجات
print(x.grad.numpy())         # التدرجات بالنسبة لـ x: يجب أن تكون [2, 2, 2]
```

للت export إلى NumPy، استخدم `.numpy()`—تتراكم التدرجات ما لم يتم إعادة تعيينها.

#### الشبكات العصبية والتدريب
يتضمن TinyGrad طبقات ومحسّنات أساسية. إليك مثال MLP بسيط:

```python
from tinygrad.nn import Linear, optim

# تعريف نموذج بسيط (مثلاً، طبقة خطية)
model = Linear(3, 1)          # إدخال 3، إخراج 1

# بيانات وهمية
x = Tensor.rand(4, 3)         # دفعة من 4 عينات، 3 ميزات
y_true = Tensor.rand(4, 1)    # الهدف

# التمرير الأمامي
pred = model(x).sigmoid()      # بافتراض تصنيف ثنائي

# الخسارة (مثلاً، MSE)
loss = ((pred - y_true) ** 2).mean()

# الانتشار الخلفي والتحسين
loss.backward()
optim.Adam([model], lr=0.01).step()
```

لشبكات الالتواء، استخدم `Conv2d` من `tinygrad.nn`.

### الميزات المتقدمة
- **دوال الخسارة والتفعيل**: متوفرة في `tinygrad.nn` (مثلاً، `sigmoid`, `relu`, `cross_entropy`).
- **المحسّنات**: `SGD`, `Adam` في `tinygrad.nn.optim`.
- **الطبقات**: `Linear`, `Conv2d`, `BatchNorm`، إلخ.
- **الحفظ/التحميل**: يمكن حفظ النماذج كقواميس حالة (مشابه لـ PyTorch).
- **وحدة معالجة الرسومات/التسريع**: يمكن لـ TinyGrad العمل على وحدة معالجة الرسومات عبر واجهة PyTorch الخلفية: `TESOR_SET_DEVICE='cuda:0'`. كما يدعم Metal على macOS.
- **أمثلة الرؤية/إزالة الضوضاء**: يحتوي المستودع على أمثلة مثل تدريب ResNet على MNIST.

للحصول على أمثلة كاملة، تحقق من مجلد `examples/` في مستودع GitHub، مثل تصنيف الصور أو التعلم المعزز.

### مثال: تدريب شبكة عصبية
إليك نصاً بسيطاً وكاملاً لتدريب شبكة على بوابة XOR:

```python
from tinygrad import Tensor
from tinygrad.nn import Linear, optim

# البيانات
X = Tensor([[0,0], [0,1], [1,0], [1,1]])
Y = Tensor([[0], [1], [1], [0]])

# النموذج
model = Linear(2, 1)
opt = optim.Adam([model], lr=0.1)

Tensor.training = True

for epoch in range(1000):
    pred = model(X).sigmoid()
    loss = ((pred - Y) ** 2).mean()
    loss.backward()
    opt.step()
    opt.zero_grad()

# الاختبار
print(model(Tensor([[0,0]])))  # يجب أن يكون ~0
```

### نصائح وتحذيرات
- **تبديل الوضع**: ضع دائماً `Tensor.training = True` للتدريب؛ بدّل إلى `False` للاستدلال لتجنب حساب التدرجات.
- **معالجة الشكل**: تتبع الموترات البث الشبيه بـ NumPy، ولكن انتبه للفيض في النماذج الصغيرة.
- **تصحيح الأخطاء**: استخدم `.numpy()` لفحص القيم. إذا فشل شيء، تحقق من عدم تطابق الأشكال.
- **الأداء**: إنها بطيئة للنماذج الكبيرة—استخدم PyTorch للإنتاج.
- **التحديثات**: يتطور TinyGrad بسرعة؛ تحقق من المستودع للحصول على أحدث نسخة.

لمزيد من التفاصيل، قم بزيارة [docs/README.md](https://github.com/geohot/tinygrad/blob/master/README.md) أو شغّل الأمثلة محلياً. إذا كان لديك حالة استخدام محددة، قدم المزيد من التفاصيل للحصول على نصيحة مخصصة!