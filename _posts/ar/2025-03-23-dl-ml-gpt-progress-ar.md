---
audio: false
generated: true
lang: ar
layout: post
title: التعلم العميق والتعلم الآلي وتطور جي بي تي (2010-2025)
translated: true
type: note
---

إليك جدولًا زمنيًا مفصلاً لأهم الإنجازات في التعلم العميق، والتعلم الآلي، ونماذج GPT من عام 2010 حتى تاريخ قطع معلوماتي في أكتوبر 2024:

## أوائل العقد 2010: الأسس

- **2012: AlexNet** - حققت بنية الشبكة العصبية التلافيفية (CNN) التي قدمها كريزيفسكي وسوتسكيفر وهينتون فوزًا في مسابقة ImageNet، مما أدى إلى خفض معدلات الخطأ من 26% إلى 15.3%، مما أطلق ثورة التعلم العميق في مجال الرؤية الحاسوبية.

- **2013: Word2Vec** - قدم ميكولوف تقنيات تضمين الكلمات التي تمثل الكلمات كمتجهات بناءً على السياق، مما مكّن من الفهم الدلالي.

- **2014: الشبكات الخصومية التوليدية (GANs)** - قدم غودفيلو إطار عمل تتنافس فيه شبكات المولد والمميز، مما مكّن من توليد الصور الواقعية.

- **2014: نماذج التسلسل إلى التسلسل (Sequence-to-Sequence)** - طور سوتسكيفر وفينياولز ولي نماذج للترجمة الآلية يمكنها تعيين تسلسلات الإدخال إلى تسلسلات الإخراج.

## منتصف العقد 2010: ظهور النماذج الأساسية

- **2015: ResNet** - قدم هو وزملاؤه اتصالات المتبقيات (residual connections)، مما مكّن من تدريب شبكات أعمق بكثير (152+ طبقة) والفوز بمسابقة ImageNet بمعدل خطأ 3.57%.

- **2015: تسوية الدُفعات (Batch Normalization)** - طور إيوفي وسزيجدي تقنية لتحقيق استقرار وتسريع تدريب الشبكات العصبية.

- **2015: آلية الانتباه (Attention Mechanism)** - قدم بهادانو آلية الانتباه للترجمة الآلية العصبية، مما سمح للنماذج بالتركيز على الأجزاء ذات الصلة من تسلسلات الإدخال.

- **2016: AlphaGo** - هزم نظام DeepMind بطل العالم لي سيدول في لعبة Go، مدمجًا بين التعلم العميق المعزز وبحث شجرة مونت كارلو.

## أواخر العقد 2010: ثورة المحولات (Transformers)

- **2017: بنية المحول (Transformer Architecture)** - قدم فاسواني وزملاؤه ورقة "الانتباه هو كل ما تحتاجه"، مستبدلاً الشبكات العصبية المتكررة (RNNs) بآليات الانتباه الذاتي.

- **2018: BERT** - حقق نموذج التمثيلات الترميزية ثنائية الاتجاه من المحولات (Bidirectional Encoder Representations from Transformers) من Google نتائج متطورة في فهم اللغة الطبيعية.

- **2018: GPT-1** - أصدرت OpenAI أول نموذج محول توليدي مُدرّب مسبقًا (Generative Pre-trained Transformer) يحتوي على 117 مليون معلمة، مُدرّب على مجموعة BookCorpus.

- **2019: GPT-2** - قامت OpenAI بالزيادة في الحجم إلى 1.5 مليار معلمة، مُظهرة قدرات مفاجئة في التعلم دون أمثلة (zero-shot)، ولكنها امتنعت في البداية عن الإصدار الكامل بسبب مخاوف سوء الاستخدام.

## أوائل العقد 2020: التوسع في الحجم والتعددية النمطية

- **2020: GPT-3** - أصدرت OpenAI نموذجًا يحتوي على 175 مليار معلمة يُظهر قدرات ملحوظة في التعلم القليل (few-shot learning) عبر المهام دون الحاجة إلى ضبط دقيق.

- **2021: DALL-E** - أظهرت OpenAI إمكانية قيام نماذج المحولات بتوليد الصور من الأوصاف النصية.

- **2021: Codex** - أظهر نموذج توليد الكود من OpenAI، الذي يشغل GitHub Copilot، قدرات في البرمجة.

- **2021: نماذج الانتشار (Diffusion Models)** - قدمت GLIDE و DALL-E 2 و Stable Diffusion جودة فائقة في توليد الصور.

- **2022: ChatGPT** - حازت واجهة المحادثة من OpenAI لنماذج GPT على اعتماد جماهيري غير مسبوق (100 مليون مستخدم في شهرين).

- **2022: PaLM** - أظهر نموذج Google الذي يحتوي على 540 مليار معلمة قدرات استدلالية.

- **2022: Chinchilla** - أظهرت DeepMind قوانين التوسع المثلى التي تشير إلى أن النماذج الأصغر حجمًا مع المزيد من البيانات يمكنها التفوق على النماذج الأكبر.

## 2024-2023: نماذج اللغة الكبيرة متعددة الوسائط والاستدلال

- **2023: GPT-4** - نموذج OpenAI متعدد الوسائط مع تحسينات في القدرات الاستدلالية والسلامة وفهم الصور.

- **2023: Claude** - أصدرت Anthropic الذكاء الاصطناعي الدستوري (constitutional AI) مركزًا على الفائدة والسلامة والصدق.

- **2023: LLaMA** - أصدرت Meta نماذج لغة كبيرة مفتوحة الأوزان، مما حفز الابتكار مفتوح المصدر.

- **2023: نماذج خليط الخبراء (Mixture-of-Experts - MoE)** - أظهرت نماذج مثل Mixtral 8x7B مكاسب في الكفاءة من خلال تنشيط الأجزاء ذات الصلة فقط من الشبكة لكل مدخل.

- **2024-2023: النماذج متعددة الوسائط** - اكتسبت النماذج مثل GPT-4V و Gemini و Claude 3 وغيرها قدرات عبر الرؤية والصوت واللغة.

- **2024: Sora** - أنتج نموذج تحويل النص إلى فيديو من OpenAI مقاطع فيديو واقعية لمشاهد معقدة.

- **2024: تحسينات الاستدلال** - اكتسبت النماذج قدرات محسنة في التسلسل الفكري (Chain-of-Thought)، مع نماذج متخصصة في الرياضيات والبرمجة والاستدلال العلمي.

- **2024: أطر عمل الوكلاء (Agent Frameworks)** - سمحت الأنظمة التي تتضمن التخطيط واستخدام الأدوات والذاكرة للنماذج بإكمال مهام معقدة متعددة الخطوات.

## الاتجاهات الرئيسية خلال هذه الفترة:

1. **قوانين التوسع (Scaling Laws)**: تحسينات أداء متسقة من خلال زيادة حجم النموذج، وحجم مجموعة البيانات، والحوسبة.

2. **القدرات الناشئة (Emergent Abilities)**: ظهور قدرات مثل التعلم في السياق (in-context learning) بشكل غير متوقع عند عتبات حجم معينة.

3. **التعددية النمطية (Multimodality)**: دمج النص والصور والصوت والفيديو في نماذج موحدة.

4. **التعلم المعزز من التغذية الراجعة البشرية (RLHF)**: محاذاة النماذج مع تفضيلات البشر.

5. **التعميم (Democratization)**: نمو النماذج مفتوحة المصدر والأطر المتاحة مما يخفض الحواجز أمام تطوير التعلم الآلي.

تمثل هذه الفترة ربما أسرع تقدم تكنولوجي في أي مجال في تاريخ البشرية، محولة التعلم الآلي من تخصص أكاديمي متخصص إلى تكنولوجيا متعددة الأغراض ذات تأثير عالمي.