---
audio: false
generated: true
image: false
lang: ar
layout: post
model: x-ai/grok-4-fast
title: التكميم الرباعي البت لتحسين كفاءة النماذج اللغوية الكبيرة
translated: true
type: note
---

### ما هو التكميم 4-بت (في سياق QLoRA أو GPTQ)؟

التكميم 4-بت هو تقنية لتقليل البصمة الذاكرة لنماذج اللغة الكبيرة (LLMs) بشكل كبير عن طريق تخزين كل وزن باستخدام 4 بت فقط بدلاً من الدقة المعتادة 16-بت (FP16/BF16) أو 32-بت (FP32).

النماذج العادية (كاملة الدقة):
- FP32: 4 بايت لكل معامل → نموذج 7B ≈ 28 جيجابايت
- FP16/BF16: 2 بايت لكل معامل → نموذج 7B ≈ 14 جيجابايت

التكميم 4-بت:
- ~0.5 بايت لكل معامل → نموذج 7B ≈ 3.5–4 جيجابايت (تقريبًا 1/4 إلى 1/8 الحجم الأصلي اعتمادًا على الطريقة الدقيقة)

هناك نهجان رئيسيان 4-بت نشهدهما اليوم بشكل شائع:

| الطريقة | الاسم الكامل                   | الورقة البحثية الرئيسية / السنة | حالة الاستخدام النموذجية         | الخصائص الرئيسية                                                                 |
|---------|---------------------------------|----------------------------------|----------------------------------|---------------------------------------------------------------------------------|
| GPTQ    | GPTQ                            | 2023                             | التكميم بعد التدريب (للاستدلال فقط) | لمرة واحدة، دقيق جدًا، لا يحتاج إلى إعادة تدريب. يقوم بتقريب الأوزان إلى 4-بت بعد التدريب. |
| QLoRA   | Quantized Low-Rank Adaptation | 2023 (يونيو)                     | الضبط الفعال / ضبط التعليمات      | يجمع بين التخزين 4-بت + محولات LoRA + محسنات مُجدولة. يسمح بضبط نماذج 65B+ على وحدة معالجة رسومية واحدة 24–48 جيجابايت. |

#### QLoRA بتفصيل أكثر (هو ما يعنيه الناس عادةً عندما يقولون "4-بت QLoRA")
تقوم QLoRA بأربعة أشياء ذكية في وقت واحد:

1.  **تكميم NormalFloat 4-بت (NF4)**
    - نوع بيانات خاص 4-بت مُحسّن للأوزان موزعة بشكل طبيعي (معظم أوزان LLM تكون ≈ Gaussian بعد التدريب).
    - أفضل من INT4 العادي؛ مثالي نظريًا للبيانات موزعة بشكل طبيعي.

2.  **التكميم المزدوج**
    - حتى ثوابت التكميم (عوامل القياس) يتم تكميها من FP16 → 8-بت، مما يوفر بضع ميغابايتات إضافية.

3.  **المحسنات المُجدولة**
    - يتم تخزين حالات المُحسّن (لحظات AdamW) في ذاكرة الوصول العشوائي للـ CPU ويتم ترحيلها إلى وحدة المعالجة الرسومية باستخدام ذاكرة NVIDIA الموحدة. يمنع نفاد الذاكرة (OOM) أثناء التدريب.

4.  **محولات LoRA**
    - يقوم بتدريب مصفوفات صغيرة ذات رتبة منخفضة فقط (r=64 أو أقل) بينما يظل النموذج الأساسي 4-بت مجمدًا.

النتيجة: يمكنك ضبط نموذج 65B مثل Llama/Mistral بالكامل على وحدة معالجة رسومية واحدة من نوع 48 جيجابايت RTX A6000 أو حتى نموذج 70B على وحدة معالجة رسومية واحدة من نوع 80 جيجابايت A100 باستخدام QLoRA، بينما الضبط الكامل العادي سيتطلب 8×A100 أو أكثر.

#### GPTQ (المركز على الاستدلال)
- يتم بعد انتهاء التدريب.
- يستخدم معلومات من الدرجة الثانية (Hessian) لتقليل خطأ التقريب عند ضغط الأوزان إلى 4-بت.
- دقيق للغاية — عادةً ما يكون تدهور الـ perplexity أقل من 0.1 مقارنة بـ FP16.
- شائع الاستخدام مع أدوات مثل AutoGPTQ, ExLlama, vLLM, و llama.cpp (GGUF لديها أيضًا أوضاع تشبه GPTQ).

### Gradient Checkpointing (يُسمى أيضًا "activation checkpointing")

تقنية منفصلة تمامًا لتوفير الذاكرة تُستخدم أثناء التدريب.

كيف تعمل:
- بشكل طبيعي، تحتاج الانتشار العكسي إلى تخزين جميع التنشيطات الوسيطة من المرور الأمامي → تزداد الذاكرة خطيًا مع عمق النموذج/حجم الدفعة/طول التسلسل.
- يقوم Gradient Checkpointing بمقايضة الحساب بالذاكرة:
    - أثناء المرور الأمامي، يحفظ فقط بعض التنشيطات "نقاط التفتيش".
    - أثناء المرور الخلفي، يعيد حساب التنشيطات المفقودة على الفور.

مدى توفير الذاكرة النموذجي: 50–80% من ذاكرة التنشيط، على حساب تدريب أبطأ بنسبة ~20–30%.

يتم استخدامه دائمًا تقريبًا مع QLoRA أو أي ضبط لنماذج كبيرة لأنه حتى مع أوزان 4-بت، لا تزال التنشيطات تستهلك ذاكرة وحدة المعالجة الرسومية بشكل هائل.

### ملخص سريع

"التكميم 4-بت (QLoRA-style أو GPTQ) + Gradient Checkpointing" هي الوصفة الحديثة من أجل:

- تشغيل أو ضبط نماذج ضخمة (33B, 70B, 405B) على وحدات معالجة رسومية استهلاكية أو مؤسسية مفردة.
- 4-بت QLoRA-style → الضبط باستخدام ذاكرة وحدة معالجة رسومية منخفضة جدًا.
- 4-بت GPTQ-style → استدلال سريع باستخدام ذاكرة وحدة معالجة رسومية منخفضة جدًا.
- Gradient Checkpointing → يجعل عملية الانتشار العكسي الفعلية ممكنة على التسلسلات الطويلة دون نفاد الذاكرة (OOM).

معًا، هم السبب في أنك ترى أشخاصًا يضبطون نموذج Llama-3-70B على وحدة معالجة رسومية مفردة من نوع 3090/4090 اليوم.