---
audio: false
generated: true
lang: ar
layout: post
title: شجرة القرار
translated: true
type: note
---

**شجرة القرار** هي أداة لنمذجة التنبؤ تُستخدم في التعلم الآلي وتحليل البيانات لاتخاذ قرارات بناءً على بيانات الإدخال. إنها تمثل القرارات والعواقب المحتملة لها، بما في ذلك نتائج الأحداث العرضية، في هيكل يشبه الشجرة. تُستخدم أشجار القرار على نطاق واسع لمهام مثل التصنيف (على سبيل المثال، التنبؤ بما إذا كان العميل سيشتري منتجًا) والانحدار (على سبيل المثال، التنبؤ بأسعار المنازل). إنها بديهية وسهلة التفسير وفعالة لكل من مجموعات البيانات البسيطة والمعقدة.

سيشرح هذا الدليل الشامل ما هي شجرة القرار، وكيف تعمل، ومكوناتها، وعملية بنائها، والمزايا، والقيود، والاعتبارات العملية، جنبًا إلى جنب مع الأمثلة.

---

### **ما هي شجرة القرار؟**

شجرة القرار هي تمثيل يشبه المخطط الانسيابي للقرارات والنتائج المحتملة لها. وهي تتكون من العقد والفروع:
*   **العقد**: تمثل القرارات أو الشروط أو النتائج.
*   **الفروع**: تمثل النتائج المحتملة لقرار أو شرط.
*   **الأوراق**: تمثل الناتج النهائي (على سبيل المثال، تسمية فئة للتصنيف أو قيمة رقمية للانحدار).

تُستخدم أشجار القرار في التعلم الخاضع للإشراف، حيث يتعلم النموذج من بيانات التدريب المُصنفة للتنبؤ بالنتائج لبيانات جديدة غير مرئية. إنها متعددة الاستخدامات ويمكنها التعامل مع كل من البيانات الفئوية والرقمية.

---

### **مكونات شجرة القرار**

1.  **العقدة الجذرية**:
    *   العقدة العلوية في الشجرة.
    *   تمثل مجموعة البيانات بأكملها ونقطة القرار الأولية.
    *   تنقسم بناءً على السمة التي توفر أكبر قدر من المعلومات أو تقلل عدم اليقين إلى أقصى حد.

2.  **العقد الداخلية**:
    *   العقد الواقعة بين الجذر والأوراق.
    *   تمثل نقاط قرار وسيطة بناءً على سمات وشروط محددة (على سبيل المثال، "هل العمر > 30؟").

3.  **الفروع**:
    *   اتصالات بين العقد.
    *   تمثل نتيجة قرار أو شرط (على سبيل المثال، "نعم" أو "لا" للانقسام الثنائي).

4.  **العقد الورقية**:
    *   عقد نهائية تمثل الناتج النهائي.
    *   في التصنيف، تمثل الأوراق تسميات الفئة (على سبيل المثال، "اشترِ" أو "لا تشترِ").
    *   في الانحدار، تمثل الأوراق قيمًا رقمية (على سبيل المثال، سعر متوقع).

---

### **كيف تعمل شجرة القرار؟**

تعمل شجرة القرار عن طريق تقسيم بيانات الإدخال بشكل متكرر إلى مناطق بناءً على قيم السمات، ثم اتخاذ قرار بناءً على الفئة الأغلبية أو القيمة المتوسطة في تلك المنطقة. إليك شرحًا خطوة بخطوة لكيفية عملها:

1.  **بيانات الإدخال**:
    *   تحتوي مجموعة البيانات على سمات (متغيرات مستقلة) ومتغير هدف (متغير تابع).
    *   على سبيل المثال، في مجموعة بيانات للتنبؤ بما إذا كان العميل سيشتري منتجًا، قد تشمل السمات العمر والدخل ووقت التصفح، ويكون الهدف هو "اشترِ" أو "لا تشترِ".

2.  **تقسيم البيانات**:
    *   تختار الخوارزمية سمة وعتبة (على سبيل المثال، "العمر > 30") لتقسيم البيانات إلى مجموعات فرعية.
    *   الهدف هو إنشاء تقسيمات تعظم فصل الفئات (للتقسيم) أو تقلل التباين (للانحدار).
    *   تشمل معايير التقسيم مقاييس مثل **عدم نقاء Gini**، أو **كسب المعلومات**، أو **تقليل التباين** (سيتم شرحها لاحقًا).

3.  **التقسيم المتكرر**:
    *   تكرر الخوارزمية عملية التقسيم لكل مجموعة فرعية، مما يخلق عقدًا وفروعًا جديدة.
    *   يستمر هذا حتى يتم استيفاء معيار التوقف (على سبيل المثال، أقصى عمق، الحد الأدنى من العينات لكل عقدة، أو عدم وجود تحسن إضافي).

4.  **تعيين المخرجات**:
    *   بمجرد توقف التقسيم، يتم تعيين ناتج نهائي لكل عقدة ورقة.
    *   للتصنيف، تمثل الورقة الفئة الأغلبية في تلك المنطقة.
    *   للانحدار، تمثل الورقة المتوسط (أو الوسيط) لقيم الهدف في تلك المنطقة.

5.  **التنبؤ**:
    *   للتنبؤ بنتيجة نقطة بيانات جديدة، تجتاز الشجرة من الجذر إلى الورقة، متبعة قواعد القرار بناءً على قيم سمات نقطة البيانات.
    *   توفر العقدة الورقية التنبؤ النهائي.

---

### **معايير التقسيم**

تحدد جودة التقسيم مدى جودة فصل الشجرة للبيانات. تشمل المعايير الشائعة:

1.  **عدم نقاء Gini (للتصنيف)**:
    *   يقيس عدم نقاء العقدة (مدى اختلاط الفئات).
    *   الصيغة: \( \text{Gini} = 1 - \sum_{i=1}^n (p_i)^2 \)، حيث \( p_i \) هي نسبة الفئة \( i \) في العقدة.
    *   يشير انخفاض عدم نقاء Gini إلى تقسيم أفضل (عقدة أكثر تجانسًا).

2.  **كسب المعلومات (للتصنيف)**:
    *   يعتمد على **الإنتروبيا**، التي تقيس العشوائية أو عدم اليقين في العقدة.
    *   الإنتروبيا: \( \text{Entropy} = - \sum_{i=1}^n p_i \log_2(p_i) \).
    *   كسب المعلومات = الإنتروبيا قبل التقسيم - متوسط الإنتروبيا الموزون بعد التقسيم.
    *   يشير ارتفاع كسب المعلومات إلى تقسيم أفضل.

3.  **تقليل التباين (للانحدار)**:
    *   يقيس الانخفاض في تباين متغير الهدف بعد التقسيم.
    *   التباين: \( \text{Variance} = \frac{1}{n} \sum_{i=1}^n (y_i - \bar{y})^2 \)، حيث \( y_i \) هي قيمة الهدف و \( \bar{y} \) هو المتوسط.
    *   تختار الخوارزمية التقسيم الذي يعظم تقليل التباين.

4.  **Chi-Square (للتصنيف)**:
    *   يختبر ما إذا كان التقسيم يحسن توزيع الفئات بشكل كبير.
    *   يُستخدم في بعض الخوارزميات مثل CHAID.

تقيم الخوارزمية جميع التقسيمات الممكنة لكل سمة وتختار التقسيم ذو أفضل درجة (على سبيل المثال، أقل عدم نقاء Gini أو أعلى كسب للمعلومات).

---

### **كيف يتم بناء شجرة القرار؟**

يتضمن بناء شجرة القرار الخطوات التالية:

1.  **اختيار أفضل سمة**:
    *   قيم جميع السمات ونقاط التقسيم المحتملة باستخدام المعيار المختار (على سبيل المثال، Gini، كسب المعلومات).
    *   اختر السمة والعتبة التي تفصل البيانات بشكل أفضل.

2.  **تقسيم البيانات**:
    *   قسّم مجموعة البيانات إلى مجموعات فرعية بناءً على السمة والعتبة المحددتين.
    *   أنشئ عقدًا فرعية لكل مجموعة فرعية.

3.  **كرر بشكل متكرر**:
    *   طبق نفس العملية على كل عقدة فرعية حتى يتم استيفاء شرط التوقف:
        *   الوصول إلى أقصى عمق للشجرة.
        *   الحد الأدنى لعدد العينات في العقدة.
        *   لا يوجد تحسن كبير في معيار التقسيم.
        *   تنتمي جميع العينات في العقدة إلى نفس الفئة (للتقسيم) أو لها قيم متشابهة (للانحدار).

4.  **تقليم الشجرة (اختياري)**:
    *   لمنع الإفراط في التخصيص، قلل من تعقيد الشجرة عن طريق إزالة الفروع التي تساهم قليلاً في دقة التنبؤ.
    *   يمكن أن يكون التقليم **ما قبل التقليم** (التوقف مبكرًا أثناء البناء) أو **ما بعد التقليم** (إزالة الفروع بعد البناء).

---

### **مثال: شجرة قرار التصنيف**

**مجموعة البيانات**: التنبؤ بما إذا كان العميل سيشتري منتجًا بناءً على العمر والدخل ووقت التصفح.

| العمر | الدخل | وقت التصفح | اشترى؟ |
|-----|--------|---------------|------|
| 25  | Low    | Short         | No   |
| 35  | High   | Long          | Yes  |
| 45  | Medium | Medium        | Yes  |
| 20  | Low    | Short         | No   |
| 50  | High   | Long          | Yes  |

**الخطوة 1: العقدة الجذرية**:
*   قيم جميع السمات (العمر، الدخل، وقت التصفح) للحصول على أفضل تقسيم.
*   لنفترض أن "الدخل = High" يعطي أعلى كسب للمعلومات.
*   قسّم البيانات:
    *   الدخل = High: كلها "Yes" (عقدة نقية، توقف هنا).
    *   الدخل = Low أو Medium: مختلطة (استمر في التقسيم).

**الخطوة 2: العقدة الفرعية**:
*   لمجموعة فرعية "Low أو Medium Income"، قيم السمات المتبقية.
*   لنفترض أن "العمر > 30" يعطي أفضل تقسيم:
    *   العمر > 30: غالبًا "Yes."
    *   العمر ≤ 30: كلها "No."

**الخطوة 3: توقف**:
*   جميع العقد نقية (تحتوي على فئة واحدة فقط) أو تستوفي معايير التوقف.
*   تبدو الشجرة كالتالي:
    *   الجذر: "هل الدخل High؟"
        *   نعم → الورقة: "اشترِ"
        *   لا → "هل العمر > 30؟"
            *   نعم → الورقة: "اشترِ"
            *   لا → الورقة: "لا تشترِ"

**التنبؤ**:
*   عميل جديد: العمر = 40، الدخل = Medium، وقت التصفح = Short.
*   المسار: الدخل ≠ High → العمر = 40 > 30 → تنبأ "اشترِ".

---

### **مثال: شجرة قرار الانحدار**

**مجموعة البيانات**: التنبؤ بأسعار المنازل بناءً على الحجم والموقع.

| الحجم (قدم مربع) | الموقع | السعر ($K) |
|--------------|----------|------------|
| 1000         | Urban    | 300        |
| 1500         | Suburban | 400        |
| 2000         | Urban    | 600        |
| 800          | Rural    | 200        |

**الخطوة 1: العقدة الجذرية**:
*   قيم التقسيمات (على سبيل المثال، الحجم > 1200، الموقع = Urban).
*   لنفترض أن "الحجم > 1200" يقلل التباين.
*   تقسيم:
    *   الحجم > 1200: الأسعار = {400, 600} (المتوسط = 500).
    *   الحجم ≤ 1200: الأسعار = {200, 300} (المتوسط = 250).

**الخطوة 2: توقف**:
*   العقد صغيرة بما يكفي أو أن تقليل التباين ضئيل.
*   الشجرة:
    *   الجذر: "هل الحجم > 1200؟"
        *   نعم → الورقة: تنبأ 500$K.
        *   لا → الورقة: تنبأ 250$K.

**التنبؤ**:
*   منزل جديد: الحجم = 1800، الموقع = Urban → الحجم > 1200 → تنبأ 500$K.

---

### **مزايا أشجار القرار**

1.  **القدرة على التفسير**:
    *   سهلة الفهم والتصور، مما يجعلها مثالية لشرح القرارات لأصحاب المصلحة غير التقنيين.
2.  **يتعامل مع البيانات المختلطة**:
    *   يعمل مع كل من السمات الفئوية والرقمية دون معالجة مسبقة مكثفة.
3.  **لا معلمي**:
    *   لا توجد افتراضات حول التوزيع الأساسي للبيانات.
4.  **أهمية السمة**:
    *   يحدد السمات التي تساهم بشكل أكبر في التنبؤات.
5.  **تنبؤ سريع**:
    *   بمجرد التدريب، تكون التنبؤات سريعة لأنها تتضمن مقارنات بسيطة.

---

### **قيود أشجار القرار**

1.  **الإفراط في التخصيص**:
    *   يمكن للأشجار العميقة أن تحفظ بيانات التدريب، مما يؤدي إلى تعميم ضعيف.
    *   الحل: استخدم التقليم، أو حدد أقصى عمق، أو عيّن الحد الأدنى من العينات لكل عقدة.
2.  **عدم الاستقرار**:
    *   التغييرات الصغيرة في البيانات يمكن أن تؤدي إلى أشجار مختلفة تمامًا.
    *   الحل: استخدم طرق المجموعات مثل Random Forests أو Gradient Boosting.
3.  **منحاز للفئات المسيطرة**:
    *   يكافح مع مجموعات البيانات غير المتوازنة حيث تهيمن فئة واحدة.
    *   الحل: استخدم تقنيات مثل ترجيح الفئة أو زيادة العينات.
4.  **نهج جشع**:
    *   يتم اختيار التقسيمات بناءً على التحسين المحلي، مما قد لا يؤدي إلى الشجرة المثلى عالميًا.
5.  **معالجة ضعيفة للعلاقات الخطية**:
    *   أقل فعالية لمجموعات البيانات حيث تكون العلاقات بين السمات والهدف خطية أو معقدة.

---

### **اعتبارات عملية**

1.  **المعاملات الفائقة**:
    *   **العمق الأقصى**: يحد من عمق الشجرة لمنع الإفراط في التخصيص.
    *   **الحد الأدنى لعينات التقسيم**: الحد الأدنى لعدد العينات المطلوبة لتقسيم عقدة.
    *   **الحد الأدنى لعينات الورقة**: الحد الأدنى لعدد العينات في عقدة ورقة.
    *   **الحد الأقصى للسمات**: عدد السمات التي يجب مراعاتها لكل تقسيم.

2.  **التقليم**:
    *   ما قبل التقليم: حدد قيودًا أثناء بناء الشجرة.
    *   ما بعد التقليم: أزل الفروع بعد بناء الشجرة بناءً على أداء التحقق.

3.  **معالجة القيم المفقودة**:
    *   بعض الخوارزميات (على سبيل المثال، CART) تعين القيم المفقودة للفرع الذي يقلل الخطأ.
    *   بدلاً من ذلك، قدّر القيم المفقودة قبل التدريب.

4.  **القابلية للتوسع**:
    *   أشجار القرار فعالة حسابيًا لمجموعات البيانات الصغيرة والمتوسطة ولكن يمكن أن تكون بطيئة لمجموعات البيانات الكبيرة جدًا ذات السمات الكثيرة.

5.  **طرق المجموعات**:
    *   للتغلب على القيود، غالبًا ما تُستخدم أشجار القرار في مجموعات:
        *   **Random Forest**: يجمع بين أشجار متعددة مدربة على مجموعات فرعية عشوائية من البيانات والسمات.
        *   **Gradient Boosting**: يبني الأشجار بشكل تسلسلي، حيث يصحح كل منها أخطاء السابقة.

---

### **تطبيقات أشجار القرار**

1.  **الأعمال**:
    *   التنبؤ برحيل العملاء، التسجيل الائتماني، تجزئة التسويق.
2.  **الرعاية الصحية**:
    *   تشخيص الأمراض، التنبؤ بالمخاطر (على سبيل المثال، أمراض القلب).
3.  **التمويل**:
    *   كشف الاحتيال، التنبؤ بتخلف القروض.
4.  **معالجة اللغة الطبيعية**:
    *   تصنيف النص (على سبيل المثال، تحليل المشاعر).
5.  **مهام الانحدار**:
    *   التنبؤ بالنتائج المستمرة مثل أسعار المنازل أو توقعات المبيعات.

---

### **مثال على التصور**

لتوضيح كيف تقسم شجرة القرار البيانات، دعنا نفكر في مجموعة بيانات تصنيف بسيطة بسمتين (على سبيل المثال، العمر والدخل) وفئتين (اشترِ، لا تشترِ). أدناه مخطط مفاهيمي يوضح كيف تقسم شجرة القرار مساحة السمة.

```
chartjs
{
  "type": "scatter",
  "data": {
    "datasets": [
      {
        "label": "Buy",
        "data": [
          {"x": 35, "y": 50000},
          {"x": 45, "y": 60000},
          {"x": 50, "y": 80000}
        ],
        "backgroundColor": "#4CAF50",
        "pointRadius": 6
      },
      {
        "label": "Not Buy",
        "data": [
          {"x": 20, "y": 20000},
          {"x": 25, "y": 30000}
        ],
        "backgroundColor": "#F44336",
        "pointRadius": 6
      }
    ]
  },
  "options": {
    "scales": {
      "x": {
        "title": { "display": true, "text": "Age" },
        "min": 15,
        "max": 60
      },
      "y": {
        "title": { "display": true, "text": "Income ($)" },
        "min": 10000,
        "max": 100000
      }
    },
    "plugins": {
      "title": { "display": true, "text": "Decision Tree Feature Space" },
      "legend": { "display": true }
    }
  }
}
```

يظهر هذا المخطط نقاط البيانات في مساحة سمة ثنائية الأبعاد. قد تقسم شجرة القرار هذه المساحة (على سبيل المثال، عند العمر = 30 أو الدخل = 40000) لفصل "اشترِ" عن "لا تشترِ".

---

### **التنفيذ عمليًا**

يمكن تنفيذ أشجار القرار باستخدام مكتبات مثل:
*   **Python**: Scikit-learn (`DecisionTreeClassifier`, `DecisionTreeRegressor`), XGBoost, LightGBM.
*   **R**: `rpart`, `party`.
*   **أدوات أخرى**: Weka, MATLAB, أو Spark MLlib.

مثال في Python (Scikit-learn):
```python
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score

# Sample data
X = [[25, 20000], [35, 50000], [45, 60000], [20, 30000], [50, 80000]]  # Features: Age, Income
y = [0, 1, 1, 0, 1]  # Labels: 0 = Not Buy, 1 = Buy

# Split data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Train decision tree
clf = DecisionTreeClassifier(max_depth=3, random_state=42)
clf.fit(X_train, y_train)

# Predict
y_pred = clf.predict(X_test)

# Evaluate
print("Accuracy:", accuracy_score(y_test, y_pred))
```

---

### **الخلاصة**

أشجار القرار هي نماذج قوية قابلة للتفسير لمهام التصنيف والانحدار. تعمل عن طريق تقسيم البيانات بشكل متكرر بناءً على شروط السمات، باستخدام معايير مثل عدم نقاء Gini أو كسب المعلومات. بينما يسهل استخدامها وتصورها، فإنها يمكن أن تفرط في التخصيص وتكون غير مستقرة، مما يتم التخفيف منه غالبًا عن طريق التقليم أو استخدام طرق المجموعات مثل Random Forests.

لمزيد من الاستكشاف، جرب أشجار القرار باستخدام مجموعات بيانات حقيقية، وضبط المعاملات الفائقة، واجمعها مع طرق المجموعات لتحسين الأداء. إذا كان لديك مجموعة بيانات أو حالة استخدام محددة في الاعتبار، يمكنني تقديم مثال أو تصور مخصص!