---
audio: false
generated: true
image: false
lang: ar
layout: post
model: x-ai/grok-4-fast
title: 'vLLM: محرك خدمة فعال للنماذج اللغوية الكبيرة'
translated: true
type: note
---

### اختراع vLLM

vLLM (اختصار لـ "virtual LLM") هي مكتبة مفتوحة المصدر لاستدلال ونماذج اللغة الكبيرة (LLMs) عالية الإنتاجية وذات كفاءة في الذاكرة. تم اختراعها في أوائل عام 2023 من قبل باحثين في Sky Computing Lab التابع لـ RISELab في جامعة UC Berkeley. بدأ المشروع كمحرك استدلال متخصص مُحسّن لبطاقات NVIDIA A100 GPU ومجموعة محدودة من النماذج، لمعالجة التحديات الرئيسية في تقديم نماذج اللغة الكبيرة مثل تشتت الذاكرة وانخفاض الإنتاجية.

أهم المحطات المبكرة:
- **منتصف أبريل 2023**: أول تكامل عام مع FastChat، مشغلاً عروض LMSYS's Vicuna و Chatbot Arena.
- **يونيو 2023**: الإصدار الرسمي وإطلاق مستودع GitHub العام.
- **12 سبتمبر 2023**: نُشرت الورقة البحثية الأساسية، "Efficient Memory Management for Large Language Model Serving with PagedAttention" على arXiv، مقدمة آلية PagedAttention الأساسية التي تمكن الدُفعات المستمرة وهدر ذاكرة التخزين المؤقت (KV cache) شبه المنعدِم.

تم إنشاء مستودع GitHub (vllm-project/vllm) حوالي مايو–يونيو 2023، متزامناً مع الدفعة التطويرية الأولية.

### الصعود في الشعبية

بدأ vLLM في اكتساب زخم كبير في عام 2024، متطوراً من أداة بحثية متخصصة إلى المعيار الفعلي لتقديم نماذج اللغة الكبيرة مفتوحة المصدر. انفجرت شعبيته بسبب الإضافات السريعة للميزات (مثل التكميم، فك التشفير التخميني، دعم الوسائط المتعددة)، والتوسع في الأجهزة (NVIDIA، AMD، Google TPUs، إلخ)، واعتماده في بيئات الإنتاج من قبل شركات مثل Amazon (مشغلاً لـ Rufus خلال Prime Day 2024) و LinkedIn.

مؤشرات النمو الرئيسية من عام 2024:
- **نجوم GitHub**: نما 2.3 مرة من 14,000 (أوائل 2024) إلى 32,600 (نهاية 2024).
- **التنزيلات الشهرية**: ارتفع 4.5 مرة من 6,000 إلى 27,000.
- **استخدام GPU**: زاد حوالي 10 مرات في النصف الثاني من عام 2024.
- **المجتمع**: ارتفع عدد المساهمين 3.9 مرة إلى 740، مع ساعات عمل مكتبية كل أسبوعين وشراكات (مثل NVIDIA، IBM، AWS).

بحلول منتصف عام 2024، كان لديه حوالي 20,000 نجمة وكان يُناقش بشكل متكرر في مجتمعات الذكاء الاصطناعي لتغلبيه على البدائل في الإنتاجية. استمر الزخم حتى عام 2025:
- انضم إلى نظام PyTorch البيئي في ديسمبر 2024.
- دخل في فترة احتضان LF AI & Data في أكتوبر 2024.
- وصل إلى 50,000 نجمة بحلول يونيو 2025.

اليوم (أكتوبر 2025)، لديه أكثر من 55,000 نجمة ويدعم ما يقرب من 100 بنية نموذج، مما يجعله حجر الزاوية للنشر القابل للتوسع في الذكاء الاصطناعي.

[ورقة arXiv عن PagedAttention](https://arxiv.org/abs/2309.06180)  
[مستودع vLLM على GitHub](https://github.com/vllm-project/vllm)  
[مدونة استعراض عام 2024 لـ vLLM](https://blog.vllm.ai/2025/01/10/vllm-2024-wrapped-2025-vision.html)  
[إعلان الاندماج مع PyTorch](https://pytorch.org/blog/vllm-joins-pytorch/)