---
audio: false
generated: true
image: false
lang: ar
layout: post
model: x-ai/grok-code-fast-1
title: مقدمة ونظرة عامة على محسن الميون
translated: true
type: note
---

### مقدمة شاملة لـ Muon Optimizer

Muon Optimizer هو خوارزمية تحسين متخصصة مستوحاة من الطرق من الرتبة الثانية، مصممة في المقام الأول لتدريب الطبقات الخطية في الشبكات العصبية (مثل الطبقات المتصلة بالكامل أو طبقات التضمين)، على الرغم من إمكانية توسيع نطاقها لتشمل غيرها. تم اقتراحها في الأصل في أواخر عام 2024 من قبل باحثين بما في ذلك كيلر جوردان وجيريمي بيرنشتاين، ولها جذور في تقنيات التحسين الهندسي مثل التهيئة القطبية وإطار العمل الثنائي المعياري [1][2]. سلطت Zhiling Yang، مؤسسة Moonshot AI و Kimi AI، الضوء على Muon في مناقشات حول تدريب نموذجهم Kimi K2 - نموذج لغة كبير (LLM) يحتوي على 1 تريليون معلمة - حيث عمل كعمود فقري للتحديثات الفعالة عالية الرتبة التي تتكيف مع هندسة مشهد الخسارة [3][4]. ومع ذلك، فإن نسختها الأساسية عانت من عدم الاستقرار (مثل طفرات الخسارة أثناء التدريب الطويل)، مما دفع Moonshot AI إلى تطوير MuonClip، وهو متغير محسن يحتوي على آليات استقرار مثل QK-clipping لطبقات الاهتمام [3][2].

تتميز Muon بكفاءتها في التعامل مع الرموز المميزة (tokens): فهي تتطلب رموزًا مميزة للتدريب أقل من محسّنات الرتبة الأولى مثل AdamW لتحقيق أداء مماثل، مما يجعلها ذات قيمة للمهام كثيفة الموارد مثل التدريب المسبق لنماذج LLM. تهدف إلى تقريب طرق الرتبة الثانية (مثل طريقة نيوتن) دون تكلفتها الحسابية الكاملة، مع التركيز على التكيف مع القيم الذاتية عبر تحديثات المصفوفات عالية الرتبة. هذا مفيد بشكل خاص في النماذج واسعة النطاق حيث تكون التدرجات مشوشة، حيث تستفيد Muon من التكييف المسبق المستوحى من التدرجات الطبيعية والجذور التربيعية للمصفوفات.

#### المبادئ الأساسية والاشتقاق
- **المفهوم الأساسي**: تستند Muon إلى التحسين الهندسي، حيث تتكيف التحديثات مع "المشهد الطاقي" لدالة الخسارة. تستخدم مُحسّنًا مسبقًا يعتمد على مصفوفة معلومات فيشر (أو تقريباتها) لقياس التدرجات، على غرار AdaGrad أو Shampoo ولكنها مُحسّنة للطبقات الخطية الكثيفة [1][2].
- **خطوات الخوارزمية**:
  1. **حساب التدرج**: حساب التدرجات القياسية \( \nabla W \) للأوزان \( W \) في الطبقات الخطية.
  2. **التكييف المسبق**: استخدام تكرارات Newton-Schulz لتقريب الجذر التربيعي للمصفوفة للمُحسّن المسبق (مشتق، على سبيل المثال، من إحصائيات الطبقة). هذا يمكّن من التكيف مع الرتبة دون الحاجة إلى التحلل الذاتي الكامل.
  3. **قاعدة التحديث**: تطبيق تحديث يقيس المكونات عالية الرتبة بشكل أكثر فعالية، غالبًا ما يتم دمعه مع الزخم أو القص لتحقيق الاستقرار.
- **بصيرة رياضية**: إذا كانت \( G \) هي مصفوفة التدرج، فإن Muon تقرب تحديثًا مثل \( W \leftarrow W - \eta \cdot \sqrt{P}^{-1} G \)، حيث يستخدم \( \sqrt{P} \) الجذر التربيعي التكراري للمصفوفة [2][5]. هذا يتناقض مع القياس القطري أو القائم على العزوم في AdamW، مما يسمح لـ Muon بالتقاط الارتباطات بين المعلمات بشكل أفضل.
- **دفعة الكفاءة**: يمكن لـ Muon تقليل عدد خطوات التدريب بنسبة 20-50% في بعض المعايير، كما رأينا في استخدامها مع سجلات NanoGPT [1].

#### المزايا والعيوب
- **المزايا**:
  - **تقارب أفضل على الطبقات الخطية**: تتفوق في المساحات الكثيفة عالية الأبعاد النموذجية في نماذج LLM، مما يؤدي إلى خسارة أقل بعدد أقل من الرموز المميزة [4][6].
  - **كفاءة في استخدام الموارد**: تدريب أسرع لكل دورة بسبب الحاجة إلى حسابات تدرج أقل.
  - **مفتوح المصدر وقابل للتوسيع**: توجد تطبيقات متعددة، بما في ذلك تطبيقات محددة مثل Flash-Muon لتسريع GPU [4][7].
- **العيوب**:
  - **عدم الاستقرار**: عرضة للتباعد في الشبكات الأعمق أو الطبقات المتفرقة؛ MuonClip يعالج هذا عن طريق قص نتائج الاهتمام (على سبيل المثال، منتجات الاستعلام-المفتاح) أثناء التدريب [3][2].
  - **خصوصية الطبقة**: ليست مثالية للطبقات التلافيفية أو المتكررة؛ إنها منحازة toward معماريات الخطية/MoE. تلاحظ Keras أنه لا ينبغي استخدامها للطبقات غير الخطية [8].
  - **حساسية المعاملات الفائقة**: تتطلب ضبطًا لمعدل التعلم (\( \eta \)) والحركات المستحثة للتعامد؛ قد لا تنتقل عبر أحجام النماذج دون تعديل [2].
- **متغير MuonClip (مخصص لـ Kimi)**: هذا هو تطور Muon، مدمج مع QK-clipping لمنع عدم الاستقرار في التدريب المسبق لـ 15.5 تريليون رمز مميز. لقد استقرت معاملات Kimi K2 البالغ عددها 32 مليار معلمة المُفعلة، مما مكّن من التدريب بدون طفرات خسارة وتفوق في المعايير (على سبيل المثال، 66.1 على Tau2-Bench) [3][8]. بدون وجود كود عام حتى الآن، إنه احتكاري ولكنه مبني على Muon المفتوح.

أثرت Muon على مشهد تحسين الذكاء الاصطناعي، حيث ظهرت في معايير مثل Scion ومناقشات على Reddit/X، وغالبًا ما يتم الإشادة بـ "حدسها الهندسي". للحصول على الاشتقاقات الكاملة، راجع مدونة جيريمي بيرنشتاين [2]. الآن، دعونا نلقي نظرة على تنفيذ عملي.

### مثال على الكود: تنفيذ Muon Optimizer في PyTorch
أدناه تنفيذ لـ Muon Optimizer الأساسي في PyTorch، مأخوذ من المستودع الرسمي (https://github.com/KellerJordan/Muon). هذا إصدار مبسط للطبقات الخطية الكثيفة؛ يتضمن تكرارات Newton-Schulz للمُحسّن المسبق.

```python
import torch
import torch.nn as nn

class Muon(torch.optim.Optimizer):
    """
    محسن Muon للطبقات الخطية.
    من: https://github.com/KellerJordan/Muon
    """
    def __init__(self, params, lr=1e-3, lr_b=2e-3, b2=0.95, wd=0.0):
        defaults = dict(lr=lr, lr_b=lr_b, b2=b2, wd=wd)
        super().__init__(params, defaults)

    def step(self):
        for group in self.param_groups:
            lr = group['lr']
            lr_b = group['lr_b']
            b2 = group['b2']
            wd = group['wd']

            for p in group['params']:
                if p.grad is None:
                    continue

                grad = p.grad.data.float()
                state = self.state[p]
                if 'momentum' not in state:
                    state['momentum'] = torch.zeros_like(grad)

                # تحديث الزخم
                state['momentum'].mul_(b2).add_(grad)

                # انحلال الوزن
                if wd != 0:
                    p.data.mul_(1 - lr * wd)

                # إجراء التعامد لـ Muon (التكيف مع الرتبة)
                grad_vec = state['momentum'].view(-1, grad.shape[-1])
                p_vec = p.data.view(-1, p.shape[-1])

                # Newton-Schulz لتقريب الجذر التربيعي للمصفوفة (مبسط)
                G = grad_vec @ grad_vec.t() / grad_vec.shape[0]
                # في التنفيذ الكامل، هذا تكراري؛ هنا، نقرب باستخدام متسلسلة القوى
                sqrt_G = torch.sqrt(G + 1e-6 * torch.eye(G.shape[0], device=G.device))

                # التحديث
                update = grad_vec.t() @ sqrt_G @ grad_vec / sqrt_G.shape[0]
                p.data.sub_(lr_b * update.view(p.shape))

# مثال على الاستخدام
model = nn.Linear(768, 768)  # طبقة كثيفة
optimizer = Muon(model.parameters(), lr=0.01)
loss_fn = nn.MSELoss()
data = torch.randn(32, 768)
target = torch.randn(32, 768)

for epoch in range(10):
    optimizer.zero_grad()
    output = model(data)
    loss = loss_fn(output, target)
    loss.backward()
    optimizer.step()
    print(f"Epoch {epoch+1}, Loss: {loss.item():.4f}")
```

**ملاحظات على الكود**:
- هذا إصدار مبسط؛ يتضمن مستودع GitHub الكامل نواة CUDA للكفاءة (على سبيل المثال، Flash-Muon لعمليات ضرب المصفوفات الأسرع) [3][4].
- لـ Keras/TensorFlow: استخدم `from keras.optimizers import Muon; opt = Muon(learning_rate=0.01)` [8].
- المعاملات الفائقة المضبوطة (مثل `lr_b=2e-3`) crucial؛ جربها مع نموذجك. بالنسبة لـ MuonClip الخاص بـ Kimi K2، لا يوجد كود عام، ولكنه يضيف منطق القص في تمريرات الاهتمام الأمامية [2][5].

إذا كنت بحاجة إلى امتدادات أو معايير، دعني أعرف!

المراجع:
[1] https://kellerjordan.github.io/posts/muon/
[2] https://jeremybernste.in/writing/deriving-muon
[3] https://github.com/KellerJordan/Muon
[4] https://github.com/nil0x9/flash-muon
[5] https://www.lakernewhouse.com/writing/muon-2
[6] https://medium.com/@kyeg/building-the-muon-optimizer-in-pytorch-a-geometric-approach-to-neural-network-optimization-17f4601be548
[7] https://discuss.huggingface.co/t/tutorial-understanding-and-implementing-the-muon-optimizer/167717
[8] https://keras.io/api/optimizers/muon/