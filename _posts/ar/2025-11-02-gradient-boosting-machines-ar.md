---
audio: false
generated: true
image: false
lang: ar
layout: post
model: x-ai/grok-4-fast
title: تقديم آلات تعزيز التدرج
translated: true
type: note
---

### نظرة عامة
قدمت الورقة البحثية لعام 2001 بعنوان *"Greedy Function Approximation: A Gradient Boosting Machine"* بقلم Jerome H. Friedman آلات تعزيز التدرج (GBMs)، وهي طريقة تعلم تجميعية قوية للمهام الخاضعة للإشراف مثل الانحدار والتصنيف. تعرض هذه الورقة التعزيز كشكل من نزول التدرج الوظيفي، حيث يتم إضافة متعلمين "ضعفاء" بسيط (غالبًا أشجار القرار) بشكل تسلسلي إلى نموذج جمعي لتقليل دالة الخسارة المحددة. تعمم هذه المقاربة خوارزميات التعزيز السابقة (مثل AdaBoost) وتؤكد على التحسين الجشع في فضاء الدوال، مما يؤدي إلى نماذج عالية الدقة وقوية وقابلة للتفسير.

### ملخص (معاد صياغته)
تبني نماذج GBM نماذج تنبؤية مرنة من خلال الجمع بين المتعلمين الضعفاء بطريقة تسلسلية جمعية لتقريب مُصغِر دالة الخسارة القابلة للاشتقاق. يؤدي استخدام أشجار الانحدار كمتعلمين أساسيين إلى إجراءات تنافسية وقوية للانحدار والتصنيف. تفوقت الطريقة على البدائل مثل multivariate adaptive regression splines (MARS) في الاختبارات التجريبية، بمعدلات خطأ منخفضة عبر مجموعات بيانات متنوعة.

### الطرق الرئيسية
الفكرة الأساسية هي ملاءمة متعلمين جدد بشكل متكرر *للتدرج السالب* (الشبه باقية) للخسارة فيما يتعلق بتوقعات النموذج الحالي، محاكيةً نزول التدرج في فضاء الدوال.

- **هيكل النموذج**: النموذج النهائي هو \\( F_M(x) = \sum_{m=1}^M \beta_m h_m(x) \\)، حيث كل \\( h_m(x) \\) هو متعلم ضعيف (مثل شجرة انحدار صغيرة).
- **قاعدة التحديث**: عند التكرار \\( m \\)، احسب البواقي \\( r_{im} = -\left[ \frac{\partial L(y_i, F(x_i))}{\partial F(x_i)} \right]_{F=F_{m-1}} \\)، ثم أملِئ \\( h_m \\) لهذه البواقي عبر المربعات الصغرى. يتم تحسين حجم الخطوة \\( \gamma_m \\) بالبحث في الخط: \\( \gamma_m = \arg\min_\gamma \sum_i L(y_i, F_{m-1}(x_i) + \gamma h_m(x_i)) \\).
- **الانكماش**: قم بقياس الإضافات بواسطة \\( \nu \in (0,1] \\) (مثل \\( \nu = 0.1 \\)) لتقليل التجاوز والسماح بمزيد من التكرارات.
- **المتغير العشوائي**: أخذ عينة فرعية من البيانات (مثل 50%) في كل خطوة لتدريب أسرع وتعميم أفضل.
- **خوارزمية TreeBoost** (ملخص بالشبه كود):
  1. ابدأ بـ \\( F_0(x) \\) كقيمة ثابتة تُصغِر الخسارة.
  2. من أجل \\( m = 1 \\) إلى \\( M \\):
     - احسب الشبه باقي \\( r_{im} \\).
     - أملِئ شجرة \\( h_m \\) إلى \\( \{ (x_i, r_{im}) \} \\).
     - ابحث عن الـ \\( \gamma_m \\) الأمثل عبر البحث في الخط.
     - حدّث \\( F_m(x) = F_{m-1}(x) + \nu \gamma_m h_m(x) \\).
  3. توقف بناءً على التكرارات أو تحسن الخسارة.

تشمل الخسائر المدعومة:
- المربعات الصغرى (الانحدار): \\( L(y, F) = \frac{1}{2}(y - F)^2 \\)، البواقي = \\( y - F \\).
- الانحراف المطلق الأدنى (انحدار قوي): \\( L(y, F) = |y - F| \\).
- الاحتمال اللوغاريتمي (التصنيف الثنائي): \\( L = -\sum [y \log p + (1-y) \log(1-p)] \\)، مع \\( p = \frac{1}{1 + e^{-F}} \\)؛ البواقي = \\( y - p \\).
- خسارة Huber (مقاومة للقيم المتطرفة).

تتكيف متغيرات مثل LogitBoost مع هذا لخسائر محددة (مثل الانحراف ذي الحدين).

### الإسهامات
- **إطار عمل موحد**: يمتد بالتعزيز إلى أي خسارة قابلة للاشتقاق عبر التدرجات، موحدًا AdaBoost (خسارة أسية) وLogitBoost.
- **تحسينات عملية**: تقدم الانكماش وأخذ العينات العشوائية الفرعية لتحسين التحكم في التجاوز والحساب.
- **المرونة**: تعمل مع متعلمين أساسيين ومهام متنوعة (انحدار، تصنيف، ترتيب); تمكن الأشجار من معالجة البيانات عالية الأبعاد بكفاءة.
- **أدوات القابلية للتفسير**: تناقش أهمية المتغير (عن طريق إجمالي تقليل الشوائب) ومخططات الاعتماد الجزئي.

### النتائج الرئيسية
- **الأداء**: تتفوق نماذج GBM تجريبيًا، حيث تقلل معدلات الخطأ بنسبة 10-50% عن الخطوط الأساسية مثل الأشجار المفردة، والتجميع، والشبكات العصبية، وMARS على مجموعات بيانات من العالم الحقيقي (مثل التمويل، محاكيات الفيزياء).
- **المتانة**: يتقارب GBM العشوائي بشكل أسرع; تعزز الأشجار الأعمق (6-8 مستويات) الدقة ولكنها تحتاج إلى تنظيم. تقلل خسائر LAD/Huber من تأثير القيم المتطرفة بشكل أفضل من المربعات الصغرى.
- **الكفاءة**: سريعة للبيانات الكبيرة; تستخدم الإعدادات المثلى \\( \nu = 0.1 \\)، 1000+ تكرار، وأخذ عينات فرعية.
- تظهر المخططات تقارب خسارة سريع وتباين/انحياز منخفض مقارنة بالبدائل.

هذه الورقة وضعت الأساس للتطبيقات الحديثة مثل XGBoost وLightGBM.

### المراجع
- [Greedy Function Approximation: A Gradient Boosting Machine (PDF)](https://jerryfriedman.su.domains/ftp/trebst.pdf)