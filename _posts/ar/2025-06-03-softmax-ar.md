---
audio: false
generated: true
lang: ar
layout: post
title: دالة سوفتماكس في التعلم العميق
translated: true
type: note
---

### ما هو Softmax؟

دالة **Softmax** هي دالة رياضية تُستخدم في تعلم الآلة، وخاصة في التعلم العميق، لتحويل متجه من الدرجات الأولية (غالباً ما تسمى logits) إلى توزيع احتمالي. تضمن أن قيم المخرجات تكون موجبة ومجموعها يساوي 1، مما يجعلها قابلة للتفسير كاحتمالات.

تُعرّف دالة Softmax على النحو التالي لمتجه \\( z = [z_1, z_2, \dots, z_n] \\):

\\[
\text{Softmax}(z_i) = \frac{e^{z_i}}{\sum_{j=1}^n e^{z_j}}
\\]

حيث:
- \\( z_i \\): الدرجة المدخلة (logit) للفئة \\( i \\).
- \\( e^{z_i} \\): الأس للدرجة المدخلة، مما يضمن الإيجابية.
- \\( \sum_{j=1}^n e^{z_j} \\): مجموع الأسس لجميع الدرجات المدخلة، يُستخدم للتطبيع.
- المخرج \\( \text{Softmax}(z_i) \\) يمثل احتمال الفئة \\( i \\).

الخصائص الرئيسية:
- **نطاق المخرج**: كل قيمة مخرجات تكون بين 0 و 1.
- **المجموع يساوي 1**: مجموع كل قيم المخرجات يساوي 1، مما يجعله توزيعاً احتماليًا صالحًا.
- **يُضخم الفروقات**: الدالة الأسية في Softmax تُبرز قيم الإدخال الأكبر، مما يجعل احتمالات المخرجات أكثر حسمًا للـ logits الأكبر.

### كيف يتم تطبيق Softmax في التعلم العميق

تُستخدم دالة Softmax بشكل شائع في **الطبقة الأخيرة** من الشبكات العصبية لمهام **التصنيف متعدد الفئات**. إليك كيف يتم تطبيقها:

1. **السياق في الشبكات العصبية**:
   - في الشبكة العصبية، تنتج الطبقة الأخيرة غالبًا درجات أولية (logits) لكل فئة. على سبيل المثال، في مشكلة تصنيف ذات 3 فئات (مثل: قطة، كلب، عصفور)، قد تخرج الشبكة بقيم logits مثل \\([2.0, 1.0, 0.5]\\).
   - هذه الـ logits ليست قابلة للتفسير مباشرة كاحتمالات لأنها يمكن أن تكون سالبة، غير محدودة، ولا مجموعها يساوي 1.

2. **دور Softmax**:
   - تحول دالة Softmax هذه الـ logits إلى احتمالات. للمثال أعلاه:
     \\[
     \text{Softmax}([2.0, 1.0, 0.5]) = \left[ \frac{e^{2.0}}{e^{2.0} + e^{1.0} + e^{0.5}}, \frac{e^{1.0}}{e^{2.0} + e^{1.0} + e^{0.5}}, \frac{e^{0.5}}{e^{2.0} + e^{1.0} + e^{0.5}} \right]
     \\]
     قد ينتج عن هذا احتمالات مثل \\([0.665, 0.245, 0.090]\\)، مما يشير إلى احتمال 66.5% للفئة 1 (قطة)، و 24.5% للفئة 2 (كلب)، و 9.0% للفئة 3 (عصفور).

3. **التطبيقات**:
   - **التصنيف متعدد الفئات**: يُستخدم Softmax في مهام مثل تصنيف الصور (مثل التعرف على الأشياء في الصور)، أو معالجة اللغة الطبيعية (مثل تحليل المشاعر مع فئات متعددة)، أو أي مشكلة حيث يجب تعيين مدخل إلى واحدة من عدة فئات.
   - **حساب الخسارة**: عادةً ما يُقترن Softmax مع دالة **الخسارة الانتروبيا المتقاطعة**، التي تقيس الفرق بين توزيع الاحتمال المتوقع والتوزيع الحقيقي (التسميات المشفرة one-hot). توجه هذه الخسارة تدريب الشبكة العصبية.
   - **اتخاذ القرار**: يمكن استخدام احتمالات المخرجات لاختيار الفئة الأكثر احتمالاً (مثل أخذ الفئة ذات الأعلى احتمال).

4. **أمثلة في التعلم العميق**:
   - **تصنيف الصور**: في الشبكة العصبية التلافيفية (CNN) مثل ResNet، تنتج الطبقة المتصلة بالكامل الأخيرة logits لكل فئة (مثل 1000 فئة في ImageNet). يحول Softmax هذه إلى احتمالات للتنبؤ بالجسم في الصورة.
   - **معالجة اللغة الطبيعية**: في نماذج مثل المحولات (transformers) (مثل BERT)، يُستخدم Softmax في طبقة المخرجات لمهام مثل تصنيف النص أو التنبؤ بالكلمة التالية، حيث تكون هناك حاجة للاحتمالات عبر مفردات أو مجموعة من الفئات.
   - **تعلم التعزيز**: يمكن استخدام Softmax لتحويل درجات الإجراءات إلى احتمالات لاختيار الإجراءات في طريقة قائمة على السياسة.

5. **التنفيذ في الأطر**:
   - في أطر العمل مثل **PyTorch** أو **TensorFlow**، غالبًا ما يتم تنفيذ Softmax كدالة مدمجة:
     - PyTorch: `torch.nn.Softmax(dim=1)` أو `torch.nn.functional.softmax()`
     - TensorFlow: `tf.nn.softmax()`
   - تجمع العديد من أطر العمل Softmax مع خسارة الانتروبيا المتقاطعة في عملية واحدة (مثل `torch.nn.CrossEntropyLoss` في PyTorch) للاستقرار العددي، حيث أن حساب Softmax بشكل منفصل يمكن أن يؤدي إلى مشاكل مثل الفائض (overflow) مع الـ logits الكبيرة.

### اعتبارات عملية
- **الاستقرار العددي**: الحساب المباشر لـ Softmax يمكن أن يؤدي إلى الفائض بسبب الدالة الأسية. حيلة شائعة هي طرح أكبر قيمة logit من جميع قيم الـ logits (\\( z_i - \max(z) \\)) قبل تطبيق Softmax، وهو ما لا يغير المخرجات ولكنه يمنع الأسس الكبيرة.
- **Softmax مقابل Sigmoid**: لـ **التصنيف الثنائي**، غالبًا ما تُستخدم دالة sigmoid بدلاً من Softmax، لأنها تتعامل مع فئتين بكفاءة أكبر. يعمم Softmax دالة sigmoid إلى فئات متعددة.
- **المحددات**:
  - يفترض Softmax التبادل المتبادل (فئة واحدة صحيحة). للتصنيف متعدد العلامات (حيث يمكن أن تكون فئات متعددة صحيحة)، يُفضل استخدام sigmoid.
  - يمكن أن يكون Softmax واثقًا بشكل مفرط في التنبؤات بسبب الدالة الأسية، والتي قد تضخم الفروق الصغيرة في الـ logits.

### مثال على الحساب
لنفترض أن شبكة عصبية أخرجت قيم logits \\([1.5, 0.8, -0.2]\\) لمشكلة ذات 3 فئات:
1. احسب الأسس: \\( e^{1.5} \approx 4.482, e^{0.8} \approx 2.225, e^{-0.2} \approx 0.819 \\).
2. اجمع الأسس: \\( 4.482 + 2.225 + 0.819 = 7.526 \\).
3. احسب الاحتمالات:
   - الفئة 1: \\( \frac{4.482}{7.526} \approx 0.596 \\)
   - الفئة 2: \\( \frac{2.225}{7.526} \approx 0.296 \\)
   - الفئة 3: \\( \frac{0.819}{7.526} \approx 0.109 \\)
4. المخرجات: \\([0.596, 0.296, 0.109]\\)، وهو توزيع احتمالي صالح.

### تصور Softmax
لتوضيح كيف تحول Softmax الـ logits إلى احتمالات، ضع في الاعتبار المخطط التالي الذي يظهر الـ logits والاحتمالات المقابلة لها بعد تطبيق Softmax:

```chartjs
{
  "type": "bar",
  "data": {
    "labels": ["Class 1", "Class 2", "Class 3"],
    "datasets": [
      {
        "label": "Logits",
        "data": [1.5, 0.8, -0.2],
        "backgroundColor": "rgba(75, 192, 192, 0.6)",
        "borderColor": "rgba(75, 192, 192, 1)",
        "borderWidth": 1
      },
      {
        "label": "Softmax Probabilities",
        "data": [0.596, 0.296, 0.109],
        "backgroundColor": "rgba(255, 99, 132, 0.6)",
        "borderColor": "rgba(255, 99, 132, 1)",
        "borderWidth": 1
      }
    ]
  },
  "options": {
    "scales": {
      "y": {
        "beginAtZero": true,
        "title": {
          "display": true,
          "text": "Value"
        }
      },
      "x": {
        "title": {
          "display": true,
          "text": "Classes"
        }
      }
    },
    "plugins": {
      "legend": {
        "display": true
      },
      "title": {
        "display": true,
        "text": "Logits vs. Softmax Probabilities"
      }
    }
  }
}
```

يقارن هذا المخطط الـ logits الأولية مع الاحتمالات بعد تطبيق Softmax، مسلطًا الضوء على كيف يقوم Softmax بتطبيع القيم إلى توزيع احتمالي.

### الملخص
دالة Softmax هي حجر أساس في التصنيف متعدد الفئات في التعلم العميق، حيث تحول الدرجات الأولية إلى توزيع احتمالي. تُستخدم على نطاق واسع في طبقات مخرجات الشبكات العصبية لمهام مثل تصنيف الصور والنص، مما يمكن النماذج من تعيين احتمالات لفئات متعددة وتسهيل التدريب مع دوال الخسارة مثل الانتروبيا المتقاطعة. طبيعتها الأسية تجعلها حساسة للفروق في الـ logits، ويضمن التنفيذ الحذر الاستقرار العددي.