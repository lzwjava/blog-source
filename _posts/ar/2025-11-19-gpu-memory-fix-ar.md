---
audio: false
generated: true
image: false
lang: ar
layout: post
model: x-ai/grok-4-fast
title: إصلاح أخطاء ذاكرة وحدة معالجة الرسوميات
translated: true
type: note
---

نعم، أنت تواجه خطأ في نفاد الذاكرة (OOM) لأن حتى نموذج "125M" من نوع nanoGPT بإعداداتك الحالية يدفع حدود بطاقة GPU سعة 12 جيجابايت (مثل 3090/4090/3090 Ti/A6000). دعنا نحلل بالضبط ما الذي يستخدم VRAM وما الذي يجب أن تغيره أولاً.

### تقدير استهلاك VRAM الحالي (بإعداداتك)
- حجم النموذج: ~124 مليون معامل حقيقي → ~500–550 ميجابايت (في FP16/BF16)
- حالات المُحسّن (AdamW): ~1–1.2 جيجابايت
- التدرجات: ~500 ميجابايت
- التفعيلات (المستهلك الرئيسي):
  - batch_size = 16
  - block_size = 1024
  - gradient_accumulation_steps = 32
  → دُفعة صغيرة من 16 تسلسل × 1024 رمز = 16384 رمز لكل عملية تمرير أمامي/خلفي
  - مع 12 طبقة، 768 بُعد، 12 رأس → التفعيلات تستهلك بسهولة 9–11 جيجابايت فقط للدفعة الصغيرة الواحدة

إذن الإجمالي → أنت قريب جدًا أو تجاوزت 12 جيجابايت، خاصة بمجرد بدء عمل النفقات العامة لـ PyTorch، وذاكرة التخزين المؤقت للتحويل، والرسوم البيانية CUDA، إلخ.

### ماذا يفعل `batch_size` فعليًا؟
`batch_size` في nanoGPT هو حجم الدُفعة الصغيرة (عدد التسلسلات التي تتم معالجتها في عملية تمرير أمامي/خلفي واحدة قبل تطبيق التدرجات).

حجم الدفعة الفعلي (الإجمالي) لديك هو:

effective_batch_size = batch_size × gradient_accumulation_steps  
= 16 × 32 = 512 تسلسل

هذا الرقم 512 هو المهم لجودة التدرج/الضجيج. الدفعة الصغيرة (16) تؤثر في الغالب على VRAM والسرعة.

### أفضل الحلول (مرتبة حسب الفعالية لبطاقتك 12 جيجابايت)

| الخيار | القيم الجديدة | الدفعة الفعالة | VRAM المُوفر | التأثير على التدريب | التوصية |
|------|------------|------------------|------------|---------------------|----------------|
| 1. تقليل الدفعة الصغيرة فقط | batch_size = 8<br>gradient_accumulation_steps = 64 | تظل 512 | ~40–50% أقل في التفعيلات → تتسع بشكل مريح | تأثير شبه معدوم على النموذج النهائي | **الخيار الأفضل** |
| 2. تقليل الدفعة الصغيرة أكثر | batch_size = 4<br>gradient_accumulation_steps = 128 | تظل 512 | أكثر أمانًا، أبطأ قليلاً | لا يزال الفرق ضئيلاً | إذا استمر OOM |
| 3. تقليل الاثنين (أسهل ولكن أسوأ قليلاً) | batch_size = 8<br>gradient_accumulation_steps = 32 | تصبح 256 | توفير كبير في VRAM | تدرجات أكثر ضجيجًا، قد تحتاج 10–20% خطوات أكثر للتقارب | مقبول |
| 4. تقليل حجم النموذج | n_layer=9 or n_embd=512 | نموذج أصغر | توفير هائل في VRAM | تفقد السعة | فقط إذا أردت تجارب سريعة جدًا |

### الإعداد الموصى به والذي سيتسع بالتأكيد على 12 جيجابايت
```python
batch_size = 8                    # ← التغيير الرئيسي (كان 16)
gradient_accumulation_steps = 64  # ← زيادة للحفاظ على الدفعة الفعالة 512
block_size = 1024                 # ابق كما هو
# كل شيء آخر بدون تغيير
```

سيؤدي هذا إلى تقليل ذاكرة التفعيلات تقريبًا إلى النمع مع الحفاظ على نفس جودة التدرج. سيكون التدريب أبطأ بنحو ~10–15% في الوقت الفعلي (المزيد من خطوات التجميع = المزيد من النفقات العامة لـ Python)، ولكنه سيعمل بالفعل بدلاً من التعطل.

### الوضع الآمن الإضافي (إذا استمر OOM بعد ما سبق)
```python
batch_size = 4
gradient_accumulation_steps = 128   # الدفعة الفعالة تظل 512
# أو حتى batch_size=2 + 256 إذا كانت بطاقتك محدودة تمامًا بـ 12 جيجابايت
```

### تعديلات صغيرة أخرى توفر بضع مئات من الميجابايت
```python
compile = False          # التحويل يمكنه استخدام 1–2 جيجابايت إضافية في ذاكرة التخزين المؤقت في التشغيل الأول
torch.set_float32_matmul_precision('medium')  # احذفها أو اضبطها على 'high' إذا كانت مضبوطة لديك
```

### TL;DR (ملخص سريع)
نعم — غيّر batch_size إلى 8 و gradient_accumulation_steps إلى 64.  
ستحافظ على نفس حجم الدفعة الفعالة (512)، نفس جودة النموذج، نفس عدد الرموز التي تتم رؤيتها، لكن VRAM سينخفض بما يكفي ليتسع بشكل مريح على 12 جيجابايت.

ستكون بخير بعد هذا التغيير. تدريب سعيد!