---
audio: false
generated: true
lang: ar
layout: post
title: آلية كيو كي في في المحولات
translated: true
type: note
---

آلية **المفتاح (K)، والاستعلام (Q)، والقيمة (V)** هي مكون أساسي لآلية **الانتباه الذاتي** في نماذج اللغة الكبيرة (LLMs) القائمة على المحولات (Transformer). هذه الآلية، التي تم تقديمها في الورقة البحثية المؤثرة *"Attention is All You Need"* (Vaswani et al., 2017)، تمكن النماذج من وزن أهمية الكلمات المختلفة في التسلسل عند معالجة النص أو توليده. أدناه، أقدم مقدمة شاملة عن كيفية عمل آلية **K, Q, V** في سياق نماذج لغة المحولات (Transformer LLMs)، covering حدسها، وصياغتها الرياضية، وتنفيذها في الانتباه الذاتي، ودورها في البنية الأوسع.

---

### 1. **الحدس وراء K, Q, V في الانتباه الذاتي**
تسمح آلية الانتباه الذاتي لنموذج المحول (Transformer) بمعالجة تسلسل الإدخال من خلال التركيز على الأجزاء ذات الصلة من التسلسل لكل كلمة (أو وحدة رمزية). مكونات **K, Q, V** هي اللبنات الأساسية لهذه العملية، مما يمكن النموذج من تحديد الأجزاء الأكثر صلة من الإدخال بشكل ديناميكي.

- **الاستعلام (Q):** يمثل "السؤال" الذي تطرحه وحدة رمزية حول الوحدات الرمزية الأخرى في التسلسل. لكل وحدة رمزية، يشفر متجه الاستعلام المعلومات التي تبحث عنها الوحدة الرمزية من بقية التسلسل.
- **المفتاح (K):** يمثل "الوصف" لكل وحدة رمزية في التسلسل. يشفر متجه المفتاح المعلومات التي يمكن أن تقدمها وحدة رمزية للآخرين.
- **القيمة (V):** تمثل المحتوى الفعلي أو المعلومات التي تحملها وحدة رمزية. بمجرد أن يحدد النموذج الوحدات الرمزية ذات الصلة (عبر تفاعلات Q و K)، فإنه يسترد متجهات القيمة المقابلة لبناء الناتج.

يحدد التفاعل بين **Q** و **K** مقدار الاهتمام الذي يجب أن تدفعه كل وحدة رمزية لكل وحدة رمزية أخرى، ثم يتم ترجيح متجهات **V** ودمجها بناءً على هذا الاهتمام لإنتاج الناتج لكل وحدة رمزية.

فكر في الأمر مثل البحث في المكتبة:
- **الاستعلام (Query)**: استعلام البحث الخاص بك (مثل "التعلم الآلي").
- **المفتاح (Key)**: عناوين الكتب أو البيانات الوصفية في المكتبة، والتي تقارنها باستعلامك للعثور على الكتب ذات الصلة.
- **القيمة (Value)**: المحتوى الفعلي للكتب التي تستردها بعد تحديد الكتب ذات الصلة.

---

### 2. **كيف تعمل K, Q, V في الانتباه الذاتي**
تحسب آلية الانتباه الذاتي مجموعًا مرجحًا لمتجهات **القيمة (V)**، حيث يتم تحديد الأوزان من خلال التشابه بين متجهات **الاستعلام (Q)** و **المفتاح (K)**. فيما يلي تفصيل خطوة بخطوة للعملية:

#### الخطوة 1: تمثيل الإدخال
- الإدخال إلى طبقة المحول (Transformer) هو تسلسل من الوحدات الرمزية (مثل الكلمات أو أجزاء الكلمات)، كل منها ممثلة بمتجه تضمين عالي الأبعاد (مثل البعد \\( d_{\text{model}} = 512 \\)).
- لتسلسل مكون من \\( n \\) وحدة رمزية، يكون الإدخال عبارة عن مصفوفة \\( X \in \mathbb{R}^{n \times d_{\text{model}}} \\)، حيث كل صف هو تضمين وحدة رمزية.

#### الخطوة 2: التحويلات الخطية لتوليد K, Q, V
- لكل وحدة رمزية، يتم حساب ثلاثة متجهات: **الاستعلام (Q)**، و **المفتاح (K)**، و **القيمة (V)**. يتم الحصول عليها عن طريق تطبيق تحويلات خطية مُتعلمة على تضمينات الإدخال:
  \\[
  Q = X W_Q, \quad K = X W_K, \quad V = X W_V
  \\]
  - \\( W_Q, W_K, W_V \in \mathbb{R}^{d_{\text{model}} \times d_k} \\) هي مصفوفات أوزان مُتعلمة.
  - عادةً، \\( d_k = d_v \\)، وغالبًا ما يتم ضبطها على \\( d_{\text{model}} / h \\) (حيث \\( h \\) هو عدد رؤوس الانتباه، الموضح لاحقًا).
  - النتيجة هي:
    - \\( Q \in \mathbb{R}^{n \times d_k} \\): مصفوفة الاستعلام لجميع الوحدات الرمزية.
    - \\( K \in \mathbb{R}^{n \times d_k} \\): مصفوفة المفتاح لجميع الوحدات الرمزية.
    - \\( V \in \mathbb{R}^{n \times d_v} \\): مصفوفة القيمة لجميع الوحدات الرمزية.

#### الخطوة 3: حساب درجات الانتباه
- تحسب آلية الانتباه مقدار اهتمام كل وحدة رمزية بكل وحدة رمزية أخرى عن طريق حساب **الضرب النقطي** بين متجه الاستعلام لوحدة رمزية واحدة ومتجهات المفتاح لجميع الوحدات الرمزية:
  \\[
  \text{درجات الانتباه} = Q K^T
  \\]
  - هذا ينتج مصفوفة \\( \in \mathbb{R}^{n \times n} \\)، حيث يمثل كل إدخال \\( (i, j) \\) التشابه غير المعياري بين استعلام الوحدة الرمزية \\( i \\) ومفتاح الوحدة الرمزية \\( j \\).
- لتحقيق استقرار التدرجات ومنع القيم الكبيرة، يتم قياس الدرجات بالجذر التربيعي لبعد المفتاح:
  \\[
  \text{الدرجات المعيارية} = \frac{Q K^T}{\sqrt{d_k}}
  \\]
  - هذا يسمى **الانتباه بالضرب النقطي المعياري (scaled dot-product attention)**.

#### الخطوة 4: تطبيق Softmax للحصول على أوزان الانتباه
- يتم تمرير الدرجات المعيارية عبر دالة **softmax** لتحويلها إلى احتمالات (أوزان انتباه) مجموعها 1 لكل وحدة رمزية:
  \\[
  \text{أوزان الانتباه} = \text{softmax}\left( \frac{Q K^T}{\sqrt{d_k}} \right)
  \\]
  - النتيجة هي مصفوفة \\( \in \mathbb{R}^{n \times n} \\)، حيث يمثل كل صف توزيع الانتباه لوحدة رمزية على جميع الوحدات الرمزية في التسلسل.
  - تشير أوزان الانتباه العالية إلى أن الوحدات الرمزية المقابلة ذات صلة عالية ببعضها البعض.

#### الخطوة 5: حساب الناتج
- يتم استخدام أوزان الانتباه لحساب مجموع مرجح لمتجهات **القيمة (V)**:
  \\[
  \text{ناتج الانتباه} = \text{softmax}\left( \frac{Q K^T}{\sqrt{d_k}} \right) V
  \\]
  - الناتج هو مصفوفة \\( \in \mathbb{R}^{n \times d_v} \\)، حيث كل صف هو تمثيل جديد لوحدة رمزية، يدمج معلومات من جميع الوحدات الرمزية الأخرى بناءً على أهميتها.

#### الخطوة 6: الانتباه متعدد الرؤوس
- في الممارسة العملية، تستخدم المحولات **الانتباه متعدد الرؤوس**، حيث يتم تنفيذ العملية المذكورة أعلاه عدة مرات بالتوازي (باستخدام \\( W_Q, W_K, W_V \\) مختلفة) لالتقاط أنواع مختلفة من العلاقات:
  - يتم تقسيم الإدخال إلى \\( h \\) رأس، لكل منها متجهات \\( Q, K, V \\) أصغر ببعد \\( d_k = d_{\text{model}} / h \\).
  - يحسب كل رأس ناتج الانتباه الخاص به.
  - يتم ربط مخرجات جميع الرؤوس وتمريرها عبر تحويل خطي نهائي:
    \\[
    \text{MultiHead}(Q, K, V) = \text{Concat}(\text{head}_1, \text{head}_2, \dots, \text{head}_h) W_O
    \\]
    حيث \\( W_O \in \mathbb{R}^{h \cdot d_v \times d_{\text{model}}} \\) هي مصفوفة إسقاط ناتج مُتعلمة.

---

### 3. **دور K, Q, V في نماذج لغة المحولات (Transformer LLMs)**
يتم استخدام آلية **K, Q, V** في أجزاء مختلفة من بنية المحول (Transformer)، اعتمادًا على نوع الانتباه:

- **الانتباه الذاتي في المشفر (مثل BERT):**
  - جميع الوحدات الرمزية تنتبه إلى جميع الوحدات الرمزية الأخرى في تسلسل الإدخال (انتباه ثنائي الاتجاه).
  - يتم اشتقاق \\( Q, K, V \\) جميعها من نفس تسلسل الإدخال \\( X \\).
  - هذا يسمح للنموذج بالتقاط السياق من الوحدات الرمزية السابقة واللاحقة، وهو أمر مفيد لمهام مثل تصنيف النص أو الإجابة على الأسئلة.

- **الانتباه الذاتي في فك الشفرة (مثل GPT):**
  - في النماذج التوليدية التلقائية (autoregressive) مثل GPT، يستخدم فك الشفرة **الانتباه الذاتي المقنع** لمنع الانتباه إلى الوحدات الرمزية المستقبلية (حيث يولد النموذج النص بشكل تسلسلي).
  - يضمن القناع أنه لكل وحدة رمزية \\( i \\)، يتم ضبط درجات الانتباه للوحدات الرمزية \\( j > i \\) على \\(-\infty\\) قبل softmax، مما يعطيها وزنًا صفريًا بشكل فعال.
  - لا يزال يتم اشتقاق \\( Q, K, V \\) من تسلسل الإدخال، لكن الانتباه سببي (ينتبه فقط إلى الوحدات الرمزية السابقة).

- **الانتباه المتبادل في نماذج المشفر-فك الشفرة (مثل T5):**
  - في البنى المعمارية للمشفر-فك الشفرة، يستخدم فك الشفرة الانتباه المتبادل للانتباه إلى ناتج المشفر.
  - هنا، يتم اشتقاق \\( Q \\) من إدخال فك الشفرة، بينما يأتي \\( K \\) و \\( V \\) من ناتج المشفر، مما يسمح لفك الشفرة بالتركيز على الأجزاء ذات الصلة من تسلسل الإدخال عند توليد الناتج.

---

### 4. **لماذا تعمل K, Q, V بشكل جيد جدًا**
آلية **K, Q, V** قوية لعدة أسباب:
- **التسييق الديناميكي**: تسمح لكل وحدة رمزية بجمع المعلومات من الوحدات الرمزية الأخرى بناءً على محتواها، بدلاً من الاعتماد على أنماط ثابتة (كما في RNNs أو CNNs).
- **التوازي**: على عكس الشبكات العصبية المتكررة، تعالج آلية الانتباه الذاتي جميع الوحدات الرمزية في وقت واحد، مما يجعلها فعالة للغاية على الأجهزة الحديثة مثل وحدات معالجة الرسومات (GPUs).
- **المرونة**: يمكّن الانتباه متعدد الرؤوس النموذج من التقاط علاقات متنوعة (مثل العلاقات النحوية والدلالية) من خلال تعلم إسقاطات مختلفة لـ \\( Q, K, V \\).
- **القابلية للتوسع**: تتوسع الآلية بشكل جيد للتسلسلات الطويلة (على الرغم من أن التكلفة الحسابية تنمو تربيعيًا مع طول التسلسل، ويتم التخفيف من ذلك بتقنيات مثل الانتباه المتفرع أو المحولات الفعالة).

---

### 5. **ملخص رياضي**
صيغة الانتباه بالضرب النقطي المعياري هي:
\\[
\text{Attention}(Q, K, V) = \text{softmax}\left( \frac{Q K^T}{\sqrt{d_k}} \right) V
\\]
للانتباه متعدد الرؤوس:
\\[
\text{MultiHead}(Q, K, V) = \text{Concat}(\text{head}_1, \dots, \text{head}_h) W_O
\\]
حيث:
\\[
\text{head}_i = \text{Attention}(Q W_{Q_i}, K W_{K_i}, V W_{V_i})
\\]

---

### 6. **مثال عملي**
لنأخذ الجملة: *"القط جلس على السجادة."*
- **الإدخال**: يتم تحويل كل كلمة إلى متجه تضمين (عبر طبقة تضمين الكلمات، على سبيل المثال).
- **حساب Q, K, V**: لكل وحدة رمزية، يحسب النموذج متجهات الاستعلام والمفتاح والقيمة باستخدام الأوزان المُتعلمة.
- **درجات الانتباه**: بالنسبة للكلمة "القط"، قد يعطي متجه الاستعلام درجات انتباه عالية لـ "جلس" و "السجادة" لأنهما ذات صلة سياقيًا، بينما يعطي درجات أقل لـ "ال".
- **الناتج**: تنتج آلية الانتباه تمثيلاً جديدًا لـ "القط" يدمج معلومات من "جلس" و "السجادة"، مرجحًا بدرجات الانتباه الخاصة بها.

---

### 7. **الامتدادات والتحسينات**
- **الانتباه الفعال**: للتعامل مع التسلسلات الطويلة، تقلل المتغيرات مثل Performer و Linformer و Longformer من التعقيد الحسابي للانتباه.
- **الترميزات الموضعية**: نظرًا لأن الانتباه الذاتي غير حساس لترتيب الوحدات (permutation-invariant)، تضيف المحولات ترميزات موضعية إلى تضمينات الإدخال لالتقاط ترتيب الكلمات، مما يؤثر بشكل غير مباشر على \\( Q, K, V \\).
- **تراصف الطبقات**: تضع المحولات طبقات متعددة من الانتباه الذاتي فوق بعضها البعض، مما يسمح لـ \\( Q, K, V \\) بالعمل على تمثيلات مجردة بشكل متزايد.

---

### 8. **الخلاصة**
آلية **K, Q, V** هي قلب الانتباه الذاتي في المحول (Transformer)، مما يمكن نماذج اللغة الكبيرة (LLMs) من نمذجة العلاقات المعقدة في النص بكفاءة وفعالية. من خلال حساب درجات الانتباه بناءً على الاستعلامات والمفاتيح، وتجميع المعلومات من متجهات القيمة، تحقق المحولات أداءً متقدمًا في مهام مثل نمذجة اللغة والترجمة والمزيد. يعد فهم هذه الآلية مفتاحًا لفهم كيفية معالجة نماذج اللغة الحديثة (LLMs) لتوليد نص يشبه النص البشري.

إذا كنت ترغب في الخوض بشكل أعمق في أي جانب محدد (مثل الانتباه متعدد الرؤوس، أو التنفيذ البرمجي، أو التطبيقات في نماذج محددة)، دعني أعلم!