---
audio: false
generated: true
lang: ar
layout: post
title: تحسين استدعاءات أدوات الذكاء الاصطناعي عبر التنفيذ من جانب الخادم
translated: true
type: note
---

نعم، الطبيعة التبادلية للمكالمات التقليدية للأدوات أو الوظائف في روبوتات الدردشة (حيث يولد نموذج الذكاء الاصطناعي مكالمة، ويرسلها إلى العميل أو خدمة أخرى، وينتظر التنفيذ، ثم يستلم النتائج) يمكن أن تؤدي بالفعل إلى زيادة زمن الوصول، خاصة في التطبيقات التي تعمل في الوقت الفعلي. غالبًا ما يكون هذا بسبب النفقات العامة للشبكة، والتسلسل/إلغاء التسلسل، والحاجة إلى جولات متعددة من واجهات برمجة التطبيقات. اقتراحك باستخدام دوال لامدا أو تصميم واجهة برمجة تطبيقات تقبل كود بايثون للتنفيذ من جانب الخادم هو طريقة ذكية للتخفيف من هذا - أي نقل الحساب إلى الخادم لتقليل التفاعلات بين العميل والخادم. سأقوم بتحليل هذا من الناحية المفاهيمية، ومناقشة الجدوى، والتطبيقات المحتملة، والمفاضلات.

### لماذا يمكن أن تكون مكالمات الأدوات التقليدية بطيئة
في العديد من أطر عمل الذكاء الاصطناعي (مثل LangChain، أو استدعاء الدوال في OpenAI، أو أدوات Anthropic):
- ينتج النموذج استدعاء أداة منظمًا (مثل JSON مع اسم الدالة والوسائط).
- ينفذ العميل (أو الوكيل) الدالة محليًا أو عبر واجهة برمجة تطبيقات أخرى.
- تُضاف النتائج إلى سجل المحادثة وتُرسل مرة أخرى إلى النموذج لخطوة الاستدلال التالية.
هذه الحلقة يمكن أن تضيف ثوانٍ من التأخير لكل دورة، مما يزيد الأمر سوءًا في المهام المعقدة مثل تحليل البيانات أو التفكير متعدد الخطوات.

### استخدام دوال لامدا أو تنفيذ الكود من جانب الخادم
فكرتك تتماشى مع نماذج التنفيذ "بلا خادم" أو "المعزولة"، حيث يولد الذكاء الاصطناعي كودًا (أو snippet يشبه اللامدا) يتم تشغيله مباشرة على الخادم الذي يستضيف النموذج. هذا يبقي كل شيء في بيئة واحدة، مما يقلل جولات الاتصال إلى مجرد مكالمة واحدة لواجهة برمجة التطبيقات من المستخدم في أحسن الأحوال.

- **نهج دوال لامدا**: تسمح خدمات مثل AWS Lambda، أو Google Cloud Functions، أو Azure Functions بتنفيذ snippets صغيرة ومؤقتة من كود بايثون عند الطلب دون الحاجة إلى إدارة خوادم. في سياق الذكاء الاصطناعي:
  - يمكن للواجهة الخلفية للروبوت الدردش أن تغلف نموذج الذكاء الاصطناعي (عبر OpenAI API على سبيل المثال) وتدمج Lambda كأداة.
  - يولد النموذج تعبير لامدا أو دالة قصيرة، يتم استدعاؤها من جانب الخادم.
  - الإيجابيات: قابلة للتطوير، الدفع حسب الاستخدام، ووقت تشغيل سريع (غالبًا <100 ميلي ثانية للبدء البارد).
  - السلبيات: وقت تنفيذ محدود (مثل 15 دقيقة كحد أقصى على AWS)، وتحتاج إلى إدارة الحالة إذا امتدت المهمة عبر عدة استدعاءات.
  - مثال: يمكن لوكيل ذكاء اصطناعي أن يولد لامدا لمعالجة البيانات (مثل `lambda x: sum(x) if isinstance(x, list) else 0`)، ويرسلها إلى نقطة نهاية Lambda، ويحصل على النتائج مباشرة.

- **تصميم واجهة برمجة تطبيقات تقبل وتنفذ كود بايثون**:
  - نعم، هذا ممكن تمامًا وهو موجود بالفعل في أنظمة الإنتاج. المفتاح هو **العزل** لمنع المخاطر الأمنية مثل تنفيذ الكود التعسفي (مثل حذف الملفات أو إجراء مكالمات شبكية).
  - آلية العمل: تستقبل نقطة نهاية واجهة برمجة التطبيقات snippet كود (كسلسلة نصية)، وتقوم بتشغيلها في بيئة معزولة، وتلتقط الناتج/الأخطاء، وتعيد النتائج. يمكن لنموذج الذكاء الاصطناعي أن يولد وي "ستدعي" هذا الكود بشكل تكراري دون مغادرة الخادم.
  - الفوائد:
    - يقلل زمن الوصول: يحدث التنفيذ في نفس مركز البيانات الذي يوجد فيه النموذج، غالبًا في أجزاء من الثانية.
    - يمكن من مهام معقدة: مثل معالجة البيانات، محاكاة الرياضيات، أو التعامل مع الملفات دون الحاجة إلى أدوات خارجية.
    - جلسات ذات حالة: بعض التطبيقات تحافظ على بيئة تشبه REPL عبر المكالمات.
  - الإجراءات الأمنية:
    - استخدام الحاويات (Docker)، أو micro-VMs (Firecracker)، أو مترجمات بايثون مقيدة (مثل PyPy sandboxing أو globals مقيدة).
    - تقييد الموارد: حصص CPU/الوقت، عدم الوصول إلى الشبكة، وحدات مسموح بها فقط (مثل numpy، pandas، ولكن ليس os أو subprocess).
    - توفر مكتبات مثل `restrictedpython` أو أدوات مثل E2B/Firecracker بيئات معزولة جاهزة.

### أمثلة وتطبيقات من العالم الحقيقي
تدعم عدة منصات ذكاء اصطناعي هذا بدرجات متفاوتة:
- **Assistants API من OpenAI مع Code Interpreter**: يسمح للنموذج بكتابة وتشغيل كود بايثون في بيئة معزولة على خوادم OpenAI. يمكن للنموذج رفع الملفات، وتنفيذ الكود، والتكرار على النتائج—كل ذلك من جانب الخادم. لا حاجة للتنفيذ من جانب العميل.
- **تنفيذ الكود في Gemini API من جوجل**: يوفر بيئة معزولة مدمجة لبايثون حيث يولد النموذج ويشغل الكود بشكل تكراري، ويتعلم من المخرجات دون مكالمات خارجية.
- **حلول مخصصة**:
  - **E2B Sandbox**: SDK/API لإنشاء بيئات معزولة قائمة على السحابة مع نواة Jupyter. يمكن لوكلاء الذكاء الاصطناعي إرسال كود لتشغيله بأمان، وهو مثالي لأدوات تحليل البيانات.
  - **Modal Sandboxes**: منصة لتشغيل الكود المُولد بواسطة الذكاء الاصطناعي في بيئات معزولة، غالبًا ما تُستخدم لوكلاء LLM.
  - **SandboxAI (مفتوح المصدر)**: وقت تشغيل مخصص لتنفيذ كود بايثون المُولد بواسطة الذكاء الاصطناعي في بيئات معزولة.
  - للبناء الذاتي: أنشئ خادم FastAPI أو Flask يقبل الكود عبر POST، ويستخدم `exec()` في نطاق اسمي مقيد، أو يشغل حاوية Docker لكل طلب.

من ناحية الكود، قد تبدو نقطة نهاية واجهة برمجة التطبيقات البسيطة كالتالي (كود زائف للتوضيح):

```python
from fastapi import FastAPI
import restrictedpython  # For safe execution

app = FastAPI()

@app.post("/execute")
def execute_code(code: str):
    safe_globals = {"__builtins__": restrictedpython.safe_builtins}  # Restricted env
    try:
        result = {}
        exec(code, safe_globals, result)  # Run code
        return {"output": result.get("output")}
    except Exception as e:
        return {"error": str(e)}
```

ادمج هذا مع روبوتك الدردش: يولد الذكاء الاصطناعي الكود بناءً على إدخال المستخدم، يستدعي نقطة النهاية داخليًا، ويدمج النتائج.

### العيوب المحتملة وأفضل الممارسات
- **المخاطر الأمنية**: تنفيذ الكود التعسفي خطير—عزله دائمًا. تجنبه إذا كنت تتعامل مع مستخدمين غير موثوق بهم.
- **التكلفة والقابلية للتطوير**: يستخدم التنفيذ من جانب الخادم قدرة حوسبة أكثر؛ راقب باستخدام أدوات مثل New Relic للتحسين.
- **تصحيح الأخطاء**: يمكن أن تدور الأخطاء في الكود المُولد في حلقة إذا لم يتم التعامل معها جيدًا.
- **بدائل للتحسين**:
  - تجميع استدعاءات الأدوات: جمّع وظائف متعددة في جولة اتصال واحدة بالخادم.
  - البث: استخدم واجهات برمجة التطبيقات التي تبث الردود أثناء تنفيذ الأدوات بالتوازي.
  - جانب العميل إذا كان زمن الوصول مقبولاً: للمهام غير الحساسة، نفذ محليًا.
- إذا كنت تبني من الصفر، ابدأ بأطر عمل مثل LangChain أو Vercel's AI SDK، التي تدعم أدوات مخصصة ويمكن تكوينها للتشغيل من جانب الخادم.

هذا النهج يمكن أن يجعل روبوتات الدردشة أسرع بكثير وأكثر قدرة، خاصة للمهام الحسابية. إذا كنت تنفذ هذا، ففكر في البدء بالميزات المدمجة في OpenAI أو Gemini لتحقيق نتائج سريعة.

[Building a Sandboxed Environment for AI generated Code Execution](https://anukriti-ranjan.medium.com/building-a-sandboxed-environment-for-ai-generated-code-execution-e1351301268a)  
[substratusai/sandboxai: Run AI generated code in isolated sandboxes](https://github.com/substratusai/sandboxai)  
[Build AI data analyst with sandboxed code execution using TS ... - E2B](https://e2b.dev/blog/build-ai-data-analyst-with-sandboxed-code-execution-using-typescript-and-gpt-4o)  
[Code execution | Gemini API](https://ai.google.dev/gemini-api/docs/code-execution)  
[Assistants Code Interpreter - OpenAI API](https://platform.openai.com/docs/assistants/tools/code-interpreter)  
[Modal Sandboxes](https://modal.com/use-cases/sandboxes)  
[Optimizing AI chatbot performance with New Relic AI monitoring](https://newrelic.com/blog/how-to-relic/optimizing-ai-chatbot-performance)