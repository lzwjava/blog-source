---
audio: false
generated: true
lang: ar
layout: post
title: المحوِّل
translated: true
type: note
---

**المحول (Transformer)** هو بنية شبكة عصبية ثورية أصبحت أساس معظم نماذج اللغة الكبيرة (LLMs) الحديثة. تم تقديمه في الورقة البحثية المؤثرة عام 2017 بعنوان "Attention Is All You Need" بقلم فاسواني وآخرون، وقد غير منذ ذلك الحين مجال معالجة اللغة الطبيعية (NLP) بشكل جذري.

على عكس البنى المعمارية السائدة السابقة مثل الشبكات العصبية المتكررة (RNNs)، التي تعالج البيانات المتسلسلة خطوة بخطوة، فإن المحول يعالج تسلسلات الإدخال بالكامل **بشكل متوازٍ**. هذا الاختلاف الرئيسي يسمح بتحسينات كبيرة في سرعة التدريب والقدرة على التقاط التبعيات طويلة المدى داخل النص.

إليك نظرة متعمقة على المكونات والمفاهيم الرئيسية لبنية المحول في سياق نماذج اللغة الكبيرة:

**1. الفكرة الأساسية: آلية الانتباه (Attention Mechanism)**

الابتكار المركزي للمحول هو **آلية الانتباه**، وخاصة **الانتباه الذاتي (self-attention)**. تتيح هذه الآلية للنموذج أن يزن أهمية الكلمات (أو الرموز) المختلفة في تسلسل الإدخال عند معالجة كلمة محددة. بدلاً من الاعتماد فقط على الكلمات التي تسبقها مباشرة (مثل الشبكات العصبية المتكررة)، يمكّن الانتباه الذاتي النموذج من مراعاة السياق بأكمله لفهم المعنى والعلاقات بين الكلمات.

فكر في الأمر بهذه الطريقة: عندما تقرأ جملة، لا تعالج كل كلمة بمعزل عن غيرها. يعتبر عقلك جميع الكلمات في وقت واحد لفهم المعنى العام وكيف تساهم كل كلمة فيه. آلية الانتباه الذاتي تحاكي هذا السلوك.

**كيف يعمل الانتباه الذاتي (مبسط):**

لكل كلمة في تسلسل الإدخال، يحسب المحول ثلاثة متجهات:

*   **الاستعلام (Query - Q):** يمثل ما تبحث عنه الكلمة الحالية في الكلمات الأخرى.
*   **المفتاح (Key - K):** يمثل المعلومات التي "تحتويها" كل كلمة أخرى.
*   **القيمة (Value - V):** يمثل المعلومات الفعلية التي تحملها كل كلمة أخرى والتي قد تكون ذات صلة.

تقوم آلية الانتباه الذاتي بعد ذلك بالخطوات التالية:

1.  **حساب درجات الانتباه:** يتم حساب حاصل الضرب النقطي بين متجه الاستعلام للكلمة الحالية ومتجه المفتاح لكل كلمة أخرى في التسلسل. تشير هذه الدرجات إلى مدى أهمية معلومات كل كلمة أخرى للكلمة الحالية.
2.  **قياس الدرجات:** يتم تقسيم الدرجات على الجذر التربيعي لبُعد متجهات المفتاح (`sqrt(d_k)`). يساعد هذا القياس في استقرار المتجهات التدرجية أثناء التدريب.
3.  **تطبيق Softmax:** يتم تمرير الدرجات المُقاسَة عبر دالة softmax، التي تقوم بتطبيعها إلى احتمالات بين 0 و 1. تمثل هذه الاحتمالات **أوزان الانتباه** – أي مقدار "الانتباه" الذي يجب أن تدفعه الكلمة الحالية لكل من الكلمات الأخرى.
4.  **حساب القيم الموزونة:** يتم ضرب متجه القيمة لكل كلمة في وزن الانتباه المقابل لها.
5.  **جمع القيم الموزونة:** يتم جمع متجهات القيمة الموزونة لإنتاج **متجه الإخراج** للكلمة الحالية. يحتوي متجه الإخراج هذا الآن على معلومات من جميع الكلمات ذات الصلة في تسلسل الإدخال، مُوزونة بأهميتها.

**2. الانتباه متعدد الرؤوس (Multi-Head Attention)**

لتعزيز قدرة النموذج على التقاط أنواع مختلفة من العلاقات، يستخدم المحول **الانتباه متعدد الرؤوس**. بدلاً من أداء آلية الانتباه الذاتي مرة واحدة فقط، فإنه يؤديها عدة مرات بالتوازي مع مجموعات مختلفة من مصفوفات الأوزان للاستعلام والمفتاح والقيمة. كل "رأس" يتعلم التركيز على جوانب مختلفة من العلاقات بين الكلمات (مثل الاعتماديات النحوية، والروابط الدلالية). ثم يتم ربط مخرجات جميع رؤوس الانتباه معًا وتحويلها خطيًا لإنتاج الناتج النهائي لطبقة الانتباه متعددة الرؤوس.

**3. الترميز الموضعي (Positional Encoding)**

نظرًا لأن المحول يعالج جميع الكلمات بالتوازي، فإنه يفقد المعلومات المتعلقة **بترتيب** الكلمات في التسلسل. لمعالجة هذه المشكلة، يتم إضافة **ترميز موضعي** إلى تضمينات الإدخال. هذه الترميزات هي متجهات تمثل موقع كل كلمة في التسلسل. تكون عادةً أنماطًا ثابتة (مثل الدوال الجيبية) أو تضمينات مُتعلمة. من خلال إضافة الترميزات الموضعية، يمكن للمحول فهم الطبيعة التسلسلية للغة.

**4. مداخن المُشَفِّر (Encoder) وفَك التشفير (Decoder)**

تتكون بنية المحول عادةً من جزأين رئيسيين: **مُشَفِّر** و**فَك تشفير**، كلاهما مكون من طبقات متطابقة متعددة مكدسة فوق بعضها البعض.

*   **المُشَفِّر (Encoder):** دور المُشَفِّر هو معالجة تسلسل الإدخال وإنشاء تمثيل غني له. تحتوي كل طبقة مشفر عادةً على:
    *   طبقة فرعية لل**انتباه الذاتي متعدد الرؤوس**.
    *   طبقة فرعية ل**شبكة عصبية تقدمية (feed-forward)**.
    *   **وصلات متبقية (Residual connections)** حول كل طبقة فرعية، تليها **تطبيع طبقة (layer normalization)**. تساعد الوصلات المتبقية في تدفق التدرج أثناء التدريب، ويستقر تطبيع الطبقة عمليات التنشيط.

*   **فَك التشفير (Decoder):** دور فَك التشفير هو إنشاء تسلسل الإخراج (مثل الترجمة الآلية أو توليد النص). تحتوي كل طبقة فك تشفير عادةً على:
    *   طبقة فرعية لل**انتباه الذاتي متعدد الرؤوس المقنع (masked)**. يمنع "التقنيع" فَك التشفير من النظر إلى الرموز المستقبلية في تسلسل الهدف أثناء التدريب، مما يضمن أنه يستخدم فقط الرموز التي تم إنشاؤها مسبقًا للتنبؤ بالرمز التالي.
    *   طبقة فرعية لل**انتباه متعدد الرؤوس** تنتبه إلى ناتج المُشَفِّر. هذا يسمح لفَك التشفير بالتركيز على الأجزاء ذات الصلة من تسلسل الإدخال أثناء إنشاء الإخراج.
    *   طبقة فرعية ل**شبكة عصبية تقدمية**.
    *   **وصلات متبقية** و**تطبيع طبقة** مشابهة للمُشَفِّر.

**5. الشبكات التقدمية (Feed-Forward Networks)**

تحتوي كل طبقة من طبقات المُشَفِّر وفَك التشفير على شبكة عصبية تقدمية (FFN). يتم تطبيق هذه الشبكة على كل رمز بشكل مستقل وتساعد في معالجة التمثيلات التي تعلمتها آليات الانتباه بشكل أكبر. تتكون عادةً من تحويلين خطيين مع دالة تنشيط غير خطية (مثل ReLU) بينهما.

**كيفية استخدام المحولات في نماذج اللغة الكبيرة:**

تعتمد نماذج اللغة الكبيرة في المقام الأول على بنية المحول **ذات فَك التشفير فقط** (مثل نماذج GPT) أو بنية **المُشَفِّر-فَك التشفير** (مثل T5).

*   **نماذج فَك التشفير فقط:** يتم تدريب هذه النماذج للتنبؤ بالرمز التالي في التسلسل بالنظر إلى الرموز السابقة. تقوم بتكديس طبقات متعددة من فَك التشفير. يتم تمرير المطالبة (prompt) المدخلة عبر الطبقات، وتتنبأ الطبقة الأخيرة بتوزيع الاحتمالات عبر المفردات للرمز التالي. من خلال أخذ العينات التلقائية من هذا التوزيع، يمكن للنموذج توليد نص متماسك وذو صلة سياقية.

*   **نماذج المُشَفِّر-فَك التشفير:** تأخذ هذه النماذج تسلسل إدخال وتنشئ تسلسل إخراج. تستخدم بنية المُشَفِّر-فَك التشفير الكاملة. يعالج المُشَفِّر الإدخال، ويستخدم فَك التشفير ناتج المُشَفِّر لتوليد تسلسل الهدف. هذه النماذج مناسبة جيدًا لمهام مثل الترجمة والتلخيص والإجابة على الأسئلة.

**فهم الأهمية بعمق:**

تأثير بنية المحول على نماذج اللغة الكبيرة عميق:

*   **معالجة التبعيات طويلة المدى:** تسمح آلية الانتباه الذاتي للنموذج بربط الكلمات المتباعدة في التسلسل مباشرة، متغلبًا على قيود الشبكات العصبية المتكررة في التقاط السياق طويل المدى.
*   **المعالجة المتوازية:** معالجة التسلسل بالكامل بشكل متوازٍ تقلل بشكل كبير من وقت التدريب وتسمح باستخدام مجموعات بيانات ونماذج أكبر بكثير.
*   **القابلية للتوسع:** تتوسع بنية المحول بشكل جيد مع زيادة حجم البيانات والنموذج، مما أدى إلى تطوير نماذج لغة كبيرة قوية للغاية بمليارات أو حتى تريليونات من المعاملات.
*   **الفهم السياقي:** تمكن آلية الانتباه النموذج من فهم سياق الكلمات بطريقة أكثر دقة، مما يؤدي إلى فهم أفضل للغة وتوليدها.
*   **أساس للابتكار:** شكلت بنية المحول أساسًا للعديد من التطورات والاختلافات، مما أدى إلى المشهد المتنوع لنماذج اللغة الكبيرة الحديثة.

باختصار، غيرت بنية المحول، بآلية الانتباه الذاتي الأساسية فيها، وقدراتها على المعالجة المتوازية، وبنية المُشَفِّر-فَك التشفير (أو اختلافات فَك التشفير فقط)، مجال معالجة اللغة الطبيعية وهي القوة الدافعة وراء القدرات الرائعة لنماذج اللغة الكبيرة الحديثة.