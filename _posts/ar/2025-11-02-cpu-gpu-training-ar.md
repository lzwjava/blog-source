---
audio: false
generated: true
image: false
lang: ar
layout: post
model: x-ai/grok-4-fast
title: أداء التدريب للمعالج المركزي مقابل معالج الرسوميات
translated: true
type: note
---

انخفاض نسبة MFU (0.01%) أثناء تدريب NanoGPT على حاسوبك المحمول ناتج عن استخدام علم `--device=cpu` في أمر التشغيل، مما يجبر الحمل بالكامل على التشغيل على المعالج المركزي بدلاً من الاستفادة من أي تسريع بواسطة معالج الرسوميات. إليك تحليلًا لسبب حدوث ذلك ومقارنته بإعدادك لكرت 4070 Ti:

### الأسباب الرئيسية لانخفاض MFU على الحاسوب المحمول
- **عدم التوافق بين بنية المعالج المركزي ومعالج الرسوميات**: النماذج التحويلية مثل المستخدمة في NanoGPT (حتى مع هذا الإعداد البسيط: 4 طبقات، 128 بُعد تضمين، حجم الدُفعة 12) تعتمد بشكل كبير على عمليات المصفوفات المتوازية (مثل الانتباه وطبقات التغذية الأمامية) التي يتفوق فيها معالج الرسوميات في معالجتها عبر آلاف النوى وذاكرة النطاق الترددي العالي. أما المعالجات المركزية، حتى الحديثة منها في الحواسيب المحمولة (من المحتمل أن جهازك من فئة Intel Alder Lake-P استنادًا إلى معلومات kernel/platform)، فتعالج هذه العمليات بشكل تسلسلي أو بتوازي محدود. PyTorch على المعالج المركزي يستخدم مكتبات BLAS المحسنة (مثل OpenBLAS) ولكنه لا يزال يحقق أقل من 1% من إنتاجية FLOPs مقارنة بمعالج الرسوميات لمثل هذه النماذج. تقيس MFU *الاستفادة نسبة إلى ذروة FLOPs النظرية*، لذا فإن التشغيل المعتمد على المعالج المركزي يبلغ بشكل طبيعي قيمًا صغيرة مثل 0.01%—هذا ليس "عطلًا"، ولكنه ببساطة غير فعال لهذه المهمة.

- **عدم وجود تفريغ للحمل على معالج الرسوميات هنا**: عتاد حاسوبك المحمول (Intel UHD Graphics من فئة Alder Lake-P) لا يتوافق مع CUDA، لذا فإن PyTorch يستخدم المعالج المركزي افتراضيًا بدون تعديلات. يظهر ناتج `get_gpu_info.py` معالج رسوميات مدمج من Intel مسمى خطأً باسم "AMD" (على الأرجح خطأ في البرنامج النصي في تحليل `lspci`)، ولكن حتى لو كان قابلاً للاستخدام، فإن PyTorch القياسي لا يقوم بتسريع التدريب على معالجات الرسوميات المدمجة من Intel/AMD مباشرة. ستحتاج إلى إضافات مثل oneAPI من Intel (عبر `torch.backends.mps` أو الامتدادات) أو ROCm لمعالجات AMD، ولكن هذا لا يزال تجريبيًا ولن يضاهي أداء NVIDIA.

- **نطاق النموذج/الحمل**: هذا نموذج مصغر على مجموعة بيانات صغيرة (Shakespeare chars، block_size=64). على المعالج المركزي، فإن النفقات العامة من تحميل البيانات، وحلقات Python، والعمليات غير المرتبطة بـ FLOPs هي المهيمنة، مما يسحب MFN إلى أسفل أكثر. الإعدادات max_iters=2000 و log_interval=1 تعني نقاط فحص وتقييم متكررة، مما يضخم اختناقات المعالج المركزي.

### المقارنة مع 4070 Ti (نسبة MFU 10%)
- **فجوة إنتاجية العتاد**: يمكن لكرت 4070 Ti (سلسلة RTX 40، ~29 TFLOPs FP16) معالجة هذا النموذج بسرعة تزيد من 10 إلى 20 ضعفًا مقارنة بمعالج حاسوب محمول مركزي (~0.5-1 TFLOPs فعال لتعلم الآلة). تعتبر نسبة MFU البالغة 10% جيدة لـ NanoGPT على نموذج صغير—فهي ليست 100% بسبب النفقات العامة لإطلاق kernel، وقيود عرض النطاق الترددي للذاكرة، وأحجام الدُفعات غير المثالية. زيادة batch_size (مثلاً إلى 128+) أو استخدام FP16/bfloat16 يمكن أن ترفع النسبة إلى 15-20%، ولكن إعدادك محافظ.

- **وضع معالج الرسوميات الضمني**: في إعداد 4070 Ti، من المحتمل أنك تشغل بـ `--device=cuda` (الإفتراضي في NanoGPT إذا كان متاحًا)، مما يمكن التوازي الكامل للموترات ونواة cuBLAS/cuDNN. هذا وحده يعزز MFU عن طريق التحسين للعتاد.

| الجانب | الحاسوب المحمول (المعالج المركزي) | 4070 Ti (معالج الرسوميات) |
|--------|---------------------------------|--------------------------|
| **الجهاز** | المعالج المركزي (مُجبر) | معالج رسوميات CUDA |
| **ذروة FLOPs** | ~0.5-1 TFLOPs (محسن لتعلم الآلة) | ~29 TFLOPs (FP16) |
| **نسبة MFU المُحققة** | 0.01% (النفقات العامة للمعالج المركزي هي المهيمنة) | 10% (إمكانات التوازي تظهر) |
| **تقدير زمن التدريب (2000 تكرار)** | 10-30+ دقيقة (الإدخال/الإخراج بطيء) | <1 دقيقة (سريع جدًا) |
| **الاختناقات** | التنفيذ التسلسلي، قيود الذاكرة العشوائية | عرض نطاق الذاكرة، كفاءة kernel |

### إصلاحات سريعة لتحسين التدريب على الحاسوب المحمول
- **التبديل إلى معالج الرسوميات إذا أمكن**: إذا كان لديك إمكانية الوصول إلى كرت 4070 Ti عن بُعد (مثلاً عبر SSH + `torchrun`)، فشغل التدريب هناك. لمعالجات الرسوميات المدمجة من Intel، جرب:
  ```
  pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cpu  # تأكد من التراجع للمعالج المركزي
  # أو لـ oneAPI من Intel (تجريبي):
  conda install intel-extension-for-pytorch -c intel
  ```
  ثم اختبر `--device=cuda` (سيعطي خطأ) أو استخدم `device='xpu'` مع تفعيل IPEX. توقع تحقيق 1-5% MFU في أفضل الأحوال—لا يزال غير جيد.

- **تحسين التشغيل على المعالج المركزي**:
  - زد `batch_size` إلى 24-48 إذا كانت الذاكرة العشوائية تسمح (إعداد Ubuntu 24.04 الخاص بك يحتوي على الكثير مع glibc2.39).
  - أضف `--compile=True` (ميزة dynamo من PyTorch 2.0+ لتسريع المعالج المركزي، أسرع بنسبة ~20-50%).
  - استخدم `num_workers=4` في أداة تحميل البيانات لتوازي الإدخال/الإخراج.
  - شغل باستخدام `taskset -c 0-7` لتثبيت العمل على نوى الأداء (تحقق باستخدام `lscpu`).

- **قياس الأداء**: أعد التشغيل باستخدام `--device=cuda` على جهاز 4070 Ti لمقارنة مباشرة عادلة (يجب أن تصل إلى ~10% MFU). إذا شاركت رابط/إصدار مستودع NanoGPT المحدد، يمكنني اقتراح تعديلات على الإعدادات.

هذا السلوك متوقع—المعالجات المركزية ليست مصممة للتدريب الفعلي على نماذج تعلم الآلة. إذا استمرت المشكلة، الصق سجل التدريب الكامل لتحليل أعمق!