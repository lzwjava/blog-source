---
audio: false
generated: true
image: false
lang: ar
layout: post
model: x-ai/grok-4-fast
title: فهم طبقة الإسقاط في PyTorch
translated: true
type: note
---

### فهم `drop = nn.Dropout(config.dropout)`

في PyTorch (مكتبة التعلم العميق الشهيرة)، `nn.Dropout` هي وحدة مضمنة تُستخدم لتطبيق **التنظيم بالإسقاط (Dropout Regularization)** أثناء تدريب النموذج. تساعد هذه التقنية في منع التكيف المفرط (Overfitting) عن طريق "إسقاط" (أي جعلها صفرًا) جزء عشوائي من عناصر المدخلات في الـ Tensor في كل مرور أمامي (forward pass). تكون نشطة فقط أثناء التدريب؛ أما أثناء التقييم/الاستدلال، فلا تقوم بإسقاط أي شيء وتقوم بقياس القيم المتبقية للحفاظ على الحجم المتوقع للمخرجات.

تفصيل سطر الكود الخاص بك:
```python
drop = nn.Dropout(config.dropout)
```
- `nn.Dropout`: هذا هو صنف PyTorch الخاص بطبقة الإسقاط.
- `config.dropout`: هذه عادةً ما تكون قيمة عشرية (مثل 0.1 أو 0.5) من كائن/قاموس التهيئة، تمثل **احتمال الإسقاط** `p`. وتعني "أسقط هذه النسبة المئوية من العناصر."
  - على سبيل المثال، إذا كانت `config.dropout = 0.2`، فسيتم جعل 20% من العناصر في المدخلات صفرًا بشكل عشوائي.
- `drop = ...`: هذا ينشئ نسخة (instance) من وحدة الإسقاط ويعينها للمتغير `drop`. يمكنك بعد ذلك استخدامها مثل أي طبقة أخرى في الشبكة العصبية الخاصة بك (مثلًا في `nn.Sequential` أو دالة forward).

#### كيف يعمل الإسقاط عند استدعاء `drop(x)`
لا، `drop(x)` **لا** تعني "جعل كل شيء صفرًا". بدلاً من ذلك:
- تأخذ الـ tensor المدخلة `x` (مثل التفعيلات من الطبقة السابقة).
- تختار العناصر **عشوائيًا** لإسقاطها بناءً على الاحتمال `p` (من `config.dropout`).
  - يتم جعل العناصر المسقطة تساوي 0.
  - يتم قياس العناصر غير المسقطة بـ `1 / (1 - p)` للحفاظ على المجموع المتوقع كما هو (هذا يتجنب التدني الحسابي - underflow - أثناء التدريب).
- يحدث هذا **فقط أثناء التدريب** (وضع `model.train()`). في وضع التقييم (`model.eval()`)، تمرر `x` دون تغيير.
- مثال: إذا كان `x` هو tensor مثل `[1.0, 2.0, 3.0, 4.0]` و `p=0.5`، فقد يكون الناتج المحتمل `[0.0, 4.0, 0.0, 8.0]` (تم إسقاط 50%، وتم قياس العناصر الباقية بـ 2x). لكن النتيجة عشوائية، لذا تختلف في كل مرة.

#### مثال بسيط على الكود
إليك snippet صغير من PyTorch للتوضيح:
```python
import torch
import torch.nn as nn

# افترض أن config.dropout = 0.5 (إسقاط بنسبة 50%)
config = type('Config', (), {'dropout': 0.5})()
drop = nn.Dropout(config.dropout)

# tensor مدخلات عينة
x = torch.tensor([1.0, 2.0, 3.0, 4.0])

# أثناء التدريب
torch.manual_seed(42)  # لإمكانية إعادة النتائج
print(drop(x))  # الناتج المحتمل: tensor([0., 0., 0., 8.])  # إسقاط عشوائي + قياس

# أثناء التقييم
drop.eval()
print(drop(x))  # الناتج: tensor([1., 2., 3., 4.])  # لا توجد تغييرات
```

#### لماذا نستخدم الإسقاط؟
- **يمنع التكيف المفرط**: يجبر النموذج على عدم الاعتماد بشدة على أي عصبون (Neuron) فردي.
- شائع في طبقات التغذية الأمامية / MLP، أو RNNs، أو بعد التضمينات (Embeddings) (مثلًا في المحولات - Transformers).
- اضبط `p` عبر البحث عن المعاملات الفائقة (Hyperparameter Search); إذا كانت القيمة مرتفعة جدًا (مثل >0.5) فقد يؤدي ذلك إلى التكيف الأقل من المطلوب (Underfitting).

للحصول على الوثائق الكاملة، راجع [مرجع PyTorch nn.Dropout](https://pytorch.org/docs/stable/generated/torch.nn.Dropout.html). إذا كان هذا الكود من نموذج محدد (مثل BERT)، فأخبرني للمزيد من السياق!