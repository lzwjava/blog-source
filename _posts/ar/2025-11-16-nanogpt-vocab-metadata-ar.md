---
audio: false
generated: true
image: false
lang: ar
layout: post
model: x-ai/grok-4-fast
title: ملف بيانات التعريف لمفردات نانو جي بي تي
translated: true
type: note
---

في مشروع **nanoGPT** (https://github.com/karpathy/nanoGPT)، يُعتبر ملف **meta.pkl** ملف pickle اختياريًا يخزن المفردات وبيانات وصف الترميز الخاصة بمجموعة البيانات. يتم استخدامه عندما تقوم بالتدريب على مجموعة بيانات نصية مخصصة خاصة بك (وليست مجموعات البيانات الافتراضية مثل OpenWebText أو Shakespeare على مستوى الحروف).

### ما يحتويه ملف meta.pkl عادةً
إنه قاموس يحتوي على المفاتيح التالية (مثال):

```python
{
    'vocab_size': 50257,       # حجم المفردات
    'itos': {0: 'A', 1: 'B', ...},  # رَقَم → سلسلة نصية (الوحدة الرمزية)
    'stoi': {'A': 0, 'B': 1, ...},  # سلسلة نصية → رَقَم (الوحدة الرمزية)
    'encoder': ...,            # دالة الترميز
    'decoder': ...,            # دالة فك الترميز
}
```

### متى يقوم nanoGPT بتحميل أو إنشاء meta.pkl

1. أثناء التدريب (`train.py`):
   - إذا كان الملف `data/your_dataset/meta.pkl` موجودًا بالفعل → فإنه يقوم بتحميل المفردات من هناك.
   - إذا كان **غير** موجود → فإن nanoGPT يبني مفردات جديدة من النص الموجود في مجموعة البيانات الخاصة بك (عادةً باستخدام SentencePiece، أو tiktoken، أو مفردات بسيطة على مستوى الحروف) ويحفظها كملف `meta.pkl` لاستخدامها في المستقبل.

2. أثناء أخذ العينات/الاستدلال (`sample.py`):
   - عندما ترى الرسالة  
     `No meta.pkl found, assuming GPT-2 encodings...`  
     فهذا يعني أن nanoGPT لم يتمكن من العثور على ملف meta.pkl في دليل البيانات الخاص بك.
   - لذلك يتراجع إلى المرمِّز الافتراضي GPT-2 BPE (نفس المرمِّز المستخدم في GPT-2 من OpenAI) مع حجم مفردات يساوي 50257. ولهذا يعمل نموذجك الذي يحتوي على 29.94 مليون معلمة (وهو تكوين GPT-2 124M الأصغر ولكن يتم الإبلاغ عنه بشكل خاطئ في بعض الأحيان) بشكل جيد حتى بدون وجود meta.pkl عندما تتدرب على OpenWebText أو مجموعات بيانات مشابهة.

### باختصار:

| الموقف                              | وظيفة meta.pkl                                      |
|----------------------------------------|----------------------------------------------------------|
| التدريب على مجموعة بيانات **مخصصة**      | يخزن المفردات المخصصة الخاصة بك لضمان اتساق الترميز وفك الترميز |
| التدريب على OpenWebText/Shakespeare   | غير مطلوب → يستخدم nanoGPT المرمِّز الافتراضي GPT-2  |
| عدم تمكن `sample.py` من العثور على meta.pkl        | يتراجع إلى المرمِّز GPT-2 (وهو ما رأيته)    |

إذا قمت في أي وقت بالتدريب على مجموعة البيانات الخاصة بك وأردت الحصول على ترميز قابل للتكرار (أو حجم مفردات مختلف)، فستلاحظ أن nanoGPT ينشئ ملف meta.pkl تلقائيًا في جولة التدريب الأولى. بعد ذلك، سيعيد استخدامه ولن تحصل على تحذير "assuming GPT-2 encodings" بعد الآن.