---
audio: false
generated: true
image: false
lang: ar
layout: post
model: x-ai/grok-4-fast
title: ملخص محاضرة مدخل إلى المحولات
translated: true
type: note
---

### ملخص محاضرة أندريه كارباثي "مقدمة إلى المحولات (Transformers)"

محاضرة أندريه كارباثي، وهي جزء من دورة ستانفورد CS25 Transformers United، عبارة عن غوص عميق وسهل للمبتدئين في بنية المحولات (Transformer) — العمود الفقري لنماذج الذكاء الاصطناعي الحديثة مثل GPT و BERT. مُقدَّمة في حوالي ساعة، تستخدم المحاضرة رسومًا بيانية بديهية، وقياسات، ومقتطفات من التعليمات البرمجية (بما في ذلك عرض حي لتنفيذه "NanoGPT") لتبسيط كيفية عمل المحولات. يتتبع كارباثي تاريخها، يحلل آلياتها، ويستكشف تنوعها عبر مجالات تتجاوز اللغة. إليك نظرة عامة منظمة للنقاط الرئيسية:

#### سياق الدورة والصورة الكبيرة
- **لماذا تهم المحولات**: مقدَّمة في ورقة 2017 "Attention is All You Need"، أحدثت المحولات ثورة في الذكاء الاصطناعي منذ ذلك الحين، وسيطرت على معالجة اللغة الطبيعية (NLP)، ورؤية الكمبيوتر، وعلم الأحياء (مثل AlphaFold)، والروبوتات، والمزيد. إنها ليست للنص فقط — فهي إطار مرن لأي بيانات متسلسلة.
- **أهداف الدورة**: هذه هي المحاضرة الافتتاحية لسلسلة عن أساسيات المحولات، والانتباه الذاتي (self-attention)، والتطبيقات. تغطي الجلسات المستقبلية نماذج مثل BERT/GPT ومحادثات ضيوف حول الاستخدامات الواقعية. يؤكد كارباثي على أن المحولات هي خوارزمية تعلم "موحدة"، تجمع بين مجالات الذكاء الاصطناعي الفرعية نحو نماذج قابلة للتطوير وقائمة على البيانات.

#### التطور التاريخي
- **من النماذج المبكرة إلى الاختناقات**: بدأ الذكاء الاصطناعي اللغوي بشبكات عصبية بسيطة (2003) تتنبأ بالكلمات التالية عبر多层感知. أضافت RNNs / LSTMs (2014) معالجة التسلسل لمهام مثل الترجمة لكنها واجهت حدودًا: "مختنقات المشفر (encoder)" الثابتة ضغطت المدخلات بأكملها في متجه واحد، مما أدى إلى فقدان التفاصيل عبر التسلسلات الطويلة.
- **صعود آلية الانتباه (Attention)**: عالجت آليات الانتباه (التي صاغها يان ليكون) هذه المشكلة بالسماح لوحدات فك الترميز (decoders) بإجراء "بحث مرن" للأجزاء ذات الصلة من المدخلات عبر المجموع الموزون. كانت الطفرة في عام 2017 بتجاهل RNNs تمامًا، والرهان على أن "الانتباه هو كل ما تحتاجه" للمعالجة المتوازية — مما جعلها أسرع وأكثر قوة.

#### الآليات الأساسية: الانتباه الذاتي وتمرير الرسائل
- **الرموز (Tokens) كعُقَد**: فكر في بيانات الإدخال (مثل الكلمات) كـ "رموز" في رسم بياني. الانتباه الذاتي يشبه تبادل العُقَد للرسائل: كل رمز يُنشِئ **استعلامات (queries)** (ما أبحث عنه)، **مفاتيح (keys)** (ما أقدمه)، و**قيم (values)** (حمولة بياناتي). يحدد تشابه الضرب النقطي بين الاستعلامات/المفاتيح أوزان الانتباه (عبر softmax)، ثم تضرب الأوزان القيم للحصول على تحديث يراعي السياق.
- **الانتباه متعدد الرؤوس (Multi-Head Attention)**: تشغيل هذا في "رؤوس" متوازية بأوزان مختلفة للحصول على منظورات أكثر ثراءً، ثم دمج النتائج.
- **الإخفاء السببي (Causal Masking)**: في وحدات فك الترميز (للتوليد)، يتم إخفاء الرموز المستقبلية لمنع "الغش" أثناء التنبؤ.
- **الترميز الموضعي (Positional Encoding)**: تعالج المحولات مجموعات، وليست تسلسلات، لذا تتم إضافة ترميزات قائمة على Sine إلى التضمينات (embeddings) لحقن معلومات الترتيب.
- **البديهة**: إنها اتصال يعتمد على البيانات — الرموز "تتحاور" بحرية (في المشفر) أو بشكل سببي (في وحدة فك الترميز)، capturing التبعيات طويلة المدى دون اختناقات تسلسلية.

#### البنية الكاملة: الاتصال + الحساب
- **إعداد المشفر/وحدة فك الترميز (Encoder-Decoder)**: يقوم المشفر بتوصيل الرموز بالكامل لتدفق ثنائي الاتجاه؛ تضيف وحدة فك الترميز انتباهًا متقاطعًا (cross-attention) لمخرجات المشفر وانتباهًا ذاتيًا سببيًا (causal self-attention) للتوليد التلقائي (autoregressive).
- **هيكل الكتلة (Block)**: تكديس طبقات بالتناوب:
  - **مرحلة الاتصال**: الانتباه الذاتي/المتقاطع متعدد الرؤوس (تمرير الرسائل).
  - **مرحلة الحساب**: الشبكة الأمامية (Feed-forward MLP) (معالجة الرمز الفردي مع عدم خطية ReLU).
- **إضافات للاستقرار**: اتصالات متبقية (Residual connections) (إضافة المدخلات إلى المخرجات)، تطبيع الطبقة (layer normalization).
- **لماذا تعمل**: قابلة للتوزيع على وحدات معالجة الرسومات (GPUs)، معبرة عن الأنماط المعقدة، وتتوسع مع البيانات/القدرة الحاسوبية.

#### التطبيق العملي: البناء والتدريب باستخدام NanoGPT
- **تنفيذ مصغر**: يقدم كارباثي عرضًا لـ NanoGPT — وهو محول (Transformer) صغير من نوع وحدة فك الترميز فقط مكتوب بـ PyTorch. يتدرب على النص (مثل شكسبير) للتنبؤ بالأحرف/الكلمات التالية.
  - **تحضير البيانات**: تحويل النص إلى رموز رقمية (Tokenize)، تجميعها في سياقات ذات حجم ثابت (مثل 1024 رمز).
  - **المرور الأمامي (Forward Pass)**: تضمين الرموز + الترميزات الموضعية → كتل المحولات → القيم الأولية (logits) → دالة الخسارة الانتروبية المتقاطعة (cross-entropy loss) (الأهداف = مدخلات مُزاحة).
  - **التوليد**: البدء بمطالبة (prompt)، أخذ عينات من الرموز التالية بشكل تلقائي (autoregressively)، مع مراعاة حدود السياق.
- **نصائح التدريب**: حجم الدفعة (Batch size) × طول التسلسل للكفاءة؛ يتوسع لنماذج ضخمة مثل GPT-2.
- **المتغيرات**: مشفر فقط (مثل BERT للتصنيف عبر الإخفاء)؛ مشفر/وحدة فك ترميز كاملة للترجمة.

#### التطبيقات والقدرات الخارقة
- **أبعد من النص**: تحويل الصور/الصوت إلى بقع (patches) وتحويلها إلى رموز — يتعامل الانتباه الذاتي مع "الاتصال" غير الإقليدي عبر البقع، مما يمكّن محولات الرؤية (Vision Transformers - ViT).
- **التعلم في السياق (In-Context Learning)**: إدخال أمثلة في المطالبات؛ تتعلم النماذج المهام على الفور (التعلم الفوقي - meta-learning)، دون الحاجة إلى ضبط دقيق (fine-tuning). مع البيانات الضخمة، تبرز التحيزات الدنيا.
- **المرونة**: معالجة حالات/إجراءات التعلم المعزز (RL) كلـ "لغة"، أو إضافة ذاكرة خارجية لسياقات أطول. الهجينة مع نماذج الانتشار (diffusion models) للمهام التكرارية مثل توليد الصور.

#### التحديات والتوقعات المستقبلية
- **العقبات**: الحساب التربيعي (انتباه O(n²))، التحكم في الهلوسة (hallucination)، التعامل مع السياقات الطويلة، محاذاة الدماغ.
- **التأثير**: المحولات كـ "حواسيب للأغراض العامة" للنص — التدريب على بيانات صعبة لتشغيل "برامج" مثل الاستدلال أو توليد الكود.
- **ما التالي**: نماذج مخصصة لمجال معين (مثل GPTs للتمويل)، التكامل متعدد الوسائط، أدوات أفضل للاستدلال/الصوت/الفن. يلمح كارباثي إلى تعديلات الكفاءة وطموحات "GPT++".

تتميز هذه المحاضرة بقياساتها البصرية (مثل تشبيه الانتباه بـ "المؤشر" في الترجمة) وشرح التعليمات البرمجية، مما يجعل الأفكار المجردة ملموسة. يُعتبر هذا الملخص من أفضل المقدمات لأنه يبني الحدس دون إرباك بالرياضيات المعقدة — مثالي للقادمين الجدد.

[ملخص محاضرة ستانفورد CS25: Transformers United](https://www.deciphr.ai/podcast/stanford-cs25-v2-i-introduction-to-transformers-w-andrej-karpathy-2)  
[ملاحظات مقدمة إلى المحولات](https://ersantana.com/llm/stanford_cs25_transformers/02_intro_to_transformers)  
[فيديو يوتيوب](https://www.youtube.com/watch?v=XfpMkf4rD6E)