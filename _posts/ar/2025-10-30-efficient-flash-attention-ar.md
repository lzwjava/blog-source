---
audio: false
generated: true
image: false
lang: ar
layout: post
model: x-ai/grok-4-fast
title: انتباه الفلاش الفعال في PyTorch
translated: true
type: note
---

### الانتباه الفعال مع نواة CUDA للانتباه السريع (Flash Attention) في PyTorch

سطر الكود الذي شاركته ينفذ **الانتباه القائم على الضرب النقطي المعياري** (الآلية الأساسية في نماذج المحولات Transformer) باستخدام الدالة `torch.nn.functional.scaled_dot_product_attention` في PyTorch. تم تصميم هذه الدالة لتكون فعالة وتقوم تلقائيًا بتوجيه الحساب إلى تنفيذات مُحسّنة مثل **الانتباه السريع (Flash Attention)** عند استيفاء الشروط (مثل التشغيل على CUDA مع أشكال موتر متوافقة وعدم وجود أقنعة مخصصة). سأقوم بشرحها خطوة بخطوة، بما في ذلك كيف يتناسب الانتباه السريع مع العملية، وأشكال الوسائط، ولماذا هي فعالة.

#### خلفية سريعة عن الانتباه القائم على الضرب النقطي المعياري
في نماذج المحولات (Transformers)، يحسب الانتباه مقدار "اهتمام" كل موضع في التسلسل بالمواضع الأخرى. الصيغة هي:

\\[
\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right) V
\\]

- \\(Q\\): مصفوفة الاستعلام (ما نستعلم عنه).
- \\(K\\): مصفوفة المفاتيح (ما نقوم بمطابقته).
- \\(V\\): مصفوفة القيم (ما نسترجعه).

الحساب الساذج لهذا يتطلب إنشاء مصفوفة انتباه كبيرة بحجم \\(N \times N\\) (حيث \\(N\\) هو طول التسلسل)، مما يستخدم ذاكرة بسعة \\(O(N^2)\\) — وهذا أمر سيء للتسلسلات الطويلة (مثلاً \\(N > 10k\\)).

**الانتباه السريع (Flash Attention)** (الذي قدمه Tri Dao وآخرون في 2022) يُصلح هذا المشكلة بتقنية **دمج النواة (kernel fusion)** باستخدام CUDA. حيث يحسب الانتباه **على الطاير** في بلوكات (كتل)، متجنبًا إنشاء المصفوفة الكاملة في الذاكرة. هذا يقلل الذاكرة إلى \\(O(N)\\) ويسرع العملية بمقدار 2-4 مرات على وحدات معالجة الرسومات، خاصة للسياقات الطويلة. PyTorch تدمجه بسلاسة عبر هذه الدالة — لا حاجة لنواة مخصصة.

#### كيف يستخدم الكود الانتباه السريع
```python
y = torch.nn.functional.scaled_dot_product_attention(
    q, k, v, 
    attn_mask=None, 
    dropout_p=self.dropout if self.training else 0, 
    is_causal=True
)
```
- هذا يحسب الانتباه الذاتي السببي (شائع في النماذج الانحدارية الذاتية مثل GPT، حيث لا يمكن للرموز المستقبلية الاهتمام بالرموز الماضية).
- **توجيه الانتباه السريع**: PyTorch تتحقق من الشروط أثناء التشغيل:
  - الجهاز: CUDA (GPU).
  - أنواع البيانات: float16/bfloat16 (أو float32 مع محاذير).
  - الأشكال: متوافقة (انظر أدناه).
  - الأقنعة: `attn_mask=None` و `is_causal=True` يُمكنّن القناع السببي داخليًا دون إنشائه فعليًا في الذاكرة.
  - لا توجد قيود أخرى (مثل عدم وجود `attn_mask` مخصص أو أبعاد رأس معينة تكسر التقسيم إلى بلوكات).

  إذا تحققت الشروط، تستخدم نواة الانتباه السريع 2 (أو 3 في إصدارات PyTorch الأحدث). وإلا، تعود إلى التنفيذ القياسي (الأبطأ، الأكثر استهلاكًا للذاكرة). يمكنك التحقق من ذلك باستخدام `torch.backends.cuda.sdp_kernel(enable_flash=True, enable_math=False)` لإجبار/تمكينه.

- **الإسقاط (Dropout)**: يُطبق أثناء التدريب (`dropout_p > 0`) على أوزان الانتباه لتنظيم النموذج. في وضع التقييم، تكون قيمته 0.
- المخرج `y`: نفس شكل `v`، ويمثل القيم بعد تطبيق الانتباه.

#### أشكال المتطلبات والوسائط
جميع المدخلات (`q`, `k`, `v`) يجب أن يكون لها أشكال متطابقة وأن تكون على نفس الجهاز/نوع البيانات. دالة PyTorch تدعم **الانتباه المجمع (batched)** و **متعدد الرؤوس (multi-head)** بمرونة. إليك التفصيل:

| الوسيط | الشكل (الأولوية للمجموعة، الإعداد الافتراضي) | الوصف | المتطلبات |
|----------|------------------------------|-------------|--------------|
| **q** (الاستعلام) | `(B, S_q, H, D)` أو `(B, S_q, E)` | - `B`: حجم المجموعة (مثلاً 32).<br>- `S_q`: طول تسلسل الاستعلام (مثلاً 512).<br>- `H`: عدد الرؤوس (مثلاً 8؛ اختياري إذا كان رأس واحد).<br>- `D`: بُعد الرأس (مثلاً 64؛ `E = H * D` لبُعد التضمين المسطح). | - `S_q` يجب أن تطابق `S_k` للانتباه الذاتي.<br>- للانتباه السريع: `D` ≤ 256 (الأمثل)، لكن حتى 512 يعمل. |
| **k** (المفتاح) | `(B, S_k, H, D)` أو `(B, S_k, E)` | نفس `q`، لكن `S_k` هو طول تسلسل المفتاح (غالبًا = `S_q`). | - يجب أن يكون قابلاً للبث إلى شكل `q`. |
| **v** (القيمة) | `(B, S_v, H, D)` أو `(B, S_v, E)` | نفس `k`، `S_v` عادةً = `S_k`. | - شكل المخرج `y` يطابق `v`. |
| **attn_mask** | `(B, H, S_q, S_k)` أو `(S_q, S_k)` (قابل للبث) | قناع إضافي اختياري (مثلاً `-inf` للمواضع المُقنّعة). هنا: `None`. | - للانتباه السريع: تجنبه إذا أمكن؛ استخدم `is_causal` بدلاً من ذلك. |
| **dropout_p** | عدد قياسي (float) | معدل الإسقاط (0.0-1.0). | - Float32. |
| **is_causal** | منطقي (Bool) | يُمكنّن القناع السببي المثلثي السفلي (لا يطلع على المستقبل). هنا: `True`. | - للانتباه السريع: مُفضل على الأقنعة اليدوية. |

- **الأولوية للمجموعة مقابل الأولوية للرأس**: الإعداد الافتراضي هو `batch_first=True` (الأشكال كما بالأعلى). عيّن `batch_first=False` للحصول على `(H, B, S, D)`.
- **انتباه متقاطع (Cross-Attention)**: في حالة نموذج المُشفر-فك الشفرة (encoder-decoder)، `S_q` (طول فك الشفرة) يمكن أن يختلف عن `S_k = S_v` (طول المُشفر).
- **الحالات الخاصة للانتباه السريع**:
  - طول التسلسل \\(S \leq 8192\\) (الأطول قد يؤدي للعودة للتنفيذ القياسي).
  - لا يوجد دعم للمجموعات غير المنتظمة أو بعض الأقنعة المتفرقة.
  - يمكن تمكينه بـ: `torch.backends.cuda.enable_flash_sdp(True)`.

#### مثال للاستخدام والتحقق
إليك مثالاً بسيطًا في PyTorch (بافتراض وجود CUDA):
```python
import torch
import torch.nn.functional as F

B, S, H, D = 2, 128, 8, 64  # المجموعة=2، التسلسل=128، الرؤوس=8، بُعد الرأس=64
q = torch.randn(B, S, H, D, device='cuda', dtype=torch.float16)
k = torch.randn(B, S, H, D, device='cuda', dtype=torch.float16)
v = torch.randn(B, S, H, D, device='cuda', dtype=torch.float16)

# تمكين الانتباه السريع
with torch.backends.cuda.sdp_kernel(enable_flash=True, enable_math=False):
    y = F.scaled_dot_product_attention(q, k, v, is_causal=True)

print(y.shape)  # torch.Size([2, 128, 8, 64])
```

هذا يعمل أسرع بحوالي ~3 مرات من الحساب اليدوي `q @ k.transpose(-2,-1)` + softmax + `@ v` للتسلسلات الكبيرة \\(S\\).

#### لماذا هي فعالة؟
- **الذاكرة**: القياسي: \\(O(B \cdot H \cdot S^2 \cdot 4\\) بايت (لـ float32). الانتباه السريع: \\(O(B \cdot H \cdot S \cdot D)\\) — يتدرج خطيًا مع \\(S\\).
- **الحساب**: النواة المدمجة تقلل من عمليات إطلاق النواة (softmax + matmul في خطوة واحدة).
- **المقايضات**: استقرار عددي أقل قليلاً في float16 (استخدم bfloat16 للأفضل)، لكنه ضئيل.

للتعمق أكثر، راجع [ورقة الانتباه السريع](https://arxiv.org/abs/2205.14135) أو [توثيق PyTorch](https://pytorch.org/docs/stable/generated/torch.nn.functional.scaled_dot_product_attention.html). إذا شاركت المزيد من السياق (مثل النموذج الكامل)، يمكنني التحسين!