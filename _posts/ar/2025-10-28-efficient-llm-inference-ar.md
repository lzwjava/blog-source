---
audio: false
generated: true
image: false
lang: ar
layout: post
model: x-ai/grok-4-fast
title: الاستدلال الفعال للـ LLM باستخدام vLLM
translated: true
type: note
---

### vLLM: الاستدلال والخدمة الفعالة لنماذج اللغة الكبيرة

vLLM هي مكتبة مفتوحة المصدر مصممة من أجل الاستدلال والخدمة السريعة والموفرة للذاكرة لنماذج اللغة الكبيرة (LLMs). تم تطويرها في البداية في مختبر Sky Computing بجامعة كاليفورنيا، بيركلي، وهي الآن مشروع يقوده المجتمع ويُستخدم على نطاق واسع في بيئات الإنتاج لنشر نماذج اللغة الكبيرة مثل Llama أو متغيرات GPT. يكمن الابتكار الأساسي فيها في **PagedAttention**، وهي تقنية تعامل ذاكرة التخزين المؤقت للمفاتيح-القيم (KV) مثل صفحات الذاكرة الظاهرية، مما يقلل الهدر ويمكن من تحقيق معدل إنتاجية أعلى من خلال التخصيص الديناميكي للكتل غير المتجاورة.

#### آلية العمل
- **المعالجة المجمعة المستمرة**: على عكس الأنظمة التقليدية التي تنتظر اكتمال الدُفعات، يقوم vLLM بإضافة وإزالة الطلبات ديناميكيًا أثناء التنفيذ، مما يقلل من وقت الخمول لوحدة معالجة الرسومات أثناء فك الترميز.
- **إدارة الذاكرة**: تتجنب PagedAttention التجزئة في ذاكرة التخزين المؤقت للمفاتيح-القيم (والتي تزداد مع طول التسلسل)، مما يدعم سياقات أطول دون حدوث أخطاء نفاد الذاكرة (OOM).
- **التنفيذ المُحسّن**: يستخدم الرسوم البيانية CUDA/HIP لإطلاق النواة بشكل أسرع، ومتكامل مع FlashAttention/FlashInfer لحساب الاهتمام، ويدعم التكميم (مثل AWQ, GPTQ, FP8) لتقليل استخدام الذاكرة حتى 4 مرات.
- **الميزات المتقدمة**: تشمل فك الترميز التخميني (لتخمين الرموز والتحقق منها)، والملء المسبق المجزأ (للمدخلات الطويلة)، وتعددية LoRA (تكييف النماذج على الطاير)، والتوزيع المتوازي (الموتر، خط الأنابيب، الخبير).

يقدم vLLM خادم واجهة برمجة تطبيقات متوافق مع OpenAI، ويتكامل بسلاسة مع نماذج Hugging Face، ويعمل على أجهزة متنوعة (وحدات معالجة الرسومات من NVIDIA/AMD/Intel، ووحدات TPU، ووحدات CPU). إنه مثالي لسيناريوهات الإنتاجية العالية، حيث يحقق تسريعًا يتراوح من 2 إلى 10 مرات مقارنة بالمعايير الأساسية مثل Hugging Face Transformers في معايير أداء الخدمة.

#### حالات الاستخدام الرئيسية
- الخدمة عبر الإنترنت للدردشات الآلية أو واجهات برمجة التطبيقات مع مخرجات متدفقة.
- الاستدلال المجمّع دون اتصال بالإنترنت لمهام مثل التلخيص.
- التوسع إلى مجموعات متعددة من وحدات معالجة الرسومات دون حاجة إلى بنية تحتية مخصصة معقدة.

### Ray: إطار العمل الموحد لتوسيع نطاق تطبيقات الذكاء الاصطناعي وبايثون

Ray هو إطار عمل موزّع مفتوح المصدر يسهل عملية توسيع نطاق كود بايثون – خاصة أحمال عمل الذكاء الاصطناعي/التعلم الآلي – من جهاز واحد إلى مجموعات ضخمة. تم إنشاؤه بواسطة Anyscale (ذات جذور من جامعة كاليفورنيا، بيركلي)، وهو يلغي تعقيدات الأنظمة الموزعة مثل الجدولة، وتحمل الأخطاء، والتنسيق، مما يسمح للمطورين بالتركيز على المنطق.

#### المكونات الرئيسية
- **Ray Core**: الأساس – أدوات أساسية بأسلوب بايثون للمهام (الدوال المتوازية)، والمشغّلين (الخدمات ذات الحالة)، والكائنات (مشاركة البيانات الموزعة). وهو يتعامل تلقائيًا مع التوسع التلقائي، وإعادة المحاولة، وتخصيص الموارد.
- **مكتبات Ray للذكاء الاصطناعي**: أدوات متخصصة في مجالات معينة مبنية على Core:
  - **Ray Data**: ETL قابلة للتوسع للمعالجة المسبقة لمجموعات البيانات.
  - **Ray Train**: التدريب الموزع مع تكاملات (PyTorch, TensorFlow, Hugging Face).
  - **Ray Tune**: تحسين المعاملات الفائقة على نطاق واسع.
  - **Ray Serve**: نشر النماذج للاستدلال، مع التوجيه، والمعالجة المجمعة، واختبار A/B.
  - **RLlib**: أدوات تعزيز التعلم.
- **مجموعات Ray**: طبقة البنية التحتية المدارة للنشر على السحابة (AWS, GCP)، أو Kubernetes، أو محليًا، مع التوسع التلقائي بناءً على الطلب.

#### آلية العمل
يعمل Ray كبرنامج خفي (Daemon) على العقد، مشكلًا مجموعة مع عقدة رئيسية للتنسيق. يمكنك تزيين الدوال بـ `@ray.remote` لتحقيق التوازي، وهو يقوم بتوزيع التنفيذ عبر وحدات CPU/GPU. بالنسبة للتعلم الآلي، تتولى مكتبات مثل Serve نقاط نهاية HTTP وموازنة الحمل، بينما تضمن Core التوسع مع تحمل الأخطاء (مثل إعادة تشغيل المهام الفاشلة).

#### حالات الاستخدام الرئيسية في الذكاء الاصطناعي/التعلم الآلي
- خطوات العمل الشاملة: تحميل البيانات → التدريب → الضبط → الخدمة.
- مسحات تحسين المعاملات الفائقة على مئات من وحدات معالجة الرسومات.
- أساطيل الاستدلال في الوقت الفعلي، كما في أنظمة التوصية.
- التطبيقات الموزعة المخصصة، مثل المحاكاة.

تكمن قوة Ray في واجهة برمجة التطبيقات الموحدة: اكتب الكود مرة واحدة، وقم بتوسيع نطاقه في أي مكان، مع الحد الأدنى من الكود المتكرر مقارنة بـ Spark أو Dask.

### العلاقة بين vLLM و Ray

في أنظمة مثل SLOs-Serve (من مناقشتنا السابقة)، يتعامل vLLM مع المعالجة المجمعة/الاستدلال منخفض المستوى لنماذج LLM، بينما يقوم Ray بتنسيق عمليات النشر متعددة النسخ المتماثلة، وتوجيه الطلبات عبر العقد للتعامل مع الزيادات المفاجئة والتوسع.

[توثيق vLLM](https://docs.vllm.ai/en/latest/)  
[توثيق Ray](https://docs.ray.io/en/latest/ray-overview/index.html)