---
audio: false
lang: ar
layout: post
title: Deepseek - محادثة
translated: true
---

أ: كنت أتصفح تقرير DeepSeek-V3 الفني، وأشعر بالدهشة من حجم هذا النموذج. 671 مليار معامل، ولكن فقط 37 مليار مفعل لكل رمز؟ هذا هو هيكل MoE الضخم. كيف يعمل ذلك؟

ب: نعم، إنه إنجاز كبير! DeepSeek-V3 مبني على إطار Mixture-of-Experts (MoE)، الذي يسمح له بتفعيل مجموعة فرعية فقط من المعاملات لكل رمز. بشكل خاص، يستخدم 256 خبيرًا موجهًا، ولكن فقط 8 يتم تفعيلها لكل رمز. هذا يجعله فعّالًا بشكل كبير مقارنة بالنماذج الكثيفة، حيث تكون جميع المعاملات نشطة لكل رمز.

أ: هذا منطقي. ولكن كيف يقرر أي خبيرين يتم تفعيلهما؟ هل هو عشوائي، أو هناك نوع من آلية التوجيه؟

ب: سؤال جيد! التوجيه يعتمد على درجات التعلق بين الرموز والخبراء. يتم تعيين درجة لكل رمز لكل خبير، وتتم تفعيل أعلى K خبراء الذين لديهم أعلى الدرجات. DeepSeek-V3 يستخدم وظيفة سيجموئيد لحساب هذه الدرجات، مما يساعد في توازن الحمل بين الخبراء.

أ: آه، فلا هو عشوائي - إنه يتم تعلمه خلال التدريب. ولكن هل ذلك لا يؤدي إلى استخدام غير متوازن للخبراء؟ سمعت أن ذلك مشكلة شائعة في نماذج MoE.

ب: بالضبط! يمكن أن يكون استخدام غير متوازن للخبراء مشكلة، ولكن DeepSeek-V3 يقدم استراتيجية بدون خسائر مساعدة لتسوية ذلك. بدلاً من إضافة مصطلح خسارة منفصل لتشجيع التوازن، فإنه يعدل ديناميكيًا معاملًا للخطأ لكل خبير. إذا كان خبيرًا مفرطًا في الحمل، يتم تقليل معامله، وإذا كان خبيرًا تحت الحمل، يتم زيادة معامله. هذا يحافظ على التوازن دون تدهور أداء النموذج.

أ: هذا ذكي. لذلك، لا يوجد خسائر مساعدة يعني أقل تدخل مع الهدف الرئيسي للتدريب. ولكن كيف هذا مقارنة بنماذج MoE التقليدية التي تستخدم خسائر مساعدة؟

ب: بالضبط. نماذج MoE التقليدية تستخدم خسائر مساعدة لتشجيع التوازن، ولكن هذه الخسائر يمكن أن تؤثر على الأداء أحيانًا. استراتيجية DeepSeek-V3 بدون خسائر مساعدة تجنب هذا التنازع. في الواقع، أظهرت دراسات إزالة الخسائر أنها تتفوق باستمرار على النماذج التي تعتمد على خسائر مساعدة، وخاصة في مهام مثل البرمجة والرياضيات.

أ: مثير للاهتمام. تحدثًا عن البرمجة والرياضيات، لاحظت أن DeepSeek-V3 يؤدي بشكل استثنائي في مراجعات مثل HumanEval و MATH. ما هو السحر هناك؟

ب: جزء كبير منه هو الهدف متعدد الرموز التنبؤية (MTP). بدلاً من التنبؤ بالرمز التالي فقط، DeepSeek-V3 ينبؤ بعديد من الرموز المستقبلية في كل موقع. هذا يثخن الإشارة التدريبية ويساعد النموذج على التخطيط، وهو مفيد بشكل خاص في المهام التي تتطلب التفكير التسلسلي مثل البرمجة والرياضيات.

أ: انتظر، فهل هو ينبؤ برموز متعددة في نفس الوقت؟ كيف يعمل ذلك أثناء الاستنتاج؟ هل يستخدم MTP، أو هو فقط للتدريب؟

ب: أثناء الاستنتاج، يمكن التخلص من وحدات MTP، ويصبح النموذج يعمل مثل نموذج تنبؤي تقليدي. ولكن هنا الجزء المثير: يمكن إعادة استخدام وحدات MTP للتشفير التخميني، مما يسرع من التوليد عن طريق التنبؤ برموز متعددة في نفس الوقت ثم التحقق منها.

أ: هذا حيلة جميلة. فمثلًا، مثل الحصول على فوائد MTP أثناء التدريب ثم استخدامها لتسريع الاستنتاج. ولكن ماذا عن آلية الانتباه؟ رأيت شيئًا عن الانتباه المتعدد للاتجاهات (MLA). كيف يتناسب ذلك؟

ب: MLA هو أحد الابتكارات الرئيسية. يقلل من حجم الذاكرة عن طريق ضغط مخزن Key-Value (KV). بدلاً من تخزين مفاتيح الانتباه والقيم الكاملة، يستخدم ضغط مشترك منخفض الترتيب لتمثيلهم. هذا يقلل بشكل كبير من حجم مخزن KV أثناء الاستنتاج بينما يحافظ على الأداء المقارن مع الانتباه المتعدد للاتجاهات.

أ: هذا فوز كبير في الكفاءة. ولكن هل ضغط المعلومات لا يؤدي إلى فقدان بعض المعلومات؟ كيف يحافظ على الأداء؟

ب: نقطة جيدة. التصميم لضغط الحفاظ على المعلومات الأكثر أهمية عن طريق التركيز على المتجهات اللاتينية التي تكتسب الميزات الأساسية للمفاتيح والقيم. يستخدم النموذج أيضًا التضمين الدوراني للموقع (RoPE) للحفاظ على معلومات الموقع، مما يساعد في تقليل أي خسائر من الضغط.

أ: فهمت. لذلك، MLA كل شيء عن الكفاءة دون التضحية بدرجة كبيرة من الأداء. ولكن ماذا عن التدريب؟ تدريب نموذج بهذا الحجم يجب أن يكون مكلفًا للغاية. كيف يدير DeepSeek-V3 تخفيض التكاليف؟

ب: كفاءة التدريب هي التركيز الرئيسي. DeepSeek-V3 يستخدم إطار FP8 للدقة المختلطة، الذي يقلل من استخدام الذاكرة ويسرع من الحساب. يستخدم أيضًا خوارزمية DualPipe للتوازي في الأنابيب، التي تقلل من فقاعات الأنابيب وتغطي الحساب بالاتصال. هذه التحسينات تسمح للنموذج بالتدريب على 14.8 تريليون رمز مع 2.788 مليون ساعة GPU H800 فقط.

أ: ذلك مدهش. ولكن التدريب FP8 يمكن أن يكون صعبًا - كيف يديرون مشاكل الدقة؟ سمعت أن التدريب بدقة منخفضة يمكن أن يؤدي إلى عدم الاستقرار.

ب: أنت على حق. التدريب FP8 صعب بسبب النطاق الديناميكي المحدود. DeepSeek-V3 يعالج ذلك باستخدام تكميم دقيق، حيث يتم تجميع التفعيلات والمفاتيح في ألواح أو كتل أصغر وتتم تقييما بشكل مستقل. هذا يقلل من تأثير المتطرفين ويحافظ على استقرار التدريب. يستخدمون أيضًا تجميع دقة عالية لأهم العمليات لضمان الدقة.

أ: هذا منطقي. لذلك، هو توازن بين الكفاءة والدقة، ولكنهم نجحوا في تحقيق توازن جيد. ولكن ماذا عن البيانات؟ 14.8 تريليون رمز هو مجموعة بيانات هائلة. ما نوع البيانات التي يتم تدريبها عليها؟

ب: المجموعة البيانات متنوعة ومتعددة الجودة، مع التركيز على النصوص الإنجليزية والصينية. تشمل أيضًا كمية كبيرة من البيانات الرياضية والبرمجية، مما يساعد النموذج على التفوق في هذه المجالات. يتم تحسين خط البيانات لتقلل من التكرار بينما يحافظ على التنوع، ويستخدمون تقنيات مثل حزم الوثائق لضمان سلامة البيانات.

أ: ذلك يفسر الأداء القوي في مهام البرمجة والرياضيات. ولكن ماذا عن الأداء متعدد اللغات؟ هل يدير اللغات الأخرى جيدًا؟

ب: نعم، DeepSeek-V3 يتم تدريبه على مجموعة بيانات متعددة اللغات، ويؤدي بشكل جيد في مراجعات مثل MMMLU، التي تشمل مهام غير الإنجليزية. إنه قوي بشكل خاص في الصينية، يتفوق على نماذج مثل Qwen2.5 في مراجعات الصينية مثل C-Eval و CMMLU.

أ: ذلك مدهش. ولكن ماذا عن مهام السياق الطويل؟ رأيت أنه يدعم حتى 128K رمز. كيف يدير هذه المدخلات الطويلة؟

ب: DeepSeek-V3 يمدد طول السياق في مرحلتين: أولاً إلى 32K رمز، ثم إلى 128K رمز باستخدام تقنية YaRN. هذا يسمح له بمعالجة مهام السياق الطويل مثل ملخص الوثائق واسترجاع البيانات بشكل فعال. كما أنه يؤدي بشكل جيد في اختبار "Needle In A Haystack" الذي يقييم فهم السياق الطويل.

أ: ذلك هو تحسين كبير على النماذج السابقة. ولكن ماذا عن التوزيع؟ كيف يديرون الاستنتاج لهذا النموذج الكبير؟

ب: يتم التعامل مع الاستنتاج على مجموعة H800، مع GPUs متصلة باستخدام NVLink و InfiniBand. استراتيجية التوزيع تفصل مراحل التعبئة والتشفير لضمان كفاءة عالية ووقت استجابة منخفض. يستخدمون أيضًا خبراء متكررين لتوازن الحمل أثناء الاستنتاج، مما يساعد على الحفاظ على الكفاءة.

أ: هناك الكثير من التحسينات. ولكن ما هي القيود؟ بالتأكيد، نموذج بهذا الحجم لديه بعض التنازلات.

ب: واحدة من القيود هي حجم وحدة التوزيع. DeepSeek-V3 يتطلب مجموعة كبيرة من أجل الاستنتاج الفعال، وهذا يمكن أن يكون تحديًا للفرق الصغيرة. هناك أيضًا مجال للتحسن في سرعة التوليد، على الرغم من أن التشفير التخميني مع MTP يساعد.

أ: معقول. ولكن بشكل عام، يبدو وكأنه خطوة كبيرة إلى الأمام. ما القادم ل DeepSeek-V3؟ هل هناك اتجاهات مستقبلية يدرسونها؟

ب: يدرسون عدة مجالات، مثل تحسين الهيكل لتقديم طول السياق اللانهائي، استكشاف مصادر إشارات تدريب إضافية، وتحسين قدرات التفكير المنطقي للنموذج. يعملون أيضًا على طرق تقييم أكثر شمولية لتحسين تقييم أداء النموذج.

أ: يبدو أنهم لا يتوقفون عن التقدم. شكرًا لك على مروري على كل هذا - DeepSeek-V3 بالتأكيد هو تغيير في مجال النماذج المفتوحة LLM.

ب: بالتأكيد! إنه مثير للاهتمام رؤية مدى تقدم النماذج المفتوحة. DeepSeek-V3 يوسع الحدود، وأنا لا أستطيع الانتظار لرؤية ما سيقومون به في المستقبل.

أ: ذكرت أن DeepSeek-V3 يستخدم تدريب FP8 الدقة المختلطة. أنا فضولي - كيف هذا مقارنة مع BF16 أو FP16؟ هل FP8 مستقر بما يكفي لتدريب نموذج بهذا الحجم؟

ب: سؤال جيد. FP8 بالتأكيد أكثر تحديًا بسبب النطاق الديناميكي المحدود، ولكن DeepSeek-V3 يستخدم استراتيجية تكميم دقيقة لتخفيف ذلك. على سبيل المثال، يتم تجميع التفعيلات في ألواح 1x128، والمفاتيح في ألواح 128x128. يتم تقييما كل مجموعة بشكل مستقل، مما يساعد في التعامل مع المتطرفين ويحافظ على استقرار التدريب.

أ: مثير للاهتمام. لذلك، ليس مجرد تكميم FP8 - إنه أكثر تعقيدًا. ولكن هل ذلك لا يجلب عبء إضافي لإدارة جميع هذه المجموعات والمعاملات؟

ب: نعم، ولكن العبء قليل مقارنة بالفوائد. المفتاح هو أن FP8 يقلل من استخدام الذاكرة ويسرع من الحساب، وهو حاسم لتدريب نموذج بهذا الحجم. يستخدمون أيضًا تجميع دقة عالية لأهم العمليات، مثل ضربات المصفوفات، لضمان الاستقرار الرقمي.

أ: فهمت. لذلك، هو توازن بين الدقة والكفاءة، ولكنهم نجحوا في تحقيق توازن جيد. ولكن ماذا عن خوارزمية DualPipe؟ كيف تعمل؟

ب: DualPipe مصممة لتقلل من فقاعات الأنابيب في التوازي في الأنابيب. تغطي الحساب والاتصال عن طريق تقسيم كل قطعة من العمل إلى أربعة مكونات: الانتباه، إرسال الكل لكل، MLP، وجمع الكل لكل. أثناء المرور الخلفي، يتم تقسيم الحساب إلى "الخلف للدخول" و"الخلف للوزن"، مما يسمح بتغطية أكثر كفاءة.