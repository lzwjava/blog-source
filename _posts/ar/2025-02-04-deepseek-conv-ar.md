---
audio: false
generated: false
lang: ar
layout: post
title: DeepSeek - محادثة
translated: true
type: note
---

أ: كنت أقرأ التقرير الفني لـ DeepSeek-V3، وأنا معجب حقًا بحجم هذا النموذج. 671 مليار معلمة، ولكن يتم تفعيل 37 مليار فقط لكل رمز؟ هذا نموذج MoE ضخم. كيف يعمل حتى؟

ب: نعم، إنه إنجاز رائع! تم بناء DeepSeek-V3 على إطار عمل Mixture-of-Experts (MoE)، الذي يسمح له بتفعيل مجموعة فرعية فقط من المعلمات لكل رمز. على وجه التحديد، يستخدم 256 خبيرًا موجهًا، ولكن يتم تفعيل 8 فقط لكل رمز. هذا يجعله فعالاً بشكل لا يصدق مقارنة بالنماذج الكثيفة، حيث تكون جميع المعلمات نشطة لكل رمز.

أ: هذا منطقي. ولكن كيف يقرر أي الخبراء سيتم تفعيلهم؟ هل هو عشوائي، أم أن هناك نوعًا من آلية التوجيه؟

ب: سؤال رائع! يعتمد التوجيه على درجات تقارب الرمز إلى الخبير. يتم تعيين درجة لكل خبير لكل رمز، ويتم تفعيل أفضل K خبير بأعلى الدرجات. يستخدم DeepSeek-V3 دالة sigmoid لحساب هذه الدرجات، مما يساعد في موازنة الحمل عبر الخبراء.

أ: آه، إذن ليس الأمر عشوائيًا — إنه يتم تعلمه أثناء التدريب. ولكن ألا يؤدي هذا إلى استخدام غير متوازن للخبراء؟ سمعت أن هذه مشكلة شائعة في نماذج MoE.

ب: بالضبط! يمكن أن يكون الاستخدام غير المتوازن للخبراء مشكلة، لكن DeepSeek-V3 يقدم استراتيجية خالية من الخسائر المساعدة للتعامل مع هذا. بدلاً من إضافة مصطلح خسارة منفصل لتشجيع موازنة الحمل، فإنه يضبط مصطلح التحيز ديناميكيًا لكل خبير. إذا كان أحد الخبراء مثقلًا، يتم تقليل تحيزه، وإذا كان غير مثقل، يتم زيادة التحيز. هذا يحافظ على توازن الحمل دون تدهور أداء النموذج.

أ: هذا ذكي. إذن، عدم وجود خسارة مساعدة يعني تداخلًا أقل مع الهدف الرئيسي للتدريب. ولكن كيف يقارن هذا بنماذج MoE التقليدية التي تستخدم خسائر مساعدة؟

ب: صحيح. غالبًا ما تستخدم نماذج MoE التقليدية خسائر مساعدة لتشجيع موازنة الحمل، ولكن هذه الخسائر يمكن أن تضر بالأداء في بعض الأحيان. يتجنب نهج DeepSeek-V3 الخالي من الخسائر المساعدة هذه المقايضة. في الواقع، تظهر دراسات الإزالة أنه يتفوق باستمرار على النماذج التي تعتمد على الخسائر المساعدة، خاصة في مهام مثل البرمجة والرياضيات.

أ: مثير للاهتمام. بالحديث عن البرمجة والرياضيات، لاحظت أن DeepSeek-V3 يؤدي أداءً استثنائيًا في معايير مثل HumanEval وMATH. ما السر وراء ذلك؟

ب: جزء كبير منه هو هدف التنبؤ متعدد الرموز (MTP). بدلاً من مجرد التنبؤ بالرمز التالي، يتنبأ DeepSeek-V3 بعدة رموز مستقبلية في كل موضع. هذا يكثف إشارة التدريب ويساعد النموذج على التخطيط المسبق، وهو مفيد بشكل خاص للمهام التي تتطلب تفكيرًا تسلسليًا، مثل البرمجة والرياضيات.

أ: انتظر، إذن هو يتنبأ بعدة رموز في وقت واحد؟ كيف يعمل هذا أثناء الاستدلال؟ هل لا يزال يستخدم MTP، أم أنه مخصص للتدريب فقط؟

ب: أثناء الاستدلال، يمكن التخلص من وحدات MTP، ويتصرف النموذج مثل نموذج انحداري قياسي. ولكن إليك الجزء الرائع: يمكن إعادة استخدام وحدات MTP للفك التخميني، مما يسرع التوليد من خلال التنبؤ بعدة رموز بالتوازي ثم التحقق منها.

أ: هذه خدعة رائعة. إذن، الأمر يشبه الحصول على فوائد MTP أثناء التدريب ثم استخدامها لتسريع الاستدلال. ولكن ماذا عن آلية الانتباه؟ رأيت شيئًا عن Multi-head Latent Attention (MLA). كيف يتناسب هذا؟

ب: MLA هو ابتكار رئيسي آخر. يقلل البصمة الذاكرة عن طريق ضغط ذاكرة التخزين المؤقت للمفاتيح والقيم (KV). بدلاً من تخزين مفاتيح وقيم الانتباه الكاملة، يستخدم الضغط المشترك ذا الرتبة المنخفضة لتمثيلها. هذا يقلل بشكل كبير من حجم ذاكرة التخزين المؤقت KV أثناء الاستدلال مع الحفاظ على أداء قابل للمقارنة مع الانتباه متعدد الرؤوس القياسي.

أ: هذا فوز كبير للكفاءة. ولكن ألا يقدم الضغط بعض فقدان المعلومات؟ كيف يحافظ على الأداء؟

ب: نقطة جيدة. تم تصميم الضغط للحفاظ على المعلومات الأكثر أهمية من خلال التركيز على المتجهات الكامنة التي تلتقط الميزات الأساسية للمفاتيح والقيم. يستخدم النموذج أيضًا Rotary Positional Embedding (RoPE) للحفاظ على المعلومات الموضعية، مما يساعد في التخفيف من أي فقدان بسبب الضغط.

أ: فهمت. إذن، MLA هو فوز للجميع — يقلل استخدام الذاكرة دون التضحية بكثير من الأداء. ولكن ماذا عن التدريب؟ تدريب نموذج بهذا الحجم يجب أن يكون مكلفًا للغاية. كيف يتمكن DeepSeek-V3 من خفض التكاليف؟

ب: كفاءة التدريب هي محور رئيسي. يستخدم DeepSeek-V3 إطار عمل دقة مختلطة من نوع FP8، مما يقلل استخدام الذاكرة ويسرع الحساب. كما يوظف خوارزمية DualPipe للتوازي في خطوط الأنابيب، مما يقلل من فقاعات خطوط الأنابيب ويتداخل الحساب مع الاتصال. تتيح هذه التحسينات تدريب النموذج على 14.8 تريليون رمز باستخدام 2.788 مليون ساعة فقط من معالجات H800 GPU.

أ: هذا مثير للإعجاب. لكن تدريب FP8 يمكن أن يكون صعبًا — كيف يتعاملون مع مشكلات الدقة؟ سمعت أن التدريب بدقة منخفضة يمكن أن يؤدي إلى عدم استقرار.

ب: أنت محق. تدريب FP8 صعب بسبب النطاق الديناميكي المحدود. يتعامل DeepSeek-V3 مع هذا من خلال التكميم الدقيق، حيث يتم تجميع عمليات التنشيط والأوزان في مجموعات أو كتل أصغر ويتم تحجيمها بشكل مستقل. هذا يقلل من تأثير القيم المتطرفة ويحافظ على استقرار التدريب. كما يستخدمون التراكم عالي الدقة للعمليات الحرجة للحفاظ على الدقة.

أ: هذا منطقي. إذن، إنه توازن بين الكفاءة والدقة. ولكن ماذا عن البيانات؟ 14.8 تريليون رمز هي مجموعة بيانات ضخمة. على أي نوع من البيانات تم تدريبه؟

ب: مجموعة البيانات متنوعة وعالية الجودة، مع تركيز على النصوص الإنجليزية والصينية. وهي تشمل أيضًا كمية كبيرة من بيانات الرياضيات والبرمجة، مما يساعد النموذج على التفوق في تلك المجالات. تم تحسين خط أنابيب البيانات لتقليل التكرار مع الحفاظ على التنوع، ويستخدمون تقنيات مثل تجميع المستندات لضمان سلامة البيانات.

أ: هذا يفسر الأداء القوي في مهام البرمجة والرياضيات. ولكن ماذا عن الأداء متعدد اللغات؟ هل يتعامل مع اللغات الأخرى بشكل جيد؟

ب: نعم، تم تدريب DeepSeek-V3 على مجموعة نصوص متعددة اللغات، ويؤدي أداءً جيدًا في معايير مثل MMMLU، التي تتضمن مهامًا غير إنجليزية. إنه قوي بشكل خاص في الصينية، متفوقًا على نماذج مثل Qwen2.5 في المعايير الصينية مثل C-Eval وCMMLU.

أ: هذا مثير للإعجاب. ولكن ماذا عن مهام السياق الطويل؟ رأيت أنه يدعم حتى 128 ألف رمز. كيف يتعامل مع مثل هذه المدخلات الطويلة؟

ب: يوسع DeepSeek-V3 طول السياق في مرحلتين: أولاً إلى 32 ألف رمز ثم إلى 128 ألف رمز باستخدام تقنية YaRN. هذا يسمح له بالتعامل مع مهام السياق الطويل مثل تلخيص المستندات واسترجاعها بفعالية. كما أنه يؤدي أداءً جيدًا في اختبار 'Needle In A Haystack'، الذي يقيم فهم السياق الطويل.

أ: هذا تحسن كبير عن النماذج السابقة. ولكن ماذا عن النشر؟ كيف يتعاملون مع الاستدلال لنموذج بهذا الحجم؟

ب: يتم التعامل مع الاستدلال على مجموعة H800، مع اتصال معالجات GPU باستخدام NVLink وInfiniBand. تفصل استراتيجية النشر بين مراحل التعبئة المسبقة وفك التشفير لضمان كل من الإنتاجية العالية والكمون المنخفض. كما يستخدمون خبراء زائدين لموازنة الحمل أثناء الاستدلال، مما يساعد في الحفاظ على الكفاءة.

أ: هذه الكثير من التحسينات. ولكن ما هي القيود؟ بالتأكيد، لنموذج بهذا الحجم بعض المقايضات.

ب: أحد القيود هو حجم وحدة النشر. يتطلب DeepSeek-V3 مجموعة كبيرة نسبيًا للاستدلال الفعال، مما قد يمثل تحديًا للفرق الأصغر. هناك أيضًا مجال لتحسين سرعة التوليد، على الرغم من أن فك التشفير التخميني مع MTP يساعد.

أ: هذا معقول. ولكن بشكل عام، يبدو أنه خطوة كبيرة إلى الأمام. ما التالي لـ DeepSeek-V3؟ هل هناك أي اتجاهات مستقبلية يستكشفونها؟

ب: إنهم ينظرون في عدة مجالات، مثل تحسين الهندسة المعمارية لدعم طول سياق لا نهائي، واستكشاف مصادر إشارات تدريب إضافية، وتعزيز قدرات التفكير في النموذج. كما يعملون على طرق تقييم أكثر شمولاً لتقييم أداء النموذج بشكل أفضل.

أ: يبدو أنهم لا يبطئون في أي وقت قريب. شكرًا لك على walked me through كل هذا — DeepSeek-V3 هو بالتأكيد محول للعبة في مجال LLM مفتوح المصدر.

ب: Absolutely! إنه من المثير رؤية إلى أي مدى وصلت النماذج مفتوحة المصدر. DeepSeek-V3 يدفع الحدود، ولا يمكنني الانتظار لرؤية ما سيفعلونه بعد.

أ: لقد ذكرت أن DeepSeek-V3 يستخدم تدريبًا بدقة مختلطة من نوع FP8. أنا فضولي — كيف يقارن هذا بـ BF16 أو FP16؟ هل FP8 مستقر حقًا بما يكفي لتدريب نموذج بهذا الحجم؟

ب: هذا سؤال رائع. FP8 هو بالفعل أكثر تحديًا بسبب نطاقه الديناميكي المحدود، لكن DeepSeek-V3 يستخدم استراتيجية تكميم دقيقة للتخفيف من هذا. على سبيل المثال، يتم تجميع عمليات التنشيط في مجموعات 1x128، ويتم تجميع الأوزان في كتل 128x128. يتم تحجيم كل مجموعة بشكل مستقل، مما يساعد في التعامل مع القيم المتطرفة ويحافظ على استقرار التدريب.

أ: مثير للاهتمام. إذن،它不是 مجرد تكميم FP8 شامل — إنه أكثر دقة. ولكن ألا يقدم هذا عبئًا إضافيًا لإدارة كل هذه المجموعات وعوامل التحجيم؟

ب: نعم، لكن العبء ضئيل مقارنة بالفوائد. النقطة الرئيسية هي أن FP8 يقلل استخدام الذاكرة ويسرع الحساب، وهو أمر بالغ الأهمية لتدريب نموذج بهذا الحجم. كما يستخدمون التراكم عالي الدقة للعمليات الحرجة، مثل ضرب المصفوفات، لضمان الاستقرار العددي.

أ: فهمت. إذن، إنها مقايضة بين الدقة والكفاءة، لكنهم تمكنوا من تحقيق توازن جيد. ماذا عن خوارزمية DualPipe؟ كيف تعمل؟

ب: تم تصميم DualPipe لتقليل فقاعات خطوط الأنابيب في التوازي لخطوط الأنابيب. إنه يتداخل الحساب والاتصال عن طريق تقسيم كل جزء من العمل إلى أربعة مكونات: الانتباه، وإرسال all-to-all، وMLP، وجمع all-to-all. أثناء التمريرات الخلفية، يقسم الحساب further إلى 'خلفي للإدخال' و'خلفي للأوزان'، مما يسمح بتداخل أكثر كفاءة.

أ: هذا يبدو معقدًا، لكنه منطقي. إذن، إنه يخفي بشكل أساسي عبء الاتصال من خلال تداخله مع الحساب. كيف يقارن هذا بطرق التوازي لخطوط الأنابيب الأخرى مثل 1F1B أو Zero Bubble؟

ب: لدى DualPipe فقاعات خط أنابيب أقل مقارنة بـ 1F1B وZero Bubble. كما يسمح بالجدولة ثنائية الاتجاه، حيث يتم تغذية الدُفعات الصغيرة من كلا طرفي خط الأنابيب. هذا يقلل further من وقت الخمول ويحسن الكفاءة overall. في الواقع، يحقق DualPipe عبء اتصال all-to-all near-zero، وهو أمر crucial لتوسيع نطاق نماذج MoE.

أ: هذا مثير للإعجاب. ولكن ماذا عن استخدام الذاكرة؟ هل يتطلب DualPipe ذاكرة أكثر من الطرق الأخرى؟

ب: إنه يتطلب ذاكرة أكثر قليلاً لأنه يحتفظ بنسختين من معلمات النموذج، لكن الزيادة يمكن التحكم فيها. يتم تحسين البصمة الذاكرة من خلال تقنيات مثل إعادة حساب RMSNorm وMLA up-projections، مما يلغي الحاجة إلى تخزين عمليات التنشيط الوسيطة.

أ: آه، إذن هم يتاجرون بقليل من الذاكرة للحصول على كفاءة أفضل. هذا يبدو مقايضة عادلة. بالحديث عن الذاكرة، كيف يتعاملون مع ذاكرة التخزين المؤقت KV لطول سياق كبير كهذا؟ 128 ألف رمز must تتطلب ذاكرة تخزين مؤقت huge.

ب: هذا هو المكان where يبرز MLA حقًا. من خلال ضغط ذاكرة التخزين المؤقت KV، فإنهم يقللون بشكل كبير من حجمها. بدلاً من تخزين مفاتيح وقيم الانتباه الكاملة، يقومون بتخزين متجهات كامنة مضغوطة، وهي أصغر بكثير. هذا يسمح لـ DeepSeek-V3 بالتعامل مع السياقات الطويلة دون مواجهة اختناقات الذاكرة.

أ: هذا حل ذكي. ولكن ماذا عن جودة الانتباه؟ هل يؤثر الضغط على قدرة النموذج على الانتباه إلى الرموز الصحيحة؟

ب: تم تصميم الضغط للحفاظ على المعلومات الأكثر أهمية، لذا فإن التأثير على جودة الانتباه ضئيل. كما يستخدمون RoPE (Rotary Positional Embedding) للحفاظ على المعلومات الموضعية، مما يساعد النموذج على فهم المواضع النسبية للرموز حتى مع المفاتيح والقيم المضغوطة.

أ: هذا منطقي. إذن، MLA هو فوز للجميع — يقلل استخدام الذاكرة دون التضحية بكثير من الأداء. ولكن ماذا عن بيانات التدريب؟ لقد ذكرت أنها 14.8 تريليون رمز. كيف يضمنون جودة وتنوع مجموعة بيانات ضخمة كهذه؟

ب: تمت تنقية مجموعة البيانات بعناية لتشمل رموزًا عالية الجودة ومتنوعة. يقومون بتحسين خط أنابيب البيانات لتقليل التكرار مع الحفاظ على التنوع، ويستخدمون تقنيات مثل تجميع المستندات لضمان سلامة البيانات. تتضمن المجموعة مزيجًا من النصوص الإنجليزية والصينية، مع التركيز على عينات الرياضيات والبرمجة.

أ: هذا يفسر الأداء القوي في مهام البرمجة والرياضيات. ولكن ماذا عن المهام متعددة اللغات؟ هل يتعامل مع اللغات الأخرى بشكل جيد؟

ب: نعم، تم تدريب DeepSeek-V3 على مجموعة نصوص متعددة اللغات، ويؤدي أداءً جيدًا في معايير مثل MMMLU، التي تتضمن مهامًا غير إنجليزية. إنه قوي بشكل خاص في الصينية، متفوقًا على نماذج مثل Qwen2.5 في المعايير الصينية مثل C-Eval وCMMLU.

أ: هذا مثير للإعجاب. ولكن ماذا عن مهام السياق الطويل؟ رأيت أنه يدعم حتى 128 ألف رمز. كيف يتعامل مع مثل هذه المدخلات الطويلة؟

ب: يوسع DeepSeek-V3 طول السياق في مرحلتين: أولاً إلى 32 ألف رمز ثم إلى 128 ألف رمز باستخدام تقنية YaRN. هذا يسمح له بالتعامل مع مهام السياق الطويل مثل تلخيص المستندات واسترجاعها بفعالية. كما أنه يؤدي أداءً جيدًا في اختبار 'Needle In A Haystack'، الذي يقيم فهم السياق الطويل.

أ: هذا تحسن كبير عن النماذج السابقة. ولكن ماذا عن النشر؟ كيف يتعاملون مع الاستدلال لنموذج بهذا الحجم؟

ب: يتم التعامل مع الاستدلال على مجموعة H800، مع اتصال معالجات GPU باستخدام NVLink وInfiniBand. تفصل استراتيجية النشر بين مراحل التعبئة المسبقة وفك التشفير لضمان كل من الإنتاجية العالية والكمون المنخفض. كما يستخدمون خبراء زائدين لموازنة الحمل أثناء الاستدلال، مما يساعد في الحفاظ على الكفاءة.

أ: هذه الكثير من التحسينات. ولكن ما هي القيود؟ بالتأكيد، لنموذج بهذا الحجم بعض المقايضات.

ب: أحد القيود هو حجم وحدة النشر. يتطلب DeepSeek-V3 مجموعة كبيرة نسبيًا للاستدلال الفعال، مما قد يمثل تحديًا للفرق الأصغر. هناك أيضًا مجال لتحسين سرعة التوليد، على الرغم من أن فك التشفير التخميني مع MTP يساعد.

أ: هذا معقول. ولكن بشكل عام، يبدو أنه خطوة كبيرة إلى الأمام. ما التالي لـ DeepSeek-V3؟ هل هناك أي اتجاهات مستقبلية يستكشفونها؟

ب: إنهم ينظرون في عدة مجالات، مثل تحسين الهندسة المعمارية لدعم طول سياق لا نهائي، واستكشاف مصادر إشارات تدريب إضافية، وتعزيز قدرات التفكير في النموذج. كما يعملون على طرق تقييم أكثر شمولاً لتقييم أداء النموذج بشكل أفضل.

أ: يبدو أنهم لا يبطئون في أي وقت قريب. شكرًا لك على walked me through كل هذا — DeepSeek-V3 هو بالتأكيد محول للعبة في مجال LLM مفتوح المصدر.

ب: Absolutely! إنه من المثير رؤية إلى أي مدى وصلت النماذج مفتوحة المصدر. DeepSeek-V3 يدفع الحدود، ولا يمكنني الانتظار لرؤية ما سيفعلونه بعد.