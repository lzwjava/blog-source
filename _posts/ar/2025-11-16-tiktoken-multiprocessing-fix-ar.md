---
audio: false
generated: true
image: false
lang: ar
layout: post
model: x-ai/grok-4-fast
title: إصلاح خطأ التخليل في تعدد المعالجة لتوكتوكين
translated: true
type: note
---

خطأك شائع جدًا عند استخدام `datasets.map(..., num_proc>1)` مع مُرمِّز مخصص (مثل مُرمِّز Tiktoken المستخدم في سكريبت nanoGPT's FineWeb) — حيث يقوم المُرمِّز بتحميل ملف BPE (`cl100k_base.tiktoken` أو ما شابه) ومقبض هذا الملف **لا يمكن تسليله (pickled)** عند إرساله إلى العمليات الفرعية على Linux (معالجة متعددة قائمة على الـ fork).

تتعطل العمليات الفرعية فورًا → "أحد العمليات الفرعية توقف بشكل مفاجئ".

### الإصلاح السريع (يعمل بنسبة 99% من الوقت)

قم بتعديل `data/fineweb/prepare.py` وأجبر عملية الترميز على التشغيل في **العملية الرئيسية فقط**:

```python
# حول السطر 80-85، ابحث عن استدعاء .map() وغير num_proc=8 → num_proc=1

tokenized = raw_datasets.map(
    tokenize,
    num_proc=1,               # ← غير هذا السطر (أو احذف الوسيط ببساطة)
    desc="tokenizing the splits",
)
```

سيكون أبطأ (نواة واحدة)، ولكنه سيعمل بموثوقية. على FineWeb-10B، هذا يأخذ حوالي 30-60 دقيقة اعتمادًا على وحدة المعالجة المركزية/القرص لديك.

### إصلاح أفضل (احتفظ بالمعالجة المتعددة، ولا يزال سريعًا)

اجعل المُرمِّز يحمّل بكسل داخل عملية العامل بدلاً من النطاق العام.

استبدل إنشاء المُرمِّز الحالي (حوالي السطر 40-50) بهذا النمط:

```python
import tiktoken

# احذف السطر العام encoder = tiktoken.get_encoding("cl100k_base")

def get_encoder():
    """قم بتحميل المُرمِّز بكسل في كل عملية فرعية"""
    return tiktoken.get_encoding("cl100k_base")

def tokenize(example):
    enc = get_encoder()                                   # ← محمل في كل عملية
    ids = enc.encode_ordinary(example["text"])
    ids.append(enc.eot_token)  # أو أيًا كان ما يفعله السكريبت
    return {"ids": ids, "len": len(ids)}
```

ثم احتفظ بـ `num_proc=8` (أو أي رقم). بهذه الطريقة، كل عامل يحمل نسخته الخاصة من ملف .tiktoken بعد التفرع → لا توجد مشاكل في التسليلة (pickling).

### بديل أسرع (موصى به للتشغيل المتكرر)

قم بتحميل أجزاء FineWeb-Edu ذات السعة 10B/100B مسبقًا من Hugging Face واستخدم السكريبت المحدث من Karpathy الذي يعمل مباشرة على ملفات parquet/arrow بدلاً من ملف .txt الضخم:

```bash
# أسرع بكثير ويتجنب هذا الخطأ تمامًا
python data/fineweb/prepare_fineweb.py --version 10B
# أو 100B
```

السكريبت الجديد (تمت إضافته ~2024) يستخدم مجموعة بيانات HF الرسمية وهو آمن للمعالجة المتعددة.

### الملخص

| الحل                          | السرعة     | التعقيد | التوصية |
|-----------------------------------|-----------|------------|----------------|
| `num_proc=1`                      | بطيء      | سطر واحد   | إصلاح سريع     |
| مُرمِّز كسول داخل الدالة tokenize() | سريع      | تعديل بسيط | الأفضل لملفات .txt  |
| استخدام سكريبت prepare_fineweb.py الجديد | الأسرع   | أمر جديد   | موصى به        |

اختر أحد هذه الحلول وستتجاوز هذا الخطأ فورًا. دعني أعرف أي طريق سلكت وسأعطيك التصحيح الدقيق إذا لزم الأمر!