---
audio: false
generated: false
lang: ar
layout: post
title: من الشبكة العصبية إلى GPT
translated: true
---

### مقاطع فيديو على YouTube

أندريه كارباثي - لنبني GPT: من الصفر، في الكود، موضحًا بالتفصيل.

عمر جميل - الاهتمام هو كل ما تحتاجه (المحول) - شرح النموذج (بما في ذلك الرياضيات)، الاستدلال والتدريب

StatQuest مع Josh Starmer - شبكات Transformer العصبية، أساس ChatGPT، موضحة بوضوح!!!

باسكال بوربارت - محاضرة CS480/680 رقم 19: الانتباه وشبكات المحولات (Attention and Transformer Networks)

الهاكر الذكي الاصطناعي - مايكل فاي - دليل مصور لشبكات Transformers العصبية: شرح خطوة بخطوة

### كيف أتعلم

بمجرد أن قرأت نصف كتاب "الشبكات العصبية والتعلم العميق"، بدأت في تكرار مثال الشبكة العصبية للتعرف على الأرقام المكتوبة بخط اليد. قمت بإنشاء مستودع على GitHub، https://github.com/lzwjava/neural-networks-and-zhiwei-learning.

هذا هو الجزء الصعب حقًا. إذا استطاع شخص ما كتابته من الصفر دون نسخ أي كود، فهذا يعني أنه يفهمه جيدًا.

لا يزال كود النسخ المتماثل الخاص بي يفتقر إلى تنفيذ `update_mini_batch` و `backprop`. ومع ذلك، من خلال مراقبة المتغيرات بعناية في مرحلة تحميل البيانات، التغذية الأمامية، والتقييم، حصلت على فهم أفضل بكثير للمتجهات، الأبعاد، المصفوفات، وشكل الكائنات.

وبدأت في تعلم كيفية تنفيذ GPT والمحولات (transformer). من خلال تضمين الكلمات (word embedding) والتشفير الموضعي (positional encoding)، يتم تحويل النص إلى أرقام. ثم، في الجوهر، لا يوجد فرق بينها وبين الشبكة العصبية البسيطة المستخدمة في التعرف على الأرقام المكتوبة بخط اليد.

محاضرة أندريه كارباثي "Let's build GPT" ممتازة جدًا. إنه يشرح الأشياء بشكل جيد.

السبب الأول هو أنه يتم بالفعل من الصفر. نرى أولاً كيفية إنشاء النص. إنه نوع من الضبابية والعشوائية. السبب الثاني هو أن أندريه كان بإمكانه شرح الأشياء بشكل بديهي للغاية. قام أندريه بمشروع nanoGPT لعدة أشهر.

لقد خطرت لي فكرة جديدة للحكم على جودة المحاضرة. هل يمكن للمؤلف حقًا كتابة هذه الأكواد؟ لماذا لا أفهم وأي موضوع يفتقده المؤلف؟ بالإضافة إلى هذه الرسوم البيانية والرسوم المتحركة الأنيقة، ما هي عيوبها ونقاط ضعفها؟

بالعودة إلى موضوع تعلم الآلة نفسه. كما ذكر أندريه، هناك الإسقاط (Dropout)، الاتصال المتبقي (Residual Connection)، الانتباه الذاتي (Self-Attention)، الانتباه متعدد الرؤوس (Multi-Head Attention)، والانتباه المقنع (Masked Attention).

من خلال مشاهدة المزيد من الفيديوهات أعلاه، بدأت أفهم قليلاً.

من خلال الترميز الموضعي باستخدام دوال الجيب وجيب التمام، نحصل على بعض الأوزان. ومن خلال تضمين الكلمات، نقوم بتحويل الكلمات إلى أرقام.

$$
    PE_{(pos,2i)} = \sin\left(\frac{pos}{10000^{2i/d_{model}}}\right) \\
    PE_{(pos,2i+1)} = \cos\left(\frac{pos}{10000^{2i/d_{model}}}\right)
$$

> خرجت البيتزا من الفرن وكان طعمها لذيذًا.

في هذه الجملة، كيف يعرف الخوارزمية ما إذا كانت تشير إلى البيتزا أو الفرن؟ كيف نحسب التشابهات لكل كلمة في الجملة؟

نريد مجموعة من الأوزان. إذا استخدمنا شبكة المحولات (transformer) للقيام بمهمة الترجمة، في كل مرة نقوم بإدخال جملة، يمكنها إخراج الجملة المقابلة بلغة أخرى.

حول الضرب النقطي هنا. أحد الأسباب التي تجعلنا نستخدم الضرب النقطي هنا هو أن الضرب النقطي يأخذ في الاعتبار كل رقم في المتجه. ماذا لو استخدمنا الضرب النقطي المربع؟ نقوم أولاً بحساب مربع الأرقام، ثم نجعلها تقوم بالضرب النقطي. ماذا لو قمنا ببعض الضرب النقطي المعكوس؟

حول الجزء المقنع هنا، نقوم بتغيير أرقام نصف المصفوفة إلى ما لا نهاية سالبة. ثم نستخدم softmax لجعل القيم تتراوح من 0 إلى 1. ماذا لو قمنا بتغيير الأرقام في الجزء السفلي الأيسر إلى ما لا نهاية سالبة؟

### الخطة

استمر في قراءة الأكواد والأوراق البحثية ومشاهدة الفيديوهات. فقط استمتع واتبع فضولي.

https://github.com/karpathy/nanoGPT

https://github.com/jadore801120/attention-is-all-you-need-pytorch