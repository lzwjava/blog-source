---
audio: false
generated: true
image: false
lang: ar
layout: post
title: 'vLLM: تقديم نموذج لغوي كبير عالي الأداء'
translated: true
type: note
---

vLLM هي مكتبة عالية الأداء لاستدلال ونماذج اللغة الكبيرة (LLM) وخدمتها. فيما يلي شرح لأبرز الميزات التي ذكرتها، مُقسّم من أجل الوضوح:

### 1. **إنتاجية خدمة من الطراز الأول**
   - **المعنى**: صُمم vLLM لتعظيم عدد الطلبات أو الرموز المميزة التي تتم معالجتها في الثانية، مما يوفر إنتاجية عالية لاستدلال نماذج اللغة الكبيرة.
   - **كيفية تحقيق ذلك**: يقوم بتحسين خطورة الاستدلال بالكامل، من معالجة الطلبات إلى تنفيذ النموذج، مما يقلل من الحمل الزائد ويستفيد من مسرعات الأجهزة (مثل وحدات معالجة الرسومات) بكفاءة. يضمن هذا أوقات استجابة سريعة حتى تحت الأحمال الثقيلة.

### 2. **إدارة فعالة لذاكرة الانتباه للمفاتيح والقيم مع PagedAttention**
   - **المعنى**: PagedAttention هي تقنية إدارة ذاكرة لآلية الانتباه في نماذج اللغة الكبيرة القائمة على المحولات (Transformers).
   - **الشرح**: في المحولات، تقوم آلية الانتباه بتخزين موترات المفاتيح والقيم (KV) لكل رمز مميز، مما يمكن أن يستهلك ذاكرة GPU كبيرة. تقوم PagedAttention بتقسيم ذاكرة التخزين المؤقت هذه إلى "صفحات" أصغر يمكن إدارتها، على غرار الذاكرة الظاهرية في أنظمة التشغيل. يقلل هذا من تجزئة الذاكرة، ويسمح بإعادة استخدام فعالة للذاكرة، ويدعم نماذج أكبر أو تسلسلات أطول دون نفاد ذاكرة GPU.

### 3. **المعالجة الدفعية المستمرة للطلبات الواردة**
   - **المعنى**: تقوم المعالجة الدفعية المستمرة بتجميع الطلبات الواردة ديناميكيًا لمعالجتها معًا، مما يحسن الكفاءة.
   - **الشرح**: بدلاً من معالجة كل طلب على حدة، يقوم vLLM بمعالجة عدة طلبات دفعة واحدة في الوقت الفعلي عند وصولها. يقوم بضبط حجم الدفعة وتكوينها ديناميكيًا، مما يقلل وقت الخمول ويعظم استخدام GPU. يكون هذا مفيدًا بشكل خاص للتعامل مع أحمال العمل المتغيرة في سيناريوهات الخدمة الواقعية.

### 4. **تنفيذ نموذج سريع مع CUDA/HIP Graph**
   - **المعنى**: تُستخدم رسوم CUDA/HIP لتحسين تنفيذ GPU من خلال تحديد سلسلة من العمليات مسبقًا.
   - **الشرح**: عادةً ما تتضمن عمليات GPU إطلاق نواة متعددة، مما يترتب عليه حمل زائد. تسمح رسوم CUDA/HIP لـ vLLM بتسجيل سلسلة من العمليات (مثل ضرب المصفوفات، والتفعيل) في رسم بياني قابل للتنفيذ واحد، مما يقلل من حمل الإطلاق ويحسن سرعة التنفيذ. يكون هذا فعالاً بشكل خاص للمهام المتكررة في استدلال نماذج اللغة الكبيرة.

### 5. **التكميم: GPTQ, AWQ, AutoRound, INT4, INT8, و FP8**
   - **الممعنى**: يقلل التكميم من دقة أوزان النموذج والتفعيلات (على سبيل المثال، من نقطة عائمة 32 بت إلى تنسيقات أقل عددًا من البتات) لتوفير الذاكرة وتسريع الحساب.
   - **الشرح**:
     - **GPTQ**: طريقة تكميم ما بعد التدريب تضغط الأوزان إلى 4 بت أو أقل، مع الحفاظ على دقة عالية.
     - **AWQ (تكميم الأوزان الحساس للتفعيل)**: يحسن التكميم من خلال النظر في توزيعات التفعيل، مما يحسن الأداء لنماذج محددة.
     - **AutoRound**: تقنية تكميم آلية تضبط قرارات التقليل لتقليل فقدان الدقة.
     - **INT4/INT8**: تكميم قائم على الأعداد الصحيحة (4 بت أو 8 بت)، مما يقلل البصمة الذاكرية ويمكن من حساب أسرع على الأجهزة المتوافقة.
     - **FP8**: تنسيق نقطة عائمة 8 بت، يوازن بين الدقة والكفاءة، خاصة على وحدات معالجة الرسومات الحديثة التي تدعم FP8 (مثل NVIDIA H100).
   - **التأثير**: تقلل طرق التكميم هذه من استخدام الذاكرة، مما يسمح بنماذج أكبر لتناسب وحدات معالجة الرسومات وتسريع الاستدلال دون فقدان كبير في الدقة.

### 6. **نواة CUDA المُحسّنة، بما في ذلك التكامل مع FlashAttention و FlashInfer**
   - **المعنى**: يستخدم vLLM نواة CUDA مُحسّنة للغاية (كود GPU منخفض المستوى) مصممة خصيصًا لنماذج اللغة الكبيرة، بما في ذلك آليات الانتباه المتقدمة مثل FlashAttention و FlashInfer.
   - **الشرح**:
     - **نواة CUDA**: هذه هي برامج GPU مخصصة ومحسنة لعمليات نماذج اللغة الكبيرة المحددة، مثل ضرب المصفوفات أو حسابات الانتباه، مما يقلل وقت التنفيذ.
     - **FlashAttention**: خوارزمية انتباه عالية الكفاءة تقلل من الوصول إلى الذاكرة والحساب من خلال إعادة صياغة آلية الانتباه لتقليل العمليات الزائدة عن الحاجة. إنها سريعة بشكل خاص للتسلسلات الطويلة.
     - **FlashInfer**: امتداد أو بديل لـ FlashAttention، يحسن أداء الانتباه进一步 لسيناريوهات استخدام أو أجهزة محددة.
   - **التأثير**: تجعل هذه التحسينات حسابات الانتباه أسرع وأكثر كفاءة في استخدام الذاكرة، وهو أمر بالغ الأهمية لنماذج اللغة الكبيرة القائمة على المحولات.

### 7. **فك الترميز التخميني**
   - **المعنى**: يُسرّع فك الترميز التخميني توليد النص من خلال التنبؤ بعدة رموز مميزة في وقت واحد والتحقق منها لاحقًا.
   - **الشرح**: بدلاً من توليد رمز مميز واحد في كل مرة، يستخدم vLLM نموذجًا أصغر حجمًا وأسرع (أو إرشاديًا) للتنبؤ بعدة رموز مميزة بالتوازي. ثم يقوم النموذج الرئيسي بالتحقق من هذه التوقعات في تمريرة واحدة. إذا كانت صحيحة، يقلل هذا من عدد تقييمات النموذج، مما يسرع عملية التوليد. إذا كانت غير صحيحة، فإنه يعود إلى فك الترميز القياسي، مما يضمن الدقة.
   - **التأثير**: تعزز هذه التقنية الإنتاجية بشكل كبير لمهام توليد النص مثل الدردشة أو سرد القصص.

### 8. **الملء المسبق المجزأ**
   - **المعنى**: يقسم الملء المسبق المجزأ المعالجة الأولية (الملء المسبق) لمطالط الإدخال إلى أجزاء أصغر لتحسين الذاكرة والحساب.
   - **الشرح**: في نماذج اللغة الكبيرة، تتضمن مرحلة الملء المسبق حساب ذاكرة التخزين المؤقت للانتباه للمفاتيح والقيم (KV) للمطالط قبل توليد الرموز المميزة. بالنسبة للمطالط الطويلة، يمكن أن يكون هذا مكثفًا للذاكرة. تعالج عملية الملء المسبق المجزأ المطالط في مقاطع أصغر، مما يقلل من استخدام ذاكرة الذروة ويسمح بالتعامل بكفاءة مع التسلسلات الطويلة.
   - **التأثير**: يمكن vLLM من التعامل مع مدخلات أطول دون الحاجة إلى ذاكرة GPU مفرطة، مما يحسن قابلية التوسع.

### الملخص
تأتي سرعة vLLM من مزيج من إدارة الذاكرة المُحسّنة (PagedAttention، الملء المسبق المجزأ)، والتعامل الفعال مع الطلبات (المعالجة الدفعية المستمرة)، وتسريع الأجهزة (رسوم CUDA/HIP، النواة المُحسّنة، FlashAttention/FlashInfer)، وضغط النموذج (التكميم). تقنيات مثل فك الترميز التخميني تعزز further سرعة التوليد. معًا، تجعل هذه الميزات vLLM أداة قوية لنشر نماذج اللغة الكبيرة على نطاق واسع بإنتاجية عالية وكُمون منخفض.

**المراجع**:  
[وثائق vLLM](https://vllm.ai/)  
[ورقة بحثية عن FlashAttention](https://arxiv.org/abs/2205.14135)