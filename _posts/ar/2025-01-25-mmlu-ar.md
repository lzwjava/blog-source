---
audio: true
lang: ar
layout: post
title: معيار MMLU
translated: true
---

هذه التدوينة تقيم نموذجًا لغويًا على معيار MMLU (فهم اللغة متعدد المهام الضخم).

معيار MMLU هو اختبار شامل لقدرة النموذج على أداء مهام متنوعة عبر مجموعة واسعة من الموضوعات. يتكون من أسئلة اختيار من متعدد تغطي مجالات متنوعة مثل الرياضيات، التاريخ، القانون، والطب.

**روابط مجموعة البيانات:**

*   [Papers with Code](https://paperswithcode.com/dataset/mmlu)
*   [Hugging Face Datasets](https://huggingface.co/datasets/cais/mmlu)

```python
import torch
from datasets import load_dataset
import requests
import json
from tqdm import tqdm

# تحميل مجموعة بيانات MMLU
subject = "college_computer_science"  # اختر الموضوع الخاص بك
dataset = load_dataset("cais/mmlu", subject, split="test")

# تنسيق النص بدون أمثلة قليلة
def format_mmlu_prompt(example):
    prompt = "الأسئلة التالية هي أسئلة اختيار من متعدد حول {}".format(subject.replace("_", " "))
    prompt += ". يرجى الإجابة بحرف الخيار الصحيح (A، B، C، أو D) فقط."
    prompt += " أجب بالحرف فقط. لا حاجة لتفسير."
    
    # إضافة السؤال الحالي
    prompt += f"السؤال: {example['question']}\n"
    prompt += "الخيارات:\nA. {}\nB. {}\nC. {}\nD. {}\n".format(*example['choices'])
    return prompt

# حلقة التقييم
correct = 0
total = 0

for i, example in tqdm(enumerate(dataset), total=len(dataset), desc="Evaluating"):
    prompt = format_mmlu_prompt(example)
    
    # إرسال طلب إلى llama-server
    url = "http://localhost:8080/v1/chat/completions"
    headers = {"Content-Type": "application/json"}
    data = {
        "messages": [{"role": "user", "content": prompt}]
    }
    
    print(f"Input to API: {data}")
    response = requests.post(url, headers=headers, data=json.dumps(data))
    
    if response.status_code == 200:
        output_text = response.json()["choices"][0]["message"]["content"]
        predicted_answer = output_text.strip()[0] if len(output_text.strip()) > 0 else ""
        print(f"Output from API: {output_text}")
    else:
        predicted_answer = ""
        print(f"Error: {response.status_code} - {response.text}")
    
    # المقارنة مع الإجابة الصحيحة
    
    answer_map = {0: "A", 1: "B", 2: "C", 3: "D"}
    ground_truth_answer = answer_map.get(example["answer"], "")
    is_correct = predicted_answer.upper() == ground_truth_answer
    if is_correct:
        correct += 1
    total += 1
    
    print(f"السؤال: {example['question']}")
    print(f"الخيارات: A. {example['choices'][0]}, B. {example['choices'][1]}, C. {example['choices'][2]}, D. {example['choices'][3]}")
    print(f"الإجابة المتوقعة: {predicted_answer}, الإجابة الصحيحة: {ground_truth_answer}, صحيحة: {is_correct}")
    print("-" * 30)

    if (i+1) % 10 == 0:
        accuracy = correct / total
        print(f"تمت معالجة {i+1}/{len(dataset)}. الدقة الحالية: {accuracy:.2%} ({correct}/{total})")


# حساب الدقة
accuracy = correct / total
print(f"الموضوع: {subject}")
print(f"الدقة: {accuracy:.2%} ({correct}/{total})")
```

السجل:

```bash
% python scripts/mmlu.py

Evaluating:   9%| 9/100 [01:31<15:19, 10.10s/it]Processed 10/100. Current Accuracy: 0.00% (0/10)
Evaluating:  19%| 19/100 [03:14<12:47,  9.47s/it]Processed 20/100. Current Accuracy: 0.00% (0/20)
Evaluating:  26%| 26/100 [04:30<13:44, 11.14s/it]

...

Processed 100/100. Current Accuracy: 40.00% (40/100)
Evaluating: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [15:15<00:00,  9.16s/it]
Subject: college_computer_science
Accuracy: 40.00% (40/100)
```