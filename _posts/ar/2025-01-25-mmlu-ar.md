---
audio: true
lang: ar
layout: post
title: معيار MMLU
translated: true
---

## مقدمة

هذا المنشور يقيم نموذج لغة باستخدام معيار MMLU (فهم اللغة متعدد المهام الضخم).

معيار MMLU هو اختبار شامل لقدرة النموذج على أداء مهام مختلفة عبر مجموعة واسعة من الموضوعات. يتكون من أسئلة متعددة الخيارات تغطي مجالات متنوعة مثل الرياضيات، التاريخ، القانون، والطب.

**روابط مجموعة البيانات:**

*   [Papers with Code](https://paperswithcode.com/dataset/mmlu)
*   [Hugging Face Datasets](https://huggingface.co/datasets/cais/mmlu)

## llama-server

لتشغيل llama-server:

```bash
build/bin/llama-server -m models/7B/mistral-7b-instruct-v0.2.Q4_K_M.gguf --port 8080
```

## معيار MMLU

لتشغيل كود معيار MMLU:

```python
import torch
from datasets import load_dataset
import requests
import json
from tqdm import tqdm

# تحميل مجموعة بيانات MMLU
subject = "college_computer_science"  # اختر الموضوع الخاص بك
dataset = load_dataset("cais/mmlu", subject, split="test")

# تنسيق النص بدون أمثلة قليلة
def format_mmlu_prompt(example):
    prompt = "The following are multiple-choice questions about {}".format(subject.replace("_", " "))
    prompt += ". Please answer with the letter of the correct choice (A, B, C, or D) only."
    prompt += " Answer the letter only. No explanation is needed."
    
    # إضافة السؤال الحالي
    prompt += f"Question: {example['question']}\n"
    prompt += "Choices:\nA. {}\nB. {}\nC. {}\nD. {}\n".format(*example['choices'])
    return prompt

# حلقة التقييم
correct = 0
total = 0

for i, example in tqdm(enumerate(dataset), total=len(dataset), desc="Evaluating"):
    prompt = format_mmlu_prompt(example)
    
    # إرسال طلب إلى llama-server
    url = "http://localhost:8080/v1/chat/completions"
    headers = {"Content-Type": "application/json"}
    data = {
        "messages": [{"role": "user", "content": prompt}]
    }
    
    print(f"Input to API: {data}")
    response = requests.post(url, headers=headers, data=json.dumps(data))
    
    if response.status_code == 200:
        output_text = response.json()["choices"][0]["message"]["content"]
        predicted_answer = output_text.strip()[0] if len(output_text.strip()) > 0 else ""
        print(f"Output from API: {output_text}")
    else:
        predicted_answer = ""
        print(f"Error: {response.status_code} - {response.text}")
    
    # المقارنة مع الإجابة الصحيحة
    
    answer_map = {0: "A", 1: "B", 2: "C", 3: "D"}
    ground_truth_answer = answer_map.get(example["answer"], "")
    is_correct = predicted_answer.upper() == ground_truth_answer
    if is_correct:
        correct += 1
    total += 1
    
    print(f"Question: {example['question']}")
    print(f"Choices: A. {example['choices'][0]}, B. {example['choices'][1]}, C. {example['choices'][2]}, D. {example['choices'][3]}")
    print(f"Predicted Answer: {predicted_answer}, Ground Truth: {ground_truth_answer}, Correct: {is_correct}")
    print("-" * 30)

    if (i+1) % 10 == 0:
        accuracy = correct / total
        print(f"Processed {i+1}/{len(dataset)}. Current Accuracy: {accuracy:.2%} ({correct}/{total})")


# حساب الدقة
accuracy = correct / total
print(f"Subject: {subject}")
print(f"Accuracy: {accuracy:.2%} ({correct}/{total})")
```

السجل:

```bash
% python scripts/mmlu.py

Evaluating:   9%| 9/100 [01:31<15:19, 10.10s/it]Processed 10/100. Current Accuracy: 0.00% (0/10)
Evaluating:  19%| 19/100 [03:14<12:47,  9.47s/it]Processed 20/100. Current Accuracy: 0.00% (0/20)
Evaluating:  26%| 26/100 [04:30<13:44, 11.14s/it]

...

Processed 100/100. Current Accuracy: 40.00% (40/100)
Evaluating: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [15:15<00:00,  9.16s/it]
Subject: college_computer_science
Accuracy: 40.00% (40/100)
```

## ollama

```bash
Processed 100/100. Current Accuracy: 40.00% (40/100)
Evaluating: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [02:18<00:00,  1.39s/it]
Subject: college_computer_science
Accuracy: 40.00% (40/100)
```

يبدو أن برنامج ollama أسرع بكثير من llama-server.

بالنسبة لـ ollama، يتم استخدام معلمات مثل.

```bash
/Applications/Ollama.app/Contents/Resources/ollama runner --model /Users/lzwjava/.ollama/models/blobs/sha256-ff82381e2bea77d91c1b824c7afb83f6fb73e9f7de9dda631bcdbca564aa5435 --ctx-size 8192 --batch-size 512 --n-gpu-layers 33 --threads 4 --parallel 4 --port 51151
```

## Deepseek

تقييم Deepseek.

```python
import torch
from datasets import load_dataset
import requests
import json
from tqdm import tqdm
import os
from openai import OpenAI
from dotenv import load_dotenv

load_dotenv()

# تحميل مجموعة بيانات MMLU
subject = "college_computer_science"  # اختر الموضوع الخاص بك
dataset = load_dataset("cais/mmlu", subject, split="test")

# تنسيق النص بدون أمثلة قليلة
def format_mmlu_prompt(example):
    prompt = "The following are multiple-choice questions about {}".format(subject.replace("_", " "))
    prompt += ". Please answer with the letter of the correct choice (A, B, C, or D) only."
    prompt += " Answer the letter only. Do not need Explanation."
    
    # إضافة السؤال الحالي
    prompt += f"Question: {example['question']}\n"
    prompt += "Choices:\nA. {}\nB. {}\nC. {}\nD. {}\n".format(*example['choices'])
    return prompt

# حلقة التقييم
correct = 0
total = 0

# تهيئة عميل DeepSeek
api_key = os.environ.get("DEEPSEEK_API_KEY")
if not api_key:
    print("Error: DEEPSEEK_API_KEY environment variable not set.")
    exit()
client = OpenAI(api_key=api_key, base_url="https://api.deepseek.com")


for i, example in tqdm(enumerate(dataset), total=len(dataset), desc="Evaluating"):
    prompt = format_mmlu_prompt(example)
    
    # إرسال طلب إلى واجهة برمجة تطبيقات DeepSeek
    try:
        response = client.chat.completions.create(
            model="deepseek-chat",
            messages=[
                {"role": "user", "content": prompt}
            ],
            max_tokens=100
        )
        if response and response.choices:
            output_text = response.choices[0].message.content.strip()
            predicted_answer = output_text.strip()[0] if len(output_text.strip()) > 0 else ""
            print(f"Output from API: {output_text}")
        else:
            predicted_answer = ""
            print("Error: No response from the API.")
    except Exception as e:
        predicted_answer = ""
        print(f"Error during API call: {e}")
    
    # المقارنة مع الإجابة الصحيحة
    
    answer_map = {0: "A", 1: "B", 2: "C", 3: "D"}
    ground_truth_answer = answer_map.get(example["answer"], "")
    is_correct = predicted_answer.upper() == ground_truth_answer
    if is_correct:
        correct += 1
    total += 1
    
    print(f"Question: {example['question']}")
    print(f"Choices: A. {example['choices'][0]}, B. {example['choices'][1]}, C. {example['choices'][2]}, D. {example['choices'][3]}")
    print(f"Predicted Answer: {predicted_answer}, Ground Truth: {ground_truth_answer}, Correct: {is_correct}")
    print("-" * 30)

    if (i+1) % 10 == 0:
        accuracy = correct / total
        print(f"Processed {i+1}/{len(dataset)}. Current Accuracy: {accuracy:.2%} ({correct}/{total})")


# حساب الدقة
accuracy = correct / total
print(f"Subject: {subject}")
print(f"Accuracy: {accuracy:.2%} ({correct}/{total})")
```

النتيجة:

```bash
Processed 100/100. Current Accuracy: 78.00% (78/100)
Evaluating: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [03:44<00:00,  2.24s/it]
Subject: college_computer_science
Accuracy: 78.00% (78/100)
```