---
audio: true
lang: ar
layout: post
title: تعليمات متعددة اللغات، معيار
translated: true
---

## المقدمة

يقدر هذا المنشور نموذج اللغة باستخدام مقياس MMLU (فهم اللغة متعددة المهام).

يقيس MMLU قدرة النموذج على أداء مهام متنوعة عبر مجموعة متنوعة من الموضوعات. ويحتوي على اسئلة متعددة الاختيار تغطي مجالات مثل الرياضيات، التاريخ، القانون والطب.

**رابطات البيانات:**

*   [أبحاث مع الكود](https://paperswithcode.com/dataset/mmlu)
*   [مجموعات Hugging Face](https://huggingface.co/datasets/cais/mmlu)

## llama-server

لتشغيل llama-server:

```bash
build/bin/llama-server -m models/7B/mistral-7b-instruct-v0.2.Q4_K_M.gguf --port 8080
```

## مقياس MMLU

يتم تقييم هذا المقياس باستخدام ثلاثة خلفيات مختلفة: `ollama`، `llama-server` و `deepseek`.

لاندفاع الكود MMLU:

```python
import torch
from datasets import load_dataset
import requests
import json
from tqdm import tqdm
import argparse
import os
from openai import OpenAI
from dotenv import load_dotenv
import time

load_dotenv()

# إعداد تحليل الجدول
parser = argparse.ArgumentParser(description="تقييم مجموعة بيانات MMLU مع خلفيات مختلفة.")
parser.add_argument("--type", type=str, default="ollama", choices=["ollama", "llama", "deepseek", "gemini", "deepseek-r1"], help="نوع الخلفية: ollama, llama, deepseek, أو gemini")
args = parser.parse_args()

# تحميل مجموعة بيانات MMLU
subject = "college_computer_science"  # اختر موضوعك
dataset = load_dataset("cais/mmlu", subject, split="test")

# تنسيق التوجيه بدون أمثلة قليلة
def format_mmlu_prompt(example):
    prompt = "الآتي هو الأسئلة المتعددة الاختيار حول {}".format(subject.replace("_", " "))
    prompt += ". يرجى الرد بالحرف الصحيح (أ، ب، ج، أو د) فقط."
    prompt += " الرد بالحرف فقط. لا يجب تقديم التفسير."

    # إضافة السؤال الحالي
    prompt += f"سؤال: {example['question']}\n"
    prompt += "الاختيارات:\nA. {}\nB. {}\nC. {}\nD. {}\n".format(*example['choices'])
    return prompt

# تهيئة عميل DeepSeek إذا لزم الأمر
def initialize_deepseek_client():
    api_key = os.environ.get("DEEPSEEK_API_KEY")
    if not api_key:
        print("خطأ: لم يتم تعيين متغير البيئة DEEPSEEK_API_KEY.")
        exit()
    return OpenAI(api_key=api_key, base_url="https://api.deepseek.com")

def call_gemini_api(prompt, retries=3, backoff_factor=1):
    gemini_api_key = os.environ.get("GEMINI_API_KEY")
    if not gemini_api_key:
        print("خطأ: لم يتم تعيين متغير البيئة GEMINI_API_KEY.")
        exit()
    url = f"https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash:generateContent"
    params = {"key": gemini_api_key}
    payload = {"contents": [{"parts": [{"text": prompt}]}]}

    for attempt in range(retries):
        response = requests.post(url, json=payload, params=params)
        if response.status_code != 429:
            return response.json()
        time.sleep(backoff_factor * (2 ** attempt))  # العودة المتداعية
    return None

import re

def process_ollama_response(response):
    if response.status_code == 200:
        output_text = response.json()["choices"][0]["message"]["content"]
        match = re.search(r"Answer:\s*([A-D])", output_text, re.IGNORECASE)
        if match:
            predicted_answer = match.group(1).upper()
        else:
            predicted_answer = output_text.strip()[0] if len(output_text.strip()) > 0 else ""
        print(f"الخروج من API: {output_text}")
        return predicted_answer
    else:
        print(f"خطأ: {response.status_code} - {response.text}")
        return ""

def process_llama_response(response):
    if response.status_code == 200:
        output_text = response.json()["choices"][0]["message"]["content"]
        predicted_answer = output_text.strip()[0] if len(output_text.strip()) > 0 else ""
        print(f"الخروج من API: {output_text}")
        return predicted_answer
    else:
        print(f"خطأ: {response.status_code} - {response.text}")
        return ""

def process_deepseek_response(client, prompt):
    try:
        response = client.chat.completions.create(
            model="deepseek-chat",
            messages=[
                {"role": "user", "content": prompt}
            ],
            max_tokens=100
        )
        if response and response.choices:
            output_text = response.choices[0].message.content.strip()
            predicted_answer = output_text.strip()[0] if len(output_text.strip()) > 0 else ""
            print(f"الخروج من API: {output_text}")
            return predicted_answer
        else:
            print("خطأ: لم يتم الحصول على إجابة من API.")
            return ""
    except Exception as e:
        print(f"خطأ أثناء اتصال API: {e}")
        return ""

def process_deepseek_r1_response(client, prompt, retries=3, backoff_factor=1):
    for attempt in range(retries):
        try:
            response = client.chat.completions.create(
                model="deepseek-reasoner",
                messages=[
                    {"role": "user", "content": prompt}
                ],
                max_tokens=100
            )
            if response and response.choices:
                output_text = response.choices[0].message.content.strip()
                predicted_answer = output_text.strip()[0] if len(output_text.strip()) > 0 else ""
                print(f"الخروج من API: {output_text}")
                return predicted_answer
            else:
                print("خطأ: لم يتم الحصول على إجابة من API.")
                return ""
        except Exception as e:
            if "502" in str(e):
                print(f"خطأ البوابة السريعة (502) أثناء اتصال API، المحاولة مرة أخرى بعد {backoff_factor * (2 ** attempt)} ثانية...")
                time.sleep(backoff_factor * (2 ** attempt))
            else:
                print(f"خطأ أثناء اتصال API: {e}")
                return ""
    print("تم الوصول إلى الحد الأقصى للمحاولات، العودة بإجابة فارغة.")
    return ""

def process_gemini_response(prompt):
    json_response = call_gemini_api(prompt)
    if not json_response:
        print("لم يتم الحصول على إجابة من API Gemini بعد المحاولات.")
        return ""
    if 'candidates' not in json_response or not json_response['candidates']:
        print("لم يتم العثور على مرشحين في الإجابة، المحاولة مرة أخرى...")
        json_response = call_gemini_api(prompt)
        if not json_response or 'candidates' not in json_response or not json_response['candidates']:
            print("لم يتم العثور على مرشحين في الإجابة بعد المحاولة.")
            return ""

    first_candidate = json_response['candidates'][0]
    if 'content' in first_candidate and 'parts' in first_candidate['content']:
        first_part = first_candidate['content']['parts'][0]
        if 'text' in first_part:
            output_text = first_part['text']
            predicted_answer = output_text.strip()[0] if len(output_text.strip()) > 0 else ""
            print(f"الخروج من API: {output_text}")
            return predicted_answer
        else:
            print("لم يتم العثور على نص في الإجابة")
            return ""
    else:
        print("تنسيق الإجابة غير متوقع: نقص المضمون أو الأجزاء")
        return ""

def evaluate_model(args, dataset):
    correct = 0
    total = 0
    client = None
    if args.type == "deepseek" or args.type == "deepseek-r1":
        client = initialize_deepseek_client()

    for i, example in tqdm(enumerate(dataset), total=len(dataset), desc="التقييم"):
        prompt = format_mmlu_prompt(example)
        predicted_answer = ""

        if args.type == "ollama":
            url = "http://localhost:11434/v1/chat/completions"
            data = {
                "messages": [{"role": "user", "content": prompt}],
                "model": "mistral:7b"
            }
            headers = {"Content-Type": "application/json"}
            print(f"المدخل إلى API: {data}")
            response = requests.post(url, headers=headers, data=json.dumps(data))
            predicted_answer = process_ollama_response(response)

        elif args.type == "llama":
            url = "http://localhost:8080/v1/chat/completions"
            data = {
                "messages": [{"role": "user", "content": prompt}]
            }
            headers = {"Content-Type": "application/json"}
            print(f"المدخل إلى API: {data}")
            response = requests.post(url, headers=headers, data=json.dumps(data))
            predicted_answer = process_llama_response(response)

        elif args.type == "deepseek":
            predicted_answer = process_deepseek_response(client, prompt)

        elif args.type == "deepseek-r1":
            predicted_answer = process_deepseek_r1_response(client, prompt)

        elif args.type == "gemini":
            predicted_answer = process_gemini_response(prompt)
        else:
            raise ValueError("نوع الخلفية غير صالح")

        answer_map = {0: "A", 1: "B", 2: "C", 3: "D"}
        ground_truth_answer = answer_map.get(example["answer"], "")
        is_correct = predicted_answer.upper() == ground_truth_answer
        if is_correct:
            correct += 1
        total += 1

        print(f"سؤال: {example['question']}")
        print(f"الاختيارات: A. {example['choices'][0]}, B. {example['choices'][1]}, C. {example['choices'][2]}, D. {example['choices'][3]}")
        print(f"الإجابة المخطوة: {predicted_answer}, الحقائق: {ground_truth_answer}, صحيح: {is_correct}")
        print("-" * 30)

        if (i+1) % 10 == 0:
            accuracy = correct / total
            print(f"تم معالجة {i+1}/{len(dataset)}. الدقة الحالية: {accuracy:.2%} ({correct}/{total})")

    return correct, total

# حلقة التقييم
correct, total = evaluate_model(args, dataset)

# حساب الدقة
accuracy = correct / total
print(f"الموضوع: {subject}")
print(f"الدقة: {accuracy:.2%} ({correct}/{total})")
```

## النتائج

### التقييم بدون أمثلة

| النموذج                     | الأسلوب                      | الموضوع                        | الدقة   |
|---------------------------|--------------------------|--------------------------------|------------|
| mistral-7b-instruct-v0.2, Q4_K_M | macOS m2, 16GB, llama-server | MMLU college_computer_science | 40.00% (40/100) |
| Mistral-7B-Instruct-v0.3, Q4_0  | macOS m2, 16GB, ollama      | MMLU college_computer_science | 40.00% (40/100) |
| deepseek v3 (API)               | API, 2025.1.25           | MMLU college_computer_science | 78.00% (78/100) |
| gemini-1.5-flash (API)          | API, 2025.1.25           | MMLU college_computer_science | 72.00% (72/100) |
| deepseek r1 (API)               | API, 2025.1.26           | MMLU college_computer_science | 87.14% (61/70) |
| Mistral Small Latest (API) | API, 2025.01.31 | MMLU college_computer_science | 65.00% (65/100) |
| Mistral Large Latest (API) | API, 2025.01.31 | MMLU college_computer_science | 73.00% (73/100) |
| Mistral Small 2501 (API)   | API, 2025.01.31 | MMLU college_computer_science | 66.00% (66/100) |

### الرسم

هنا سنخلق رسمًا بناءً على الجدول أعلاه.

```python
import matplotlib.pyplot as plt
import os

# بيانات العينة (قم باستبدالها ببياناتك الفعلية)
models = ['mistral-7b-instruct-v0.2 (llama.cpp)', 'Mistral-7B-Instruct-v0.3 (ollama)', 'deepseek v3 (API)', 'gemini-1.5-flash (API)', 'deepseek r1 (API)']
accuracy = [40.00, 40.00, 78.00, 72.00, 87.14]
subject = "college_computer_science"

# إنشاء الرسم الشريطي
plt.figure(figsize=(10, 6))
plt.bar(models, accuracy, color=['skyblue', 'lightcoral', 'lightgreen', 'gold', 'lightcoral'])
plt.xlabel('النموذج')
plt.ylabel('الدقة (%)')
plt.title(f'دقة مقياس MMLU لـ {subject}')
plt.ylim(0, 100)  # تعيين الحد الأقصى للأحرف إلى 0-100 للنسبة المئوية
plt.xticks(rotation=45, ha="right")  # دوران أسماء المحور السيني لأفضل قابلية القراءة
plt.tight_layout()

# إضافة قيم الدقة فوق العناصر
for i, val in enumerate(accuracy):
    plt.text(i, val + 1, f'{val:.2f}%', ha='center', va='bottom')

# حفظ الرسم كملف JPG في الدليل الحالي
plt.savefig(os.path.join(os.path.dirname(__file__), f'mmlu_accuracy_chart.jpg'))
plt.show()

```

{: .centered }
![](assets/images/mmlu/mmlu_accuracy_chart.jpg)
*دقة مقياس MMLU*{: .caption }