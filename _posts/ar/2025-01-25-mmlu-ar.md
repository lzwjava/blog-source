---
audio: true
lang: ar
layout: post
title: معيار MMLU
translated: true
---

## مقدمة

هذا المنشور يقيم نموذج لغة باستخدام معيار MMLU (فهم اللغة متعدد المهام الضخم).

معيار MMLU هو اختبار شامل لقدرة النموذج على أداء مهام متنوعة عبر مجموعة واسعة من الموضوعات. يتكون من أسئلة متعددة الخيارات تغطي مجالات متنوعة مثل الرياضيات، التاريخ، القانون، والطب.

**روابط مجموعة البيانات:**

*   [Papers with Code](https://paperswithcode.com/dataset/mmlu)
*   [Hugging Face Datasets](https://huggingface.co/datasets/cais/mmlu)

## llama-server

لتشغيل llama-server:

```bash
build/bin/llama-server -m models/7B/mistral-7b-instruct-v0.2.Q4_K_M.gguf --port 8080
```

## معيار MMLU

هذا النص البرمجي يقيم معيار MMLU باستخدام ثلاث واجهات خلفية مختلفة: `ollama`، `llama-server`، و `deepseek`.

لتشغيل كود تقييم معيار MMLU:

```python
import torch
from datasets import load_dataset
import requests
import json
from tqdm import tqdm
import argparse
import os
from openai import OpenAI
from dotenv import load_dotenv

load_dotenv()

# إعداد تحليل الوسائط
parser = argparse.ArgumentParser(description="Evaluate MMLU dataset with different backends.")
parser.add_argument("--type", type=str, default="ollama", choices=["ollama", "llama", "deepseek"], help="Backend type: ollama, llama, or deepseek")
args = parser.parse_args()

# تحميل مجموعة بيانات MMLU
subject = "college_computer_science"  # اختر الموضوع الخاص بك
dataset = load_dataset("cais/mmlu", subject, split="test", cache_dir="./.cache")

# تنسيق النص بدون أمثلة قليلة
def format_mmlu_prompt(example):
    prompt = "The following are multiple-choice questions about {}".format(subject.replace("_", " "))
    prompt += ". Please answer with the letter of the correct choice (A, B, C, or D) only."
    prompt += " Answer the letter only. Do not need Explanation."
    
    # إضافة السؤال الحالي
    prompt += f"Question: {example['question']}\n"
    prompt += "Choices:\nA. {}\nB. {}\nC. {}\nD. {}\n".format(*example['choices'])
    return prompt

# حلقة التقييم
correct = 0
total = 0

# تهيئة عميل DeepSeek إذا لزم الأمر
if args.type == "deepseek":
    api_key = os.environ.get("DEEPSEEK_API_KEY")
    if not api_key:
        print("Error: DEEPSEEK_API_KEY environment variable not set.")
        exit()
    client = OpenAI(api_key=api_key, base_url="https://api.deepseek.com")


for i, example in tqdm(enumerate(dataset), total=len(dataset), desc="Evaluating"):
    prompt = format_mmlu_prompt(example)
    
    # إرسال طلب إلى الواجهة الخلفية
    if args.type == "ollama":
        url = "http://localhost:11434/v1/chat/completions"
        data = {
            "messages": [{"role": "user", "content": prompt}],
            "model": "mistral:7b"
        }
        headers = {"Content-Type": "application/json"}
        print(f"Input to API: {data}")
        response = requests.post(url, headers=headers, data=json.dumps(data))
        if response.status_code == 200:
            output_text = response.json()["choices"][0]["message"]["content"]
            predicted_answer = output_text.strip()[0] if len(output_text.strip()) > 0 else ""
            print(f"Output from API: {output_text}")
        else:
            predicted_answer = ""
            print(f"Error: {response.status_code} - {response.text}")
    elif args.type == "llama":
        url = "http://localhost:8080/v1/chat/completions"
        data = {
            "messages": [{"role": "user", "content": prompt}]
        }
        headers = {"Content-Type": "application/json"}
        print(f"Input to API: {data}")
        response = requests.post(url, headers=headers, data=json.dumps(data))
        if response.status_code == 200:
            output_text = response.json()["choices"][0]["message"]["content"]
            predicted_answer = output_text.strip()[0] if len(output_text.strip()) > 0 else ""
            print(f"Output from API: {output_text}")
        else:
            predicted_answer = ""
            print(f"Error: {response.status_code} - {response.text}")
    elif args.type == "deepseek":
        try:
            response = client.chat.completions.create(
                model="deepseek-chat",
                messages=[
                    {"role": "user", "content": prompt}
                ],
                max_tokens=100
            )
            if response and response.choices:
                output_text = response.choices[0].message.content.strip()
                predicted_answer = output_text.strip()[0] if len(output_text.strip()) > 0 else ""
                print(f"Output from API: {output_text}")
            else:
                predicted_answer = ""
                print("Error: No response from the API.")
        except Exception as e:
            predicted_answer = ""
            print(f"Error during API call: {e}")
    else:
        raise ValueError("Invalid backend type")
    
    # المقارنة مع الإجابة الصحيحة
    
    answer_map = {0: "A", 1: "B", 2: "C", 3: "D"}
    ground_truth_answer = answer_map.get(example["answer"], "")
    is_correct = predicted_answer.upper() == ground_truth_answer
    if is_correct:
        correct += 1
    total += 1
    
    print(f"Question: {example['question']}")
    print(f"Choices: A. {example['choices'][0]}, B. {example['choices'][1]}, C. {example['choices'][2]}, D. {example['choices'][3]}")
    print(f"Predicted Answer: {predicted_answer}, Ground Truth: {ground_truth_answer}, Correct: {is_correct}")
    print("-" * 30)

    if (i+1) % 10 == 0:
        accuracy = correct / total
        print(f"Processed {i+1}/{len(dataset)}. Current Accuracy: {accuracy:.2%} ({correct}/{total})")


# حساب الدقة
accuracy = correct / total
print(f"Subject: {subject}")
print(f"Accuracy: {accuracy:.2%} ({correct}/{total})")
```

## النتائج

### التقييم بدون أمثلة

| النموذج                     | الطريقة                      | الموضوع                        | الدقة   |
|---------------------------|--------------------------|--------------------------------|------------|
| mistral-7b-instruct-v0.2, Q4_K_M | macOS m2, 16GB, llama-server | MMLU college_computer_science | 40.00% (40/100) |
| Mistral-7B-Instruct-v0.3, Q4_0  | macOS m2, 16GB, ollama      | MMLU college_computer_science | 40.00% (40/100) |
| deepseek v3               | API, 2025.1.25           | MMLU college_computer_science | 78.00% (78/100) |
| gemini-1.5-flash          | API, 2025.1.25           | MMLU college_computer_science | 78.00% (78/100) |