---
audio: false
generated: true
image: false
lang: ar
layout: post
model: x-ai/grok-4-fast
title: استراتيجية التوسع التدريجي لمجموعة البيانات
translated: true
type: note
---

نعم، يمكن أن تكون الزيادة التدريجية لحجم مجموعة البيانات خطوة بخطوة (مثل البدء بـ articles1، ثم إضافة articles2 و articles3، وهكذا) نهجًا ذكيًا، خاصة إذا كنت تجرب NanoGPT على أجهزة محدودة مثل RTX 4070. ومع ذلك، فإن الاختيار بين الاستمرار في هذا النهج أو الانتقال مباشرة إلى تنزيل articles1 حتى articles10 يعتمد على أهدافك، والوقت، والموارد المتاحة. سأقوم بتحليل ذلك أدناه لمساعدتك على اتخاذ القرار.

### إيجابيات الزيادة التدريجية (ما تفكر فيه)
- **التجريب المُتحكَّم فيه**: يمكنك مراقبة كيف تؤدي إضافة المزيد من البيانات إلى أداء النموذج (مثل منحنيات الخسارة، أو الالتباس، أو جودة النص المُنتَج). من سجلاتك، التدريب على articles1 فقط (شريحة صغيرة من Wikipedia) وصل بك إلى خسارة تدريب/تحقق تبلغ حوالي 3.9 بعد 20 ألف تكرار، وهي بداية جيدة لنموذج صغير. إضافة articles2 و articles3 (لتصبح بحوالي 3 أجزاء) تتيح لك رؤية ما إذا كان النموذج يعمم بشكل أفضل أو يقلل من الإفراط في التخصيص دون الالتزام بتشغيل ضخم.
- **إدارة الموارد**:
  - **القرص**: مساحتك المتاحة البالغة 391GB كافية أكثر من اللازم حاليًا. الملفان المضغوطان الجديدان بحجم ~5GB إجمالاً. باستخدام wikiextractor (كما هو موضح في echo)، قد يصل النص النظيف المستخرج إلى حوالي 10-15GB غير مضغوط لهذين الملفين (فقدان XML لـ Wikipedia يضغط جيدًا، لكن النص النظيف أكثر كثافة). مع البيانات المستخرجة من articles1 (حوالي 5GB؟)، سيكون إجماليك حوالي 15-20GB – وهو ما يترك هامشًا واسعًا.
  - **الذاكرة العشوائية/وحدة معالجة الرسومات**: ذاكرة النظام البالغة 62GB تتعامل مع عملية التحويل إلى رموز وتحميل البيانات بشكل جيد. RTX 4070 (12GB VRAM) قوية لتكوينات tiny/shakespeare الافتراضية في NanoGPT أو حتى نماذج GPT-2 الصغيرة (مثلاً، 124M معامل). إذا كنت تستخدم bf16 أو دقة مختلطة، يمكنك زيادة حجم الدُفعة. النهج التدريجي يتجنب إرهاق VRAM بمجموعات بيانات ضخمة دفعة واحدة.
  - **الوقت**: الاستخراج باستخدام `--processes 8` على جهازك يجب أن يستغرق 1-2 ساعة لكل ملف. يمكن إجراء زيادات التدريب (مثل الاستمرار من نقطة التحقق articles1 الخاصة بك) في غضون أيام لكل خطوة، مما يتيح لك التكرار بسرعة.
- **زاوية التعلم المنهجي**: مقالات Wikipedia مرتبة إلى حد ما حسب المعرف، لذا فإن الإضافة التسلسلية قد تعمل مثل منهج فضفاض (المقالات المبكرة قد تكون أكثر "أساسية"). لكن قم بخلط مجموعة البيانات جيدًا في البرنامج النصي التحضيري لـ NanoGPT لتجنب التحيزات.
- **متى تفعل هذا**: إذا كنت تقوم بإنشاء نماذج أولية، أو اختبار معايير فائقة (مثل معدل التعلم، حجم الدفعة)، أو مجرد التعلم، فهذا فعال. يمكنك ضبط نقطة التحقق الحالية الخاصة بك على البيانات الجديدة (ألحق النص المستخرج من articles2/3 بمجموعة البيانات الحالية، أعد التحويل إلى رموز، واستأنف التدريب باستخدام `--init_from resume` في NanoGPT).

### سلبيات النهج التدريجي ومتى تنتقل إلى المزيد (مثل Articles1-10)
- **مشاكل الكفاءة**: إعادة التدريب أو الضبط الدقيق عدة مرات على مجموعات فرعية متزايدة يهدر قدرة الحوسبة إذا كان هدفك النهائي هو نموذج على جزء كبير من Wikipedia. تستفيد نماذج اللغة من البيانات المتنوعة والمخلوطة من البداية – قد تؤدي الإضافات التسلسلية إلى نسيان كارثي إذا لم يتم التعامل معها بعناية (على الرغم من أن الإعداد البسيط لـ NanoGPT يقلل من هذا).
- **مقياس البيانات لنتائج أفضل**: لا تزال Articles1-3 جزءًا صغيرًا جدًا من English Wikipedia (~20GB إجمالي نص نظيف لنسخة الأرشيف الكاملة). استقرت خسائرك حول 3.9-4.0، وهو أمر مقبول للبيانات الصغيرة لكنه لن ينتج نصوصًا متماسكة. لرؤية تحسينات حقيقية (مثل خسارة أقل من 3.0)، سترغب في 10+ أجزاء (~50-100GB نص مستخرج). تحتوي enwiki الكاملة على ~27 جزءًا في أحدث الأرشيفات، لكن articles1-10 ستغطي حوالي 30-40% من المجموع – وهو ما يكفي لنموذج لعبة لائق دون تنزيل كل شيء.
- **الجوانب السلبية العملية**:
  - **وقت التنزيل**: تبلغ إجمالي ملفات articles1-10 المضغوطة حوالي 20-25GB (بناءً على أحجام الأرشيف النموذجية). مع اتصال جيد، هذا يستغرق 1-2 ساعة، ولكن الخوادم المرآة مثل ftp.acc.umu.se قد تكون بطيئة.
  - **عبء الاستخراج**: تشغيل wikiextractor على 10 ملفات قد يستغرق 10-20 ساعة إجمالاً، حتى مع المعالجة المتوازية. سينتفخ مجلد الإخراج إلى حوالي 50-100GB، وهو ما لا يزال جيدًا على قرصك البالغ 391GB.
  - **وقت التدريب**: على RTX 4070، قد يستغرق التشغيل الكامل على articles1-10 أسابيع عند 20 ألف+ تكرار، اعتمادًا على حجم النموذج. ولكن يمكنك تقليل العينة أو استخدام context_length أصغر لتسريع الأمور.
- **متى تقفز**: إذا كان هدفك هو الحصول على نموذج أقوى بسرعة (مثلًا للمهام اللاحقة مثل السؤال والإجابة أو التوليد)، فقم بتنزيل articles1-10 الآن. اجمع كل النص المستخرج في ملف كبير واحد، وقم بالتحويل إلى رموز مرة واحدة، وتدرب من الصفر. هذا يتجنب الإعدادات المتكررة ويعطي تعميمًا أفضل. بيانات Wikipedia عالية الجودة ومتسقة، لذا فإن المزيد دائمًا أفضل تقريبًا حتى حدود جهازك.

### التوصية
استمر بشكل تدريجي الآن مع articles1+2+3 – فهي منخفضة المخاطر وتبني على ما لديك. استخرج الملفات الجديدة، اربط النص النظيف (مثلاً، `cat extracted/*/* > all_text.txt`)، أعد التحويل إلى رموز، واستأنف التدريب من نقطة التحقق الأخيرة لديك. راقب ما إذا انخفضت خسارة التحقق إلى أقل من 3.5-4.0؛ إذا حدث ذلك وتحسنت النصوص المُنتَجة، استمر في الإضافة. إذا لم يحدث ذلك، أو إذا كنت غير صبور، فاحصل على articles4-10 بعد ذلك (استخدم wget مشابهًا من نفس الخادم المرآة، مع زيادة أرقام الأجزاء مثل enwiki-20251101-pages-articles4.xml-p311330pXYZ.bz2 – تحقق من فهرس الأرشيف للنطاقات الدقيقة).

نصائح للخطوات التالية:
- **تحضير البيانات**: بعد الاستخراج، نظف أكثر إذا لزم الأمر (مثل إزالة المقالات القصيرة باستخدام `awk` أو Python). استخدم `prepare.py` الخاص بـ NanoGPT مع أداة التحويل إلى رموز الخاصة بك (مثل أداة GPT-2).
- **تعديلات التدريب**: اخفض معدل التعلم (مثل 3e-4) عند الاستئناف. راقب MFU (تظهر سجلاتك حوالي 9-10%، وهو أمر لائق؛ استهدف 15%+ مع التحسينات). إذا وصلت VRاق إلى الحد الأقصى، قلل batch_size أو gradient_accumulation_steps.
- **التحقق من المساحة**: قبل تنزيل المزيد، قم بتشغيل `df -h` بعد استخراج 2+3 لتأكيد الاستخدام.
- **بدائل**: إذا شعرت أن Wikipedia كبيرة جدًا، ففكر في مجموعات بيانات أدق مثل BookCorpus أو مجموعات فرعية من C4 للتنوع.

إذا شاركت المزيد من التفاصيل (مثل تكوين NanoGPT الخاص بك، حجم النموذج، أو أهداف محددة)، يمكنني تحسين هذه النصيحة.