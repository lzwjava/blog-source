---
audio: false
generated: true
image: false
lang: ar
layout: post
model: x-ai/grok-4-fast
title: نظرة عامة على محول الرؤية
translated: true
type: note
---

### مقدمة حول Vision Transformer (ViT)

Vision Transformer (ViT) هو بنية شبكة عصبية تقوم بتكييف نموذج المحول (transformer) - الذي صُمم في الأصل لمعالجة اللغة الطبيعية (NLP) - لتلائم مهام الرؤية الحاسوبية، وخاصة تصنيف الصور. تم تقديمه في عام 2020 من قبل Alexey Dosovitskiy وزملاؤه في جوجل في الورقة البحثية *An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale*، يعامل ViT الصور على أنها تسلسلات من الرقع (patches) بدلاً من معالجتها باستخدام الشبكات العصبية التلافيفية (CNNs) التقليدية. وهذا يسمح له بالاستفادة من نقاط قوة المحول في التعامل مع التبعيات طويلة المدى والحساب المتوازي.

أظهر ViT أداءً منافسًا أو متفوقًا على الـ CNNs في مجموعات البيانات واسعة النطاق مثل ImageNet، خاصة عند التدريب المسبق على كميات هائلة من البيانات (مثل JFT-300M). المتغيرات مثل DeiT (Data-efficient Image Transformers) تجعله أكثر كفاءة لمجموعات البيانات الأصغر. اليوم، نماذج مستوحاة من ViT تدعم العديد من مهام الرؤية في نماذج مثل DALL-E و Stable Diffusion والمصنفات الحديثة.

### كيف يعمل ViT: البنية العامة وسير العمل

الفكرة الأساسية لـ ViT هي "تحويل" الصورة إلى تسلسل من الرقع ذات الحجم الثابت، بشكل مشابه لكيفية تقسيم النص إلى كلمات أو وحدات (tokens). ثم تتم معالجة هذا التسلسل بواسطة مشفر محول (transformer encoder) قياسي (بدون وحدة فك تشفير، على عكس نماذج النص التوليدية). إليك تفصيل خطوة بخطوة لكيفية عمله:

1. **المعالجة المسبقة للصورة واستخراج الرقع**:
   - ابدأ بصورة إدخال بحجم \\(H \times W \times C\\) (الارتفاع × العرض × القنوات، مثلاً 224 × 224 × 3 للـ RGB).
   - قسّم الصورة إلى رقع غير متداخلة بحجم ثابت \\(P \times P\\) (مثلاً 16 × 16 بكسل). هذا يُنتج \\(N = \frac{HW}{P^2}\\) رقعة (مثلاً 196 رقعة لصورة 224×224 مع رقع 16×16).
   - يتم تسطيح كل رقعة إلى متجه أحادي البعد بطول \\(P^2 \cdot C\\) (مثلاً 768 بُعدًا لـ 16×16×3).
   - لماذا الرقع؟ البكسل الخام سيخلق تسلسلاً طويلاً بشكل غير عملي (ملايين البكسل للصورة عالية الدقة)، لذا تعمل الرقع كـ "كلمات بصرية" لتقليل الأبعاد.

2. **تضمين الرقع (Patch Embedding)**:
   - طبق إسقاطًا خطيًا قابلًا للتعلم (طبقة fully connected بسيطة) على كل متجه رقعة مسطح، لتعيينه إلى بُعد تضمين ثابت \\(D\\) (مثلاً 768، ليطابق محولات من نوع BERT).
   - هذا يُنتج \\(N\\) متجه تضمين، كل منها بحجم \\(D\\).
   - اختياريًا، أضف تضمين رمز [CLS] الخاص (متجه قابل للتعلم بحجم \\(D\\)) في بداية التسلسل، مشابهًا لـ BERT لمهام التصنيف.

3. **تضمينات المواقع (Positional Embeddings)**:
   - أضف تضمينات مواقع أحادية البعد قابلة للتعلم إلى تضمينات الرقع لتشفير المعلومات المكانية (المحولات غير حساسة للتبديل بدون هذا).
   - التسلسل الكامل للإدخال أصبح الآن: \\([ \text{[CLS]}, \text{patch}_1, \text{patch}_2, \dots, \text{patch}_N ] + \text{positions}\\)، مصفوفة بالشكل \\((N+1) \times D\\).

4. **كتل مشفر المحول (Transformer Encoder Blocks)**:
   - أدخل التسلسل إلى \\(L\\) طبقة مشفر محول متراصة (مثلاً 12 طبقة).
   - كل طبقة تتكون من:
     - **الانتباه الذاتي متعدد الرؤوس (MSA)**: يحسب درجات الانتباه بين جميع أزواج الرقع (بما في ذلك [CLS]). هذا يسمح للنموذج بالتقاط العلاقات العالمية، مثل "أذن هذه القطة مرتبطة بالشعيرة الموجودة على بعد 100 رقعة"، على عكس المجالات الاستقبالية المحلية في الـ CNNs.
       - الصيغة: Attention(Q, K, V) = \\(\text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right) V\\)، حيث Q, K, V هي إسقاطات للإدخال.
     - **الشبكة العصبية متعددة الطبقات (MLP)**: شبكة feed-forward (طبقتين خطيتين مع تنشيط GELU) تطبق موضعيًا.
     - تطبيع الطبقة (Layer normalization) ووصلات التخطي (residual connections): الإدخال + MSA → Norm → MLP → Norm + الإدخال.
   - المخرجات: تسلسل من التضمينات المكررة، لا يزال بالشكل \\((N+1) \times D\\).

5. **رأس التصنيف (Classification Head)**:
   - لتصنيف الصور، استخرج مخرجات رمز [CLS] (أو خذ متوسط جميع تضمينات الرقع).
   - مرره عبر رأس MLP بسيط (مثلاً طبقة أو طبقتين خطيتين) لإخراج قيم logits للفئات.
   - أثناء التدريب، استخدم دالة الخسارة الانتروبيا المتقاطعة (cross-entropy) على البيانات المُعنونة. التدريب المسبق غالبًا ما يتضمن التنبؤ بالرقع المُقنعة (masked patch prediction) أو مهام أخرى ذاتية الإشراف.

**المعاملات الرئيسية** (من نموذج ViT-Base الأصلي):
- حجم الرقعة \\(P\\): 16
- بُعد التضمين \\(D\\): 768
- عدد الطبقات \\(L\\): 12
- عدد الرؤوس: 12
- عدد المعاملات: ~86 مليون

ViT يتوسع جيدًا: النماذج الأكبر (مثل ViT-Large مع \\(D=1024\\), \\(L=24\\)) تؤدي أداءً أفضل لكنها تحتاج إلى مزيد من البيانات/القدرة الحاسوبية.

**التدريب والاستدلال**:
- **التدريب**: من البداية للنهاية (End-to-end) على البيانات المُعنونة؛ يستفيد بشكل هائل من التدريب المسبق على مليارات الصور.
- **الاستدلال**: تمريرة أمامية (Forward pass) عبر المشفر (~O(N²) وقتًا بسبب الانتباه، لكنه فعال مع تحسينات مثل FlashAttention).
- على عكس الـ CNNs، لا يمتلك ViT تحيزات استقرائية (inductive biases) مثل ثبات الانزياح (translation invariance) — كل شيء يتم تعلمه.

### المقارنة مع محولات النص: أوجه التشابه والاختلاف

ViT هو في الأساس *نفس البنية* مثل جزء المشفر في محولات النص (مثل BERT)، ولكن تم تكييفه للبيانات البصرية ثنائية الأبعاد. إليك مقارنة جنبًا إلى جنب:

| الجانب              | محول النص (مثل BERT)                  | Vision Transformer (ViT)                       |
|---------------------|------------------------------------------------|------------------------------------------------|
| **تمثيل الإدخال** | تسلسل من الوحدات (tokens) (كلمات/أجزاء كلمات) مُضمنة في متجهات. | تسلسل من رقع الصور المُضمنة في متجهات. الرقع تشبه "الوحدات البصرية (visual tokens)". |
| **طول التسلسل** | متغير (مثلاً 512 وحدة لجملة).   | ثابت بناءً على حجم الصورة/حجم الرقعة (مثلاً 197 مع [CLS]). |
| **تشفير الموقع** | أحادي البعد (مطلق أو نسبي) لترتيب الكلمات.     | أحادي البعد (قابل للتعلم) لترتيب الرقع (مثلاً تسطيح بترتيب الصفوف). لا توجد بنية ثنائية الأبعاد مدمجة. |
| **الآلية الأساسية**  | الانتباه الذاتي (Self-attention) عبر الوحدات لنمذجة التبعيات. | الانتباه الذاتي عبر الرقع — نفس الرياضيات، لكنه ينتبه إلى "العلاقات" المكانية بدلاً من العلاقات النحوية. |
| **المخرجات/المهام**    | المشفر للتصنيف/نمذجة اللغة المقنعة (Masked LM); وحدة فك التشفير للتوليد. | مشفر فقط للتصنيف؛ يمكن توسيعه للكشف/التجزئة (detection/segmentation). |
| **نقاط القوة**       | يتعامل مع تبعيات النص طويلة المدى.         | السياق الشامل في الصور (مثلاً فهم المشهد بالكامل). |
| **نقاط الضعف**      | يحتاج إلى مجموعات نصية ضخمة.                      | جائع للبيانات؛ يواجه صعوبات في مجموعات البيانات الصغيرة بدون تدريب مسبق على الـ CNNs. |
| **أسلوب التنبؤ**| التنبؤ بالوحدة التالية (Next-token prediction) في وحدات فك التشفير (توليد تسلسلي). | لا يوجد تنبؤ "تسلسلي" بطبيعته — يصنف الصورة بأكملها بشكل كلي. |

في الجوهر، ViT هو تبديل "جاهز للاستخدام": استبدل تضمينات الوحدات (token embeddings) بتضمينات الرقع (patch embeddings)، وستحصل على نموذج رؤية. كلاهما يعتمد على الانتباه لوزن العلاقات في تسلسل، لكن النص تسلسلي/خطي بطبيعته، بينما الصور مكانية (يتعلم ViT هذا عبر الانتباه).

### معالجة مسألة "الوحدة التالية" مقابل "البكسل التالي" في ViT

لا، ViT *لا* يتنبأ بـ "البكسل التالي" مثلما يتنبأ محول النص بـ "الوحدة التالية" في التوليد التسلسلي (مثل GPT). إليك السبب:

- **محولات النص (التسلسلية)**: في نماذج مثل GPT، يولد فك الشفرة بشكل تسلسلي — وحدة واحدة في كل مرة، مع التكييف بناءً على جميع الوحدات السابقة. هذا يشبه التنبؤ بكسل بكسل في بعض نماذج توليد الصور (مثل PixelRNN)، لكنه غير فعال.

- **نهج ViT**: ViT هو *غير تسلسلي* (non-autoregressive) وكلي. يقوم بمعالجة الصورة *بالكامل* (جميع الرقع) بشكل متوازٍ عبر المشفر. لا يوجد تنبؤ تسلسلي "للتالي" أثناء الاستدلال — يحسب الانتباه العلاقات عبر *جميع* الرقع في وقت واحد. لمهام التوليد (مثل إكمال الصور المُقنعة)، قد تستخدم متغيرات ViT وحدة فك تشفير للتنبؤ بالرقع المُقنعة بشكل تسلسلي، لكن ViT الأساسي مخصص للتصنيف، وليس التوليد.

- **لماذا لا نستخدم البكسل؟** معاملة البكسل الفردي كوحدات (tokens) سيزيد طول التسلسل بشكل هائل (مثلاً 224×224×3 = ~150 ألف وحدة)، مما يجعل حساب الانتباه التربيعي غير مجدٍ حسابيًا (O(N²) = تريليونات العمليات). تقوم الرقع بتجميع ~256 بكسل (16×16) في "وحدة" واحدة، مما يحافظ على N قابلاً للإدارة (~200).

إذا كنت تريد توليدًا بصريًا (مثل بكسل تالي أو رقعة تالية)، فابحث عن نماذج مثل VQ-VAE + Transformer أو Parti، التي تبني على أفكار ViT.

لمزيد من التفاصيل، راجع الورقة البحثية الأصلية: [An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale](https://arxiv.org/abs/2010.11929). إذا كنت تريد أمثلة برمجية (مثلاً في PyTorch) أو مقارنات بمتغيرات محددة، دعني أعلم!