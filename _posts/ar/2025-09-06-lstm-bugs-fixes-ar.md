---
audio: false
date: 2025-09-06
generated: true
image: false
lang: ar
layout: post
title: أخطاء وتصحيحات تنفيذ LSTM
translated: true
type: note
---

حسناً—إليك شرح واضح وشامل لما يفعله هذا السكريبت لبناء LSTM من الصفر، بالإضافة إلى الأخطاء الدقيقة التي ستؤدي إلى تعطيل التدريب وكيفية إصلاحها. سأحافظ على تسلسل منطقي وسهولة القراءة (بدون جداول عريضة).

# ما يحاول الكود القيام به

أنت تنفذ نموذج لغة LSTM بسيط على مستوى الحروف/الكلمات باستخدام NumPy:

1. **الإعداد والأحجام**

* `hidden_size`, `vocab_size`, مجموعات البيانات (`training_set`, `validation_set`, `test_set`)، خرائط الرمز↔الفهرس، ودوال مثل `sigmoid`, `tanh`, `softmax`, `one_hot_encode_sequence` من المتوقع أن تأتي من `data.py` / `utils.py`.
* المدخلات في كل خطوة زمنية هي متجه one-hot `x ∈ R^{vocab_size×1}`.
* تقوم بدمج الحالة المخفية السابقة والمدخل الحالي:
  `z = [h_{t-1}; x_t] ∈ R^{(hidden_size+vocab_size)×1}`.
  لهذا السبب `W_f, W_i, W_g, W_o ∈ R^{hidden_size×(hidden_size+vocab_size)}`.

2. **تهيئة المعاملات**

* `init_lstm(...)` تخصص الأوزان للأربع بوابات (النسيان f، الإدخال i، المرشح g، الإخراج o)، بالإضافة إلى إسقاط الإخراج `W_v` إلى المفردات.
* `init_orthogonal` تجعل كل مصفوفة أوزان متعامدة (عبر QR). هذا خيار جيد لـ RNNs/LSTMs.

3. **المرور الأمامي (لكل تسلسل)**
   لكل خطوة زمنية:

* بناء `z = [h_prev; x]`.
* حساب البوابات:

  * `f_t = σ(W_f z + b_f)`
  * `i_t = σ(W_i z + b_i)`
  * `g_t = tanh(W_g z + b_g)`
  * `C_t = f_t ⊙ C_{t-1} + i_t ⊙ g_t`
  * `o_t = σ(W_o z + b_o)`
  * `h_t = o_t ⊙ tanh(C_t)`
* الإسقاط إلى المفردات و softmax:

  * `v_t = W_v h_t + b_v`
  * `ŷ_t = softmax(v_t)`
* يخزن الكود الكثير من القيم الوسيطة (`z_s, f_s, i_s, g_s, C_s, o_s, h_s, v_s, output_s`) للانتشار العكسي.

4. **الخسارة**

* الانتروبيا المتقاطعة في كل خطوة مع هدف one-hot:
  `L_t = − log ŷ_t[target]`
  (يستخدم الكود `-mean(log(ŷ)*one_hot)` بشكل عنصري، وهو مكافئ لـ `-log(ŷ[target])` حتى عامل قياس ثابت.)

5. **المرور العكسي (BPTT عبر LSTM)**

* تدرجات LSTM الكلاسيكية، مجمعة من `t = T−1 → 0`.
* قص معيار التدرج العام، ثم تحديث SGD.

6. **حلقة التدريب**

* لكل عصر: التقييم على مجموعة التحقق (الخسارة فقط)، ثم التدريب على مجموعة التدريب (الخسارة + التدرجات + التحديث). أخيراً، رسم بياني لخسائر التدريب/التحقق.

---

# مشاكل حرجة (هذه ستؤدي إلى كسر التعلم بصمت)

## 1) فهرسة خاطئة بمقدار واحد لـ `h` و `C` في الانتشار العكسي

في `forward`، تدفع الحالات *الأولية* أولاً:

* `h_s[0] = h_init`، ثم بعد الخطوة 0 تضع `h_0` → لذا `h_s` لها طول `T+1` مع `h_s[t+1] = h_t`.
* نفس الشيء لـ `C_s`: `C_s[0] = C_init`، ثم `C_s[t+1] = C_t`.

لكن في `backward(...)` تستخدم `h[t]` و `C[t]` كما لو كانت `h_t` و `C_t`. ليست كذلك؛ إنها مُزاحة بمقدار واحد.

**الإصلاح (قاعدة بسيطة):**

* استخدم `h[t+1]` حيث تريد `h_t`.
* استخدم `C[t+1]` حيث تريد `C_t`.
* لـ "الحالة السابقة للخلية" تريد `C_prev = C[t]` (وليس `C[t-1]`).

لذلك داخل حلقة `for t in reversed(range(T)):`:

* الحالة الحالية: `h_t = h[t+1]`, `C_t = C[t+1]`
* الحالة السابقة: `C_{t-1} = C[t]`

سطرك الحالي:

```python
C_prev = C[t - 1]
```

خاطئ لـ `t==0` (يلتف إلى العنصر الأخير) وخاطئ بمقدار واحد بشكل عام. يجب أن يكون:

```python
C_prev = C[t]       # الحالة السابقة للخلية
# واستخدم C_t = C[t+1] كـ "حالية"
```

وأي مكان تستخدم فيه `h[t]` بقصد الحالة المخفية الحالية، غيره إلى `h[t+1]`.

## 2) مشتقات خاطئة للعديد من البوابات

تطبق أحياناً الدالة غير الخطية مرة أخرى بدلاً من مشتقتها، أو تنسى علامة المشتق.

* **مسار حالة الخلية:**
  الصحيح:
  `dC_t += dh_t ⊙ o_t ⊙ (1 - tanh(C_t)^2)`
  كودك:

  ```python
  dC += dh * o[t] * tanh(tanh(C[t]), derivative=True)
  ```

  هذا تطبيق `tanh` مرتين. استبدله بـ:

  ```python
  dC += dh * o_t * (1 - np.tanh(C_t)**2)
  ```

* **بوبة النسيان:**
  الصحيح: `df = dC_t ⊙ C_{t-1} ⊙ f_t ⊙ (1 - f_t)`
  كودك:

  ```python
  df = dC * C_prev
  df = sigmoid(f[t]) * df
  ```

  ناقص مصطلح المشتق. يجب أن يكون:

  ```python
  df = dC * C_prev
  df *= f[t] * (1 - f[t])      # إذا كان f[t] يخزن ناتج σ قبل التنشيط
  ```

* **بوبة الإدخال:**
  فعلت:

  ```python
  di = dC * g[t]
  di = sigmoid(i[t], True) * di
  ```

  هذا جيد **إذا** `sigmoid(x, True)` تُرجع σ’(x) *وليس* σ(x). الأكثر قوة (مطابق لكيفية تخزينك `i[t]` كناتج للبوابة) هو:

  ```python
  di = dC * g[t]
  di *= i[t] * (1 - i[t])
  ```

* **بوبة المرشح:**
  فعلت:

  ```python
  dg = dC * i[t]
  dg = tanh(g[t], derivative=True) * dg
  ```

  إذا كان `g[t]` يخزن `tanh(preact)`، فإن `tanh’(preact) = 1 - g[t]^2`. لذا:

  ```python
  dg = dC * i[t]
  dg *= (1 - g[t]**2)
  ```

* **بوبة الإخراج:**
  فعلت:

  ```python
  do = dh * tanh(C[t])
  do = sigmoid(o[t], derivative=True) * do
  ```

  مع إصلاح الفهرسة (`C_t = C[t+1]`, `o_t = o[t]`) والمشتق كما ورد أعلاه:

  ```python
  do = dh * np.tanh(C_t)
  do *= o[t] * (1 - o[t])
  ```

* **تدرج الخلية التالي:**
  الصحيح:

  ```python
  dC_next = dC * f[t]
  ```

## 3) استخدام `h[0]` / `C[0]` لتحديد حجم `dh_next` و `dC_next`

تريد أشكال الـ h/C *الحالية* (نهاية التسلسل)، وليس الأصفار الأولية. استخدم:

```python
dh_next = np.zeros_like(h[-1])
dC_next = np.zeros_like(C[-1])
```

## 4) استقرار الحساب للانتروبيا المتقاطعة

`loss += -np.mean(np.log(outputs[t]) * targets[t])` سيكون جيداً إذا كانت `softmax` تثبت/تضيف إبسيلون داخلياً. إذا لم تكن كذلك، أضف إبسيلون صغير:

```python
eps = 1e-12
loss += -np.sum(targets[t] * np.log(outputs[t] + eps))
```

## 5) تعديلات استقرار التدريب

* **انحياز بوابة النسيان:** هيئ `b_f` إلى قيمة موجبة (مثلاً، `1.0`) حتى يميل LSTM في بداية التدريب إلى الاحتفاظ بالذاكرة:

  ```python
  b_f = np.ones((hidden_size, 1)) * 1.0
  ```
* **معدل التعلم:** `lr=1e-1` مع SGD خام يكون عدوانياً. ابدأ بـ `1e-2` أو استخدم Adam.
* **قص التدرج:** جيد. فكر في معايير لكل مصفوفة إذا لزم الأمر.

---

# نواة مصححة لحلقة الانتشار العكسي

هذا الرسم التخطيطي يظهر إصلاحات الفهرسة والمشتقات. يفترض أن `f[t], i[t], g[t], o[t]` هي *مخرجات* دوالها غير الخطية (كما في المرور الأمامي الخاص بك):

```python
T = len(outputs)
dh_next = np.zeros_like(h[-1])
dC_next = np.zeros_like(C[-1])

for t in reversed(range(T)):
    # أسماء مختصرة مع فهرسة صحيحة
    y_hat = outputs[t]                 # إخراج softmax
    y_true = targets[t]
    h_t   = h[t+1]
    C_t   = C[t+1]
    C_tm1 = C[t]

    # طبقة الإخراج
    dv = y_hat.copy()
    dv[np.argmax(y_true)] -= 1
    W_v_d += dv @ h_t.T
    b_v_d += dv

    # الانتشار العكسي إلى h
    dh = W_v.T @ dv
    dh += dh_next

    # بوابة الإخراج
    do = dh * np.tanh(C_t)
    do *= o[t] * (1 - o[t])
    W_o_d += do @ z[t].T
    b_o_d += do

    # حالة الخلية
    dC = dC_next + dh * o[t] * (1 - np.tanh(C_t)**2)

    # المرشح (g)، الإدخال (i)، النسيان (f)
    dg = dC * i[t]
    dg *= (1 - g[t]**2)               # بما أن g[t] = tanh(preact)

    di = dC * g[t]
    di *= i[t] * (1 - i[t])

    df = dC * C_tm1
    df *= f[t] * (1 - f[t])

    # اجمع تدرجات أوزان/انحيازات البوابة
    W_g_d += dg @ z[t].T;  b_g_d += dg
    W_i_d += di @ z[t].T;  b_i_d += di
    W_f_d += df @ z[t].T;  b_f_d += df

    # انشر إلى z واقسم إلى جزء h_{t-1}
    dz = (W_f.T @ df) + (W_i.T @ di) + (W_g.T @ dg) + (W_o.T @ do)
    dh_next = dz[:hidden_size, :]
    dC_next = dC * f[t]
```

---

# تفاصيل أصغر ولكن مهمة وتوصيات

* **التهيئة المتعامدة:** جيدة للأوزان المتكررة. لأجزاء المدخلات، يمكنك أيضاً القياس بـ `1/√fan_in`. ليس حرجاً هنا.
* **التجميع:** الكود يدرب على تسلسل واحد في كل مرة (بدون تجميع). جيد من أجل الوضوح، لكنه بطيء/صاخب. التجميع المصغر سيتطلب تغييرات دقيقة في الأشكال.
* **الإجبار بالمعلم:** أنت تستخدم الرمز التالي الصحيح كهدف؛ جيد تماماً. إذا قمت يوماً بتوليد حر، فاختر عينة من مخرجات `softmax` وادفعها مرة أخرى إلى النموذج.
* **الإبلاغ عن الخسارة:** تجمع خسائر كل خطوة زمنية عبر التسلسلات ثم المتوسط حسب طول مجموعة البيانات؛ هذا يخلط بين المقاييس إذا اختلفت التسلسلات في الطول. فكر في التطبيع حسب إجمالي الرموز.
* **الرسم:** يعمل محلياً. في البيئات بدون واجهة رسومية، تذكر تبديل الواجهات الخلفية أو استخدام savefig.

---

# ملخص سريع للإصلاحات التي يجب عليك إجراؤها الآن

1. **الفهرسة**: استبدل كل `h[t]` بـ `h[t+1]`، وكل `C[t]` بـ `C[t+1]`، واجعل `C_prev = C[t]` في الانتشار العكسي.
2. **المشتقات**:

   * استخدم `(1 - tanh(C_t)**2)` لمسار حالة الخلية.
   * استخدم `gate * (1 - gate)` لبوابات σ حيث أنك خزنت مخرجات البوابة.
   * استخدم `(1 - g[t]**2)` لبوبة المرشح حيث أن `g[t]=tanh(preact)`.
3. **التهيئة**: اجعل `b_f` يساوي تقريباً `1.0`.
4. **معدل التعلم**: اخفض إلى `1e-2` (أو انتقل إلى Adam).
5. **الاستقرار**: أضف `eps` في `log`.

قم بإجراء هذه التغييرات ويجب أن تبدأ خسارتك في الانخفاض بطريقة أكثر معقولية. إذا أردت، الصق `backward(...)` المحدثة وسأفحصها من أجل السلامة.