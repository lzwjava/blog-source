---
audio: false
generated: true
image: false
lang: ar
layout: post
model: x-ai/grok-4-fast
title: لماذا يتم تجنب الدالة السينية في النماذج اللغوية الكبيرة
translated: true
type: note
---

### لماذا لا تُستخدم الدالة السيجمويد في نماذج GPT/LLM

تعتمد نماذج اللغة الكبيرة مثل GPT على بنى المحولات، حيث تلعب دوال التنشيط دورًا رئيسيًا في إدخال اللاخطية في طبقات التغذية الأمامية ومعطيات الانتباه. بينما كانت دالة السيجمويد - المُعرَّفة كـ \\(\sigma(x) = \frac{1}{1 + e^{-x}}\\) والتي تضغط المدخلات إلى المجال (0، 1) - شائعة في الشبكات العصبية المبكرة (مثلًا للتصنيف الثنائي)، إلا أنها تُتجنب إلى حد كبير في نماذج LLM الحديثة للتنشيطات الداخلية. إليك الأسباب، مُقسمة حسب القضايا الرئيسية:

#### 1. **مشكلة اختفاء التدرجات**
   - الدالة السيجمويد تشبع في النهايات: بالنسبة لـ \\(x\\) الموجب الكبير، \\(\sigma(x) \approx 1\\)؛ وبالنسبة لـ \\(x\\) السالب الكبير، \\(\sigma(x) \approx 0\\).
   - مشتقتها هي \\(\sigma'(x) = \sigma(x)(1 - \sigma(x))\\)، والتي تقترب من الصفر في هذه المناطق. أثناء الانتشار العكسي، يؤدي هذا إلى "اختفاء" التدرجات (أي تصبح صغيرة جدًا)، مما يعيق التعلم في الطبقات العميقة.
   - المحولات في نماذج LLM عميقة جدًا (مثلًا، GPT-4 لديه 100+ طبقة)، لذا فإن هذا يعيق كفاءة التدريب. البدائل مثل ReLU (\\(f(x) = \max(0, x)\\)) أو GELU (التي ناقشناها سابقًا) تتجنب التشبع الكامل للمدخلات السالبة، مما يسمح بمرور أفضل للتدرجات.

#### 2. **مخرجات غير مركزة عند الصفر**
   - دالة السيجمويد تنتج دائمًا قيمًا موجبة (من 0 إلى 1)، مما يُسبب انحيازًا في تحديثات الأوزان أثناء التحسين. هذا يؤدي إلى مسارات نزول تدرج "متعرجة"، مما يبطئ التقارب مقارنة بالدوال المركزة عند الصفر مثل tanh أو GELU.
   - في المحولات، تقوم طبقات التغذية الأمامية بمعالجة التضمينات عالية الأبعاد، وتساعد دوال التنشيط المركزة عند الصفر في الحفاظ على استقرار انتشار الإشارة عبر وصلات البقايا.

#### 3. **ضعف الأداء التجريبي**
   - تظهر التجارب الواسعة أن السيجمويد يتأخر عن دوال التنشيط الحديثة في مهام معالجة اللغات الطبيعية. استخدمت المحولات المبكرة (مثل GPT الأصلي) ReLU للسرعة والبساطة. انتقلت النماذج اللاحقة مثل GPT-2/3 إلى GELU لسلوكها الاحتمالي السلس، والذي يعزز تجريبيًا الدقة والتقارب دون عيوب السيجمويد.
   - تظهر متغيرات مثل SiLU في بعض نماذج LLM (مثل InstructGPT)، ولكن هذا ليس سيجمويدًا عاديًا - فهو مضروب في \\(x\\) لتصحيح مركزية الصفر وتقليل التشبع.

#### 4. **اعتبارات طبقة المخرجات**
   - للتنبؤ بالرمز التالي في نماذج LLM، تستخدم الطبقة الأخيرة **softmax** على القيم اللوغاريتمية، وليس السيجمويد. قد تعمل السيجمويد للقرارات الثنائية ولكنها تفشل في المفردات متعددة الفئات (مثلًا 50 ألف+ رمز)، حيث أن المخرجات لا sum إلى 1، مما يؤدي إلى احتمالات غير مستقرة.
   - تضمن softmax توزيعًا صالحًا، وهو أمر بالغ الأهمية لدالة الخسارة الانتروبية المتقاطعة.

#### متى لا تزال السيجمويد تُستخدم؟
- نادرًا في الطبقات المخفية لـ LLM، ولكن أحيانًا في نقاط متخصصة مثل البوابات الثنائية أو RNNs/LSTMs الأقدم (قبل عصر المحولات).
- في نماذج الرؤية الحاسوبية، تُستخدم أحيانًا لتطبيع البكسل، ولكن ليس في نماذج LLM التي تركز على النص.

باختصار، أناقة السيجمويد من الثمانينيات لا تتوسع لتناسب عمق ومقياس نماذج LLM. توفر ReLU/GELU/Swish تدريبًا أسرع وأكثر موثوقية مع مخاطر أقل. إذا كنت تقوم بضبط نموذج، ابدأ بـ GELU لتوافقها مع المحولات.

#### المراجع
- [Linear Layers and Activation Functions in Transformer Models](https://machinelearningmastery.com/linear-layers-and-activation-functions-in-transformer-models/)
- [Deep Learning 101: Transformer Activation Functions Explainer](https://www.saltdatalabs.com/blog/deep-learning-101-transformer-activation-functions-explainer-relu-leaky-relu-gelu-elu-selu-softmax-and-more)
- [Common Activation Functions](https://www.aussieai.com/book/ch21-common-activation-functions)
- [Understanding Logits, Sigmoid, Softmax, and Cross-Entropy Loss](https://wandb.ai/amanarora/Written-Reports/reports/Understanding-Logits-Sigmoid-Softmax-and-Cross-Entropy-Loss-in-Deep-Learning--Vmlldzo0NDMzNTU3)