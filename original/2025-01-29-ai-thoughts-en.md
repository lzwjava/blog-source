---
audio: true
lang: en
layout: post
title: AI Thoughts
translated: false
---

- Satya Nadella mentioned Jevons paradox. It is worth to learn.

- Yin Wang: There is no "intelligence" in artificial intelligence, no "neural" in neural network, no "learning" in machine learning, and no "depth" in deep learning. There is no "depth" in deep learning. What really works in this field is called "calculus". So I prefer to call this field "differentiable computing", and the process of building models is called "differentiable programming".

- Yin Wang: Machine learning is really useful, one might even say beautiful theory, because it is simply calculus after a makeover! It is the old and great theory of Newton, Leibniz, in a simpler, elegant and powerful form. Machine learning is basically the use of calculus to derive and fit some functions, and deep learning is the fitting of more complex functions. 

- Currently, large language models canâ€™t filter by file language like YAML or Python. However, a significant portion of information in the real world is organized this way. This means that we could train large language models using files.

- For training large language models, we could develop a system that finds exact matches. Perhaps we can combine the KMP (Knuth-Morris-Pratt) search algorithm with transformer architecture to enhance search capabilities.

- There are no technological secrets. Open source will reveal all the secrets that are closely guarded.

- [New Platforms Powered by AI Workflows](./ai-workflow-en)

- [The Next Direction of AI Code Editors](./ai-code-en)

- [How I Live Well in the AI and Blockchain Era](./ai-blockchain-en)


