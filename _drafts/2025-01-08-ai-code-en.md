---
audio: true  
lang: en  
layout: post  
title: The Next Direction of AI Code Editors
---

Recently, I was working on adding an `xelatex` pipeline to GitHub Actions.

I encountered an issue with the `fontawesome5` package in the GitHub flow. The solution provided by 4o-mini (installing TeX Live 2021 and using `tlmgr install fontawesome5`) didn't work for me. However, 4o suggested a better approach: upgrading to TeX Live 2023 and still using `tlmgr` to install `fontawesome5`. While this didn't completely fix the problem, switching to TeX Live 2023 significantly improved the situation.

I used ChatGPT to help figure out the problem. For more details, check out [What ChatGPT O1 Can Do That 4o-mini Cannot](./o1-en).

At this point, I didn’t use editors like Cursor or Windsurf, though I did try them in another project. The issue with these code editors is that they only capture local test output, which limits their functionality in cloud environments.

In workflows like GitHub Actions, Jenkins jobs, or any code deployment or testing flow, code editors need to be better integrated. They should provide seamless interaction with the cloud and CI/CD processes.

This integration also applies to other content creation tools—whether for text, images, audio, or video. These tools should be integrated with A/B testing systems. AI tools could generate content, and A/B testing tools could provide feedback. This dynamic is similar to Reinforcement Learning from Human Feedback (RLHF), where AI models improve over time based on real-world feedback.

This concept of extending RLHF beyond just model outputs—into real-world testing and deployment environments—seems like a promising direction for improvement in both code editors and AI-driven content creation tools.

