$ python train.py config/train_fineweb.py 
Overriding config with config/train_fineweb.py:
out_dir = 'out-fineweb'
eval_interval = 500       # Evaluate more often on small data
eval_iters = 200
log_interval = 200         # Log more frequently
always_save_checkpoint = True

wandb_log = False          # Optional
wandb_project = 'fineweb'
wandb_run_name = '125M-single-parquet-4070'

dataset = 'fineweb'       # Assumes you adapted prepare.py for your single file
gradient_accumulation_steps = 32     # Effective batch size: 16 * 32 = 512 sequences
batch_size = 16
block_size = 1024                    # Matches FineWeb's processing

# Model (~125M parameters) – perfect for 12 GB VRAM
n_layer = 12
n_head = 12
n_embd = 384
dropout = 0.1                        # Add 0.1 if overfitting
learning_rate = 1e-3                 # Slightly lower for smaller data
max_iters = 20000                     # ~3B tokens seen (adjust up to 10000 if loss keeps dropping)
warmup_iters = 500                   # Shorter warmup
lr_decay_iters = 20000
min_lr = 1e-4
beta2 = 0.99

# Extras for speed/stability
compile = True            # PyTorch compile for 20–30% faster training
bias = False              # Like LLaMA/Mistral
weight_decay = 0.1
tokens per iteration will be: 524,288
Initializing a new model from scratch
defaulting to vocab_size of GPT-2 to 50304 (50257 rounded up for efficiency)
number of parameters: 40.56M
/home/lzw/projects/nanoGPT/train.py:196: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(dtype == 'float16'))
num decayed parameter tensors: 50, with 40,943,616 parameters
num non-decayed parameter tensors: 25, with 9,600 parameters
using fused AdamW: True
compiling the model... (takes a ~minute)
W1118 22:27:11.793000 50169 torch/_inductor/utils.py:1250] [0/0] Not enough SMs to use max_autotune_gemm mode
step 0: train loss 10.8684, val loss 10.8727
iter 0: loss 10.8720, time 32025.45ms, mfu -100.00%
iter 200: loss 6.2977, time 3917.80ms, mfu 12.87%
iter 400: loss 5.7609, time 3903.44ms, mfu 12.87%
step 500: train loss 5.4354, val loss 5.5328
saving checkpoint to out-fineweb
iter 600: loss 5.0956, time 3902.93ms, mfu 12.88%
iter 800: loss 4.9073, time 3904.79ms, mfu 12.88%
step 1000: train loss 4.4487, val loss 4.4593
saving checkpoint to out-fineweb
iter 1000: loss 4.4568, time 19156.54ms, mfu 11.85%
iter 1200: loss 4.4111, time 3909.66ms, mfu 11.96%
iter 1400: loss 4.3724, time 3909.21ms, mfu 12.05%
step 1500: train loss 4.1890, val loss 4.1375
saving checkpoint to out-fineweb
iter 1600: loss 4.3850, time 3909.62ms, mfu 12.14%
iter 1800: loss 4.1081, time 3909.42ms, mfu 12.21%
step 2000: train loss 4.0389, val loss 4.0277
saving checkpoint to out-fineweb
iter 2000: loss 4.2072, time 19182.88ms, mfu 11.25%
iter 2200: loss 4.1567, time 3910.08ms, mfu 11.42%
iter 2400: loss 4.1961, time 3908.72ms, mfu 11.57%
step 2500: train loss 3.9865, val loss 3.9424
saving checkpoint to out-fineweb
iter 2600: loss 3.9387, time 3909.66ms, mfu 11.70%
iter 2800: loss 3.9624, time 3909.59ms, mfu 11.82%
step 3000: train loss 3.9311, val loss 3.8878
saving checkpoint to out-fineweb
iter 3000: loss 4.0684, time 19221.97ms, mfu 10.90%
iter 3200: loss 3.9408, time 3909.13ms, mfu 11.10%
iter 3400: loss 4.2235, time 3909.93ms, mfu 11.28%
step 3500: train loss 3.8817, val loss 3.8374
saving checkpoint to out-fineweb
iter 3600: loss 4.1006, time 3906.84ms, mfu 11.44%
iter 3800: loss 3.8431, time 3907.82ms, mfu 11.59%
step 4000: train loss 3.8773, val loss 3.8000
saving checkpoint to out-fineweb
iter 4000: loss 3.8870, time 19157.84ms, mfu 10.69%
iter 4200: loss 3.9914, time 3905.68ms, mfu 10.91%
iter 4400: loss 3.7332, time 3907.58ms, mfu 11.11%
step 4500: train loss 3.8343, val loss 3.7954
saving checkpoint to out-fineweb
iter 4600: loss 3.9596, time 3908.77ms, mfu 11.29%
iter 4800: loss 3.8978, time 3909.84ms, mfu 11.45%
step 5000: train loss 3.8059, val loss 3.7625
saving checkpoint to out-fineweb
iter 5000: loss 3.9563, time 19174.23ms, mfu 10.57%
iter 5200: loss 3.8964, time 3907.74ms, mfu 10.80%
iter 5400: loss 4.0245, time 3908.84ms, mfu 11.01%
step 5500: train loss 3.7853, val loss 3.7234
saving checkpoint to out-fineweb
iter 5600: loss 3.9167, time 3907.10ms, mfu 11.20%
iter 5800: loss 3.8505, time 3907.14ms, mfu 11.37%
step 6000: train loss 3.7781, val loss 3.7283
saving checkpoint to out-fineweb
iter 6000: loss 3.9905, time 19179.77ms, mfu 10.50%
iter 6200: loss 3.9503, time 3909.70ms, mfu 10.74%
iter 6400: loss 3.9785, time 3909.48ms, mfu 10.95%
step 6500: train loss 3.7695, val loss 3.7120
saving checkpoint to out-fineweb
iter 6600: loss 3.7405, time 3908.76ms, mfu 11.15%
iter 6800: loss 3.7985, time 3906.52ms, mfu 11.32%
step 7000: train loss 3.7480, val loss 3.6934
saving checkpoint to out-fineweb
iter 7000: loss 3.7648, time 19165.71ms, mfu 10.45%
iter 7200: loss 3.8180, time 3906.45ms, mfu 10.70%
iter 7400: loss 3.8648, time 3907.92ms, mfu 10.92%
step 7500: train loss 3.7529, val loss 3.6745
saving checkpoint to out-fineweb
iter 7600: loss 3.9458, time 3909.85ms, mfu 11.12%
iter 7800: loss 3.8352, time 3911.32ms, mfu 11.29%
step 8000: train loss 3.7291, val loss 3.6538
saving checkpoint to out-fineweb
iter 8000: loss 3.8495, time 19196.47ms, mfu 10.43%
iter 8200: loss 3.8432, time 3908.03ms, mfu 10.67%
iter 8400: loss 3.6108, time 3907.02ms, mfu 10.90%
step 8500: train loss 3.7126, val loss 3.6461
saving checkpoint to out-fineweb
iter 8600: loss 3.7332, time 3910.39ms, mfu 11.10%
iter 8800: loss 3.9011, time 3909.98ms, mfu 11.28%
step 9000: train loss 3.7168, val loss 3.6381
saving checkpoint to out-fineweb
iter 9000: loss 3.7706, time 19191.11ms, mfu 10.41%
iter 9200: loss 3.8354, time 3909.42ms, mfu 10.66%
iter 9400: loss 3.7596, time 3909.45ms, mfu 10.88%
step 9500: train loss 3.7001, val loss 3.6480
saving checkpoint to out-fineweb
iter 9600: loss 3.7120, time 3909.48ms, mfu 11.08%
iter 9800: loss 3.8578, time 3907.57ms, mfu 11.27%
step 10000: train loss 3.7090, val loss 3.6567
saving checkpoint to out-fineweb
iter 10000: loss 4.1096, time 19205.37ms, mfu 10.40%
iter 10200: loss 3.7663, time 3908.24ms, mfu 10.65%
iter 10400: loss 3.9917, time 3909.46ms, mfu 10.88%
step 10500: train loss 3.6888, val loss 3.6172
saving checkpoint to out-fineweb
iter 10600: loss 3.7558, time 3909.18ms, mfu 11.08%
iter 10800: loss 3.7824, time 3908.00ms, mfu 11.26%
step 11000: train loss 3.6715, val loss 3.6098
saving checkpoint to out-fineweb
iter 11000: loss 3.8501, time 19213.47ms, mfu 10.40%
iter 11200: loss 3.6792, time 3910.74ms, mfu 10.65%
iter 11400: loss 3.5391, time 3926.47ms, mfu 10.86%
step 11500: train loss 3.6816, val loss 3.6020
saving checkpoint to out-fineweb
iter 11600: loss 3.7743, time 3929.50ms, mfu 11.06%
iter 11800: loss 3.7255, time 3907.61ms, mfu 11.24%
step 12000: train loss 3.6685, val loss 3.6079
saving checkpoint to out-fineweb
iter 12000: loss 3.7942, time 19202.69ms, mfu 10.38%
iter 12200: loss 3.8372, time 3907.00ms, mfu 10.63%
iter 12400: loss 3.5992, time 3910.79ms, mfu 10.86%
step 12500: train loss 3.6680, val loss 3.5913
saving checkpoint to out-fineweb
iter 12600: loss 3.7861, time 3909.49ms, mfu 11.06%
iter 12800: loss 3.7368, time 3926.44ms, mfu 11.24%
step 13000: train loss 3.6527, val loss 3.6024
saving checkpoint to out-fineweb
iter 13000: loss 3.8387, time 19192.21ms, mfu 10.38%
iter 13200: loss 3.8159, time 3911.46ms, mfu 10.63%
iter 13400: loss 3.7275, time 3915.12ms, mfu 10.86%
step 13500: train loss 3.6565, val loss 3.5725
saving checkpoint to out-fineweb
iter 13600: loss 3.7071, time 3913.89ms, mfu 11.06%
iter 13800: loss 3.8243, time 3913.89ms, mfu 11.24%
step 14000: train loss 3.6398, val loss 3.5787
saving checkpoint to out-fineweb
iter 14000: loss 3.6814, time 19188.54ms, mfu 10.38%
iter 14200: loss 3.6838, time 3912.31ms, mfu 10.63%
iter 14400: loss 3.7865, time 3910.91ms, mfu 10.86%
step 14500: train loss 3.6415, val loss 3.5696
saving checkpoint to out-fineweb
iter 14600: loss 3.7201, time 3913.59ms, mfu 11.06%
iter 14800: loss 3.7794, time 3912.33ms, mfu 11.24%
      

step 15000: train loss 3.6372, val loss 3.5705
saving checkpoint to out-fineweb
iter 15000: loss 3.6716, time 19208.18ms, mfu 10.38%
iter 15200: loss 3.7170, time 3911.00ms, mfu 10.63%
iter 15400: loss 3.8040, time 3910.89ms, mfu 10.86%
step 15500: train loss 3.6177, val loss 3.5430
saving checkpoint to out-fineweb
iter 15600: loss 3.7249, time 3912.97ms, mfu 11.06%
iter 15800: loss 3.6279, time 3913.47ms, mfu 11.24%
step 16000: train loss 3.6125, val loss 3.5485
saving checkpoint to out-fineweb
iter 16000: loss 3.5554, time 19166.20ms, mfu 10.38%
iter 16200: loss 3.7276, time 3910.05ms, mfu 10.63%
iter 16400: loss 3.7917, time 3909.23ms, mfu 10.86%
step 16500: train loss 3.6216, val loss 3.5467
saving checkpoint to out-fineweb
iter 16600: loss 3.7947, time 3911.63ms, mfu 11.06%
iter 16800: loss 3.6167, time 3931.45ms, mfu 11.24%
step 17000: train loss 3.6259, val loss 3.5490
saving checkpoint to out-fineweb
iter 17000: loss 3.6237, time 19189.95ms, mfu 10.38%
iter 17200: loss 3.6776, time 3910.27ms, mfu 10.63%
iter 17400: loss 3.5136, time 3925.30ms, mfu 10.85%
step 17500: train loss 3.6206, val loss 3.5464
saving checkpoint to out-fineweb
iter 17600: loss 3.6747, time 3930.91ms, mfu 11.05%
iter 17800: loss 3.7370, time 3930.56ms, mfu 11.22%
step 18000: train loss 3.6200, val loss 3.5434
saving checkpoint to out-fineweb
iter 18000: loss 3.7272, time 19173.48ms, mfu 10.36%
iter 18200: loss 3.6442, time 3911.52ms, mfu 10.62%
iter 18400: loss 3.5789, time 3912.78ms, mfu 10.84%
step 18500: train loss 3.6205, val loss 3.5386
saving checkpoint to out-fineweb
iter 18600: loss 3.7887, time 3927.51ms, mfu 11.04%
iter 18800: loss 3.7195, time 3911.32ms, mfu 11.23%
step 19000: train loss 3.6067, val loss 3.5332
saving checkpoint to out-fineweb
iter 19000: loss 3.7686, time 19168.77ms, mfu 10.37%
iter 19200: loss 3.7079, time 3912.16ms, mfu 10.62%
iter 19400: loss 3.6577, time 3910.62ms, mfu 10.85%
step 19500: train loss 3.5939, val loss 3.5283
saving checkpoint to out-fineweb
iter 19600: loss 3.5648, time 3911.22ms, mfu 11.05%
iter 19800: loss 3.7198, time 3911.56ms, mfu 11.23%
step 20000: train loss 3.6087, val loss 3.5228
saving checkpoint to out-fineweb
iter 20000: loss 3.7213, time 19164.30ms, mfu 10.37%
