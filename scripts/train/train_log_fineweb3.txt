$ python train.py config/train_fineweb.py 
Overriding config with config/train_fineweb.py:
out_dir = 'out-fineweb'
eval_interval = 500       # Evaluate more often on small data
eval_iters = 200
log_interval = 200         # Log more frequently
always_save_checkpoint = True

wandb_log = False          # Optional
wandb_project = 'fineweb'
wandb_run_name = 'fineweb'

dataset = 'fineweb'       # Assumes you adapted prepare.py for your single file
gradient_accumulation_steps = 64     # Effective batch size: 16 * 32 = 512 sequences
batch_size = 8
block_size = 1024                    # Matches FineWeb's processing

# Model (~125M parameters) – perfect for 12 GB VRAM
n_layer = 12
n_head = 12
n_embd = 768
dropout = 0.0                        # Add 0.1 if overfitting
learning_rate = 3e-4                # Slightly lower for smaller data
max_iters = 12000                     # ~3B tokens seen (adjust up to 10000 if loss keeps dropping)
warmup_iters = 500                   # Shorter warmup
lr_decay_iters = 12000
min_lr = 3e-5
beta2 = 0.99

# Extras for speed/stability
compile = True            # PyTorch compile for 20–30% faster training
bias = False              # Like LLaMA/Mistral
weight_decay = 0.1
tokens per iteration will be: 524,288
Initializing a new model from scratch
defaulting to vocab_size of GPT-2 to 50304 (50257 rounded up for efficiency)
number of parameters: 123.59M
/home/lzw/projects/nanoGPT/train.py:196: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(dtype == 'float16'))
num decayed parameter tensors: 50, with 124,354,560 parameters
num non-decayed parameter tensors: 25, with 19,200 parameters
using fused AdamW: True
compiling the model... (takes a ~minute)
step 0: train loss 10.9974, val loss 10.9913
iter 0: loss 10.9934, time 29485.98ms, mfu -100.00%
iter 200: loss 6.4872, time 9337.04ms, mfu 15.38%
iter 400: loss 5.7746, time 9342.61ms, mfu 15.38%
step 500: train loss 5.4383, val loss 5.5055
saving checkpoint to out-fineweb
iter 600: loss 5.1956, time 9356.06ms, mfu 15.38%
iter 800: loss 4.6810, time 9348.78ms, mfu 15.38%
step 1000: train loss 4.3717, val loss 4.3850
saving checkpoint to out-fineweb
iter 1000: loss 4.3124, time 29151.16ms, mfu 14.33%
iter 1200: loss 3.9229, time 9345.99ms, mfu 14.44%
iter 1400: loss 4.1939, time 9344.88ms, mfu 14.53%
step 1500: train loss 4.0668, val loss 4.0119
saving checkpoint to out-fineweb
iter 1600: loss 4.3124, time 9351.70ms, mfu 14.61%
iter 1800: loss 4.1410, time 9352.67ms, mfu 14.69%
step 2000: train loss 3.9058, val loss 3.8626
saving checkpoint to out-fineweb
iter 2000: loss 4.3298, time 29290.42ms, mfu 13.71%
iter 2200: loss 4.1424, time 9354.33ms, mfu 13.87%
iter 2400: loss 3.6960, time 9355.21ms, mfu 14.02%
step 2500: train loss 3.8149, val loss 3.7711
saving checkpoint to out-fineweb
iter 2600: loss 3.8866, time 9345.39ms, mfu 14.16%
iter 2800: loss 3.9183, time 9349.75ms, mfu 14.28%
step 3000: train loss 3.7425, val loss 3.6867
saving checkpoint to out-fineweb
iter 3000: loss 3.8541, time 29294.70ms, mfu 13.34%
iter 3200: loss 3.4133, time 9348.21ms, mfu 13.54%
iter 3400: loss 3.6525, time 9350.70ms, mfu 13.72%
step 3500: train loss 3.6812, val loss 3.6413
saving checkpoint to out-fineweb
iter 3600: loss 3.6020, time 9348.39ms, mfu 13.89%
iter 3800: loss 3.6309, time 9346.71ms, mfu 14.04%
step 4000: train loss 3.6270, val loss 3.5957
saving checkpoint to out-fineweb
iter 4000: loss 3.6054, time 29160.76ms, mfu 13.13%
iter 4200: loss 3.7042, time 9347.75ms, mfu 13.35%
iter 4400: loss 3.8140, time 9345.66ms, mfu 13.55%
step 4500: train loss 3.5952, val loss 3.5599
saving checkpoint to out-fineweb
iter 4600: loss 3.5297, time 9349.28ms, mfu 13.73%
iter 4800: loss 3.5119, time 9349.00ms, mfu 13.90%
step 5000: train loss 3.5667, val loss 3.4873
saving checkpoint to out-fineweb
iter 5000: loss 3.5075, time 29193.39ms, mfu 13.00%
iter 5200: loss 3.9663, time 9351.09ms, mfu 13.23%
iter 5400: loss 3.6010, time 9349.13ms, mfu 13.45%
step 5500: train loss 3.5283, val loss 3.4705
saving checkpoint to out-fineweb
iter 5600: loss 3.5342, time 9349.78ms, mfu 13.64%
iter 5800: loss 3.4533, time 9353.24ms, mfu 13.81%
step 6000: train loss 3.5134, val loss 3.4676
saving checkpoint to out-fineweb
iter 6000: loss 3.4520, time 29190.70ms, mfu 12.92%
iter 6200: loss 3.4927, time 9346.22ms, mfu 13.17%
iter 6400: loss 3.4445, time 9347.41ms, mfu 13.39%
step 6500: train loss 3.4863, val loss 3.4414
saving checkpoint to out-fineweb
iter 6600: loss 3.2797, time 9348.36ms, mfu 13.58%
iter 6800: loss 3.4814, time 9353.40ms, mfu 13.76%
step 7000: train loss 3.4679, val loss 3.4243
saving checkpoint to out-fineweb
iter 7000: loss 3.5679, time 29197.59ms, mfu 12.88%
iter 7200: loss 3.4975, time 9347.54ms, mfu 13.13%
iter 7400: loss 3.3723, time 9349.65ms, mfu 13.35%
step 7500: train loss 3.4431, val loss 3.3857
saving checkpoint to out-fineweb
iter 7600: loss 3.6006, time 10068.68ms, mfu 13.44%
33iter 7800: loss 3.2876, time 9349.59ms, mfu 13.63%
step 8000: train loss 3.4250, val loss 3.4081
saving checkpoint to out-fineweb
iter 8000: loss 3.4604, time 29171.89ms, mfu 12.76%
iter 8200: loss 3.1644, time 9354.18ms, mfu 13.02%
iter 8400: loss 3.6256, time 9365.64ms, mfu 13.25%
step 8500: train loss 3.4029, val loss 3.3670
saving checkpoint to out-fineweb
iter 8600: loss 3.3450, time 9361.79ms, mfu 13.46%
iter 8800: loss 3.4914, time 9348.70ms, mfu 13.65%
step 9000: train loss 3.4109, val loss 3.3741
saving checkpoint to out-fineweb
iter 9000: loss 3.3298, time 29191.48ms, mfu 12.78%
iter 9200: loss 3.4240, time 9356.80ms, mfu 13.04%
iter 9400: loss 3.0869, time 9369.40ms, mfu 13.27%
step 9500: train loss 3.4034, val loss 3.3709
saving checkpoint to out-fineweb
iter 9600: loss 3.3608, time 9357.43ms, mfu 13.47%
iter 9800: loss 3.3042, time 9350.16ms, mfu 13.66%
step 10000: train loss 3.3852, val loss 3.3581
saving checkpoint to out-fineweb
iter 10000: loss 3.2899, time 29202.67ms, mfu 12.79%
iter 10200: loss 3.4035, time 9402.66ms, mfu 13.04%
